I1004 10:37:27.085311      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-245756238
I1004 10:37:27.085508      15 e2e.go:241] Starting e2e run "556d30ef-563d-4aec-981d-1f183c0315cb" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1570185445 - Will randomize all specs
Will run 215 of 4413 specs

Oct  4 10:37:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 10:37:27.336: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Oct  4 10:37:27.488: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Oct  4 10:37:27.847: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Oct  4 10:37:27.847: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Oct  4 10:37:27.847: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Oct  4 10:37:28.276: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Oct  4 10:37:28.276: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Oct  4 10:37:28.276: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Oct  4 10:37:28.276: INFO: e2e test version: v1.15.3
Oct  4 10:37:28.277: INFO: kube-apiserver version: v1.15.3
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:37:28.277: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
Oct  4 10:37:28.328: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Oct  4 10:37:28.336: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Oct  4 10:37:43.396: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:37:43.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5400" for this suite.
Oct  4 10:37:49.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:37:49.557: INFO: namespace pods-5400 deletion completed in 6.150532023s

• [SLOW TEST:21.280 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:37:49.558: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct  4 10:37:49.608: INFO: Waiting up to 5m0s for pod "pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0" in namespace "emptydir-1636" to be "success or failure"
Oct  4 10:37:49.613: INFO: Pod "pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.197938ms
Oct  4 10:37:51.618: INFO: Pod "pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010225287s
Oct  4 10:37:53.624: INFO: Pod "pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0155882s
STEP: Saw pod success
Oct  4 10:37:53.624: INFO: Pod "pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0" satisfied condition "success or failure"
Oct  4 10:37:53.628: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0 container test-container: <nil>
STEP: delete the pod
Oct  4 10:37:53.655: INFO: Waiting for pod pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0 to disappear
Oct  4 10:37:53.659: INFO: Pod pod-bdf2243c-9547-4388-bcd8-d87c0c2e04f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:37:53.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1636" for this suite.
Oct  4 10:37:59.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:37:59.824: INFO: namespace emptydir-1636 deletion completed in 6.15874951s

• [SLOW TEST:10.266 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:37:59.827: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-237de9ab-4387-47be-be66-85c0a7259798 in namespace container-probe-8014
Oct  4 10:38:03.887: INFO: Started pod liveness-237de9ab-4387-47be-be66-85c0a7259798 in namespace container-probe-8014
STEP: checking the pod's current state and verifying that restartCount is present
Oct  4 10:38:03.891: INFO: Initial restart count of pod liveness-237de9ab-4387-47be-be66-85c0a7259798 is 0
Oct  4 10:38:21.940: INFO: Restart count of pod container-probe-8014/liveness-237de9ab-4387-47be-be66-85c0a7259798 is now 1 (18.048467175s elapsed)
Oct  4 10:38:41.996: INFO: Restart count of pod container-probe-8014/liveness-237de9ab-4387-47be-be66-85c0a7259798 is now 2 (38.104548042s elapsed)
Oct  4 10:39:02.047: INFO: Restart count of pod container-probe-8014/liveness-237de9ab-4387-47be-be66-85c0a7259798 is now 3 (58.155529971s elapsed)
Oct  4 10:39:22.098: INFO: Restart count of pod container-probe-8014/liveness-237de9ab-4387-47be-be66-85c0a7259798 is now 4 (1m18.206261781s elapsed)
Oct  4 10:40:22.258: INFO: Restart count of pod container-probe-8014/liveness-237de9ab-4387-47be-be66-85c0a7259798 is now 5 (2m18.366381546s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:40:22.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8014" for this suite.
Oct  4 10:40:28.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:40:28.421: INFO: namespace container-probe-8014 deletion completed in 6.143753075s

• [SLOW TEST:148.595 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:40:28.423: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Oct  4 10:40:28.471: INFO: Waiting up to 5m0s for pod "downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb" in namespace "downward-api-3079" to be "success or failure"
Oct  4 10:40:28.478: INFO: Pod "downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.725658ms
Oct  4 10:40:30.483: INFO: Pod "downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01139753s
Oct  4 10:40:32.489: INFO: Pod "downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017521045s
STEP: Saw pod success
Oct  4 10:40:32.489: INFO: Pod "downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb" satisfied condition "success or failure"
Oct  4 10:40:32.493: INFO: Trying to get logs from node lab1-k8s-node-1 pod downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb container dapi-container: <nil>
STEP: delete the pod
Oct  4 10:40:32.529: INFO: Waiting for pod downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb to disappear
Oct  4 10:40:32.533: INFO: Pod downward-api-fb56b226-acd4-4ebf-adb1-721c6d0481eb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:40:32.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3079" for this suite.
Oct  4 10:40:38.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:40:38.687: INFO: namespace downward-api-3079 deletion completed in 6.145647657s

• [SLOW TEST:10.264 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:40:38.687: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Oct  4 10:41:08.782: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:41:08.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1004 10:41:08.782321      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-6179" for this suite.
Oct  4 10:41:14.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:41:14.934: INFO: namespace gc-6179 deletion completed in 6.147664675s

• [SLOW TEST:36.247 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:41:14.936: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Oct  4 10:41:14.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-8415'
Oct  4 10:41:15.249: INFO: stderr: ""
Oct  4 10:41:15.249: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Oct  4 10:41:16.254: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:41:16.254: INFO: Found 0 / 1
Oct  4 10:41:17.255: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:41:17.255: INFO: Found 0 / 1
Oct  4 10:41:18.255: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:41:18.255: INFO: Found 1 / 1
Oct  4 10:41:18.255: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct  4 10:41:18.259: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:41:18.259: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Oct  4 10:41:18.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 logs redis-master-ck989 redis-master --namespace=kubectl-8415'
Oct  4 10:41:18.356: INFO: stderr: ""
Oct  4 10:41:18.356: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Oct 10:41:17.994 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Oct 10:41:17.994 # Server started, Redis version 3.2.12\n1:M 04 Oct 10:41:17.994 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Oct 10:41:17.994 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Oct  4 10:41:18.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 log redis-master-ck989 redis-master --namespace=kubectl-8415 --tail=1'
Oct  4 10:41:18.446: INFO: stderr: ""
Oct  4 10:41:18.446: INFO: stdout: "1:M 04 Oct 10:41:17.994 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Oct  4 10:41:18.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 log redis-master-ck989 redis-master --namespace=kubectl-8415 --limit-bytes=1'
Oct  4 10:41:18.548: INFO: stderr: ""
Oct  4 10:41:18.548: INFO: stdout: " "
STEP: exposing timestamps
Oct  4 10:41:18.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 log redis-master-ck989 redis-master --namespace=kubectl-8415 --tail=1 --timestamps'
Oct  4 10:41:18.635: INFO: stderr: ""
Oct  4 10:41:18.635: INFO: stdout: "2019-10-04T10:41:17.99462706Z 1:M 04 Oct 10:41:17.994 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Oct  4 10:41:21.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 log redis-master-ck989 redis-master --namespace=kubectl-8415 --since=1s'
Oct  4 10:41:21.220: INFO: stderr: ""
Oct  4 10:41:21.221: INFO: stdout: ""
Oct  4 10:41:21.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 log redis-master-ck989 redis-master --namespace=kubectl-8415 --since=24h'
Oct  4 10:41:21.312: INFO: stderr: ""
Oct  4 10:41:21.312: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Oct 10:41:17.994 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Oct 10:41:17.994 # Server started, Redis version 3.2.12\n1:M 04 Oct 10:41:17.994 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Oct 10:41:17.994 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Oct  4 10:41:21.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-8415'
Oct  4 10:41:21.397: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 10:41:21.397: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Oct  4 10:41:21.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8415'
Oct  4 10:41:21.495: INFO: stderr: "No resources found.\n"
Oct  4 10:41:21.495: INFO: stdout: ""
Oct  4 10:41:21.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -l name=nginx --namespace=kubectl-8415 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct  4 10:41:21.564: INFO: stderr: ""
Oct  4 10:41:21.564: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:41:21.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8415" for this suite.
Oct  4 10:41:43.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:41:43.710: INFO: namespace kubectl-8415 deletion completed in 22.140244282s

• [SLOW TEST:28.774 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:41:43.711: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Oct  4 10:41:43.744: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct  4 10:41:43.756: INFO: Waiting for terminating namespaces to be deleted...
Oct  4 10:41:43.760: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-1 before test
Oct  4 10:41:43.768: INFO: kube-proxy-xvmz9 from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.769: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 10:41:43.769: INFO: calico-node-s4qhf from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.769: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 10:41:43.769: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.769: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 10:41:43.769: INFO: calico-kube-controllers-5b485dc7c7-d4x2r from kube-system started at 2019-10-04 10:22:36 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.769: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct  4 10:41:43.769: INFO: nodelocaldns-9rl9p from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.769: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 10:41:43.769: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-njvm2 from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:41:43.769: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:41:43.769: INFO: 	Container systemd-logs ready: true, restart count 0
Oct  4 10:41:43.769: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-2 before test
Oct  4 10:41:43.792: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-vxftt from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:41:43.792: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:41:43.792: INFO: 	Container systemd-logs ready: true, restart count 0
Oct  4 10:41:43.792: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.792: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 10:41:43.792: INFO: kube-proxy-bpwhk from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.792: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 10:41:43.792: INFO: calico-node-zrmvj from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.792: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 10:41:43.792: INFO: nodelocaldns-5kkm7 from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.792: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 10:41:43.792: INFO: sonobuoy from heptio-sonobuoy started at 2019-10-04 10:37:02 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.792: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct  4 10:41:43.792: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-3 before test
Oct  4 10:41:43.809: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.809: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 10:41:43.809: INFO: kube-proxy-nt6kv from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.809: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 10:41:43.809: INFO: calico-node-cqqg4 from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.809: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 10:41:43.809: INFO: nodelocaldns-f9sqh from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 10:41:43.809: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 10:41:43.809: INFO: sonobuoy-e2e-job-55aaa13fda1d4ddb from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:41:43.809: INFO: 	Container e2e ready: true, restart count 0
Oct  4 10:41:43.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:41:43.809: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-rsvwx from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:41:43.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:41:43.809: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ca6b94161bed29], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:41:44.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8435" for this suite.
Oct  4 10:41:50.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:41:51.006: INFO: namespace sched-pred-8435 deletion completed in 6.149469004s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.295 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:41:51.006: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Oct  4 10:41:51.042: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-245756238 proxy --unix-socket=/tmp/kubectl-proxy-unix593856085/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:41:51.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7543" for this suite.
Oct  4 10:41:57.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:41:57.246: INFO: namespace kubectl-7543 deletion completed in 6.148592136s

• [SLOW TEST:6.240 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:41:57.249: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 10:41:57.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9413'
Oct  4 10:41:57.387: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct  4 10:41:57.387: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Oct  4 10:41:57.396: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Oct  4 10:41:57.399: INFO: scanned /root for discovery docs: <nil>
Oct  4 10:41:57.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9413'
Oct  4 10:42:13.220: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Oct  4 10:42:13.220: INFO: stdout: "Created e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06\nScaling up e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Oct  4 10:42:13.220: INFO: stdout: "Created e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06\nScaling up e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Oct  4 10:42:13.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9413'
Oct  4 10:42:13.301: INFO: stderr: ""
Oct  4 10:42:13.301: INFO: stdout: "e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06-x6m8g "
Oct  4 10:42:13.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06-x6m8g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9413'
Oct  4 10:42:13.380: INFO: stderr: ""
Oct  4 10:42:13.380: INFO: stdout: "true"
Oct  4 10:42:13.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06-x6m8g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9413'
Oct  4 10:42:13.457: INFO: stderr: ""
Oct  4 10:42:13.457: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Oct  4 10:42:13.457: INFO: e2e-test-nginx-rc-c756460e248374d083b4050ef4c65d06-x6m8g is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Oct  4 10:42:13.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete rc e2e-test-nginx-rc --namespace=kubectl-9413'
Oct  4 10:42:13.545: INFO: stderr: ""
Oct  4 10:42:13.545: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:42:13.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9413" for this suite.
Oct  4 10:42:35.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:42:35.711: INFO: namespace kubectl-9413 deletion completed in 22.156084667s

• [SLOW TEST:38.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:42:35.712: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Oct  4 10:42:35.761: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7098" to be "success or failure"
Oct  4 10:42:35.768: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.505315ms
Oct  4 10:42:37.773: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011628769s
Oct  4 10:42:39.778: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016353098s
STEP: Saw pod success
Oct  4 10:42:39.778: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Oct  4 10:42:39.782: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Oct  4 10:42:39.812: INFO: Waiting for pod pod-host-path-test to disappear
Oct  4 10:42:39.816: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:42:39.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7098" for this suite.
Oct  4 10:42:45.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:42:45.987: INFO: namespace hostpath-7098 deletion completed in 6.165027392s

• [SLOW TEST:10.275 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:42:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-aa755bfd-cb86-470b-b5bc-0ca525304ec6
STEP: Creating secret with name s-test-opt-upd-a73b5e6b-277b-4237-bd03-04e8240bfed9
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-aa755bfd-cb86-470b-b5bc-0ca525304ec6
STEP: Updating secret s-test-opt-upd-a73b5e6b-277b-4237-bd03-04e8240bfed9
STEP: Creating secret with name s-test-opt-create-4b99dfad-9260-447f-9c6b-82f1bbdcc890
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:42:50.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2990" for this suite.
Oct  4 10:43:12.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:43:12.328: INFO: namespace secrets-2990 deletion completed in 22.16494495s

• [SLOW TEST:26.340 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:43:12.330: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-mm5g
STEP: Creating a pod to test atomic-volume-subpath
Oct  4 10:43:12.390: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mm5g" in namespace "subpath-1747" to be "success or failure"
Oct  4 10:43:12.397: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.437284ms
Oct  4 10:43:14.401: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878063s
Oct  4 10:43:16.406: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.015784995s
Oct  4 10:43:18.411: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 6.020438673s
Oct  4 10:43:20.416: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 8.025305672s
Oct  4 10:43:22.421: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.030721815s
Oct  4 10:43:24.426: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 12.035724743s
Oct  4 10:43:26.432: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.04104923s
Oct  4 10:43:28.437: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.04627588s
Oct  4 10:43:30.445: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.054096468s
Oct  4 10:43:32.452: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Running", Reason="", readiness=true. Elapsed: 20.061686614s
Oct  4 10:43:34.459: INFO: Pod "pod-subpath-test-configmap-mm5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.068785908s
STEP: Saw pod success
Oct  4 10:43:34.460: INFO: Pod "pod-subpath-test-configmap-mm5g" satisfied condition "success or failure"
Oct  4 10:43:34.465: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-configmap-mm5g container test-container-subpath-configmap-mm5g: <nil>
STEP: delete the pod
Oct  4 10:43:34.495: INFO: Waiting for pod pod-subpath-test-configmap-mm5g to disappear
Oct  4 10:43:34.499: INFO: Pod pod-subpath-test-configmap-mm5g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mm5g
Oct  4 10:43:34.499: INFO: Deleting pod "pod-subpath-test-configmap-mm5g" in namespace "subpath-1747"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:43:34.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1747" for this suite.
Oct  4 10:43:40.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:43:40.649: INFO: namespace subpath-1747 deletion completed in 6.140721063s

• [SLOW TEST:28.319 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:43:40.650: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-153bd152-c599-4ff2-b627-01d140e4efca
STEP: Creating a pod to test consume secrets
Oct  4 10:43:40.752: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa" in namespace "projected-9433" to be "success or failure"
Oct  4 10:43:40.759: INFO: Pod "pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368653ms
Oct  4 10:43:42.764: INFO: Pod "pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011246193s
STEP: Saw pod success
Oct  4 10:43:42.764: INFO: Pod "pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa" satisfied condition "success or failure"
Oct  4 10:43:42.768: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct  4 10:43:42.798: INFO: Waiting for pod pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa to disappear
Oct  4 10:43:42.802: INFO: Pod pod-projected-secrets-82564377-fb06-43ef-9283-5b76f036f2fa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:43:42.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9433" for this suite.
Oct  4 10:43:48.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:43:48.967: INFO: namespace projected-9433 deletion completed in 6.160262752s

• [SLOW TEST:8.317 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:43:48.967: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Oct  4 10:43:49.000: INFO: PodSpec: initContainers in spec.initContainers
Oct  4 10:44:30.697: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7d7e410f-cc1b-4631-9dd1-026390850c2b", GenerateName:"", Namespace:"init-container-5746", SelfLink:"/api/v1/namespaces/init-container-5746/pods/pod-init-7d7e410f-cc1b-4631-9dd1-026390850c2b", UID:"a652ff59-ff5b-49ae-b8a1-b2f733d551d0", ResourceVersion:"8098", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63705782629, loc:(*time.Location)(0x7ec7a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"979356"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-dvwnp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002d82b00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dvwnp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dvwnp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-dvwnp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0005e9428), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"lab1-k8s-node-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0020eccc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0005e94f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0005e9510)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0005e9518), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0005e951c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705782629, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705782629, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705782629, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705782629, loc:(*time.Location)(0x7ec7a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.0.18", PodIP:"10.233.95.12", StartTime:(*v1.Time)(0xc002d7fd60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001b541c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001b54230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://9474cfcb8cadaa4c9ea0e199b0f9ffa4489d0092e121d6096b42cc994d4d51d1"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d7fda0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002d7fd80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:44:30.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5746" for this suite.
Oct  4 10:44:52.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:44:52.868: INFO: namespace init-container-5746 deletion completed in 22.16448369s

• [SLOW TEST:63.902 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:44:52.874: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Oct  4 10:44:52.963: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4938,SelfLink:/api/v1/namespaces/watch-4938/configmaps/e2e-watch-test-resource-version,UID:409d827e-3cdf-4a01-8aa3-a6789a5d860c,ResourceVersion:8179,Generation:0,CreationTimestamp:2019-10-04 10:44:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct  4 10:44:52.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4938,SelfLink:/api/v1/namespaces/watch-4938/configmaps/e2e-watch-test-resource-version,UID:409d827e-3cdf-4a01-8aa3-a6789a5d860c,ResourceVersion:8180,Generation:0,CreationTimestamp:2019-10-04 10:44:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:44:52.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4938" for this suite.
Oct  4 10:44:58.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:44:59.126: INFO: namespace watch-4938 deletion completed in 6.157282652s

• [SLOW TEST:6.252 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:44:59.126: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Oct  4 10:44:59.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 api-versions'
Oct  4 10:44:59.235: INFO: stderr: ""
Oct  4 10:44:59.235: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:44:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1993" for this suite.
Oct  4 10:45:05.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:45:05.396: INFO: namespace kubectl-1993 deletion completed in 6.156763968s

• [SLOW TEST:6.270 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:45:05.399: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Oct  4 10:45:05.468: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9184,SelfLink:/api/v1/namespaces/watch-9184/configmaps/e2e-watch-test-label-changed,UID:b506be45-f7b7-4194-9d01-1ca96b4b534c,ResourceVersion:8236,Generation:0,CreationTimestamp:2019-10-04 10:45:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct  4 10:45:05.468: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9184,SelfLink:/api/v1/namespaces/watch-9184/configmaps/e2e-watch-test-label-changed,UID:b506be45-f7b7-4194-9d01-1ca96b4b534c,ResourceVersion:8237,Generation:0,CreationTimestamp:2019-10-04 10:45:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Oct  4 10:45:05.468: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9184,SelfLink:/api/v1/namespaces/watch-9184/configmaps/e2e-watch-test-label-changed,UID:b506be45-f7b7-4194-9d01-1ca96b4b534c,ResourceVersion:8238,Generation:0,CreationTimestamp:2019-10-04 10:45:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Oct  4 10:45:15.508: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9184,SelfLink:/api/v1/namespaces/watch-9184/configmaps/e2e-watch-test-label-changed,UID:b506be45-f7b7-4194-9d01-1ca96b4b534c,ResourceVersion:8265,Generation:0,CreationTimestamp:2019-10-04 10:45:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct  4 10:45:15.508: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9184,SelfLink:/api/v1/namespaces/watch-9184/configmaps/e2e-watch-test-label-changed,UID:b506be45-f7b7-4194-9d01-1ca96b4b534c,ResourceVersion:8266,Generation:0,CreationTimestamp:2019-10-04 10:45:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Oct  4 10:45:15.508: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9184,SelfLink:/api/v1/namespaces/watch-9184/configmaps/e2e-watch-test-label-changed,UID:b506be45-f7b7-4194-9d01-1ca96b4b534c,ResourceVersion:8267,Generation:0,CreationTimestamp:2019-10-04 10:45:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:45:15.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9184" for this suite.
Oct  4 10:45:21.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:45:21.656: INFO: namespace watch-9184 deletion completed in 6.142598429s

• [SLOW TEST:16.258 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:45:21.658: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Oct  4 10:45:21.710: INFO: Waiting up to 5m0s for pod "pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd" in namespace "emptydir-4512" to be "success or failure"
Oct  4 10:45:21.719: INFO: Pod "pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.727289ms
Oct  4 10:45:23.725: INFO: Pod "pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014189555s
Oct  4 10:45:25.730: INFO: Pod "pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019506507s
STEP: Saw pod success
Oct  4 10:45:25.730: INFO: Pod "pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd" satisfied condition "success or failure"
Oct  4 10:45:25.734: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd container test-container: <nil>
STEP: delete the pod
Oct  4 10:45:25.762: INFO: Waiting for pod pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd to disappear
Oct  4 10:45:25.767: INFO: Pod pod-feb31063-ef4b-4e75-a6a1-e4330aa3abfd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:45:25.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4512" for this suite.
Oct  4 10:45:31.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:45:31.928: INFO: namespace emptydir-4512 deletion completed in 6.155909936s

• [SLOW TEST:10.270 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:45:31.928: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Oct  4 10:45:31.996: INFO: Waiting up to 5m0s for pod "pod-eff9b7d5-324d-40b5-810a-9b8489210098" in namespace "emptydir-5026" to be "success or failure"
Oct  4 10:45:32.001: INFO: Pod "pod-eff9b7d5-324d-40b5-810a-9b8489210098": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24894ms
Oct  4 10:45:34.006: INFO: Pod "pod-eff9b7d5-324d-40b5-810a-9b8489210098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009632466s
Oct  4 10:45:36.012: INFO: Pod "pod-eff9b7d5-324d-40b5-810a-9b8489210098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015050373s
STEP: Saw pod success
Oct  4 10:45:36.012: INFO: Pod "pod-eff9b7d5-324d-40b5-810a-9b8489210098" satisfied condition "success or failure"
Oct  4 10:45:36.016: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-eff9b7d5-324d-40b5-810a-9b8489210098 container test-container: <nil>
STEP: delete the pod
Oct  4 10:45:36.044: INFO: Waiting for pod pod-eff9b7d5-324d-40b5-810a-9b8489210098 to disappear
Oct  4 10:45:36.047: INFO: Pod pod-eff9b7d5-324d-40b5-810a-9b8489210098 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:45:36.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5026" for this suite.
Oct  4 10:45:42.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:45:42.199: INFO: namespace emptydir-5026 deletion completed in 6.146454436s

• [SLOW TEST:10.271 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:45:42.201: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 10:45:42.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25" in namespace "downward-api-2855" to be "success or failure"
Oct  4 10:45:42.263: INFO: Pod "downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.549233ms
Oct  4 10:45:44.268: INFO: Pod "downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01187597s
Oct  4 10:45:46.274: INFO: Pod "downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017843298s
STEP: Saw pod success
Oct  4 10:45:46.274: INFO: Pod "downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25" satisfied condition "success or failure"
Oct  4 10:45:46.278: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25 container client-container: <nil>
STEP: delete the pod
Oct  4 10:45:46.304: INFO: Waiting for pod downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25 to disappear
Oct  4 10:45:46.307: INFO: Pod downwardapi-volume-937853fa-9dff-4b45-a576-7d46ea70af25 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:45:46.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2855" for this suite.
Oct  4 10:45:52.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:45:52.465: INFO: namespace downward-api-2855 deletion completed in 6.152871275s

• [SLOW TEST:10.264 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:45:52.466: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Oct  4 10:45:55.039: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7353 pod-service-account-d7890591-b5b7-42a6-ab7d-ec26c42923ee -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Oct  4 10:45:55.330: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7353 pod-service-account-d7890591-b5b7-42a6-ab7d-ec26c42923ee -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Oct  4 10:45:55.573: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7353 pod-service-account-d7890591-b5b7-42a6-ab7d-ec26c42923ee -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:45:55.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7353" for this suite.
Oct  4 10:46:01.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:46:01.977: INFO: namespace svcaccounts-7353 deletion completed in 6.157336354s

• [SLOW TEST:9.511 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:46:01.977: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 10:46:02.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051" in namespace "downward-api-8250" to be "success or failure"
Oct  4 10:46:02.034: INFO: Pod "downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81018ms
Oct  4 10:46:04.038: INFO: Pod "downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010554364s
STEP: Saw pod success
Oct  4 10:46:04.038: INFO: Pod "downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051" satisfied condition "success or failure"
Oct  4 10:46:04.042: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051 container client-container: <nil>
STEP: delete the pod
Oct  4 10:46:04.077: INFO: Waiting for pod downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051 to disappear
Oct  4 10:46:04.081: INFO: Pod downwardapi-volume-0f90e218-03a2-4e28-a1e1-69a9e793d051 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:46:04.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8250" for this suite.
Oct  4 10:46:10.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:46:10.235: INFO: namespace downward-api-8250 deletion completed in 6.148866328s

• [SLOW TEST:8.258 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:46:10.236: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-76809cf9-d72c-4d91-b74d-2de49bc14ec1
STEP: Creating secret with name secret-projected-all-test-volume-7e9efe22-59fb-4827-8cbd-dcf150f1d267
STEP: Creating a pod to test Check all projections for projected volume plugin
Oct  4 10:46:10.294: INFO: Waiting up to 5m0s for pod "projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe" in namespace "projected-8524" to be "success or failure"
Oct  4 10:46:10.299: INFO: Pod "projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261574ms
Oct  4 10:46:12.304: INFO: Pod "projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009410898s
STEP: Saw pod success
Oct  4 10:46:12.304: INFO: Pod "projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe" satisfied condition "success or failure"
Oct  4 10:46:12.308: INFO: Trying to get logs from node lab1-k8s-node-1 pod projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe container projected-all-volume-test: <nil>
STEP: delete the pod
Oct  4 10:46:12.335: INFO: Waiting for pod projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe to disappear
Oct  4 10:46:12.338: INFO: Pod projected-volume-345a7711-95a3-4f60-94d5-4c7343f67dfe no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:46:12.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8524" for this suite.
Oct  4 10:46:18.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:46:18.493: INFO: namespace projected-8524 deletion completed in 6.149565393s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:46:18.493: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Oct  4 10:46:18.543: INFO: Waiting up to 5m0s for pod "downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4" in namespace "downward-api-8612" to be "success or failure"
Oct  4 10:46:18.550: INFO: Pod "downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.994549ms
Oct  4 10:46:20.555: INFO: Pod "downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011994206s
Oct  4 10:46:22.560: INFO: Pod "downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016775786s
Oct  4 10:46:24.565: INFO: Pod "downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021765731s
STEP: Saw pod success
Oct  4 10:46:24.565: INFO: Pod "downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4" satisfied condition "success or failure"
Oct  4 10:46:24.571: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4 container dapi-container: <nil>
STEP: delete the pod
Oct  4 10:46:24.603: INFO: Waiting for pod downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4 to disappear
Oct  4 10:46:24.607: INFO: Pod downward-api-5d3400ea-6f3c-46e1-8128-20d0d148f6a4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:46:24.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8612" for this suite.
Oct  4 10:46:30.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:46:30.759: INFO: namespace downward-api-8612 deletion completed in 6.146610463s

• [SLOW TEST:12.267 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:46:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Oct  4 10:46:30.844: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct  4 10:46:30.855: INFO: Waiting for terminating namespaces to be deleted...
Oct  4 10:46:30.858: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-1 before test
Oct  4 10:46:30.867: INFO: calico-kube-controllers-5b485dc7c7-d4x2r from kube-system started at 2019-10-04 10:22:36 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.867: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct  4 10:46:30.867: INFO: nodelocaldns-9rl9p from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.867: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 10:46:30.867: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-njvm2 from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:46:30.867: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:46:30.867: INFO: 	Container systemd-logs ready: true, restart count 0
Oct  4 10:46:30.867: INFO: kube-proxy-xvmz9 from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.867: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 10:46:30.868: INFO: calico-node-s4qhf from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.868: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 10:46:30.868: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.868: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 10:46:30.868: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-2 before test
Oct  4 10:46:30.878: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.878: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 10:46:30.878: INFO: kube-proxy-bpwhk from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.878: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 10:46:30.878: INFO: calico-node-zrmvj from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.879: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 10:46:30.879: INFO: nodelocaldns-5kkm7 from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.879: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 10:46:30.879: INFO: sonobuoy from heptio-sonobuoy started at 2019-10-04 10:37:02 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.879: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct  4 10:46:30.879: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-vxftt from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:46:30.879: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:46:30.879: INFO: 	Container systemd-logs ready: true, restart count 0
Oct  4 10:46:30.879: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-3 before test
Oct  4 10:46:30.886: INFO: kube-proxy-nt6kv from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.886: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 10:46:30.886: INFO: calico-node-cqqg4 from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.886: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 10:46:30.886: INFO: nodelocaldns-f9sqh from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.886: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 10:46:30.886: INFO: sonobuoy-e2e-job-55aaa13fda1d4ddb from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:46:30.886: INFO: 	Container e2e ready: true, restart count 0
Oct  4 10:46:30.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:46:30.886: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-rsvwx from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 10:46:30.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 10:46:30.886: INFO: 	Container systemd-logs ready: true, restart count 0
Oct  4 10:46:30.886: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 10:46:30.886: INFO: 	Container nginx-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node lab1-k8s-node-1
STEP: verifying the node has the label node lab1-k8s-node-2
STEP: verifying the node has the label node lab1-k8s-node-3
Oct  4 10:46:30.961: INFO: Pod sonobuoy requesting resource cpu=0m on Node lab1-k8s-node-2
Oct  4 10:46:30.961: INFO: Pod sonobuoy-e2e-job-55aaa13fda1d4ddb requesting resource cpu=0m on Node lab1-k8s-node-3
Oct  4 10:46:30.961: INFO: Pod sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-njvm2 requesting resource cpu=0m on Node lab1-k8s-node-1
Oct  4 10:46:30.961: INFO: Pod sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-rsvwx requesting resource cpu=0m on Node lab1-k8s-node-3
Oct  4 10:46:30.961: INFO: Pod sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-vxftt requesting resource cpu=0m on Node lab1-k8s-node-2
Oct  4 10:46:30.961: INFO: Pod calico-kube-controllers-5b485dc7c7-d4x2r requesting resource cpu=30m on Node lab1-k8s-node-1
Oct  4 10:46:30.961: INFO: Pod calico-node-cqqg4 requesting resource cpu=150m on Node lab1-k8s-node-3
Oct  4 10:46:30.961: INFO: Pod calico-node-s4qhf requesting resource cpu=150m on Node lab1-k8s-node-1
Oct  4 10:46:30.961: INFO: Pod calico-node-zrmvj requesting resource cpu=150m on Node lab1-k8s-node-2
Oct  4 10:46:30.961: INFO: Pod kube-proxy-bpwhk requesting resource cpu=0m on Node lab1-k8s-node-2
Oct  4 10:46:30.962: INFO: Pod kube-proxy-nt6kv requesting resource cpu=0m on Node lab1-k8s-node-3
Oct  4 10:46:30.962: INFO: Pod kube-proxy-xvmz9 requesting resource cpu=0m on Node lab1-k8s-node-1
Oct  4 10:46:30.962: INFO: Pod nginx-proxy-lab1-k8s-node-1 requesting resource cpu=25m on Node lab1-k8s-node-1
Oct  4 10:46:30.962: INFO: Pod nginx-proxy-lab1-k8s-node-2 requesting resource cpu=25m on Node lab1-k8s-node-2
Oct  4 10:46:30.962: INFO: Pod nginx-proxy-lab1-k8s-node-3 requesting resource cpu=25m on Node lab1-k8s-node-3
Oct  4 10:46:30.962: INFO: Pod nodelocaldns-5kkm7 requesting resource cpu=100m on Node lab1-k8s-node-2
Oct  4 10:46:30.962: INFO: Pod nodelocaldns-9rl9p requesting resource cpu=100m on Node lab1-k8s-node-1
Oct  4 10:46:30.962: INFO: Pod nodelocaldns-f9sqh requesting resource cpu=100m on Node lab1-k8s-node-3
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1.15ca6bd6f2033daf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-772/filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1 to lab1-k8s-node-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1.15ca6bd7356913d2], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1.15ca6bd75c2b8bd2], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1.15ca6bd760992d2d], Reason = [Created], Message = [Created container filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1.15ca6bd7702e4f65], Reason = [Started], Message = [Started container filler-pod-1844367b-c0ff-4d44-897c-871c8af774a1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c.15ca6bd6f0d07773], Reason = [Scheduled], Message = [Successfully assigned sched-pred-772/filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c to lab1-k8s-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c.15ca6bd7306ca390], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c.15ca6bd75c30086a], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c.15ca6bd75e73c84f], Reason = [Created], Message = [Created container filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c.15ca6bd76de38837], Reason = [Started], Message = [Started container filler-pod-2cfb1bb3-f619-46ad-b790-5242e2cb470c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7.15ca6bd6f14c9d55], Reason = [Scheduled], Message = [Successfully assigned sched-pred-772/filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7 to lab1-k8s-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7.15ca6bd7356dec5b], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7.15ca6bd75c3b8e2a], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7.15ca6bd75fa0e4f4], Reason = [Created], Message = [Created container filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7.15ca6bd770ca4f96], Reason = [Started], Message = [Started container filler-pod-a438b0ee-5b5e-4d18-8117-e137590779c7]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ca6bd7e23b7c81], Reason = [FailedScheduling], Message = [0/6 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 5 Insufficient cpu.]
STEP: removing the label node off the node lab1-k8s-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node lab1-k8s-node-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node lab1-k8s-node-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:46:36.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-772" for this suite.
Oct  4 10:46:42.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:46:42.228: INFO: namespace sched-pred-772 deletion completed in 6.143241456s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.468 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:46:42.228: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Oct  4 10:46:42.567: INFO: Pod name wrapped-volume-race-4ed76b3c-7a25-41a5-a7cf-5582f106ccb3: Found 0 pods out of 5
Oct  4 10:46:47.574: INFO: Pod name wrapped-volume-race-4ed76b3c-7a25-41a5-a7cf-5582f106ccb3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4ed76b3c-7a25-41a5-a7cf-5582f106ccb3 in namespace emptydir-wrapper-8861, will wait for the garbage collector to delete the pods
Oct  4 10:46:57.664: INFO: Deleting ReplicationController wrapped-volume-race-4ed76b3c-7a25-41a5-a7cf-5582f106ccb3 took: 10.395801ms
Oct  4 10:46:57.964: INFO: Terminating ReplicationController wrapped-volume-race-4ed76b3c-7a25-41a5-a7cf-5582f106ccb3 pods took: 300.229957ms
STEP: Creating RC which spawns configmap-volume pods
Oct  4 10:47:42.193: INFO: Pod name wrapped-volume-race-428b9e44-6ff3-4135-bb0e-8f99b53b1b97: Found 0 pods out of 5
Oct  4 10:47:47.212: INFO: Pod name wrapped-volume-race-428b9e44-6ff3-4135-bb0e-8f99b53b1b97: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-428b9e44-6ff3-4135-bb0e-8f99b53b1b97 in namespace emptydir-wrapper-8861, will wait for the garbage collector to delete the pods
Oct  4 10:47:59.302: INFO: Deleting ReplicationController wrapped-volume-race-428b9e44-6ff3-4135-bb0e-8f99b53b1b97 took: 11.563226ms
Oct  4 10:47:59.603: INFO: Terminating ReplicationController wrapped-volume-race-428b9e44-6ff3-4135-bb0e-8f99b53b1b97 pods took: 300.576441ms
STEP: Creating RC which spawns configmap-volume pods
Oct  4 10:48:42.523: INFO: Pod name wrapped-volume-race-19273002-4668-4526-81b8-92f981e278c6: Found 0 pods out of 5
Oct  4 10:48:47.530: INFO: Pod name wrapped-volume-race-19273002-4668-4526-81b8-92f981e278c6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-19273002-4668-4526-81b8-92f981e278c6 in namespace emptydir-wrapper-8861, will wait for the garbage collector to delete the pods
Oct  4 10:48:59.637: INFO: Deleting ReplicationController wrapped-volume-race-19273002-4668-4526-81b8-92f981e278c6 took: 23.703765ms
Oct  4 10:48:59.937: INFO: Terminating ReplicationController wrapped-volume-race-19273002-4668-4526-81b8-92f981e278c6 pods took: 300.239645ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:49:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8861" for this suite.
Oct  4 10:49:50.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:49:50.839: INFO: namespace emptydir-wrapper-8861 deletion completed in 8.148620947s

• [SLOW TEST:188.611 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:49:50.840: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-19571c4c-e07b-4589-80f0-694b10cdd63a
STEP: Creating a pod to test consume configMaps
Oct  4 10:49:50.893: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba" in namespace "configmap-7953" to be "success or failure"
Oct  4 10:49:50.901: INFO: Pod "pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.369725ms
Oct  4 10:49:52.909: INFO: Pod "pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015472365s
STEP: Saw pod success
Oct  4 10:49:52.909: INFO: Pod "pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba" satisfied condition "success or failure"
Oct  4 10:49:52.913: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 10:49:52.943: INFO: Waiting for pod pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba to disappear
Oct  4 10:49:52.947: INFO: Pod pod-configmaps-eb831e09-5d57-41ff-bc96-9e1964a165ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:49:52.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7953" for this suite.
Oct  4 10:49:58.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:49:59.097: INFO: namespace configmap-7953 deletion completed in 6.144753757s

• [SLOW TEST:8.258 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:49:59.101: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-6a792194-3c1a-4d72-9179-6b8e14edb5a7
STEP: Creating a pod to test consume secrets
Oct  4 10:49:59.166: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778" in namespace "projected-1462" to be "success or failure"
Oct  4 10:49:59.171: INFO: Pod "pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233027ms
Oct  4 10:50:01.176: INFO: Pod "pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009976239s
STEP: Saw pod success
Oct  4 10:50:01.176: INFO: Pod "pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778" satisfied condition "success or failure"
Oct  4 10:50:01.181: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778 container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 10:50:01.219: INFO: Waiting for pod pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778 to disappear
Oct  4 10:50:01.222: INFO: Pod pod-projected-secrets-cc51fb60-32f4-48c1-9214-a72e2a387778 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:50:01.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1462" for this suite.
Oct  4 10:50:07.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:50:07.384: INFO: namespace projected-1462 deletion completed in 6.156164487s

• [SLOW TEST:8.284 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:50:07.386: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct  4 10:50:07.432: INFO: Waiting up to 5m0s for pod "pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3" in namespace "emptydir-7400" to be "success or failure"
Oct  4 10:50:07.439: INFO: Pod "pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.74964ms
Oct  4 10:50:09.448: INFO: Pod "pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015510094s
STEP: Saw pod success
Oct  4 10:50:09.448: INFO: Pod "pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3" satisfied condition "success or failure"
Oct  4 10:50:09.452: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3 container test-container: <nil>
STEP: delete the pod
Oct  4 10:50:09.481: INFO: Waiting for pod pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3 to disappear
Oct  4 10:50:09.485: INFO: Pod pod-ee1ae89c-f8fc-4926-8db5-f23705d243a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:50:09.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7400" for this suite.
Oct  4 10:50:15.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:50:15.647: INFO: namespace emptydir-7400 deletion completed in 6.156754302s

• [SLOW TEST:8.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:50:15.649: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-5d83f24d-bfe5-482c-8a6b-ac79a182a284 in namespace container-probe-4603
Oct  4 10:50:19.715: INFO: Started pod test-webserver-5d83f24d-bfe5-482c-8a6b-ac79a182a284 in namespace container-probe-4603
STEP: checking the pod's current state and verifying that restartCount is present
Oct  4 10:50:19.719: INFO: Initial restart count of pod test-webserver-5d83f24d-bfe5-482c-8a6b-ac79a182a284 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:54:20.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4603" for this suite.
Oct  4 10:54:26.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:54:26.518: INFO: namespace container-probe-4603 deletion completed in 6.14340571s

• [SLOW TEST:250.869 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:54:26.519: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-41531027-c44c-4bc7-b6ea-63d94f4e2d27
STEP: Creating a pod to test consume secrets
Oct  4 10:54:26.585: INFO: Waiting up to 5m0s for pod "pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13" in namespace "secrets-9881" to be "success or failure"
Oct  4 10:54:26.594: INFO: Pod "pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.765203ms
Oct  4 10:54:28.605: INFO: Pod "pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019703227s
STEP: Saw pod success
Oct  4 10:54:28.605: INFO: Pod "pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13" satisfied condition "success or failure"
Oct  4 10:54:28.610: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13 container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 10:54:28.638: INFO: Waiting for pod pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13 to disappear
Oct  4 10:54:28.642: INFO: Pod pod-secrets-af28e179-31d0-495c-b33a-2a6559170d13 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:54:28.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9881" for this suite.
Oct  4 10:54:34.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:54:34.790: INFO: namespace secrets-9881 deletion completed in 6.142526656s

• [SLOW TEST:8.271 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:54:34.791: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct  4 10:54:37.377: INFO: Successfully updated pod "pod-update-activedeadlineseconds-59298e84-4db9-414a-a111-39b604f6eda1"
Oct  4 10:54:37.377: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-59298e84-4db9-414a-a111-39b604f6eda1" in namespace "pods-4460" to be "terminated due to deadline exceeded"
Oct  4 10:54:37.381: INFO: Pod "pod-update-activedeadlineseconds-59298e84-4db9-414a-a111-39b604f6eda1": Phase="Running", Reason="", readiness=true. Elapsed: 4.269999ms
Oct  4 10:54:39.386: INFO: Pod "pod-update-activedeadlineseconds-59298e84-4db9-414a-a111-39b604f6eda1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008939332s
Oct  4 10:54:41.391: INFO: Pod "pod-update-activedeadlineseconds-59298e84-4db9-414a-a111-39b604f6eda1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014103137s
Oct  4 10:54:41.391: INFO: Pod "pod-update-activedeadlineseconds-59298e84-4db9-414a-a111-39b604f6eda1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:54:41.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4460" for this suite.
Oct  4 10:54:47.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:54:47.541: INFO: namespace pods-4460 deletion completed in 6.144585479s

• [SLOW TEST:12.750 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:54:47.543: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 10:54:47.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-2729'
Oct  4 10:54:47.844: INFO: stderr: ""
Oct  4 10:54:47.844: INFO: stdout: "replicationcontroller/redis-master created\n"
Oct  4 10:54:47.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-2729'
Oct  4 10:54:48.047: INFO: stderr: ""
Oct  4 10:54:48.047: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Oct  4 10:54:49.053: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:54:49.053: INFO: Found 0 / 1
Oct  4 10:54:50.052: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:54:50.052: INFO: Found 1 / 1
Oct  4 10:54:50.052: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct  4 10:54:50.057: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 10:54:50.057: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct  4 10:54:50.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 describe pod redis-master-bdbqh --namespace=kubectl-2729'
Oct  4 10:54:50.155: INFO: stderr: ""
Oct  4 10:54:50.155: INFO: stdout: "Name:           redis-master-bdbqh\nNamespace:      kubectl-2729\nPriority:       0\nNode:           lab1-k8s-node-1/10.128.0.18\nStart Time:     Fri, 04 Oct 2019 10:54:47 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.233.95.41\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://1013d437124ea262e51841b869ccce16f9b6563bbc992de717860a3176322daa\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 04 Oct 2019 10:54:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mthtp (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mthtp:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mthtp\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                      Message\n  ----    ------     ----  ----                      -------\n  Normal  Scheduled  3s    default-scheduler         Successfully assigned kubectl-2729/redis-master-bdbqh to lab1-k8s-node-1\n  Normal  Pulled     2s    kubelet, lab1-k8s-node-1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, lab1-k8s-node-1  Created container redis-master\n  Normal  Started    1s    kubelet, lab1-k8s-node-1  Started container redis-master\n"
Oct  4 10:54:50.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 describe rc redis-master --namespace=kubectl-2729'
Oct  4 10:54:50.269: INFO: stderr: ""
Oct  4 10:54:50.269: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2729\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-bdbqh\n"
Oct  4 10:54:50.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 describe service redis-master --namespace=kubectl-2729'
Oct  4 10:54:50.356: INFO: stderr: ""
Oct  4 10:54:50.356: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2729\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.29.220\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.95.41:6379\nSession Affinity:  None\nEvents:            <none>\n"
Oct  4 10:54:50.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 describe node lab1-k8s-master-1'
Oct  4 10:54:50.471: INFO: stderr: ""
Oct  4 10:54:50.472: INFO: stdout: "Name:               lab1-k8s-master-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=3f73fc93-ec61-4808-88df-2580d94c1a9b\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=se-sto\n                    failure-domain.beta.kubernetes.io/zone=sto1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=lab1-k8s-master-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 04 Oct 2019 09:57:42 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 04 Oct 2019 10:22:00 +0000   Fri, 04 Oct 2019 10:22:00 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 04 Oct 2019 10:54:49 +0000   Fri, 04 Oct 2019 09:57:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 04 Oct 2019 10:54:49 +0000   Fri, 04 Oct 2019 09:57:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 04 Oct 2019 10:54:49 +0000   Fri, 04 Oct 2019 09:57:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 04 Oct 2019 10:54:49 +0000   Fri, 04 Oct 2019 10:21:14 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.128.0.14\n  ExternalIP:  212.237.150.100\n  Hostname:    lab1-k8s-master-1\nCapacity:\n attachable-volumes-cinder:  26\n cpu:                        2\n ephemeral-storage:          40470732Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     8168244Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  26\n cpu:                        1800m\n ephemeral-storage:          37297826550\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     7565844Ki\n pods:                       110\nSystem Info:\n Machine ID:                 7f528222fb4a4a5aaaf2f10537b6d1de\n System UUID:                7F528222-FB4A-4A5A-AAF2-F10537B6D1DE\n Boot ID:                    dd4412b1-8c3d-495c-af90-2d4bf396b012\n Kernel Version:             4.15.0-34-generic\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.7\n Kubelet Version:            v1.15.3\n Kube-Proxy Version:         v1.15.3\nPodCIDR:                     10.233.64.0/24\nProviderID:                  openstack:///7f528222-fb4a-4a5a-aaf2-f10537b6d1de\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-tnh8n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                calico-node-92qqk                                          150m (8%)     300m (16%)  64M (0%)         500M (6%)      33m\n  kube-system                kube-apiserver-lab1-k8s-master-1                           250m (13%)    0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                kube-controller-manager-lab1-k8s-master-1                  200m (11%)    0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                kube-proxy-qxq9c                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                kube-scheduler-lab1-k8s-master-1                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                nodelocaldns-998t4                                         100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests        Limits\n  --------                   --------        ------\n  cpu                        800m (44%)      300m (16%)\n  memory                     137400320 (1%)  678257920 (8%)\n  ephemeral-storage          0 (0%)          0 (0%)\n  attachable-volumes-cinder  0               0\nEvents:\n  Type    Reason                   Age                From                           Message\n  ----    ------                   ----               ----                           -------\n  Normal  NodeHasSufficientPID     57m (x7 over 57m)  kubelet, lab1-k8s-master-1     Node lab1-k8s-master-1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  57m (x8 over 57m)  kubelet, lab1-k8s-master-1     Node lab1-k8s-master-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    57m (x8 over 57m)  kubelet, lab1-k8s-master-1     Node lab1-k8s-master-1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 56m                kube-proxy, lab1-k8s-master-1  Starting kube-proxy.\n  Normal  Starting                 34m                kube-proxy, lab1-k8s-master-1  Starting kube-proxy.\n  Normal  NodeReady                33m                kubelet, lab1-k8s-master-1     Node lab1-k8s-master-1 status is now: NodeReady\n"
Oct  4 10:54:50.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 describe namespace kubectl-2729'
Oct  4 10:54:50.559: INFO: stderr: ""
Oct  4 10:54:50.559: INFO: stdout: "Name:         kubectl-2729\nLabels:       e2e-framework=kubectl\n              e2e-run=556d30ef-563d-4aec-981d-1f183c0315cb\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:54:50.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2729" for this suite.
Oct  4 10:55:12.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:55:12.728: INFO: namespace kubectl-2729 deletion completed in 22.163645094s

• [SLOW TEST:25.186 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:55:12.729: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-b3324151-d0d7-4084-8e56-366e7af03a3c
STEP: Creating a pod to test consume configMaps
Oct  4 10:55:12.779: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2" in namespace "projected-2442" to be "success or failure"
Oct  4 10:55:12.785: INFO: Pod "pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.742921ms
Oct  4 10:55:14.789: INFO: Pod "pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010298605s
Oct  4 10:55:16.794: INFO: Pod "pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015505909s
STEP: Saw pod success
Oct  4 10:55:16.794: INFO: Pod "pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2" satisfied condition "success or failure"
Oct  4 10:55:16.798: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 10:55:16.826: INFO: Waiting for pod pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2 to disappear
Oct  4 10:55:16.830: INFO: Pod pod-projected-configmaps-5962db9d-b430-4c92-b68c-8a7e409891a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:55:16.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2442" for this suite.
Oct  4 10:55:22.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:55:22.975: INFO: namespace projected-2442 deletion completed in 6.140311292s

• [SLOW TEST:10.246 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:55:22.977: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 10:55:23.022: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:55:27.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6576" for this suite.
Oct  4 10:56:17.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:56:17.227: INFO: namespace pods-6576 deletion completed in 50.150951464s

• [SLOW TEST:54.250 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:56:17.228: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:56:23.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4124" for this suite.
Oct  4 10:56:29.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:56:29.524: INFO: namespace namespaces-4124 deletion completed in 6.155370677s
STEP: Destroying namespace "nsdeletetest-8962" for this suite.
Oct  4 10:56:29.529: INFO: Namespace nsdeletetest-8962 was already deleted
STEP: Destroying namespace "nsdeletetest-6007" for this suite.
Oct  4 10:56:35.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:56:35.675: INFO: namespace nsdeletetest-6007 deletion completed in 6.145927193s

• [SLOW TEST:18.447 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:56:35.676: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Oct  4 10:56:35.729: INFO: Waiting up to 5m0s for pod "pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff" in namespace "emptydir-843" to be "success or failure"
Oct  4 10:56:35.734: INFO: Pod "pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.620997ms
Oct  4 10:56:37.739: INFO: Pod "pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010322278s
Oct  4 10:56:39.744: INFO: Pod "pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015473278s
STEP: Saw pod success
Oct  4 10:56:39.745: INFO: Pod "pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff" satisfied condition "success or failure"
Oct  4 10:56:39.748: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff container test-container: <nil>
STEP: delete the pod
Oct  4 10:56:39.774: INFO: Waiting for pod pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff to disappear
Oct  4 10:56:39.778: INFO: Pod pod-34742ac5-6ad1-409c-abaf-bf44b8a3f5ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:56:39.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-843" for this suite.
Oct  4 10:56:45.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:56:45.937: INFO: namespace emptydir-843 deletion completed in 6.154021593s

• [SLOW TEST:10.261 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:56:45.937: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 10:56:45.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916" in namespace "downward-api-901" to be "success or failure"
Oct  4 10:56:45.993: INFO: Pod "downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916": Phase="Pending", Reason="", readiness=false. Elapsed: 5.986909ms
Oct  4 10:56:47.998: INFO: Pod "downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010717081s
Oct  4 10:56:50.002: INFO: Pod "downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015611595s
STEP: Saw pod success
Oct  4 10:56:50.003: INFO: Pod "downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916" satisfied condition "success or failure"
Oct  4 10:56:50.007: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916 container client-container: <nil>
STEP: delete the pod
Oct  4 10:56:50.042: INFO: Waiting for pod downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916 to disappear
Oct  4 10:56:50.051: INFO: Pod downwardapi-volume-90e7decd-89c1-4392-827a-3c5c36206916 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:56:50.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-901" for this suite.
Oct  4 10:56:56.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:56:56.204: INFO: namespace downward-api-901 deletion completed in 6.147706773s

• [SLOW TEST:10.267 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:56:56.205: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct  4 10:57:04.298: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:04.303: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:06.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:06.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:08.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:08.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:10.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:10.307: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:12.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:12.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:14.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:14.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:16.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:16.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:18.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:18.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:20.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:20.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:22.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:22.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:24.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:24.309: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:26.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:26.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:28.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:28.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:30.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:30.308: INFO: Pod pod-with-prestop-exec-hook still exists
Oct  4 10:57:32.303: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Oct  4 10:57:32.308: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:57:32.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3184" for this suite.
Oct  4 10:57:54.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:57:54.478: INFO: namespace container-lifecycle-hook-3184 deletion completed in 22.152862362s

• [SLOW TEST:58.273 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:57:54.479: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1810, will wait for the garbage collector to delete the pods
Oct  4 10:57:58.598: INFO: Deleting Job.batch foo took: 16.322845ms
Oct  4 10:57:58.899: INFO: Terminating Job.batch foo pods took: 300.308689ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:58:42.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1810" for this suite.
Oct  4 10:58:48.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:58:48.257: INFO: namespace job-1810 deletion completed in 6.147974486s

• [SLOW TEST:53.778 seconds]
[sig-apps] Job
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:58:48.257: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5769/configmap-test-a86d415a-dd2a-4f6b-80c2-6b8d9eb8ab85
STEP: Creating a pod to test consume configMaps
Oct  4 10:58:48.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929" in namespace "configmap-5769" to be "success or failure"
Oct  4 10:58:48.311: INFO: Pod "pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929": Phase="Pending", Reason="", readiness=false. Elapsed: 3.873728ms
Oct  4 10:58:50.317: INFO: Pod "pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009114129s
Oct  4 10:58:52.323: INFO: Pod "pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01513816s
STEP: Saw pod success
Oct  4 10:58:52.323: INFO: Pod "pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929" satisfied condition "success or failure"
Oct  4 10:58:52.326: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929 container env-test: <nil>
STEP: delete the pod
Oct  4 10:58:52.351: INFO: Waiting for pod pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929 to disappear
Oct  4 10:58:52.355: INFO: Pod pod-configmaps-ef476808-868f-4d54-ba41-ea8855fe1929 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:58:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5769" for this suite.
Oct  4 10:58:58.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:58:58.514: INFO: namespace configmap-5769 deletion completed in 6.154162585s

• [SLOW TEST:10.257 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:58:58.515: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-1150b25a-467e-461b-a813-4c8077c8b513
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:58:58.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5333" for this suite.
Oct  4 10:59:04.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:59:04.721: INFO: namespace secrets-5333 deletion completed in 6.163434761s

• [SLOW TEST:6.207 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:59:04.722: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Oct  4 10:59:04.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-1109'
Oct  4 10:59:05.002: INFO: stderr: ""
Oct  4 10:59:05.002: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct  4 10:59:05.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:05.085: INFO: stderr: ""
Oct  4 10:59:05.085: INFO: stdout: "update-demo-nautilus-hgg9g update-demo-nautilus-mdwqx "
Oct  4 10:59:05.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:05.161: INFO: stderr: ""
Oct  4 10:59:05.161: INFO: stdout: ""
Oct  4 10:59:05.161: INFO: update-demo-nautilus-hgg9g is created but not running
Oct  4 10:59:10.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:10.243: INFO: stderr: ""
Oct  4 10:59:10.243: INFO: stdout: "update-demo-nautilus-hgg9g update-demo-nautilus-mdwqx "
Oct  4 10:59:10.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:10.322: INFO: stderr: ""
Oct  4 10:59:10.322: INFO: stdout: "true"
Oct  4 10:59:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:10.399: INFO: stderr: ""
Oct  4 10:59:10.399: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 10:59:10.399: INFO: validating pod update-demo-nautilus-hgg9g
Oct  4 10:59:10.407: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 10:59:10.407: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 10:59:10.407: INFO: update-demo-nautilus-hgg9g is verified up and running
Oct  4 10:59:10.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-mdwqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:10.498: INFO: stderr: ""
Oct  4 10:59:10.498: INFO: stdout: "true"
Oct  4 10:59:10.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-mdwqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:10.580: INFO: stderr: ""
Oct  4 10:59:10.580: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 10:59:10.580: INFO: validating pod update-demo-nautilus-mdwqx
Oct  4 10:59:10.587: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 10:59:10.587: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 10:59:10.587: INFO: update-demo-nautilus-mdwqx is verified up and running
STEP: scaling down the replication controller
Oct  4 10:59:10.589: INFO: scanned /root for discovery docs: <nil>
Oct  4 10:59:10.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1109'
Oct  4 10:59:11.705: INFO: stderr: ""
Oct  4 10:59:11.705: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct  4 10:59:11.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:11.783: INFO: stderr: ""
Oct  4 10:59:11.783: INFO: stdout: "update-demo-nautilus-hgg9g update-demo-nautilus-mdwqx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct  4 10:59:16.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:16.861: INFO: stderr: ""
Oct  4 10:59:16.861: INFO: stdout: "update-demo-nautilus-hgg9g update-demo-nautilus-mdwqx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct  4 10:59:21.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:21.938: INFO: stderr: ""
Oct  4 10:59:21.938: INFO: stdout: "update-demo-nautilus-hgg9g update-demo-nautilus-mdwqx "
STEP: Replicas for name=update-demo: expected=1 actual=2
Oct  4 10:59:26.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:27.013: INFO: stderr: ""
Oct  4 10:59:27.013: INFO: stdout: "update-demo-nautilus-hgg9g "
Oct  4 10:59:27.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:27.090: INFO: stderr: ""
Oct  4 10:59:27.090: INFO: stdout: "true"
Oct  4 10:59:27.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:27.190: INFO: stderr: ""
Oct  4 10:59:27.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 10:59:27.190: INFO: validating pod update-demo-nautilus-hgg9g
Oct  4 10:59:27.197: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 10:59:27.197: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 10:59:27.197: INFO: update-demo-nautilus-hgg9g is verified up and running
STEP: scaling up the replication controller
Oct  4 10:59:27.199: INFO: scanned /root for discovery docs: <nil>
Oct  4 10:59:27.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1109'
Oct  4 10:59:28.319: INFO: stderr: ""
Oct  4 10:59:28.319: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct  4 10:59:28.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:28.425: INFO: stderr: ""
Oct  4 10:59:28.425: INFO: stdout: "update-demo-nautilus-bb88w update-demo-nautilus-hgg9g "
Oct  4 10:59:28.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-bb88w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:28.512: INFO: stderr: ""
Oct  4 10:59:28.512: INFO: stdout: ""
Oct  4 10:59:28.512: INFO: update-demo-nautilus-bb88w is created but not running
Oct  4 10:59:33.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1109'
Oct  4 10:59:33.590: INFO: stderr: ""
Oct  4 10:59:33.590: INFO: stdout: "update-demo-nautilus-bb88w update-demo-nautilus-hgg9g "
Oct  4 10:59:33.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-bb88w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:33.663: INFO: stderr: ""
Oct  4 10:59:33.663: INFO: stdout: "true"
Oct  4 10:59:33.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-bb88w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:33.766: INFO: stderr: ""
Oct  4 10:59:33.766: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 10:59:33.766: INFO: validating pod update-demo-nautilus-bb88w
Oct  4 10:59:33.773: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 10:59:33.773: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 10:59:33.773: INFO: update-demo-nautilus-bb88w is verified up and running
Oct  4 10:59:33.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:33.851: INFO: stderr: ""
Oct  4 10:59:33.851: INFO: stdout: "true"
Oct  4 10:59:33.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-hgg9g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1109'
Oct  4 10:59:33.923: INFO: stderr: ""
Oct  4 10:59:33.923: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 10:59:33.923: INFO: validating pod update-demo-nautilus-hgg9g
Oct  4 10:59:33.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 10:59:33.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 10:59:33.928: INFO: update-demo-nautilus-hgg9g is verified up and running
STEP: using delete to clean up resources
Oct  4 10:59:33.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-1109'
Oct  4 10:59:34.020: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 10:59:34.020: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct  4 10:59:34.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1109'
Oct  4 10:59:34.116: INFO: stderr: "No resources found.\n"
Oct  4 10:59:34.116: INFO: stdout: ""
Oct  4 10:59:34.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -l name=update-demo --namespace=kubectl-1109 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct  4 10:59:34.195: INFO: stderr: ""
Oct  4 10:59:34.195: INFO: stdout: "update-demo-nautilus-bb88w\nupdate-demo-nautilus-hgg9g\n"
Oct  4 10:59:34.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1109'
Oct  4 10:59:34.802: INFO: stderr: "No resources found.\n"
Oct  4 10:59:34.802: INFO: stdout: ""
Oct  4 10:59:34.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -l name=update-demo --namespace=kubectl-1109 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct  4 10:59:34.895: INFO: stderr: ""
Oct  4 10:59:34.895: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:59:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1109" for this suite.
Oct  4 10:59:40.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:59:41.051: INFO: namespace kubectl-1109 deletion completed in 6.150464146s

• [SLOW TEST:36.328 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:59:41.051: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct  4 10:59:41.100: INFO: Waiting up to 5m0s for pod "pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4" in namespace "emptydir-8031" to be "success or failure"
Oct  4 10:59:41.109: INFO: Pod "pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.78328ms
Oct  4 10:59:43.115: INFO: Pod "pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015347608s
Oct  4 10:59:45.120: INFO: Pod "pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020839519s
STEP: Saw pod success
Oct  4 10:59:45.121: INFO: Pod "pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4" satisfied condition "success or failure"
Oct  4 10:59:45.125: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4 container test-container: <nil>
STEP: delete the pod
Oct  4 10:59:45.149: INFO: Waiting for pod pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4 to disappear
Oct  4 10:59:45.153: INFO: Pod pod-9b4f732c-7cfc-4399-aa31-b2f0c29cddb4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 10:59:45.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8031" for this suite.
Oct  4 10:59:51.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 10:59:51.317: INFO: namespace emptydir-8031 deletion completed in 6.158703729s

• [SLOW TEST:10.266 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 10:59:51.317: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7508
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7508
STEP: Creating statefulset with conflicting port in namespace statefulset-7508
STEP: Waiting until pod test-pod will start running in namespace statefulset-7508
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7508
Oct  4 10:59:55.395: INFO: Observed stateful pod in namespace: statefulset-7508, name: ss-0, uid: 7fdfa7f7-eecf-4ce7-bf80-beb254af73d1, status phase: Pending. Waiting for statefulset controller to delete.
Oct  4 10:59:55.788: INFO: Observed stateful pod in namespace: statefulset-7508, name: ss-0, uid: 7fdfa7f7-eecf-4ce7-bf80-beb254af73d1, status phase: Failed. Waiting for statefulset controller to delete.
Oct  4 10:59:55.794: INFO: Observed stateful pod in namespace: statefulset-7508, name: ss-0, uid: 7fdfa7f7-eecf-4ce7-bf80-beb254af73d1, status phase: Failed. Waiting for statefulset controller to delete.
Oct  4 10:59:55.799: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7508
STEP: Removing pod with conflicting port in namespace statefulset-7508
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7508 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Oct  4 10:59:59.831: INFO: Deleting all statefulset in ns statefulset-7508
Oct  4 10:59:59.835: INFO: Scaling statefulset ss to 0
Oct  4 11:00:19.855: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:00:19.858: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:00:19.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7508" for this suite.
Oct  4 11:00:25.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:00:26.034: INFO: namespace statefulset-7508 deletion completed in 6.14610528s

• [SLOW TEST:34.717 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:00:26.034: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:00:26.102: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6c828b57-0b7c-4913-b6fe-ec2b3d7aba40", Controller:(*bool)(0xc002b1f68e), BlockOwnerDeletion:(*bool)(0xc002b1f68f)}}
Oct  4 11:00:26.110: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9a8ffd9d-a414-46ac-97b7-f8bdb310eb12", Controller:(*bool)(0xc00292f266), BlockOwnerDeletion:(*bool)(0xc00292f267)}}
Oct  4 11:00:26.124: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a631524e-233d-4e27-b23b-af94bd2e2793", Controller:(*bool)(0xc00292f5fa), BlockOwnerDeletion:(*bool)(0xc00292f5fb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:00:31.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2407" for this suite.
Oct  4 11:00:37.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:00:37.298: INFO: namespace gc-2407 deletion completed in 6.152218565s

• [SLOW TEST:11.264 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:00:37.300: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Oct  4 11:00:37.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 --namespace=kubectl-8418 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Oct  4 11:00:39.513: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Oct  4 11:00:39.513: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:00:41.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8418" for this suite.
Oct  4 11:00:55.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:00:55.673: INFO: namespace kubectl-8418 deletion completed in 14.147413443s

• [SLOW TEST:18.373 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:00:55.673: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-ab6b2e09-7036-4484-90db-42f0b3b70da9
STEP: Creating a pod to test consume configMaps
Oct  4 11:00:55.723: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4" in namespace "configmap-649" to be "success or failure"
Oct  4 11:00:55.729: INFO: Pod "pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.822667ms
Oct  4 11:00:57.734: INFO: Pod "pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010248701s
STEP: Saw pod success
Oct  4 11:00:57.734: INFO: Pod "pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4" satisfied condition "success or failure"
Oct  4 11:00:57.738: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4 container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:00:57.765: INFO: Waiting for pod pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4 to disappear
Oct  4 11:00:57.769: INFO: Pod pod-configmaps-a5560bed-effc-440c-9c20-08860c7561c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:00:57.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-649" for this suite.
Oct  4 11:01:03.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:01:03.971: INFO: namespace configmap-649 deletion completed in 6.195969966s

• [SLOW TEST:8.298 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:01:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Oct  4 11:01:04.087: INFO: Waiting up to 5m0s for pod "client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874" in namespace "containers-7875" to be "success or failure"
Oct  4 11:01:04.099: INFO: Pod "client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874": Phase="Pending", Reason="", readiness=false. Elapsed: 12.479709ms
Oct  4 11:01:06.104: INFO: Pod "client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017339525s
Oct  4 11:01:08.110: INFO: Pod "client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022612347s
STEP: Saw pod success
Oct  4 11:01:08.110: INFO: Pod "client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874" satisfied condition "success or failure"
Oct  4 11:01:08.114: INFO: Trying to get logs from node lab1-k8s-node-1 pod client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874 container test-container: <nil>
STEP: delete the pod
Oct  4 11:01:08.139: INFO: Waiting for pod client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874 to disappear
Oct  4 11:01:08.143: INFO: Pod client-containers-cdab4f6c-dbc4-4639-8888-aa64c2c89874 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:01:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7875" for this suite.
Oct  4 11:01:14.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:01:14.306: INFO: namespace containers-7875 deletion completed in 6.157433926s

• [SLOW TEST:10.335 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:01:14.309: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:01:14.358: INFO: (0) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.564533ms)
Oct  4 11:01:14.362: INFO: (1) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.398607ms)
Oct  4 11:01:14.367: INFO: (2) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.291163ms)
Oct  4 11:01:14.371: INFO: (3) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.483274ms)
Oct  4 11:01:14.376: INFO: (4) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.29613ms)
Oct  4 11:01:14.380: INFO: (5) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.443945ms)
Oct  4 11:01:14.385: INFO: (6) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.18537ms)
Oct  4 11:01:14.389: INFO: (7) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.302865ms)
Oct  4 11:01:14.393: INFO: (8) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.156974ms)
Oct  4 11:01:14.398: INFO: (9) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.20639ms)
Oct  4 11:01:14.402: INFO: (10) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.347806ms)
Oct  4 11:01:14.407: INFO: (11) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.27399ms)
Oct  4 11:01:14.411: INFO: (12) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.106559ms)
Oct  4 11:01:14.415: INFO: (13) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.274323ms)
Oct  4 11:01:14.420: INFO: (14) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.641948ms)
Oct  4 11:01:14.424: INFO: (15) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.516032ms)
Oct  4 11:01:14.429: INFO: (16) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.494459ms)
Oct  4 11:01:14.433: INFO: (17) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.265564ms)
Oct  4 11:01:14.438: INFO: (18) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.435182ms)
Oct  4 11:01:14.442: INFO: (19) /api/v1/nodes/lab1-k8s-node-1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.404721ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:01:14.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5324" for this suite.
Oct  4 11:01:20.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:01:20.594: INFO: namespace proxy-5324 deletion completed in 6.146694321s

• [SLOW TEST:6.286 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:01:20.599: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Oct  4 11:01:20.642: INFO: Waiting up to 5m0s for pod "pod-97f12a3b-53c5-45c2-a182-338860cd2b2b" in namespace "emptydir-7643" to be "success or failure"
Oct  4 11:01:20.649: INFO: Pod "pod-97f12a3b-53c5-45c2-a182-338860cd2b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.570872ms
Oct  4 11:01:22.654: INFO: Pod "pod-97f12a3b-53c5-45c2-a182-338860cd2b2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011722651s
STEP: Saw pod success
Oct  4 11:01:22.655: INFO: Pod "pod-97f12a3b-53c5-45c2-a182-338860cd2b2b" satisfied condition "success or failure"
Oct  4 11:01:22.659: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-97f12a3b-53c5-45c2-a182-338860cd2b2b container test-container: <nil>
STEP: delete the pod
Oct  4 11:01:22.689: INFO: Waiting for pod pod-97f12a3b-53c5-45c2-a182-338860cd2b2b to disappear
Oct  4 11:01:22.693: INFO: Pod pod-97f12a3b-53c5-45c2-a182-338860cd2b2b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:01:22.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7643" for this suite.
Oct  4 11:01:28.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:01:28.857: INFO: namespace emptydir-7643 deletion completed in 6.158257846s

• [SLOW TEST:8.258 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:01:28.860: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-ckp9
STEP: Creating a pod to test atomic-volume-subpath
Oct  4 11:01:28.923: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ckp9" in namespace "subpath-2229" to be "success or failure"
Oct  4 11:01:28.927: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364651ms
Oct  4 11:01:30.932: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009253847s
Oct  4 11:01:32.937: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 4.013791177s
Oct  4 11:01:34.941: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 6.018280796s
Oct  4 11:01:36.948: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 8.024896901s
Oct  4 11:01:38.954: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 10.03104021s
Oct  4 11:01:40.959: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 12.036047718s
Oct  4 11:01:42.964: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 14.040725662s
Oct  4 11:01:44.969: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 16.045864013s
Oct  4 11:01:46.973: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 18.050571842s
Oct  4 11:01:48.978: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 20.054896894s
Oct  4 11:01:50.983: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Running", Reason="", readiness=true. Elapsed: 22.059697253s
Oct  4 11:01:52.987: INFO: Pod "pod-subpath-test-secret-ckp9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064277332s
STEP: Saw pod success
Oct  4 11:01:52.987: INFO: Pod "pod-subpath-test-secret-ckp9" satisfied condition "success or failure"
Oct  4 11:01:52.991: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-secret-ckp9 container test-container-subpath-secret-ckp9: <nil>
STEP: delete the pod
Oct  4 11:01:53.020: INFO: Waiting for pod pod-subpath-test-secret-ckp9 to disappear
Oct  4 11:01:53.024: INFO: Pod pod-subpath-test-secret-ckp9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-ckp9
Oct  4 11:01:53.024: INFO: Deleting pod "pod-subpath-test-secret-ckp9" in namespace "subpath-2229"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:01:53.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2229" for this suite.
Oct  4 11:01:59.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:01:59.183: INFO: namespace subpath-2229 deletion completed in 6.150259806s

• [SLOW TEST:30.323 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:01:59.183: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 11:01:59.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-64'
Oct  4 11:01:59.309: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct  4 11:01:59.309: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Oct  4 11:02:01.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete deployment e2e-test-nginx-deployment --namespace=kubectl-64'
Oct  4 11:02:01.442: INFO: stderr: ""
Oct  4 11:02:01.442: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:02:01.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-64" for this suite.
Oct  4 11:02:23.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:02:23.597: INFO: namespace kubectl-64 deletion completed in 22.149856438s

• [SLOW TEST:24.414 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:02:23.597: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 11:02:23.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5529'
Oct  4 11:02:23.735: INFO: stderr: ""
Oct  4 11:02:23.735: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Oct  4 11:02:23.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete pods e2e-test-nginx-pod --namespace=kubectl-5529'
Oct  4 11:02:32.066: INFO: stderr: ""
Oct  4 11:02:32.066: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:02:32.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5529" for this suite.
Oct  4 11:02:38.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:02:38.219: INFO: namespace kubectl-5529 deletion completed in 6.147776854s

• [SLOW TEST:14.622 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:02:38.220: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-97db3dc0-af83-4832-a159-343ede8736dc
STEP: Creating a pod to test consume configMaps
Oct  4 11:02:38.276: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2" in namespace "projected-6036" to be "success or failure"
Oct  4 11:02:38.283: INFO: Pod "pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.934098ms
Oct  4 11:02:40.289: INFO: Pod "pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012714641s
STEP: Saw pod success
Oct  4 11:02:40.289: INFO: Pod "pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2" satisfied condition "success or failure"
Oct  4 11:02:40.293: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:02:40.319: INFO: Waiting for pod pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2 to disappear
Oct  4 11:02:40.323: INFO: Pod pod-projected-configmaps-368e1af7-dacc-45da-ba9e-22dcaf01cee2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:02:40.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6036" for this suite.
Oct  4 11:02:46.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:02:46.477: INFO: namespace projected-6036 deletion completed in 6.148115368s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:02:46.477: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:02:46.510: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:02:48.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4905" for this suite.
Oct  4 11:03:34.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:03:34.846: INFO: namespace pods-4905 deletion completed in 46.144513065s

• [SLOW TEST:48.369 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:03:34.846: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:03:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1604" for this suite.
Oct  4 11:03:46.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:03:46.653: INFO: namespace watch-1604 deletion completed in 6.248218958s

• [SLOW TEST:11.806 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:03:46.653: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Oct  4 11:03:46.698: INFO: Waiting up to 5m0s for pod "pod-b76406c2-a82c-43b8-bdbc-a0f82166b114" in namespace "emptydir-3193" to be "success or failure"
Oct  4 11:03:46.704: INFO: Pod "pod-b76406c2-a82c-43b8-bdbc-a0f82166b114": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924416ms
Oct  4 11:03:48.710: INFO: Pod "pod-b76406c2-a82c-43b8-bdbc-a0f82166b114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011385269s
Oct  4 11:03:50.715: INFO: Pod "pod-b76406c2-a82c-43b8-bdbc-a0f82166b114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016518419s
STEP: Saw pod success
Oct  4 11:03:50.715: INFO: Pod "pod-b76406c2-a82c-43b8-bdbc-a0f82166b114" satisfied condition "success or failure"
Oct  4 11:03:50.719: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-b76406c2-a82c-43b8-bdbc-a0f82166b114 container test-container: <nil>
STEP: delete the pod
Oct  4 11:03:50.751: INFO: Waiting for pod pod-b76406c2-a82c-43b8-bdbc-a0f82166b114 to disappear
Oct  4 11:03:50.754: INFO: Pod pod-b76406c2-a82c-43b8-bdbc-a0f82166b114 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:03:50.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3193" for this suite.
Oct  4 11:03:56.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:03:56.909: INFO: namespace emptydir-3193 deletion completed in 6.149205109s

• [SLOW TEST:10.256 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:03:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-23f43466-53c5-4521-8cf7-800a7c7295ed
STEP: Creating a pod to test consume configMaps
Oct  4 11:03:56.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42" in namespace "configmap-2281" to be "success or failure"
Oct  4 11:03:56.970: INFO: Pod "pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053158ms
Oct  4 11:03:58.975: INFO: Pod "pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009008501s
Oct  4 11:04:00.980: INFO: Pod "pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013899139s
STEP: Saw pod success
Oct  4 11:04:00.980: INFO: Pod "pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42" satisfied condition "success or failure"
Oct  4 11:04:00.987: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42 container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:04:01.015: INFO: Waiting for pod pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42 to disappear
Oct  4 11:04:01.019: INFO: Pod pod-configmaps-92f9a817-542b-479d-83f3-4baaf4726b42 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:04:01.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2281" for this suite.
Oct  4 11:04:07.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:04:07.175: INFO: namespace configmap-2281 deletion completed in 6.150529958s

• [SLOW TEST:10.265 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:04:07.176: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:04:07.226: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Oct  4 11:04:12.232: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct  4 11:04:12.232: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Oct  4 11:04:12.256: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-902,SelfLink:/apis/apps/v1/namespaces/deployment-902/deployments/test-cleanup-deployment,UID:5585a190-c4b2-4c42-a900-cd2274914342,ResourceVersion:13924,Generation:1,CreationTimestamp:2019-10-04 11:04:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Oct  4 11:04:12.262: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-902,SelfLink:/apis/apps/v1/namespaces/deployment-902/replicasets/test-cleanup-deployment-55bbcbc84c,UID:5a6270ef-6c18-4835-80de-6f42c58fc3c7,ResourceVersion:13926,Generation:1,CreationTimestamp:2019-10-04 11:04:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5585a190-c4b2-4c42-a900-cd2274914342 0xc002970fa7 0xc002970fa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:04:12.262: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Oct  4 11:04:12.262: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-902,SelfLink:/apis/apps/v1/namespaces/deployment-902/replicasets/test-cleanup-controller,UID:ba81043d-9a5b-4855-aa39-2004aa54d12e,ResourceVersion:13925,Generation:1,CreationTimestamp:2019-10-04 11:04:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5585a190-c4b2-4c42-a900-cd2274914342 0xc002970ed7 0xc002970ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Oct  4 11:04:12.278: INFO: Pod "test-cleanup-controller-jmqsw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-jmqsw,GenerateName:test-cleanup-controller-,Namespace:deployment-902,SelfLink:/api/v1/namespaces/deployment-902/pods/test-cleanup-controller-jmqsw,UID:03382358-7e75-4470-bc6b-99270610c1ad,ResourceVersion:13908,Generation:0,CreationTimestamp:2019-10-04 11:04:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller ba81043d-9a5b-4855-aa39-2004aa54d12e 0xc002971977 0xc002971978}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sbdgs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sbdgs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sbdgs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029719f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002971a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:04:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:04:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:04:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:04:07 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:10.233.95.66,StartTime:2019-10-04 11:04:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:04:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ba8eeef115a0bafdb584344f20b00fe9a5b3e5eb95748f17238a98626ae41edc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:04:12.278: INFO: Pod "test-cleanup-deployment-55bbcbc84c-kmqp5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-kmqp5,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-902,SelfLink:/api/v1/namespaces/deployment-902/pods/test-cleanup-deployment-55bbcbc84c-kmqp5,UID:ae0c62bf-cfb0-4f3d-ab81-adb8ed51f12f,ResourceVersion:13928,Generation:0,CreationTimestamp:2019-10-04 11:04:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 5a6270ef-6c18-4835-80de-6f42c58fc3c7 0xc002971ae7 0xc002971ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sbdgs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sbdgs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sbdgs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002971b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002971b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:04:12.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-902" for this suite.
Oct  4 11:04:18.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:04:18.439: INFO: namespace deployment-902 deletion completed in 6.143154029s

• [SLOW TEST:11.263 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:04:18.440: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct  4 11:04:28.562: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:28.566: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:30.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:30.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:32.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:32.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:34.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:34.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:36.567: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:36.573: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:38.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:38.572: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:40.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:40.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:42.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:42.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:44.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:44.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:46.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:46.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:48.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:48.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:50.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:50.571: INFO: Pod pod-with-poststart-exec-hook still exists
Oct  4 11:04:52.566: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Oct  4 11:04:52.571: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:04:52.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9198" for this suite.
Oct  4 11:05:14.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:05:14.726: INFO: namespace container-lifecycle-hook-9198 deletion completed in 22.148514562s

• [SLOW TEST:56.286 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:05:14.726: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Oct  4 11:05:14.774: INFO: Waiting up to 5m0s for pod "client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b" in namespace "containers-3305" to be "success or failure"
Oct  4 11:05:14.781: INFO: Pod "client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.797892ms
Oct  4 11:05:16.785: INFO: Pod "client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011462333s
STEP: Saw pod success
Oct  4 11:05:16.786: INFO: Pod "client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b" satisfied condition "success or failure"
Oct  4 11:05:16.790: INFO: Trying to get logs from node lab1-k8s-node-1 pod client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b container test-container: <nil>
STEP: delete the pod
Oct  4 11:05:16.814: INFO: Waiting for pod client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b to disappear
Oct  4 11:05:16.818: INFO: Pod client-containers-10defd5a-e6a9-4345-9f99-7c4bf89e680b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:05:16.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3305" for this suite.
Oct  4 11:05:22.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:05:22.968: INFO: namespace containers-3305 deletion completed in 6.143989759s

• [SLOW TEST:8.242 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:05:22.970: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Oct  4 11:05:23.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-3147'
Oct  4 11:05:23.281: INFO: stderr: ""
Oct  4 11:05:23.281: INFO: stdout: "pod/pause created\n"
Oct  4 11:05:23.281: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Oct  4 11:05:23.281: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3147" to be "running and ready"
Oct  4 11:05:23.285: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.872981ms
Oct  4 11:05:25.290: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009005858s
Oct  4 11:05:25.290: INFO: Pod "pause" satisfied condition "running and ready"
Oct  4 11:05:25.290: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Oct  4 11:05:25.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 label pods pause testing-label=testing-label-value --namespace=kubectl-3147'
Oct  4 11:05:25.386: INFO: stderr: ""
Oct  4 11:05:25.386: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Oct  4 11:05:25.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pod pause -L testing-label --namespace=kubectl-3147'
Oct  4 11:05:25.464: INFO: stderr: ""
Oct  4 11:05:25.464: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Oct  4 11:05:25.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 label pods pause testing-label- --namespace=kubectl-3147'
Oct  4 11:05:25.545: INFO: stderr: ""
Oct  4 11:05:25.545: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Oct  4 11:05:25.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pod pause -L testing-label --namespace=kubectl-3147'
Oct  4 11:05:25.621: INFO: stderr: ""
Oct  4 11:05:25.621: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Oct  4 11:05:25.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-3147'
Oct  4 11:05:25.716: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:05:25.716: INFO: stdout: "pod \"pause\" force deleted\n"
Oct  4 11:05:25.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get rc,svc -l name=pause --no-headers --namespace=kubectl-3147'
Oct  4 11:05:25.817: INFO: stderr: "No resources found.\n"
Oct  4 11:05:25.817: INFO: stdout: ""
Oct  4 11:05:25.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -l name=pause --namespace=kubectl-3147 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct  4 11:05:25.893: INFO: stderr: ""
Oct  4 11:05:25.893: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:05:25.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3147" for this suite.
Oct  4 11:05:31.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:05:32.059: INFO: namespace kubectl-3147 deletion completed in 6.160480976s

• [SLOW TEST:9.090 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:05:32.060: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Oct  4 11:05:32.115: INFO: Pod name pod-release: Found 0 pods out of 1
Oct  4 11:05:37.121: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:05:38.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4172" for this suite.
Oct  4 11:05:44.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:05:44.294: INFO: namespace replication-controller-4172 deletion completed in 6.146189437s

• [SLOW TEST:12.234 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:05:44.294: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:05:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1715" for this suite.
Oct  4 11:06:09.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:06:09.528: INFO: namespace replication-controller-1715 deletion completed in 22.146335978s

• [SLOW TEST:25.234 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:06:09.529: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Oct  4 11:06:13.624: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct  4 11:06:13.628: INFO: Pod pod-with-prestop-http-hook still exists
Oct  4 11:06:15.628: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct  4 11:06:15.634: INFO: Pod pod-with-prestop-http-hook still exists
Oct  4 11:06:17.628: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct  4 11:06:17.634: INFO: Pod pod-with-prestop-http-hook still exists
Oct  4 11:06:19.628: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct  4 11:06:19.633: INFO: Pod pod-with-prestop-http-hook still exists
Oct  4 11:06:21.628: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct  4 11:06:21.633: INFO: Pod pod-with-prestop-http-hook still exists
Oct  4 11:06:23.628: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Oct  4 11:06:23.633: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:06:23.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6340" for this suite.
Oct  4 11:06:45.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:06:45.798: INFO: namespace container-lifecycle-hook-6340 deletion completed in 22.148487496s

• [SLOW TEST:36.270 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:06:45.798: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1004 11:06:46.438784      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct  4 11:06:46.438: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:06:46.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3263" for this suite.
Oct  4 11:06:52.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:06:52.590: INFO: namespace gc-3263 deletion completed in 6.147369089s

• [SLOW TEST:6.792 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:06:52.590: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-5552a015-5359-4b7c-8bfa-2bbf4f9b2b8f
STEP: Creating a pod to test consume secrets
Oct  4 11:06:52.641: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268" in namespace "projected-7840" to be "success or failure"
Oct  4 11:06:52.647: INFO: Pod "pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268": Phase="Pending", Reason="", readiness=false. Elapsed: 6.468227ms
Oct  4 11:06:54.652: INFO: Pod "pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011245028s
Oct  4 11:06:56.657: INFO: Pod "pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016119829s
STEP: Saw pod success
Oct  4 11:06:56.657: INFO: Pod "pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268" satisfied condition "success or failure"
Oct  4 11:06:56.669: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:06:56.695: INFO: Waiting for pod pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268 to disappear
Oct  4 11:06:56.699: INFO: Pod pod-projected-secrets-80125257-5de4-455d-8dcd-5b660ada8268 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:06:56.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7840" for this suite.
Oct  4 11:07:02.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:07:02.849: INFO: namespace projected-7840 deletion completed in 6.145520275s

• [SLOW TEST:10.259 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:07:02.850: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Oct  4 11:07:02.913: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14774,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct  4 11:07:02.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14774,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Oct  4 11:07:12.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14799,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Oct  4 11:07:12.928: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14799,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Oct  4 11:07:22.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14824,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct  4 11:07:22.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14824,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Oct  4 11:07:32.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14850,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct  4 11:07:32.950: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-a,UID:83a157b0-16f2-40a5-ba41-9d78f46a10bc,ResourceVersion:14850,Generation:0,CreationTimestamp:2019-10-04 11:07:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Oct  4 11:07:42.959: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-b,UID:618b76d3-65ac-4e45-bb2c-f5dc1fba63f2,ResourceVersion:14877,Generation:0,CreationTimestamp:2019-10-04 11:07:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct  4 11:07:42.959: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-b,UID:618b76d3-65ac-4e45-bb2c-f5dc1fba63f2,ResourceVersion:14877,Generation:0,CreationTimestamp:2019-10-04 11:07:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Oct  4 11:07:52.971: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-b,UID:618b76d3-65ac-4e45-bb2c-f5dc1fba63f2,ResourceVersion:14903,Generation:0,CreationTimestamp:2019-10-04 11:07:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct  4 11:07:52.971: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5055,SelfLink:/api/v1/namespaces/watch-5055/configmaps/e2e-watch-test-configmap-b,UID:618b76d3-65ac-4e45-bb2c-f5dc1fba63f2,ResourceVersion:14903,Generation:0,CreationTimestamp:2019-10-04 11:07:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:08:02.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5055" for this suite.
Oct  4 11:08:08.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:08:09.123: INFO: namespace watch-5055 deletion completed in 6.145327581s

• [SLOW TEST:66.273 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:08:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:08:09.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a" in namespace "projected-3549" to be "success or failure"
Oct  4 11:08:09.179: INFO: Pod "downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.41339ms
Oct  4 11:08:11.185: INFO: Pod "downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a": Phase="Running", Reason="", readiness=true. Elapsed: 2.012825711s
Oct  4 11:08:13.190: INFO: Pod "downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018098615s
STEP: Saw pod success
Oct  4 11:08:13.190: INFO: Pod "downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a" satisfied condition "success or failure"
Oct  4 11:08:13.195: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a container client-container: <nil>
STEP: delete the pod
Oct  4 11:08:13.222: INFO: Waiting for pod downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a to disappear
Oct  4 11:08:13.226: INFO: Pod downwardapi-volume-b189fa20-09fa-4da6-9439-087de83e796a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:08:13.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3549" for this suite.
Oct  4 11:08:19.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:08:19.383: INFO: namespace projected-3549 deletion completed in 6.151819174s

• [SLOW TEST:10.260 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:08:19.383: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Oct  4 11:08:19.413: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:08:22.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6971" for this suite.
Oct  4 11:08:28.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:08:28.849: INFO: namespace init-container-6971 deletion completed in 6.152565708s

• [SLOW TEST:9.465 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:08:28.849: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Oct  4 11:08:28.886: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:08:33.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8692" for this suite.
Oct  4 11:08:55.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:08:55.199: INFO: namespace init-container-8692 deletion completed in 22.144381211s

• [SLOW TEST:26.350 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:08:55.199: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:08:55.247: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e" in namespace "downward-api-5947" to be "success or failure"
Oct  4 11:08:55.252: INFO: Pod "downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.373857ms
Oct  4 11:08:57.258: INFO: Pod "downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010885011s
STEP: Saw pod success
Oct  4 11:08:57.258: INFO: Pod "downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e" satisfied condition "success or failure"
Oct  4 11:08:57.262: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e container client-container: <nil>
STEP: delete the pod
Oct  4 11:08:57.293: INFO: Waiting for pod downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e to disappear
Oct  4 11:08:57.299: INFO: Pod downwardapi-volume-ee6755de-3eaf-4b9f-8b70-ebd268f89f5e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:08:57.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5947" for this suite.
Oct  4 11:09:03.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:09:03.445: INFO: namespace downward-api-5947 deletion completed in 6.140257314s

• [SLOW TEST:8.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:09:03.449: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-54333d5d-fcf9-4b0e-96d8-943028520e16
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:09:11.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5121" for this suite.
Oct  4 11:09:33.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:09:33.724: INFO: namespace configmap-5121 deletion completed in 22.148693298s

• [SLOW TEST:30.276 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:09:33.726: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:09:33.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045" in namespace "downward-api-8658" to be "success or failure"
Oct  4 11:09:33.784: INFO: Pod "downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045": Phase="Pending", Reason="", readiness=false. Elapsed: 7.195086ms
Oct  4 11:09:35.788: INFO: Pod "downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011812037s
STEP: Saw pod success
Oct  4 11:09:35.788: INFO: Pod "downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045" satisfied condition "success or failure"
Oct  4 11:09:35.793: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045 container client-container: <nil>
STEP: delete the pod
Oct  4 11:09:35.821: INFO: Waiting for pod downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045 to disappear
Oct  4 11:09:35.825: INFO: Pod downwardapi-volume-966b84b7-f63e-41d1-bd91-875fea574045 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:09:35.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8658" for this suite.
Oct  4 11:09:41.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:09:41.981: INFO: namespace downward-api-8658 deletion completed in 6.151446228s

• [SLOW TEST:8.255 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:09:41.984: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 11:09:42.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5748'
Oct  4 11:09:42.127: INFO: stderr: ""
Oct  4 11:09:42.127: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Oct  4 11:09:47.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pod e2e-test-nginx-pod --namespace=kubectl-5748 -o json'
Oct  4 11:09:47.259: INFO: stderr: ""
Oct  4 11:09:47.259: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-10-04T11:09:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5748\",\n        \"resourceVersion\": \"15438\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5748/pods/e2e-test-nginx-pod\",\n        \"uid\": \"909949bd-508d-4bf8-9e6e-e89d6114f054\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-mgxks\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"lab1-k8s-node-1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-mgxks\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-mgxks\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-10-04T11:09:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-10-04T11:09:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-10-04T11:09:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-10-04T11:09:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://1b8b5cbd52ec92e5e972733a42b2da453a2799a2a34c460b02d65c78af2a37ca\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-10-04T11:09:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.95.80\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-10-04T11:09:42Z\"\n    }\n}\n"
STEP: replace the image in the pod
Oct  4 11:09:47.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 replace -f - --namespace=kubectl-5748'
Oct  4 11:09:47.418: INFO: stderr: ""
Oct  4 11:09:47.418: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Oct  4 11:09:47.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete pods e2e-test-nginx-pod --namespace=kubectl-5748'
Oct  4 11:09:52.068: INFO: stderr: ""
Oct  4 11:09:52.068: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:09:52.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5748" for this suite.
Oct  4 11:09:58.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:09:58.221: INFO: namespace kubectl-5748 deletion completed in 6.147294175s

• [SLOW TEST:16.237 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:09:58.221: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Oct  4 11:10:02.329: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct  4 11:10:02.335: INFO: Pod pod-with-poststart-http-hook still exists
Oct  4 11:10:04.335: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct  4 11:10:04.340: INFO: Pod pod-with-poststart-http-hook still exists
Oct  4 11:10:06.335: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Oct  4 11:10:06.339: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:10:06.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4879" for this suite.
Oct  4 11:10:28.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:10:28.488: INFO: namespace container-lifecycle-hook-4879 deletion completed in 22.144047432s

• [SLOW TEST:30.268 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:10:28.489: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9984
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Oct  4 11:10:28.555: INFO: Found 0 stateful pods, waiting for 3
Oct  4 11:10:38.561: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:10:38.561: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:10:38.561: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Oct  4 11:10:38.593: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Oct  4 11:10:48.635: INFO: Updating stateful set ss2
Oct  4 11:10:48.647: INFO: Waiting for Pod statefulset-9984/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Oct  4 11:10:58.656: INFO: Waiting for Pod statefulset-9984/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Oct  4 11:11:08.723: INFO: Found 2 stateful pods, waiting for 3
Oct  4 11:11:18.729: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:11:18.729: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:11:18.729: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Oct  4 11:11:18.757: INFO: Updating stateful set ss2
Oct  4 11:11:18.765: INFO: Waiting for Pod statefulset-9984/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Oct  4 11:11:28.794: INFO: Updating stateful set ss2
Oct  4 11:11:28.801: INFO: Waiting for StatefulSet statefulset-9984/ss2 to complete update
Oct  4 11:11:28.801: INFO: Waiting for Pod statefulset-9984/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Oct  4 11:11:38.810: INFO: Waiting for StatefulSet statefulset-9984/ss2 to complete update
Oct  4 11:11:38.810: INFO: Waiting for Pod statefulset-9984/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Oct  4 11:11:48.810: INFO: Deleting all statefulset in ns statefulset-9984
Oct  4 11:11:48.814: INFO: Scaling statefulset ss2 to 0
Oct  4 11:12:08.835: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:12:08.839: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:12:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9984" for this suite.
Oct  4 11:12:14.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:12:15.014: INFO: namespace statefulset-9984 deletion completed in 6.153632199s

• [SLOW TEST:106.525 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:12:15.014: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Oct  4 11:12:15.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-7020'
Oct  4 11:12:15.256: INFO: stderr: ""
Oct  4 11:12:15.256: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Oct  4 11:12:16.261: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:12:16.261: INFO: Found 0 / 1
Oct  4 11:12:17.261: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:12:17.261: INFO: Found 0 / 1
Oct  4 11:12:18.261: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:12:18.261: INFO: Found 1 / 1
Oct  4 11:12:18.261: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Oct  4 11:12:18.265: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:12:18.266: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct  4 11:12:18.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 patch pod redis-master-5xm5x --namespace=kubectl-7020 -p {"metadata":{"annotations":{"x":"y"}}}'
Oct  4 11:12:18.350: INFO: stderr: ""
Oct  4 11:12:18.350: INFO: stdout: "pod/redis-master-5xm5x patched\n"
STEP: checking annotations
Oct  4 11:12:18.354: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:12:18.354: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:12:18.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7020" for this suite.
Oct  4 11:12:40.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:12:40.507: INFO: namespace kubectl-7020 deletion completed in 22.146966944s

• [SLOW TEST:25.493 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:12:40.508: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4543
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Oct  4 11:12:40.571: INFO: Found 0 stateful pods, waiting for 3
Oct  4 11:12:50.577: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:12:50.577: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:12:50.577: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:12:50.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-4543 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:12:50.849: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:12:50.849: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:12:50.849: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Oct  4 11:13:00.885: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Oct  4 11:13:10.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-4543 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:13:11.187: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:13:11.187: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:13:11.187: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:13:21.222: INFO: Waiting for StatefulSet statefulset-4543/ss2 to complete update
Oct  4 11:13:21.222: INFO: Waiting for Pod statefulset-4543/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Oct  4 11:13:21.222: INFO: Waiting for Pod statefulset-4543/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Oct  4 11:13:21.222: INFO: Waiting for Pod statefulset-4543/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Oct  4 11:13:31.233: INFO: Waiting for StatefulSet statefulset-4543/ss2 to complete update
Oct  4 11:13:31.233: INFO: Waiting for Pod statefulset-4543/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Oct  4 11:13:41.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-4543 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:13:41.515: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:13:41.515: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:13:41.515: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:13:41.551: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Oct  4 11:13:51.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-4543 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:13:51.830: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:13:51.830: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:13:51.830: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:14:01.857: INFO: Waiting for StatefulSet statefulset-4543/ss2 to complete update
Oct  4 11:14:01.857: INFO: Waiting for Pod statefulset-4543/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Oct  4 11:14:01.857: INFO: Waiting for Pod statefulset-4543/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Oct  4 11:14:01.857: INFO: Waiting for Pod statefulset-4543/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Oct  4 11:14:11.866: INFO: Waiting for StatefulSet statefulset-4543/ss2 to complete update
Oct  4 11:14:11.866: INFO: Waiting for Pod statefulset-4543/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Oct  4 11:14:21.872: INFO: Waiting for StatefulSet statefulset-4543/ss2 to complete update
Oct  4 11:14:21.872: INFO: Waiting for Pod statefulset-4543/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Oct  4 11:14:31.866: INFO: Deleting all statefulset in ns statefulset-4543
Oct  4 11:14:31.870: INFO: Scaling statefulset ss2 to 0
Oct  4 11:15:11.900: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:15:11.906: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:15:11.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4543" for this suite.
Oct  4 11:15:17.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:15:18.083: INFO: namespace statefulset-4543 deletion completed in 6.154948162s

• [SLOW TEST:157.576 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:15:18.085: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:15:18.153: INFO: Create a RollingUpdate DaemonSet
Oct  4 11:15:18.161: INFO: Check that daemon pods launch on every node of the cluster
Oct  4 11:15:18.170: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:18.170: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:18.170: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:18.174: INFO: Number of nodes with available pods: 0
Oct  4 11:15:18.174: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:15:19.184: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:19.184: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:19.184: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:19.189: INFO: Number of nodes with available pods: 0
Oct  4 11:15:19.189: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:15:20.182: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:20.182: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:20.182: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:20.186: INFO: Number of nodes with available pods: 2
Oct  4 11:15:20.186: INFO: Node lab1-k8s-node-3 is running more than one daemon pod
Oct  4 11:15:21.182: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:21.182: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:21.182: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:21.187: INFO: Number of nodes with available pods: 3
Oct  4 11:15:21.187: INFO: Number of running nodes: 3, number of available pods: 3
Oct  4 11:15:21.187: INFO: Update the DaemonSet to trigger a rollout
Oct  4 11:15:21.199: INFO: Updating DaemonSet daemon-set
Oct  4 11:15:32.221: INFO: Roll back the DaemonSet before rollout is complete
Oct  4 11:15:32.230: INFO: Updating DaemonSet daemon-set
Oct  4 11:15:32.230: INFO: Make sure DaemonSet rollback is complete
Oct  4 11:15:32.236: INFO: Wrong image for pod: daemon-set-c2fpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Oct  4 11:15:32.236: INFO: Pod daemon-set-c2fpz is not available
Oct  4 11:15:32.246: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:32.246: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:32.246: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:33.251: INFO: Wrong image for pod: daemon-set-c2fpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Oct  4 11:15:33.251: INFO: Pod daemon-set-c2fpz is not available
Oct  4 11:15:33.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:33.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:33.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:34.251: INFO: Wrong image for pod: daemon-set-c2fpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Oct  4 11:15:34.251: INFO: Pod daemon-set-c2fpz is not available
Oct  4 11:15:34.256: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:34.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:34.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:35.252: INFO: Wrong image for pod: daemon-set-c2fpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Oct  4 11:15:35.252: INFO: Pod daemon-set-c2fpz is not available
Oct  4 11:15:35.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:35.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:35.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:36.251: INFO: Wrong image for pod: daemon-set-c2fpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Oct  4 11:15:36.251: INFO: Pod daemon-set-c2fpz is not available
Oct  4 11:15:36.256: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:36.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:36.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:37.253: INFO: Wrong image for pod: daemon-set-c2fpz. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Oct  4 11:15:37.253: INFO: Pod daemon-set-c2fpz is not available
Oct  4 11:15:37.258: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:37.258: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:37.258: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:38.251: INFO: Pod daemon-set-gqw2f is not available
Oct  4 11:15:38.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:38.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:15:38.257: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6615, will wait for the garbage collector to delete the pods
Oct  4 11:15:38.336: INFO: Deleting DaemonSet.extensions daemon-set took: 10.904979ms
Oct  4 11:15:38.636: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.173762ms
Oct  4 11:17:02.141: INFO: Number of nodes with available pods: 0
Oct  4 11:17:02.141: INFO: Number of running nodes: 0, number of available pods: 0
Oct  4 11:17:02.147: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6615/daemonsets","resourceVersion":"17495"},"items":null}

Oct  4 11:17:02.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6615/pods","resourceVersion":"17495"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:17:02.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6615" for this suite.
Oct  4 11:17:08.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:17:08.318: INFO: namespace daemonsets-6615 deletion completed in 6.146508254s

• [SLOW TEST:110.233 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:17:08.319: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:17:10.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8753" for this suite.
Oct  4 11:17:52.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:17:52.552: INFO: namespace kubelet-test-8753 deletion completed in 42.144676412s

• [SLOW TEST:44.233 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:17:52.552: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:17:52.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b" in namespace "projected-6590" to be "success or failure"
Oct  4 11:17:52.606: INFO: Pod "downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282667ms
Oct  4 11:17:54.614: INFO: Pod "downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01474297s
STEP: Saw pod success
Oct  4 11:17:54.614: INFO: Pod "downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b" satisfied condition "success or failure"
Oct  4 11:17:54.618: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b container client-container: <nil>
STEP: delete the pod
Oct  4 11:17:54.648: INFO: Waiting for pod downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b to disappear
Oct  4 11:17:54.651: INFO: Pod downwardapi-volume-db1e54a8-4d90-4775-98e4-095c555e143b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:17:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6590" for this suite.
Oct  4 11:18:00.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:18:00.821: INFO: namespace projected-6590 deletion completed in 6.164881225s

• [SLOW TEST:8.269 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:18:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Oct  4 11:18:00.868: INFO: Waiting up to 5m0s for pod "downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d" in namespace "downward-api-9569" to be "success or failure"
Oct  4 11:18:00.876: INFO: Pod "downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.482534ms
Oct  4 11:18:02.881: INFO: Pod "downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012519397s
STEP: Saw pod success
Oct  4 11:18:02.881: INFO: Pod "downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d" satisfied condition "success or failure"
Oct  4 11:18:02.885: INFO: Trying to get logs from node lab1-k8s-node-3 pod downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d container dapi-container: <nil>
STEP: delete the pod
Oct  4 11:18:02.916: INFO: Waiting for pod downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d to disappear
Oct  4 11:18:02.923: INFO: Pod downward-api-64854f44-9e93-429d-8e7b-018a42e5d98d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:18:02.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9569" for this suite.
Oct  4 11:18:08.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:18:09.085: INFO: namespace downward-api-9569 deletion completed in 6.154438797s

• [SLOW TEST:8.264 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:18:09.086: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Oct  4 11:18:11.152: INFO: Pod pod-hostip-00244e63-83da-485f-80f8-dfe8c64335cb has hostIP: 10.128.0.18
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:18:11.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9537" for this suite.
Oct  4 11:18:33.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:18:33.309: INFO: namespace pods-9537 deletion completed in 22.150926041s

• [SLOW TEST:24.224 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:18:33.310: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6458
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6458
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6458
Oct  4 11:18:33.365: INFO: Found 0 stateful pods, waiting for 1
Oct  4 11:18:43.370: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Oct  4 11:18:43.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:18:43.671: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:18:43.671: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:18:43.671: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:18:43.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct  4 11:18:53.681: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:18:53.681: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:18:53.699: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:18:53.699: INFO: ss-0  lab1-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:18:53.699: INFO: ss-1                   Pending         []
Oct  4 11:18:53.699: INFO: 
Oct  4 11:18:53.699: INFO: StatefulSet ss has not reached scale 3, at 2
Oct  4 11:18:54.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994627782s
Oct  4 11:18:55.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989887845s
Oct  4 11:18:56.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983706057s
Oct  4 11:18:57.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975446927s
Oct  4 11:18:58.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970139883s
Oct  4 11:18:59.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964888409s
Oct  4 11:19:00.739: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959444028s
Oct  4 11:19:01.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954683159s
Oct  4 11:19:02.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.681404ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6458
Oct  4 11:19:03.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:19:04.011: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:19:04.011: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:19:04.011: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:19:04.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:19:04.287: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct  4 11:19:04.287: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:19:04.287: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:19:04.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:19:04.567: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Oct  4 11:19:04.567: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:19:04.567: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:19:04.573: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:19:04.573: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:19:04.573: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Oct  4 11:19:04.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:19:04.816: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:19:04.816: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:19:04.816: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:19:04.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:19:05.086: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:19:05.086: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:19:05.086: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:19:05.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-6458 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:19:05.374: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:19:05.374: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:19:05.374: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:19:05.374: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:19:05.378: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Oct  4 11:19:15.392: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:19:15.392: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:19:15.392: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:19:15.405: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:15.405: INFO: ss-0  lab1-k8s-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:15.405: INFO: ss-1  lab1-k8s-node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:15.405: INFO: ss-2  lab1-k8s-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:15.406: INFO: 
Oct  4 11:19:15.406: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:16.411: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:16.411: INFO: ss-0  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:16.411: INFO: ss-1  lab1-k8s-node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:16.411: INFO: ss-2  lab1-k8s-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:16.411: INFO: 
Oct  4 11:19:16.411: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:17.417: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:17.417: INFO: ss-0  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:17.417: INFO: ss-1  lab1-k8s-node-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:17.417: INFO: ss-2  lab1-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:17.417: INFO: 
Oct  4 11:19:17.417: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:18.422: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:18.422: INFO: ss-0  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:18.422: INFO: ss-1  lab1-k8s-node-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:18.423: INFO: ss-2  lab1-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:18.423: INFO: 
Oct  4 11:19:18.423: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:19.428: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:19.429: INFO: ss-0  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:19.429: INFO: ss-1  lab1-k8s-node-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:19.429: INFO: ss-2  lab1-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:19.429: INFO: 
Oct  4 11:19:19.429: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:20.435: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:20.435: INFO: ss-0  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:20.435: INFO: ss-1  lab1-k8s-node-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:20.436: INFO: ss-2  lab1-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:20.436: INFO: 
Oct  4 11:19:20.436: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:21.440: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
Oct  4 11:19:21.440: INFO: ss-0  lab1-k8s-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:33 +0000 UTC  }]
Oct  4 11:19:21.441: INFO: ss-1  lab1-k8s-node-3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:21.441: INFO: ss-2  lab1-k8s-node-2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:18:53 +0000 UTC  }]
Oct  4 11:19:21.441: INFO: 
Oct  4 11:19:21.441: INFO: StatefulSet ss has not reached scale 0, at 3
Oct  4 11:19:22.446: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.959823092s
Oct  4 11:19:23.451: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.954475359s
Oct  4 11:19:24.456: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.474036ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6458
Oct  4 11:19:25.461: INFO: Scaling statefulset ss to 0
Oct  4 11:19:25.474: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Oct  4 11:19:25.477: INFO: Deleting all statefulset in ns statefulset-6458
Oct  4 11:19:25.482: INFO: Scaling statefulset ss to 0
Oct  4 11:19:25.493: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:19:25.496: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:19:25.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6458" for this suite.
Oct  4 11:19:31.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:19:31.671: INFO: namespace statefulset-6458 deletion completed in 6.151250942s

• [SLOW TEST:58.362 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:19:31.672: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:19:31.706: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:19:37.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5363" for this suite.
Oct  4 11:19:43.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:19:43.987: INFO: namespace custom-resource-definition-5363 deletion completed in 6.153160462s

• [SLOW TEST:12.315 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:19:43.988: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-2cbcc406-24ab-4736-a8e6-af51e134d585
STEP: Creating a pod to test consume configMaps
Oct  4 11:19:44.038: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3" in namespace "projected-2653" to be "success or failure"
Oct  4 11:19:44.043: INFO: Pod "pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.627001ms
Oct  4 11:19:46.048: INFO: Pod "pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009134379s
STEP: Saw pod success
Oct  4 11:19:46.048: INFO: Pod "pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3" satisfied condition "success or failure"
Oct  4 11:19:46.052: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:19:46.080: INFO: Waiting for pod pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3 to disappear
Oct  4 11:19:46.083: INFO: Pod pod-projected-configmaps-3ea202c3-cad8-4000-bb50-78b53f2d59e3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:19:46.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2653" for this suite.
Oct  4 11:19:52.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:19:52.241: INFO: namespace projected-2653 deletion completed in 6.153206707s

• [SLOW TEST:8.253 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:19:52.242: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:19:52.277: INFO: Creating deployment "nginx-deployment"
Oct  4 11:19:52.286: INFO: Waiting for observed generation 1
Oct  4 11:19:54.303: INFO: Waiting for all required pods to come up
Oct  4 11:19:54.312: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Oct  4 11:19:56.333: INFO: Waiting for deployment "nginx-deployment" to complete
Oct  4 11:19:56.342: INFO: Updating deployment "nginx-deployment" with a non-existent image
Oct  4 11:19:56.352: INFO: Updating deployment nginx-deployment
Oct  4 11:19:56.352: INFO: Waiting for observed generation 2
Oct  4 11:19:58.363: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Oct  4 11:19:58.367: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Oct  4 11:19:58.371: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Oct  4 11:19:58.382: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Oct  4 11:19:58.382: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Oct  4 11:19:58.386: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Oct  4 11:19:58.393: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Oct  4 11:19:58.393: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Oct  4 11:19:58.402: INFO: Updating deployment nginx-deployment
Oct  4 11:19:58.402: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Oct  4 11:19:58.413: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Oct  4 11:19:58.418: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Oct  4 11:19:58.435: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-4880,SelfLink:/apis/apps/v1/namespaces/deployment-4880/deployments/nginx-deployment,UID:fab8dd82-0f64-483d-a500-ceff9fbffd30,ResourceVersion:18588,Generation:3,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-10-04 11:19:56 +0000 UTC 2019-10-04 11:19:52 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-10-04 11:19:58 +0000 UTC 2019-10-04 11:19:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Oct  4 11:19:58.459: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-4880,SelfLink:/apis/apps/v1/namespaces/deployment-4880/replicasets/nginx-deployment-55fb7cb77f,UID:55c1d931-93f3-4a7e-8be1-e5f39e0162dd,ResourceVersion:18583,Generation:3,CreationTimestamp:2019-10-04 11:19:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment fab8dd82-0f64-483d-a500-ceff9fbffd30 0xc0022d42a7 0xc0022d42a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:19:58.459: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Oct  4 11:19:58.459: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-4880,SelfLink:/apis/apps/v1/namespaces/deployment-4880/replicasets/nginx-deployment-7b8c6f4498,UID:49965ca6-9923-42d7-9b49-9188648839b5,ResourceVersion:18581,Generation:3,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment fab8dd82-0f64-483d-a500-ceff9fbffd30 0xc0022d4377 0xc0022d4378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Oct  4 11:19:58.481: INFO: Pod "nginx-deployment-55fb7cb77f-2c6qt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2c6qt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-2c6qt,UID:f2c228bd-2b50-469a-aa0c-11fd3d9fbed8,ResourceVersion:18608,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002d4d6b7 0xc002d4d6b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4d730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4d750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-2ngqh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2ngqh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-2ngqh,UID:5bd63866-e986-4d85-8011-6c25c43a668a,ResourceVersion:18617,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002d4d7d0 0xc002d4d7d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4d860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4d880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-6kkk6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6kkk6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-6kkk6,UID:97730b01-405a-4c12-87cd-234c444d56e0,ResourceVersion:18616,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002d4d900 0xc002d4d901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4d980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4d9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-bwbvx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bwbvx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-bwbvx,UID:6e8d9597-0e00-470e-875e-26f2922afe08,ResourceVersion:18528,Generation:0,CreationTimestamp:2019-10-04 11:19:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002d4db00 0xc002d4db01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4dbc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4dbe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-10-04 11:19:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-g7v52" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-g7v52,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-g7v52,UID:df5e2b4b-0be9-4ea0-8dd2-8cf2fa9a174e,ResourceVersion:18612,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002d4dd40 0xc002d4dd41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4ddf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4de10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-hqpcs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hqpcs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-hqpcs,UID:3f45a052-1058-481e-becf-1f6630914ca9,ResourceVersion:18551,Generation:0,CreationTimestamp:2019-10-04 11:19:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002d4de77 0xc002d4de78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d4df70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d4df90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:,StartTime:2019-10-04 11:19:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-kv92n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kv92n,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-kv92n,UID:2d22c201-bd07-46bf-a432-b447904ce92c,ResourceVersion:18614,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002c92080 0xc002c92081}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c920f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.482: INFO: Pod "nginx-deployment-55fb7cb77f-mqr4z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mqr4z,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-mqr4z,UID:c30a4373-95ce-4279-80fd-94c7a1a425c4,ResourceVersion:18525,Generation:0,CreationTimestamp:2019-10-04 11:19:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002c92177 0xc002c92178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c921f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.9,PodIP:,StartTime:2019-10-04 11:19:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.483: INFO: Pod "nginx-deployment-55fb7cb77f-pwgvc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pwgvc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-pwgvc,UID:35ad8137-c66f-4fe8-a5eb-9df8457bf120,ResourceVersion:18615,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002c922e0 0xc002c922e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.483: INFO: Pod "nginx-deployment-55fb7cb77f-rfjsg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rfjsg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-rfjsg,UID:d476f2a2-9418-458d-84a0-38061de2b8ac,ResourceVersion:18613,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002c923d7 0xc002c923d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.483: INFO: Pod "nginx-deployment-55fb7cb77f-t7fs5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-t7fs5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-t7fs5,UID:4e954493-5133-45d8-9844-1563e16e274f,ResourceVersion:18523,Generation:0,CreationTimestamp:2019-10-04 11:19:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002c924c7 0xc002c924c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:,StartTime:2019-10-04 11:19:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.483: INFO: Pod "nginx-deployment-55fb7cb77f-ttpzm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ttpzm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-55fb7cb77f-ttpzm,UID:056eabf4-767b-4fde-a9d8-d02e35af5f65,ResourceVersion:18549,Generation:0,CreationTimestamp:2019-10-04 11:19:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 55c1d931-93f3-4a7e-8be1-e5f39e0162dd 0xc002c92640 0xc002c92641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c926c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c926e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:56 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:,StartTime:2019-10-04 11:19:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.483: INFO: Pod "nginx-deployment-7b8c6f4498-4dps5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4dps5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-4dps5,UID:34869250-b8c6-4afe-a812-714823c301b4,ResourceVersion:18622,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c927b0 0xc002c927b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.483: INFO: Pod "nginx-deployment-7b8c6f4498-58vvm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-58vvm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-58vvm,UID:34f6fefc-fb42-4780-8f06-b254a0a6be64,ResourceVersion:18472,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c928e0 0xc002c928e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:10.233.95.96,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://cd381ba559db290994b7bd5faa4e21d5a85321c3bd6d065884e57327f19c9e13}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-cbrcp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cbrcp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-cbrcp,UID:13897932-91d5-483f-93a5-ce7f321c514f,ResourceVersion:18486,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c92a40 0xc002c92a41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:10.233.64.17,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://75ccf894e5dc19477205576ec47e26bbb4504ce3dce4b15fd8f20cb99b22f290}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-cvzmc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cvzmc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-cvzmc,UID:d4a591d0-2997-42e6-8d98-39d8c8cf1e02,ResourceVersion:18494,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c92bb0 0xc002c92bb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.9,PodIP:10.233.74.18,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5b323f0fd23994a37941f45f61987c836602197ba70fdf078280f3bf73e69da8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-h2ffd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-h2ffd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-h2ffd,UID:a9b0fd79-d121-4b45-a5c1-04d0259df48e,ResourceVersion:18469,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c92d10 0xc002c92d11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:10.233.95.97,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9e811c4b2e39e6595802cb571eb05ac1256020c14b654cb521b9e3b3cb81d5bb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-j9z92" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j9z92,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-j9z92,UID:1c94ef0e-4c8a-467b-add2-31173d22ca61,ResourceVersion:18492,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c92ea0 0xc002c92ea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c92f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c92f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.9,PodIP:10.233.74.17,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b17d715179a30c277eb911d02e77b24e2a229c4c4392bbf8d52febb138d51244}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-kmtnh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kmtnh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-kmtnh,UID:c10855f9-a0d6-43a4-8f9b-68974ee70b3d,ResourceVersion:18591,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c93010 0xc002c93011}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c930b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-l9b5s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l9b5s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-l9b5s,UID:54a5e6f9-06d4-4838-8a4f-abe7118c64ec,ResourceVersion:18475,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c93130 0xc002c93131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c931a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c931c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:10.233.95.95,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://209cfce84a5116b0afc12540577c0445828ff41ed9707f825ffe671ce1deb770}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.484: INFO: Pod "nginx-deployment-7b8c6f4498-lxjp7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lxjp7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-lxjp7,UID:1c610d54-f44f-4552-b635-9392ee0c0990,ResourceVersion:18607,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c93290 0xc002c93291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c93320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-m96p2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m96p2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-m96p2,UID:692130a6-73f5-4ed3-89eb-8c38a597ed20,ResourceVersion:18609,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c933a0 0xc002c933a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c93430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-qxz6s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qxz6s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-qxz6s,UID:897a8b93-c010-42e3-ad96-69556dd630db,ResourceVersion:18619,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c934b0 0xc002c934b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c93540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-svzsj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-svzsj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-svzsj,UID:2c4c388c-4fda-48c3-bbb5-5a0802b05525,ResourceVersion:18497,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c935c0 0xc002c935c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c93650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.9,PodIP:10.233.74.20,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d5ebf916a4b41005466f0ba681c78b97d93f172e72c414be5e39ccfd65d820f5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-t2kh7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t2kh7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-t2kh7,UID:db06a192-b15c-4b10-b471-d19ce4f8be50,ResourceVersion:18624,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c93720 0xc002c93721}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c937b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-tljqq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tljqq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-tljqq,UID:3dcbacd0-e0b1-4339-b44d-964d9728b0c1,ResourceVersion:18606,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c93830 0xc002c93831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c93ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c93f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-v8kcs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-v8kcs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-v8kcs,UID:59c39582-39ad-4988-8390-e694cd6cb5c9,ResourceVersion:18610,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc002c93f90 0xc002c93f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214c000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214c020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-vd6sz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vd6sz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-vd6sz,UID:aae3d312-9f0f-4b46-958b-27bf3ea53337,ResourceVersion:18623,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc00214c0a0 0xc00214c0a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214c110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214c130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-w9kcm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w9kcm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-w9kcm,UID:45d6d069-1c45-4144-b886-dc8bc80f3ba6,ResourceVersion:18618,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc00214c1b0 0xc00214c1b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214c220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214c240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.485: INFO: Pod "nginx-deployment-7b8c6f4498-x7h6n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x7h6n,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-x7h6n,UID:56994a98-4f14-447a-bc89-4da6a6a224e5,ResourceVersion:18590,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc00214c2c0 0xc00214c2c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214c330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214c350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.486: INFO: Pod "nginx-deployment-7b8c6f4498-z8f7g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z8f7g,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-z8f7g,UID:071af10b-2101-41eb-a379-952f1efaf221,ResourceVersion:18597,Generation:0,CreationTimestamp:2019-10-04 11:19:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc00214c3d0 0xc00214c3d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214c440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214c460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:58 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Oct  4 11:19:58.486: INFO: Pod "nginx-deployment-7b8c6f4498-zgg5s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zgg5s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4880,SelfLink:/api/v1/namespaces/deployment-4880/pods/nginx-deployment-7b8c6f4498-zgg5s,UID:b2e6d1e4-90ad-457b-ac7b-7cedad457491,ResourceVersion:18489,Generation:0,CreationTimestamp:2019-10-04 11:19:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 49965ca6-9923-42d7-9b49-9188648839b5 0xc00214c4e0 0xc00214c4e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-f54dv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-f54dv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-f54dv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00214c550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00214c570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:19:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.9,PodIP:10.233.74.19,StartTime:2019-10-04 11:19:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-10-04 11:19:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8f714151a95fe767a6b1321db0fe4179acbfc644a54f2a5c6b0a125ff40336e4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:19:58.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4880" for this suite.
Oct  4 11:20:06.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:20:06.707: INFO: namespace deployment-4880 deletion completed in 8.208423843s

• [SLOW TEST:14.465 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:20:06.708: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1004 11:20:16.854728      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct  4 11:20:16.854: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:20:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5466" for this suite.
Oct  4 11:20:22.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:20:23.034: INFO: namespace gc-5466 deletion completed in 6.175494646s

• [SLOW TEST:16.327 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:20:23.034: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct  4 11:20:23.083: INFO: Waiting up to 5m0s for pod "pod-db588c10-6fb1-4984-afec-6809dca41661" in namespace "emptydir-6191" to be "success or failure"
Oct  4 11:20:23.089: INFO: Pod "pod-db588c10-6fb1-4984-afec-6809dca41661": Phase="Pending", Reason="", readiness=false. Elapsed: 5.532434ms
Oct  4 11:20:25.094: INFO: Pod "pod-db588c10-6fb1-4984-afec-6809dca41661": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010808065s
Oct  4 11:20:27.099: INFO: Pod "pod-db588c10-6fb1-4984-afec-6809dca41661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015701257s
STEP: Saw pod success
Oct  4 11:20:27.099: INFO: Pod "pod-db588c10-6fb1-4984-afec-6809dca41661" satisfied condition "success or failure"
Oct  4 11:20:27.103: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-db588c10-6fb1-4984-afec-6809dca41661 container test-container: <nil>
STEP: delete the pod
Oct  4 11:20:27.130: INFO: Waiting for pod pod-db588c10-6fb1-4984-afec-6809dca41661 to disappear
Oct  4 11:20:27.134: INFO: Pod pod-db588c10-6fb1-4984-afec-6809dca41661 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:20:27.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6191" for this suite.
Oct  4 11:20:33.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:20:33.285: INFO: namespace emptydir-6191 deletion completed in 6.146123681s

• [SLOW TEST:10.250 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:20:33.287: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:20:33.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c" in namespace "projected-7475" to be "success or failure"
Oct  4 11:20:33.395: INFO: Pod "downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.27672ms
Oct  4 11:20:35.399: INFO: Pod "downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011167459s
STEP: Saw pod success
Oct  4 11:20:35.399: INFO: Pod "downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c" satisfied condition "success or failure"
Oct  4 11:20:35.407: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c container client-container: <nil>
STEP: delete the pod
Oct  4 11:20:35.434: INFO: Waiting for pod downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c to disappear
Oct  4 11:20:35.439: INFO: Pod downwardapi-volume-dbf606a6-e6a9-4b4b-9b0c-3eb28b3da69c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:20:35.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7475" for this suite.
Oct  4 11:20:41.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:20:41.606: INFO: namespace projected-7475 deletion completed in 6.161705906s

• [SLOW TEST:8.319 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:20:41.607: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Oct  4 11:20:41.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-2477'
Oct  4 11:20:41.873: INFO: stderr: ""
Oct  4 11:20:41.873: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct  4 11:20:41.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2477'
Oct  4 11:20:41.955: INFO: stderr: ""
Oct  4 11:20:41.955: INFO: stdout: "update-demo-nautilus-6grs5 update-demo-nautilus-xb2c5 "
Oct  4 11:20:41.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-6grs5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:20:42.029: INFO: stderr: ""
Oct  4 11:20:42.029: INFO: stdout: ""
Oct  4 11:20:42.029: INFO: update-demo-nautilus-6grs5 is created but not running
Oct  4 11:20:47.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2477'
Oct  4 11:20:47.113: INFO: stderr: ""
Oct  4 11:20:47.113: INFO: stdout: "update-demo-nautilus-6grs5 update-demo-nautilus-xb2c5 "
Oct  4 11:20:47.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-6grs5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:20:47.196: INFO: stderr: ""
Oct  4 11:20:47.196: INFO: stdout: "true"
Oct  4 11:20:47.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-6grs5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:20:47.288: INFO: stderr: ""
Oct  4 11:20:47.288: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 11:20:47.288: INFO: validating pod update-demo-nautilus-6grs5
Oct  4 11:20:47.296: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 11:20:47.296: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 11:20:47.296: INFO: update-demo-nautilus-6grs5 is verified up and running
Oct  4 11:20:47.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-xb2c5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:20:47.384: INFO: stderr: ""
Oct  4 11:20:47.384: INFO: stdout: "true"
Oct  4 11:20:47.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-xb2c5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:20:47.466: INFO: stderr: ""
Oct  4 11:20:47.466: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 11:20:47.466: INFO: validating pod update-demo-nautilus-xb2c5
Oct  4 11:20:47.474: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 11:20:47.474: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 11:20:47.474: INFO: update-demo-nautilus-xb2c5 is verified up and running
STEP: rolling-update to new replication controller
Oct  4 11:20:47.476: INFO: scanned /root for discovery docs: <nil>
Oct  4 11:20:47.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2477'
Oct  4 11:21:09.947: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Oct  4 11:21:09.947: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct  4 11:21:09.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2477'
Oct  4 11:21:10.032: INFO: stderr: ""
Oct  4 11:21:10.032: INFO: stdout: "update-demo-kitten-cncdg update-demo-kitten-zm77h update-demo-nautilus-6grs5 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Oct  4 11:21:15.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2477'
Oct  4 11:21:15.115: INFO: stderr: ""
Oct  4 11:21:15.115: INFO: stdout: "update-demo-kitten-cncdg update-demo-kitten-zm77h "
Oct  4 11:21:15.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-kitten-cncdg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:21:15.186: INFO: stderr: ""
Oct  4 11:21:15.186: INFO: stdout: "true"
Oct  4 11:21:15.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-kitten-cncdg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:21:15.261: INFO: stderr: ""
Oct  4 11:21:15.261: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Oct  4 11:21:15.261: INFO: validating pod update-demo-kitten-cncdg
Oct  4 11:21:15.269: INFO: got data: {
  "image": "kitten.jpg"
}

Oct  4 11:21:15.269: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Oct  4 11:21:15.269: INFO: update-demo-kitten-cncdg is verified up and running
Oct  4 11:21:15.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-kitten-zm77h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:21:15.345: INFO: stderr: ""
Oct  4 11:21:15.345: INFO: stdout: "true"
Oct  4 11:21:15.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-kitten-zm77h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2477'
Oct  4 11:21:15.435: INFO: stderr: ""
Oct  4 11:21:15.435: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Oct  4 11:21:15.435: INFO: validating pod update-demo-kitten-zm77h
Oct  4 11:21:15.443: INFO: got data: {
  "image": "kitten.jpg"
}

Oct  4 11:21:15.443: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Oct  4 11:21:15.443: INFO: update-demo-kitten-zm77h is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:21:15.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2477" for this suite.
Oct  4 11:21:37.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:21:37.592: INFO: namespace kubectl-2477 deletion completed in 22.142854684s

• [SLOW TEST:55.985 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:21:37.593: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 11:21:37.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6596'
Oct  4 11:21:37.724: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct  4 11:21:37.724: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Oct  4 11:21:37.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6596'
Oct  4 11:21:37.819: INFO: stderr: ""
Oct  4 11:21:37.819: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:21:37.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6596" for this suite.
Oct  4 11:21:59.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:21:59.979: INFO: namespace kubectl-6596 deletion completed in 22.15483239s

• [SLOW TEST:22.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:21:59.980: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:22:02.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6992" for this suite.
Oct  4 11:22:44.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:22:44.208: INFO: namespace kubelet-test-6992 deletion completed in 42.148824866s

• [SLOW TEST:44.228 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:22:44.208: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Oct  4 11:22:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Oct  4 11:22:45.280: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Oct  4 11:22:47.334: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:22:49.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:22:51.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:22:53.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705784965, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:22:56.178: INFO: Waited 827.95414ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:22:56.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5121" for this suite.
Oct  4 11:23:02.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:23:02.862: INFO: namespace aggregator-5121 deletion completed in 6.246303024s

• [SLOW TEST:18.655 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:23:02.863: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Oct  4 11:23:02.919: INFO: Waiting up to 5m0s for pod "pod-74837d05-4535-40b7-8848-b0d61b0ecbc7" in namespace "emptydir-5953" to be "success or failure"
Oct  4 11:23:02.926: INFO: Pod "pod-74837d05-4535-40b7-8848-b0d61b0ecbc7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.317052ms
Oct  4 11:23:04.931: INFO: Pod "pod-74837d05-4535-40b7-8848-b0d61b0ecbc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011038747s
STEP: Saw pod success
Oct  4 11:23:04.931: INFO: Pod "pod-74837d05-4535-40b7-8848-b0d61b0ecbc7" satisfied condition "success or failure"
Oct  4 11:23:04.935: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-74837d05-4535-40b7-8848-b0d61b0ecbc7 container test-container: <nil>
STEP: delete the pod
Oct  4 11:23:04.973: INFO: Waiting for pod pod-74837d05-4535-40b7-8848-b0d61b0ecbc7 to disappear
Oct  4 11:23:04.977: INFO: Pod pod-74837d05-4535-40b7-8848-b0d61b0ecbc7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:23:04.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5953" for this suite.
Oct  4 11:23:11.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:23:11.132: INFO: namespace emptydir-5953 deletion completed in 6.150221203s

• [SLOW TEST:8.269 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:23:11.132: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1f73bc39-acdc-4a6c-a856-a6d058c48c84
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-1f73bc39-acdc-4a6c-a856-a6d058c48c84
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:23:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2790" for this suite.
Oct  4 11:23:37.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:23:37.408: INFO: namespace projected-2790 deletion completed in 22.147280818s

• [SLOW TEST:26.276 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:23:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Oct  4 11:23:39.997: INFO: Successfully updated pod "annotationupdate896a20de-b28e-41af-a42d-e729df7164ed"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:23:42.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1142" for this suite.
Oct  4 11:24:04.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:24:04.172: INFO: namespace projected-1142 deletion completed in 22.14821867s

• [SLOW TEST:26.763 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:24:04.172: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:24:04.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac" in namespace "projected-6895" to be "success or failure"
Oct  4 11:24:04.226: INFO: Pod "downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.554095ms
Oct  4 11:24:06.231: INFO: Pod "downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009973046s
Oct  4 11:24:08.236: INFO: Pod "downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015061591s
STEP: Saw pod success
Oct  4 11:24:08.236: INFO: Pod "downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac" satisfied condition "success or failure"
Oct  4 11:24:08.240: INFO: Trying to get logs from node lab1-k8s-node-3 pod downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac container client-container: <nil>
STEP: delete the pod
Oct  4 11:24:08.272: INFO: Waiting for pod downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac to disappear
Oct  4 11:24:08.276: INFO: Pod downwardapi-volume-1053795f-4dcd-4e7f-bcb4-b3c5b680cdac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:24:08.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6895" for this suite.
Oct  4 11:24:14.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:24:14.445: INFO: namespace projected-6895 deletion completed in 6.163620118s

• [SLOW TEST:10.274 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:24:14.446: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Oct  4 11:24:14.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 cluster-info'
Oct  4 11:24:14.562: INFO: stderr: ""
Oct  4 11:24:14.562: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:24:14.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8293" for this suite.
Oct  4 11:24:20.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:24:20.725: INFO: namespace kubectl-8293 deletion completed in 6.153672845s

• [SLOW TEST:6.279 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:24:20.727: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:24:20.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e" in namespace "projected-1619" to be "success or failure"
Oct  4 11:24:20.779: INFO: Pod "downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010503ms
Oct  4 11:24:22.783: INFO: Pod "downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009775348s
STEP: Saw pod success
Oct  4 11:24:22.783: INFO: Pod "downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e" satisfied condition "success or failure"
Oct  4 11:24:22.787: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e container client-container: <nil>
STEP: delete the pod
Oct  4 11:24:22.823: INFO: Waiting for pod downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e to disappear
Oct  4 11:24:22.827: INFO: Pod downwardapi-volume-de172aaf-ea97-4f06-8188-db87391a500e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:24:22.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1619" for this suite.
Oct  4 11:24:28.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:24:28.988: INFO: namespace projected-1619 deletion completed in 6.156575948s

• [SLOW TEST:8.261 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:24:28.992: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1004 11:24:39.062685      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct  4 11:24:39.062: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:24:39.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7338" for this suite.
Oct  4 11:24:45.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:24:45.226: INFO: namespace gc-7338 deletion completed in 6.15915718s

• [SLOW TEST:16.234 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:24:45.226: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-1b74c7f0-a755-444b-9e25-1a8db69bdad8
STEP: Creating a pod to test consume secrets
Oct  4 11:24:45.285: INFO: Waiting up to 5m0s for pod "pod-secrets-9093519f-753f-450a-890e-135a07754c7e" in namespace "secrets-6091" to be "success or failure"
Oct  4 11:24:45.292: INFO: Pod "pod-secrets-9093519f-753f-450a-890e-135a07754c7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.691654ms
Oct  4 11:24:47.296: INFO: Pod "pod-secrets-9093519f-753f-450a-890e-135a07754c7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011470936s
STEP: Saw pod success
Oct  4 11:24:47.296: INFO: Pod "pod-secrets-9093519f-753f-450a-890e-135a07754c7e" satisfied condition "success or failure"
Oct  4 11:24:47.301: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-9093519f-753f-450a-890e-135a07754c7e container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:24:47.335: INFO: Waiting for pod pod-secrets-9093519f-753f-450a-890e-135a07754c7e to disappear
Oct  4 11:24:47.339: INFO: Pod pod-secrets-9093519f-753f-450a-890e-135a07754c7e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:24:47.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6091" for this suite.
Oct  4 11:24:53.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:24:53.496: INFO: namespace secrets-6091 deletion completed in 6.152569267s

• [SLOW TEST:8.270 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:24:53.499: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Oct  4 11:24:56.088: INFO: Successfully updated pod "labelsupdate6e411dfd-00f7-4503-b903-7d9f96420078"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:24:58.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6844" for this suite.
Oct  4 11:25:20.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:25:20.268: INFO: namespace projected-6844 deletion completed in 22.144318239s

• [SLOW TEST:26.769 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:25:20.269: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8598
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8598
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8598
Oct  4 11:25:20.329: INFO: Found 0 stateful pods, waiting for 1
Oct  4 11:25:30.335: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Oct  4 11:25:30.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:25:30.579: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:25:30.580: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:25:30.580: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:25:30.584: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Oct  4 11:25:40.589: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:25:40.589: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:25:40.606: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999557s
Oct  4 11:25:41.611: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995313721s
Oct  4 11:25:42.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99035904s
Oct  4 11:25:43.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984969189s
Oct  4 11:25:44.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978233868s
Oct  4 11:25:45.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97306722s
Oct  4 11:25:46.638: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968367742s
Oct  4 11:25:47.642: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.963616233s
Oct  4 11:25:48.647: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.959163363s
Oct  4 11:25:49.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 954.400592ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8598
Oct  4 11:25:50.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:25:50.917: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:25:50.917: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:25:50.917: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:25:50.921: INFO: Found 1 stateful pods, waiting for 3
Oct  4 11:26:00.927: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:26:00.927: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Oct  4 11:26:00.927: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Oct  4 11:26:00.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:26:01.193: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:26:01.193: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:26:01.193: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:26:01.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:26:01.451: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:26:01.451: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:26:01.451: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:26:01.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Oct  4 11:26:01.756: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Oct  4 11:26:01.756: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Oct  4 11:26:01.756: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Oct  4 11:26:01.756: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:26:01.760: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Oct  4 11:26:11.768: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:26:11.768: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:26:11.768: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Oct  4 11:26:11.781: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999698s
Oct  4 11:26:12.787: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993866013s
Oct  4 11:26:13.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988891816s
Oct  4 11:26:14.797: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983375615s
Oct  4 11:26:15.803: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977893334s
Oct  4 11:26:16.808: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97228095s
Oct  4 11:26:17.814: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967142685s
Oct  4 11:26:18.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961786334s
Oct  4 11:26:19.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95516185s
Oct  4 11:26:20.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.563079ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8598
Oct  4 11:26:21.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:26:22.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:26:22.133: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:26:22.133: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:26:22.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:26:22.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:26:22.402: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:26:22.402: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:26:22.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec --namespace=statefulset-8598 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Oct  4 11:26:22.727: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Oct  4 11:26:22.727: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Oct  4 11:26:22.727: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Oct  4 11:26:22.727: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Oct  4 11:26:42.745: INFO: Deleting all statefulset in ns statefulset-8598
Oct  4 11:26:42.748: INFO: Scaling statefulset ss to 0
Oct  4 11:26:42.760: INFO: Waiting for statefulset status.replicas updated to 0
Oct  4 11:26:42.763: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:26:42.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8598" for this suite.
Oct  4 11:26:48.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:26:48.936: INFO: namespace statefulset-8598 deletion completed in 6.149800847s

• [SLOW TEST:88.668 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:26:48.938: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct  4 11:26:52.016: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:26:52.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3856" for this suite.
Oct  4 11:26:58.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:26:58.186: INFO: namespace container-runtime-3856 deletion completed in 6.145212318s

• [SLOW TEST:9.248 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:26:58.187: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2812.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2812.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2812.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2812.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2812.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2812.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct  4 11:27:10.290: INFO: DNS probes using dns-2812/dns-test-99a9e496-4961-4329-abbc-377d1f599d92 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:27:10.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2812" for this suite.
Oct  4 11:27:16.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:27:16.478: INFO: namespace dns-2812 deletion completed in 6.16298139s

• [SLOW TEST:18.291 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:27:16.479: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3588/secret-test-331bbc18-451e-4de0-b1ec-1a46904a013c
STEP: Creating a pod to test consume secrets
Oct  4 11:27:16.536: INFO: Waiting up to 5m0s for pod "pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332" in namespace "secrets-3588" to be "success or failure"
Oct  4 11:27:16.545: INFO: Pod "pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332": Phase="Pending", Reason="", readiness=false. Elapsed: 8.873446ms
Oct  4 11:27:18.550: INFO: Pod "pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014051734s
Oct  4 11:27:20.555: INFO: Pod "pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019082791s
STEP: Saw pod success
Oct  4 11:27:20.555: INFO: Pod "pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332" satisfied condition "success or failure"
Oct  4 11:27:20.559: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332 container env-test: <nil>
STEP: delete the pod
Oct  4 11:27:20.586: INFO: Waiting for pod pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332 to disappear
Oct  4 11:27:20.590: INFO: Pod pod-configmaps-a95f74f1-690d-4512-817e-0037ced4a332 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:27:20.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3588" for this suite.
Oct  4 11:27:26.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:27:26.740: INFO: namespace secrets-3588 deletion completed in 6.144893657s

• [SLOW TEST:10.262 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:27:26.741: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Oct  4 11:27:32.816: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1004 11:27:32.816518      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:27:32.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-178" for this suite.
Oct  4 11:27:38.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:27:38.963: INFO: namespace gc-178 deletion completed in 6.142583612s

• [SLOW TEST:12.222 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:27:38.963: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:27:39.010: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f" in namespace "projected-3117" to be "success or failure"
Oct  4 11:27:39.015: INFO: Pod "downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.645619ms
Oct  4 11:27:41.019: INFO: Pod "downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008891579s
STEP: Saw pod success
Oct  4 11:27:41.019: INFO: Pod "downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f" satisfied condition "success or failure"
Oct  4 11:27:41.023: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f container client-container: <nil>
STEP: delete the pod
Oct  4 11:27:41.061: INFO: Waiting for pod downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f to disappear
Oct  4 11:27:41.065: INFO: Pod downwardapi-volume-40ab0645-2bbd-4361-9d87-b36ec6f0ae0f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:27:41.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3117" for this suite.
Oct  4 11:27:47.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:27:47.219: INFO: namespace projected-3117 deletion completed in 6.148800489s

• [SLOW TEST:8.256 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:27:47.220: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct  4 11:27:49.288: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:27:49.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-394" for this suite.
Oct  4 11:27:55.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:27:55.467: INFO: namespace container-runtime-394 deletion completed in 6.147588793s

• [SLOW TEST:8.248 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:27:55.470: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:27:55.516: INFO: (0) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.039699ms)
Oct  4 11:27:55.521: INFO: (1) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.948036ms)
Oct  4 11:27:55.526: INFO: (2) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.375108ms)
Oct  4 11:27:55.530: INFO: (3) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.620222ms)
Oct  4 11:27:55.536: INFO: (4) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.476996ms)
Oct  4 11:27:55.541: INFO: (5) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.014512ms)
Oct  4 11:27:55.546: INFO: (6) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.891676ms)
Oct  4 11:27:55.554: INFO: (7) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.846121ms)
Oct  4 11:27:55.558: INFO: (8) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.476489ms)
Oct  4 11:27:55.563: INFO: (9) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.813676ms)
Oct  4 11:27:55.567: INFO: (10) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.244242ms)
Oct  4 11:27:55.572: INFO: (11) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.67196ms)
Oct  4 11:27:55.577: INFO: (12) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.81526ms)
Oct  4 11:27:55.582: INFO: (13) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.722131ms)
Oct  4 11:27:55.589: INFO: (14) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.794716ms)
Oct  4 11:27:55.595: INFO: (15) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.070939ms)
Oct  4 11:27:55.599: INFO: (16) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.625424ms)
Oct  4 11:27:55.604: INFO: (17) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.43536ms)
Oct  4 11:27:55.608: INFO: (18) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.51986ms)
Oct  4 11:27:55.614: INFO: (19) /api/v1/nodes/lab1-k8s-node-1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.383652ms)
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:27:55.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9879" for this suite.
Oct  4 11:28:01.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:28:01.762: INFO: namespace proxy-9879 deletion completed in 6.14401382s

• [SLOW TEST:6.292 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:28:01.763: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Oct  4 11:28:01.808: INFO: Waiting up to 5m0s for pod "var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188" in namespace "var-expansion-1081" to be "success or failure"
Oct  4 11:28:01.813: INFO: Pod "var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266537ms
Oct  4 11:28:03.824: INFO: Pod "var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015806957s
STEP: Saw pod success
Oct  4 11:28:03.824: INFO: Pod "var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188" satisfied condition "success or failure"
Oct  4 11:28:03.836: INFO: Trying to get logs from node lab1-k8s-node-1 pod var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188 container dapi-container: <nil>
STEP: delete the pod
Oct  4 11:28:03.924: INFO: Waiting for pod var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188 to disappear
Oct  4 11:28:03.928: INFO: Pod var-expansion-d37fca68-c23c-4862-8947-f20da6d0f188 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:28:03.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1081" for this suite.
Oct  4 11:28:09.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:28:10.095: INFO: namespace var-expansion-1081 deletion completed in 6.160663911s

• [SLOW TEST:8.331 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:28:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:28:10.151: INFO: Waiting up to 5m0s for pod "downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da" in namespace "projected-5858" to be "success or failure"
Oct  4 11:28:10.158: INFO: Pod "downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da": Phase="Pending", Reason="", readiness=false. Elapsed: 7.055133ms
Oct  4 11:28:12.163: INFO: Pod "downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011578769s
STEP: Saw pod success
Oct  4 11:28:12.163: INFO: Pod "downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da" satisfied condition "success or failure"
Oct  4 11:28:12.168: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da container client-container: <nil>
STEP: delete the pod
Oct  4 11:28:12.198: INFO: Waiting for pod downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da to disappear
Oct  4 11:28:12.202: INFO: Pod downwardapi-volume-278139ba-cbf3-44dc-b06b-54df8406d0da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:28:12.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5858" for this suite.
Oct  4 11:28:18.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:28:18.363: INFO: namespace projected-5858 deletion completed in 6.155622165s

• [SLOW TEST:8.268 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:28:18.365: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct  4 11:28:18.435: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:18.435: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:18.435: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:18.440: INFO: Number of nodes with available pods: 0
Oct  4 11:28:18.440: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:19.447: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:19.447: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:19.447: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:19.454: INFO: Number of nodes with available pods: 0
Oct  4 11:28:19.454: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:20.447: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:20.447: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:20.447: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:20.452: INFO: Number of nodes with available pods: 3
Oct  4 11:28:20.452: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Oct  4 11:28:20.475: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:20.475: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:20.476: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:20.481: INFO: Number of nodes with available pods: 2
Oct  4 11:28:20.481: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:21.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:21.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:21.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:21.493: INFO: Number of nodes with available pods: 2
Oct  4 11:28:21.493: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:22.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:22.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:22.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:22.492: INFO: Number of nodes with available pods: 2
Oct  4 11:28:22.492: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:23.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:23.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:23.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:23.494: INFO: Number of nodes with available pods: 2
Oct  4 11:28:23.494: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:24.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:24.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:24.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:24.492: INFO: Number of nodes with available pods: 2
Oct  4 11:28:24.492: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:25.487: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:25.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:25.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:25.492: INFO: Number of nodes with available pods: 2
Oct  4 11:28:25.492: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:28:26.487: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:26.487: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:26.488: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:28:26.494: INFO: Number of nodes with available pods: 3
Oct  4 11:28:26.494: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5484, will wait for the garbage collector to delete the pods
Oct  4 11:28:26.565: INFO: Deleting DaemonSet.extensions daemon-set took: 13.168572ms
Oct  4 11:28:26.866: INFO: Terminating DaemonSet.extensions daemon-set pods took: 301.192954ms
Oct  4 11:28:30.571: INFO: Number of nodes with available pods: 0
Oct  4 11:28:30.571: INFO: Number of running nodes: 0, number of available pods: 0
Oct  4 11:28:30.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5484/daemonsets","resourceVersion":"22189"},"items":null}

Oct  4 11:28:30.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5484/pods","resourceVersion":"22189"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:28:30.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5484" for this suite.
Oct  4 11:28:36.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:28:36.753: INFO: namespace daemonsets-5484 deletion completed in 6.154036939s

• [SLOW TEST:18.388 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:28:36.753: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Oct  4 11:28:36.793: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:28:40.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3977" for this suite.
Oct  4 11:28:46.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:28:46.915: INFO: namespace init-container-3977 deletion completed in 6.150924023s

• [SLOW TEST:10.162 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:28:46.916: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-491070b3-09e6-44f2-96f4-f2a9f3a9ca76
STEP: Creating a pod to test consume configMaps
Oct  4 11:28:46.968: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525" in namespace "configmap-1992" to be "success or failure"
Oct  4 11:28:46.974: INFO: Pod "pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525": Phase="Pending", Reason="", readiness=false. Elapsed: 5.977424ms
Oct  4 11:28:48.979: INFO: Pod "pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011095573s
STEP: Saw pod success
Oct  4 11:28:48.979: INFO: Pod "pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525" satisfied condition "success or failure"
Oct  4 11:28:48.984: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525 container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:28:49.014: INFO: Waiting for pod pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525 to disappear
Oct  4 11:28:49.018: INFO: Pod pod-configmaps-e8559a92-edbe-409e-95dc-644ed070c525 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:28:49.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1992" for this suite.
Oct  4 11:28:55.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:28:55.181: INFO: namespace configmap-1992 deletion completed in 6.157568689s

• [SLOW TEST:8.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:28:55.181: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Oct  4 11:28:55.755: INFO: created pod pod-service-account-defaultsa
Oct  4 11:28:55.755: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Oct  4 11:28:55.761: INFO: created pod pod-service-account-mountsa
Oct  4 11:28:55.761: INFO: pod pod-service-account-mountsa service account token volume mount: true
Oct  4 11:28:55.769: INFO: created pod pod-service-account-nomountsa
Oct  4 11:28:55.769: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Oct  4 11:28:55.779: INFO: created pod pod-service-account-defaultsa-mountspec
Oct  4 11:28:55.779: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Oct  4 11:28:55.787: INFO: created pod pod-service-account-mountsa-mountspec
Oct  4 11:28:55.787: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Oct  4 11:28:55.796: INFO: created pod pod-service-account-nomountsa-mountspec
Oct  4 11:28:55.796: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Oct  4 11:28:55.804: INFO: created pod pod-service-account-defaultsa-nomountspec
Oct  4 11:28:55.805: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Oct  4 11:28:55.810: INFO: created pod pod-service-account-mountsa-nomountspec
Oct  4 11:28:55.810: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Oct  4 11:28:55.819: INFO: created pod pod-service-account-nomountsa-nomountspec
Oct  4 11:28:55.819: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:28:55.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-963" for this suite.
Oct  4 11:29:01.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:29:01.992: INFO: namespace svcaccounts-963 deletion completed in 6.166877428s

• [SLOW TEST:6.811 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:29:01.994: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7ae3b208-be89-4384-8098-b10a2ccd1f8c
STEP: Creating a pod to test consume configMaps
Oct  4 11:29:02.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9" in namespace "configmap-5052" to be "success or failure"
Oct  4 11:29:02.054: INFO: Pod "pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.334846ms
Oct  4 11:29:04.059: INFO: Pod "pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011019684s
STEP: Saw pod success
Oct  4 11:29:04.059: INFO: Pod "pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9" satisfied condition "success or failure"
Oct  4 11:29:04.067: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9 container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:29:04.115: INFO: Waiting for pod pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9 to disappear
Oct  4 11:29:04.119: INFO: Pod pod-configmaps-0a34b10f-f65d-44c1-9d53-f6fefb079fc9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:29:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5052" for this suite.
Oct  4 11:29:10.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:29:10.288: INFO: namespace configmap-5052 deletion completed in 6.163903789s

• [SLOW TEST:8.294 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:29:10.288: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-8a859522-b7c8-41e3-ba38-8f1a64a8782a
STEP: Creating a pod to test consume secrets
Oct  4 11:29:10.338: INFO: Waiting up to 5m0s for pod "pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86" in namespace "secrets-3647" to be "success or failure"
Oct  4 11:29:10.344: INFO: Pod "pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86": Phase="Pending", Reason="", readiness=false. Elapsed: 5.856672ms
Oct  4 11:29:12.351: INFO: Pod "pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013219266s
STEP: Saw pod success
Oct  4 11:29:12.351: INFO: Pod "pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86" satisfied condition "success or failure"
Oct  4 11:29:12.364: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86 container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:29:12.390: INFO: Waiting for pod pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86 to disappear
Oct  4 11:29:12.394: INFO: Pod pod-secrets-6d5b73a3-e8b6-43e4-a2fc-3b428cee3d86 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:29:12.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3647" for this suite.
Oct  4 11:29:18.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:29:18.541: INFO: namespace secrets-3647 deletion completed in 6.142684382s

• [SLOW TEST:8.253 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:29:18.543: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-5731
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5731 to expose endpoints map[]
Oct  4 11:29:18.604: INFO: successfully validated that service endpoint-test2 in namespace services-5731 exposes endpoints map[] (5.052108ms elapsed)
STEP: Creating pod pod1 in namespace services-5731
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5731 to expose endpoints map[pod1:[80]]
Oct  4 11:29:20.646: INFO: successfully validated that service endpoint-test2 in namespace services-5731 exposes endpoints map[pod1:[80]] (2.03167324s elapsed)
STEP: Creating pod pod2 in namespace services-5731
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5731 to expose endpoints map[pod1:[80] pod2:[80]]
Oct  4 11:29:23.715: INFO: successfully validated that service endpoint-test2 in namespace services-5731 exposes endpoints map[pod1:[80] pod2:[80]] (3.063580225s elapsed)
STEP: Deleting pod pod1 in namespace services-5731
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5731 to expose endpoints map[pod2:[80]]
Oct  4 11:29:23.734: INFO: successfully validated that service endpoint-test2 in namespace services-5731 exposes endpoints map[pod2:[80]] (9.844237ms elapsed)
STEP: Deleting pod pod2 in namespace services-5731
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5731 to expose endpoints map[]
Oct  4 11:29:23.749: INFO: successfully validated that service endpoint-test2 in namespace services-5731 exposes endpoints map[] (5.270163ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:29:23.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5731" for this suite.
Oct  4 11:29:37.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:29:37.949: INFO: namespace services-5731 deletion completed in 14.162760848s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:19.406 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:29:37.950: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2889aeba-6c06-4d74-a83e-02dae9825e74
STEP: Creating a pod to test consume secrets
Oct  4 11:29:38.004: INFO: Waiting up to 5m0s for pod "pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454" in namespace "secrets-1805" to be "success or failure"
Oct  4 11:29:38.008: INFO: Pod "pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454": Phase="Pending", Reason="", readiness=false. Elapsed: 4.605059ms
Oct  4 11:29:40.013: INFO: Pod "pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009488373s
STEP: Saw pod success
Oct  4 11:29:40.013: INFO: Pod "pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454" satisfied condition "success or failure"
Oct  4 11:29:40.017: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454 container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:29:40.044: INFO: Waiting for pod pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454 to disappear
Oct  4 11:29:40.047: INFO: Pod pod-secrets-7aae7dd7-565e-42d6-b766-a734abacd454 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:29:40.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1805" for this suite.
Oct  4 11:29:46.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:29:46.196: INFO: namespace secrets-1805 deletion completed in 6.143028547s

• [SLOW TEST:8.246 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:29:46.197: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b85932c8-b3ad-46ad-8e3c-dabae53dfcc8
STEP: Creating configMap with name cm-test-opt-upd-41154dcb-4f99-42bf-88a2-fc409d9b3dd7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b85932c8-b3ad-46ad-8e3c-dabae53dfcc8
STEP: Updating configmap cm-test-opt-upd-41154dcb-4f99-42bf-88a2-fc409d9b3dd7
STEP: Creating configMap with name cm-test-opt-create-de42ec99-3b77-4e17-b9ae-0075fd8b86de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:29:50.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2954" for this suite.
Oct  4 11:30:12.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:30:12.534: INFO: namespace configmap-2954 deletion completed in 22.151721979s

• [SLOW TEST:26.338 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:30:12.538: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-8118e918-acaa-4e51-9d42-575952a74d52
STEP: Creating a pod to test consume configMaps
Oct  4 11:30:12.597: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b" in namespace "projected-7881" to be "success or failure"
Oct  4 11:30:12.602: INFO: Pod "pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922263ms
Oct  4 11:30:14.606: INFO: Pod "pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008377837s
STEP: Saw pod success
Oct  4 11:30:14.606: INFO: Pod "pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b" satisfied condition "success or failure"
Oct  4 11:30:14.609: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:30:14.637: INFO: Waiting for pod pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b to disappear
Oct  4 11:30:14.641: INFO: Pod pod-projected-configmaps-35cf4a81-40cd-4780-8b03-126665036c8b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:30:14.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7881" for this suite.
Oct  4 11:30:20.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:30:20.800: INFO: namespace projected-7881 deletion completed in 6.15302278s

• [SLOW TEST:8.262 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:30:20.800: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8523
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct  4 11:30:20.835: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct  4 11:30:44.951: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.47:8080/dial?request=hostName&protocol=udp&host=10.233.74.40&port=8081&tries=1'] Namespace:pod-network-test-8523 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:30:44.951: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:30:45.143: INFO: Waiting for endpoints: map[]
Oct  4 11:30:45.147: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.47:8080/dial?request=hostName&protocol=udp&host=10.233.64.46&port=8081&tries=1'] Namespace:pod-network-test-8523 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:30:45.147: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:30:45.337: INFO: Waiting for endpoints: map[]
Oct  4 11:30:45.342: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.47:8080/dial?request=hostName&protocol=udp&host=10.233.95.148&port=8081&tries=1'] Namespace:pod-network-test-8523 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:30:45.343: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:30:45.531: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:30:45.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8523" for this suite.
Oct  4 11:31:07.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:31:07.687: INFO: namespace pod-network-test-8523 deletion completed in 22.148676985s

• [SLOW TEST:46.887 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:31:07.689: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:31:07.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891" in namespace "projected-1312" to be "success or failure"
Oct  4 11:31:07.741: INFO: Pod "downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509067ms
Oct  4 11:31:09.746: INFO: Pod "downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010280716s
STEP: Saw pod success
Oct  4 11:31:09.746: INFO: Pod "downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891" satisfied condition "success or failure"
Oct  4 11:31:09.750: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891 container client-container: <nil>
STEP: delete the pod
Oct  4 11:31:09.779: INFO: Waiting for pod downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891 to disappear
Oct  4 11:31:09.783: INFO: Pod downwardapi-volume-9c2b7e6c-6052-4041-8dfb-5f0ea7c86891 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:31:09.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1312" for this suite.
Oct  4 11:31:15.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:31:15.946: INFO: namespace projected-1312 deletion completed in 6.15705551s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:31:15.946: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:31:16.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f" in namespace "projected-4576" to be "success or failure"
Oct  4 11:31:16.013: INFO: Pod "downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.541707ms
Oct  4 11:31:18.019: INFO: Pod "downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013961798s
STEP: Saw pod success
Oct  4 11:31:18.019: INFO: Pod "downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f" satisfied condition "success or failure"
Oct  4 11:31:18.026: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f container client-container: <nil>
STEP: delete the pod
Oct  4 11:31:18.050: INFO: Waiting for pod downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f to disappear
Oct  4 11:31:18.054: INFO: Pod downwardapi-volume-3b643e98-4315-429e-a5e6-e3fa7cf9529f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:31:18.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4576" for this suite.
Oct  4 11:31:24.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:31:24.212: INFO: namespace projected-4576 deletion completed in 6.152837745s

• [SLOW TEST:8.267 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:31:24.213: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Oct  4 11:31:24.260: INFO: Waiting up to 5m0s for pod "client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a" in namespace "containers-2647" to be "success or failure"
Oct  4 11:31:24.267: INFO: Pod "client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.557887ms
Oct  4 11:31:26.272: INFO: Pod "client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011265001s
STEP: Saw pod success
Oct  4 11:31:26.272: INFO: Pod "client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a" satisfied condition "success or failure"
Oct  4 11:31:26.275: INFO: Trying to get logs from node lab1-k8s-node-1 pod client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a container test-container: <nil>
STEP: delete the pod
Oct  4 11:31:26.306: INFO: Waiting for pod client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a to disappear
Oct  4 11:31:26.310: INFO: Pod client-containers-1f1e3f67-8940-40f0-a597-13fc03c1545a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:31:26.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2647" for this suite.
Oct  4 11:31:32.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:31:32.461: INFO: namespace containers-2647 deletion completed in 6.145845248s

• [SLOW TEST:8.248 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:31:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:31:56.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7209" for this suite.
Oct  4 11:32:02.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:32:02.764: INFO: namespace namespaces-7209 deletion completed in 6.151577161s
STEP: Destroying namespace "nsdeletetest-973" for this suite.
Oct  4 11:32:02.768: INFO: Namespace nsdeletetest-973 was already deleted
STEP: Destroying namespace "nsdeletetest-6788" for this suite.
Oct  4 11:32:08.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:32:08.932: INFO: namespace nsdeletetest-6788 deletion completed in 6.16461944s

• [SLOW TEST:36.469 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:32:08.933: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Oct  4 11:32:11.524: INFO: Successfully updated pod "annotationupdate0ba0b76c-eeb9-407b-8a92-26c9cdeb05f3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:32:13.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4406" for this suite.
Oct  4 11:32:35.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:32:35.695: INFO: namespace downward-api-4406 deletion completed in 22.144838013s

• [SLOW TEST:26.762 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:32:35.695: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:32:35.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 version'
Oct  4 11:32:35.812: INFO: stderr: ""
Oct  4 11:32:35.812: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:05:50Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:32:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8541" for this suite.
Oct  4 11:32:41.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:32:41.968: INFO: namespace kubectl-8541 deletion completed in 6.150875904s

• [SLOW TEST:6.273 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:32:41.970: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-097440f7-c6ce-433b-a34b-5bae00985d4d
STEP: Creating a pod to test consume secrets
Oct  4 11:32:42.023: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60" in namespace "projected-3423" to be "success or failure"
Oct  4 11:32:42.030: INFO: Pod "pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.893251ms
Oct  4 11:32:44.034: INFO: Pod "pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011599535s
Oct  4 11:32:46.039: INFO: Pod "pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016739945s
STEP: Saw pod success
Oct  4 11:32:46.040: INFO: Pod "pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60" satisfied condition "success or failure"
Oct  4 11:32:46.044: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:32:46.072: INFO: Waiting for pod pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60 to disappear
Oct  4 11:32:46.076: INFO: Pod pod-projected-secrets-90ccd918-7003-4fb3-92b3-6db9c60fac60 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:32:46.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3423" for this suite.
Oct  4 11:32:52.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:32:52.225: INFO: namespace projected-3423 deletion completed in 6.144021965s

• [SLOW TEST:10.256 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:32:52.226: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:32:54.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7634" for this suite.
Oct  4 11:33:00.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:33:00.496: INFO: namespace emptydir-wrapper-7634 deletion completed in 6.144228502s

• [SLOW TEST:8.270 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:33:00.496: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Oct  4 11:33:04.562: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-a86827cd-5397-46db-927b-37b8a68b3aae,GenerateName:,Namespace:events-9999,SelfLink:/api/v1/namespaces/events-9999/pods/send-events-a86827cd-5397-46db-927b-37b8a68b3aae,UID:0dc35dbb-5cbc-498d-87fb-e348b273ca7a,ResourceVersion:23807,Generation:0,CreationTimestamp:2019-10-04 11:33:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 529920055,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7bpch {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7bpch,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-7bpch true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038fa5f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038fa610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:33:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:33:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:33:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:33:00 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:10.233.95.156,StartTime:2019-10-04 11:33:00 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-10-04 11:33:03 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://1c6a76070a126f779e908943514b6afbe6f9cbcc08be891ac81480837d4918c9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Oct  4 11:33:06.567: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Oct  4 11:33:08.572: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:33:08.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9999" for this suite.
Oct  4 11:33:46.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:33:46.750: INFO: namespace events-9999 deletion completed in 38.162091255s

• [SLOW TEST:46.254 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:33:46.751: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Oct  4 11:33:49.839: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:33:50.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6555" for this suite.
Oct  4 11:34:12.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:34:13.032: INFO: namespace replicaset-6555 deletion completed in 22.163813697s

• [SLOW TEST:26.281 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:34:13.032: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-d640c11e-5e33-4d3e-97b5-d3ad6825ae0e
STEP: Creating configMap with name cm-test-opt-upd-0fcabdf9-4b37-450b-89eb-42b9a3c1e787
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d640c11e-5e33-4d3e-97b5-d3ad6825ae0e
STEP: Updating configmap cm-test-opt-upd-0fcabdf9-4b37-450b-89eb-42b9a3c1e787
STEP: Creating configMap with name cm-test-opt-create-07b30bda-5e07-492b-a65c-f8f27a3838e3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:34:17.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6814" for this suite.
Oct  4 11:34:39.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:34:39.364: INFO: namespace projected-6814 deletion completed in 22.144536794s

• [SLOW TEST:26.332 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:34:39.364: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:34:41.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-873" for this suite.
Oct  4 11:35:19.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:35:19.579: INFO: namespace kubelet-test-873 deletion completed in 38.140634354s

• [SLOW TEST:40.215 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:35:19.581: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3602.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3602.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3602.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3602.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct  4 11:35:21.663: INFO: DNS probes using dns-test-0a2c033f-09bd-4c8a-9483-5a9e9b8c1f9f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3602.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3602.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3602.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3602.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct  4 11:35:23.728: INFO: File wheezy_udp@dns-test-service-3.dns-3602.svc.cluster.local from pod  dns-3602/dns-test-b6ae86b3-8964-416d-bc3a-c13587f4feac contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct  4 11:35:23.734: INFO: File jessie_udp@dns-test-service-3.dns-3602.svc.cluster.local from pod  dns-3602/dns-test-b6ae86b3-8964-416d-bc3a-c13587f4feac contains 'foo.example.com.
' instead of 'bar.example.com.'
Oct  4 11:35:23.734: INFO: Lookups using dns-3602/dns-test-b6ae86b3-8964-416d-bc3a-c13587f4feac failed for: [wheezy_udp@dns-test-service-3.dns-3602.svc.cluster.local jessie_udp@dns-test-service-3.dns-3602.svc.cluster.local]

Oct  4 11:35:28.745: INFO: DNS probes using dns-test-b6ae86b3-8964-416d-bc3a-c13587f4feac succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3602.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3602.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3602.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3602.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct  4 11:35:32.829: INFO: DNS probes using dns-test-c689420b-efca-4897-a121-e86504da257a succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:35:32.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3602" for this suite.
Oct  4 11:35:38.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:35:39.033: INFO: namespace dns-3602 deletion completed in 6.143602381s

• [SLOW TEST:19.452 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:35:39.035: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:35:39.072: INFO: Creating ReplicaSet my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad
Oct  4 11:35:39.085: INFO: Pod name my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad: Found 0 pods out of 1
Oct  4 11:35:44.090: INFO: Pod name my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad: Found 1 pods out of 1
Oct  4 11:35:44.090: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad" is running
Oct  4 11:35:44.095: INFO: Pod "my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad-wxkpl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:35:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:35:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:35:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:35:39 +0000 UTC Reason: Message:}])
Oct  4 11:35:44.095: INFO: Trying to dial the pod
Oct  4 11:35:49.112: INFO: Controller my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad: Got expected result from replica 1 [my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad-wxkpl]: "my-hostname-basic-cf9aaff6-3745-495e-b194-1b28339672ad-wxkpl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:35:49.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7783" for this suite.
Oct  4 11:35:55.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:35:55.264: INFO: namespace replicaset-7783 deletion completed in 6.145809228s

• [SLOW TEST:16.229 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:35:55.266: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6809
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct  4 11:35:55.300: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct  4 11:36:17.421: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.49:8080/dial?request=hostName&protocol=http&host=10.233.74.41&port=8080&tries=1'] Namespace:pod-network-test-6809 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:36:17.421: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:36:17.602: INFO: Waiting for endpoints: map[]
Oct  4 11:36:17.607: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.49:8080/dial?request=hostName&protocol=http&host=10.233.95.165&port=8080&tries=1'] Namespace:pod-network-test-6809 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:36:17.607: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:36:17.805: INFO: Waiting for endpoints: map[]
Oct  4 11:36:17.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.64.49:8080/dial?request=hostName&protocol=http&host=10.233.64.48&port=8080&tries=1'] Namespace:pod-network-test-6809 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:36:17.810: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:36:18.002: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:36:18.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6809" for this suite.
Oct  4 11:36:40.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:36:40.157: INFO: namespace pod-network-test-6809 deletion completed in 22.149905327s

• [SLOW TEST:44.892 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:36:40.159: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-8ckb
STEP: Creating a pod to test atomic-volume-subpath
Oct  4 11:36:40.216: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8ckb" in namespace "subpath-2315" to be "success or failure"
Oct  4 11:36:40.231: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.504687ms
Oct  4 11:36:42.235: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 2.018832556s
Oct  4 11:36:44.240: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 4.023946872s
Oct  4 11:36:46.246: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 6.030226152s
Oct  4 11:36:48.251: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 8.035321981s
Oct  4 11:36:50.256: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 10.040071266s
Oct  4 11:36:52.261: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 12.044666484s
Oct  4 11:36:54.265: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 14.049222315s
Oct  4 11:36:56.270: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 16.053817662s
Oct  4 11:36:58.275: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 18.059091869s
Oct  4 11:37:00.280: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Running", Reason="", readiness=true. Elapsed: 20.063774089s
Oct  4 11:37:02.284: INFO: Pod "pod-subpath-test-downwardapi-8ckb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.06810912s
STEP: Saw pod success
Oct  4 11:37:02.284: INFO: Pod "pod-subpath-test-downwardapi-8ckb" satisfied condition "success or failure"
Oct  4 11:37:02.288: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-downwardapi-8ckb container test-container-subpath-downwardapi-8ckb: <nil>
STEP: delete the pod
Oct  4 11:37:02.313: INFO: Waiting for pod pod-subpath-test-downwardapi-8ckb to disappear
Oct  4 11:37:02.316: INFO: Pod pod-subpath-test-downwardapi-8ckb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8ckb
Oct  4 11:37:02.316: INFO: Deleting pod "pod-subpath-test-downwardapi-8ckb" in namespace "subpath-2315"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:37:02.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2315" for this suite.
Oct  4 11:37:08.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:37:08.467: INFO: namespace subpath-2315 deletion completed in 6.142964933s

• [SLOW TEST:28.308 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:37:08.469: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct  4 11:37:08.517: INFO: Waiting up to 5m0s for pod "pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32" in namespace "emptydir-5942" to be "success or failure"
Oct  4 11:37:08.521: INFO: Pod "pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139537ms
Oct  4 11:37:10.526: INFO: Pod "pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009628506s
Oct  4 11:37:12.531: INFO: Pod "pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01429749s
STEP: Saw pod success
Oct  4 11:37:12.531: INFO: Pod "pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32" satisfied condition "success or failure"
Oct  4 11:37:12.535: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32 container test-container: <nil>
STEP: delete the pod
Oct  4 11:37:12.563: INFO: Waiting for pod pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32 to disappear
Oct  4 11:37:12.567: INFO: Pod pod-eead8958-58ab-4d1b-a50e-9e2cc22ddb32 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:37:12.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5942" for this suite.
Oct  4 11:37:18.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:37:18.724: INFO: namespace emptydir-5942 deletion completed in 6.151774469s

• [SLOW TEST:10.255 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:37:18.726: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct  4 11:37:21.802: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:37:21.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2668" for this suite.
Oct  4 11:37:27.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:37:27.983: INFO: namespace container-runtime-2668 deletion completed in 6.154574377s

• [SLOW TEST:9.257 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:37:27.984: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:37:28.025: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Oct  4 11:37:29.067: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:37:29.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1242" for this suite.
Oct  4 11:37:35.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:37:35.231: INFO: namespace replication-controller-1242 deletion completed in 6.15484356s

• [SLOW TEST:7.247 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:37:35.231: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:38:35.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6525" for this suite.
Oct  4 11:38:57.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:38:57.440: INFO: namespace container-probe-6525 deletion completed in 22.149573927s

• [SLOW TEST:82.210 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:38:57.445: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9854e965-6a28-4851-85f6-335a2a195b5b
STEP: Creating a pod to test consume configMaps
Oct  4 11:38:57.506: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d" in namespace "projected-5849" to be "success or failure"
Oct  4 11:38:57.516: INFO: Pod "pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.423593ms
Oct  4 11:38:59.521: INFO: Pod "pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014432918s
STEP: Saw pod success
Oct  4 11:38:59.521: INFO: Pod "pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d" satisfied condition "success or failure"
Oct  4 11:38:59.525: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:38:59.551: INFO: Waiting for pod pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d to disappear
Oct  4 11:38:59.555: INFO: Pod pod-projected-configmaps-dd0ed5d1-debc-4211-a9e6-d3af2ca4534d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:38:59.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5849" for this suite.
Oct  4 11:39:05.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:39:05.703: INFO: namespace projected-5849 deletion completed in 6.141955278s

• [SLOW TEST:8.258 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:39:05.703: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:39:28.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9902" for this suite.
Oct  4 11:39:34.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:39:34.177: INFO: namespace container-runtime-9902 deletion completed in 6.165135371s

• [SLOW TEST:28.474 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:39:34.178: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Oct  4 11:39:34.208: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Oct  4 11:39:34.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-5395'
Oct  4 11:39:34.515: INFO: stderr: ""
Oct  4 11:39:34.515: INFO: stdout: "service/redis-slave created\n"
Oct  4 11:39:34.515: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Oct  4 11:39:34.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-5395'
Oct  4 11:39:34.761: INFO: stderr: ""
Oct  4 11:39:34.761: INFO: stdout: "service/redis-master created\n"
Oct  4 11:39:34.761: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Oct  4 11:39:34.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-5395'
Oct  4 11:39:35.009: INFO: stderr: ""
Oct  4 11:39:35.009: INFO: stdout: "service/frontend created\n"
Oct  4 11:39:35.009: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Oct  4 11:39:35.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-5395'
Oct  4 11:39:35.260: INFO: stderr: ""
Oct  4 11:39:35.260: INFO: stdout: "deployment.apps/frontend created\n"
Oct  4 11:39:35.260: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Oct  4 11:39:35.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-5395'
Oct  4 11:39:35.511: INFO: stderr: ""
Oct  4 11:39:35.511: INFO: stdout: "deployment.apps/redis-master created\n"
Oct  4 11:39:35.511: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Oct  4 11:39:35.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-5395'
Oct  4 11:39:35.785: INFO: stderr: ""
Oct  4 11:39:35.785: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Oct  4 11:39:35.786: INFO: Waiting for all frontend pods to be Running.
Oct  4 11:39:50.836: INFO: Waiting for frontend to serve content.
Oct  4 11:39:51.881: INFO: Trying to add a new entry to the guestbook.
Oct  4 11:39:51.897: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Oct  4 11:39:51.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Oct  4 11:39:52.025: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:39:52.025: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Oct  4 11:39:52.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Oct  4 11:39:52.157: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:39:52.157: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Oct  4 11:39:52.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Oct  4 11:39:52.279: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:39:52.279: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct  4 11:39:52.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Oct  4 11:39:52.373: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:39:52.373: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Oct  4 11:39:52.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Oct  4 11:39:52.488: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:39:52.488: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Oct  4 11:39:52.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-5395'
Oct  4 11:39:52.570: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 11:39:52.570: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:39:52.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5395" for this suite.
Oct  4 11:40:32.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:40:32.732: INFO: namespace kubectl-5395 deletion completed in 40.156229182s

• [SLOW TEST:58.554 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:40:32.733: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Oct  4 11:40:32.768: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-245756238 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:40:32.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-238" for this suite.
Oct  4 11:40:38.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:40:39.001: INFO: namespace kubectl-238 deletion completed in 6.148999767s

• [SLOW TEST:6.268 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:40:39.003: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:40:39.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3705" for this suite.
Oct  4 11:40:45.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:40:45.196: INFO: namespace services-3705 deletion completed in 6.152967168s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.193 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:40:45.197: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-1eca4dec-22cd-490a-a4a9-e8ff01199340
STEP: Creating a pod to test consume configMaps
Oct  4 11:40:45.250: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b" in namespace "projected-494" to be "success or failure"
Oct  4 11:40:45.255: INFO: Pod "pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114033ms
Oct  4 11:40:47.260: INFO: Pod "pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009918922s
Oct  4 11:40:49.265: INFO: Pod "pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015156892s
STEP: Saw pod success
Oct  4 11:40:49.265: INFO: Pod "pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b" satisfied condition "success or failure"
Oct  4 11:40:49.270: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:40:49.304: INFO: Waiting for pod pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b to disappear
Oct  4 11:40:49.307: INFO: Pod pod-projected-configmaps-dff4eb1f-e64d-4921-b9ad-9e14fe180c6b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:40:49.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-494" for this suite.
Oct  4 11:40:55.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:40:55.462: INFO: namespace projected-494 deletion completed in 6.148663618s

• [SLOW TEST:10.264 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:40:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct  4 11:40:55.512: INFO: Waiting up to 5m0s for pod "pod-007fc35e-04ed-4b15-9b5c-de141186122d" in namespace "emptydir-7250" to be "success or failure"
Oct  4 11:40:55.518: INFO: Pod "pod-007fc35e-04ed-4b15-9b5c-de141186122d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.953908ms
Oct  4 11:40:57.526: INFO: Pod "pod-007fc35e-04ed-4b15-9b5c-de141186122d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013845648s
STEP: Saw pod success
Oct  4 11:40:57.526: INFO: Pod "pod-007fc35e-04ed-4b15-9b5c-de141186122d" satisfied condition "success or failure"
Oct  4 11:40:57.530: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-007fc35e-04ed-4b15-9b5c-de141186122d container test-container: <nil>
STEP: delete the pod
Oct  4 11:40:57.563: INFO: Waiting for pod pod-007fc35e-04ed-4b15-9b5c-de141186122d to disappear
Oct  4 11:40:57.567: INFO: Pod pod-007fc35e-04ed-4b15-9b5c-de141186122d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:40:57.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7250" for this suite.
Oct  4 11:41:03.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:41:03.793: INFO: namespace emptydir-7250 deletion completed in 6.220101307s

• [SLOW TEST:8.331 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:41:03.795: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-cdf56fec-1da0-487a-928f-5ad0633ead94
STEP: Creating a pod to test consume secrets
Oct  4 11:41:03.921: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493" in namespace "projected-6803" to be "success or failure"
Oct  4 11:41:03.927: INFO: Pod "pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493": Phase="Pending", Reason="", readiness=false. Elapsed: 6.329295ms
Oct  4 11:41:05.932: INFO: Pod "pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011140554s
STEP: Saw pod success
Oct  4 11:41:05.932: INFO: Pod "pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493" satisfied condition "success or failure"
Oct  4 11:41:05.936: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:41:05.965: INFO: Waiting for pod pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493 to disappear
Oct  4 11:41:05.971: INFO: Pod pod-projected-secrets-7750c82a-9577-416d-9c1d-c3a4281b5493 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:41:05.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6803" for this suite.
Oct  4 11:41:11.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:41:12.132: INFO: namespace projected-6803 deletion completed in 6.155089389s

• [SLOW TEST:8.337 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:41:12.132: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:41:16.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7614" for this suite.
Oct  4 11:41:22.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:41:22.343: INFO: namespace kubelet-test-7614 deletion completed in 6.145325305s

• [SLOW TEST:10.210 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:41:22.343: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6349.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6349.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.38.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.38.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.38.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.38.109_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6349.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6349.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6349.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6349.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6349.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 109.38.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.38.109_udp@PTR;check="$$(dig +tcp +noall +answer +search 109.38.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.38.109_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct  4 11:41:24.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.455: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.490: INFO: Unable to read jessie_udp@dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.495: INFO: Unable to read jessie_tcp@dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.501: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.506: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:24.540: INFO: Lookups using dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90 failed for: [wheezy_udp@dns-test-service.dns-6349.svc.cluster.local wheezy_tcp@dns-test-service.dns-6349.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local jessie_udp@dns-test-service.dns-6349.svc.cluster.local jessie_tcp@dns-test-service.dns-6349.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local]

Oct  4 11:41:29.596: INFO: Unable to read jessie_udp@dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:29.601: INFO: Unable to read jessie_tcp@dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:29.606: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:29.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local from pod dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90: the server could not find the requested resource (get pods dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90)
Oct  4 11:41:29.642: INFO: Lookups using dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90 failed for: [jessie_udp@dns-test-service.dns-6349.svc.cluster.local jessie_tcp@dns-test-service.dns-6349.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6349.svc.cluster.local]

Oct  4 11:41:34.651: INFO: DNS probes using dns-6349/dns-test-6ae811c1-0dd6-4aa5-a478-d70d810f5f90 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:41:34.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6349" for this suite.
Oct  4 11:41:40.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:41:40.886: INFO: namespace dns-6349 deletion completed in 6.15514913s

• [SLOW TEST:18.544 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:41:40.888: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1443ef91-95b1-4fe9-99d8-462d9aa9bfb8
STEP: Creating a pod to test consume secrets
Oct  4 11:41:40.950: INFO: Waiting up to 5m0s for pod "pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2" in namespace "secrets-6620" to be "success or failure"
Oct  4 11:41:40.954: INFO: Pod "pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917373ms
Oct  4 11:41:42.959: INFO: Pod "pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009479066s
STEP: Saw pod success
Oct  4 11:41:42.959: INFO: Pod "pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2" satisfied condition "success or failure"
Oct  4 11:41:42.963: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2 container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:41:42.989: INFO: Waiting for pod pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2 to disappear
Oct  4 11:41:42.993: INFO: Pod pod-secrets-ff706fdd-b270-4a35-9504-9a4bbacb5eb2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:41:42.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6620" for this suite.
Oct  4 11:41:49.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:41:49.147: INFO: namespace secrets-6620 deletion completed in 6.147617125s

• [SLOW TEST:8.259 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:41:49.149: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4582/configmap-test-0644df68-a464-4de3-adc4-853c5f21dad2
STEP: Creating a pod to test consume configMaps
Oct  4 11:41:49.205: INFO: Waiting up to 5m0s for pod "pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db" in namespace "configmap-4582" to be "success or failure"
Oct  4 11:41:49.210: INFO: Pod "pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865894ms
Oct  4 11:41:51.216: INFO: Pod "pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011037627s
STEP: Saw pod success
Oct  4 11:41:51.216: INFO: Pod "pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db" satisfied condition "success or failure"
Oct  4 11:41:51.220: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db container env-test: <nil>
STEP: delete the pod
Oct  4 11:41:51.247: INFO: Waiting for pod pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db to disappear
Oct  4 11:41:51.251: INFO: Pod pod-configmaps-454f0d8d-377d-4e7a-9c32-c3e3382d03db no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:41:51.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4582" for this suite.
Oct  4 11:41:57.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:41:57.408: INFO: namespace configmap-4582 deletion completed in 6.150913848s

• [SLOW TEST:8.259 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:41:57.408: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad
Oct  4 11:41:57.453: INFO: Pod name my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad: Found 0 pods out of 1
Oct  4 11:42:02.458: INFO: Pod name my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad: Found 1 pods out of 1
Oct  4 11:42:02.458: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad" are running
Oct  4 11:42:02.462: INFO: Pod "my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad-bn7j2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:41:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:41:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:41:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-10-04 11:41:57 +0000 UTC Reason: Message:}])
Oct  4 11:42:02.463: INFO: Trying to dial the pod
Oct  4 11:42:07.478: INFO: Controller my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad: Got expected result from replica 1 [my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad-bn7j2]: "my-hostname-basic-8ab6eaa9-1c80-46dd-a440-fccfd9d250ad-bn7j2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:42:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2522" for this suite.
Oct  4 11:42:13.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:42:13.635: INFO: namespace replication-controller-2522 deletion completed in 6.151130683s

• [SLOW TEST:16.227 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:42:13.637: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Oct  4 11:42:13.684: INFO: Waiting up to 5m0s for pod "var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4" in namespace "var-expansion-4370" to be "success or failure"
Oct  4 11:42:13.694: INFO: Pod "var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353805ms
Oct  4 11:42:15.699: INFO: Pod "var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014507157s
STEP: Saw pod success
Oct  4 11:42:15.699: INFO: Pod "var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4" satisfied condition "success or failure"
Oct  4 11:42:15.703: INFO: Trying to get logs from node lab1-k8s-node-1 pod var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4 container dapi-container: <nil>
STEP: delete the pod
Oct  4 11:42:15.729: INFO: Waiting for pod var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4 to disappear
Oct  4 11:42:15.733: INFO: Pod var-expansion-f9d3a59d-69c5-43ec-824e-6022781d91f4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:42:15.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4370" for this suite.
Oct  4 11:42:21.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:42:21.900: INFO: namespace var-expansion-4370 deletion completed in 6.161877379s

• [SLOW TEST:8.264 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:42:21.901: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:42:45.970: INFO: Container started at 2019-10-04 11:42:23 +0000 UTC, pod became ready at 2019-10-04 11:42:45 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:42:45.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-410" for this suite.
Oct  4 11:43:07.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:43:08.117: INFO: namespace container-probe-410 deletion completed in 22.141231693s

• [SLOW TEST:46.216 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:43:08.118: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Oct  4 11:43:08.172: INFO: Waiting up to 5m0s for pod "pod-faf1309c-ee27-4bcd-8040-8f1596e59998" in namespace "emptydir-7298" to be "success or failure"
Oct  4 11:43:08.177: INFO: Pod "pod-faf1309c-ee27-4bcd-8040-8f1596e59998": Phase="Pending", Reason="", readiness=false. Elapsed: 4.948216ms
Oct  4 11:43:10.181: INFO: Pod "pod-faf1309c-ee27-4bcd-8040-8f1596e59998": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009838891s
Oct  4 11:43:12.186: INFO: Pod "pod-faf1309c-ee27-4bcd-8040-8f1596e59998": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014769175s
STEP: Saw pod success
Oct  4 11:43:12.186: INFO: Pod "pod-faf1309c-ee27-4bcd-8040-8f1596e59998" satisfied condition "success or failure"
Oct  4 11:43:12.191: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-faf1309c-ee27-4bcd-8040-8f1596e59998 container test-container: <nil>
STEP: delete the pod
Oct  4 11:43:12.217: INFO: Waiting for pod pod-faf1309c-ee27-4bcd-8040-8f1596e59998 to disappear
Oct  4 11:43:12.222: INFO: Pod pod-faf1309c-ee27-4bcd-8040-8f1596e59998 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:43:12.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7298" for this suite.
Oct  4 11:43:18.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:43:18.370: INFO: namespace emptydir-7298 deletion completed in 6.142752215s

• [SLOW TEST:10.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:43:18.370: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2190
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct  4 11:43:18.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct  4 11:43:42.514: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.74.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2190 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:43:42.515: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:43:42.738: INFO: Found all expected endpoints: [netserver-0]
Oct  4 11:43:42.743: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.95.189:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2190 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:43:42.743: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:43:42.944: INFO: Found all expected endpoints: [netserver-1]
Oct  4 11:43:42.948: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.51:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2190 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:43:42.949: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:43:43.127: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:43:43.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2190" for this suite.
Oct  4 11:44:05.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:44:05.286: INFO: namespace pod-network-test-2190 deletion completed in 22.153070069s

• [SLOW TEST:46.917 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:44:05.288: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-bac087cb-050c-4602-a4aa-6fd56edd526a
STEP: Creating a pod to test consume secrets
Oct  4 11:44:05.340: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22" in namespace "projected-2375" to be "success or failure"
Oct  4 11:44:05.345: INFO: Pod "pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22": Phase="Pending", Reason="", readiness=false. Elapsed: 5.210786ms
Oct  4 11:44:07.350: INFO: Pod "pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010044056s
STEP: Saw pod success
Oct  4 11:44:07.350: INFO: Pod "pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22" satisfied condition "success or failure"
Oct  4 11:44:07.354: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22 container projected-secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:44:07.381: INFO: Waiting for pod pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22 to disappear
Oct  4 11:44:07.385: INFO: Pod pod-projected-secrets-de67ce38-12c5-4aed-b9f0-43933a9bac22 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:44:07.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2375" for this suite.
Oct  4 11:44:13.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:44:13.537: INFO: namespace projected-2375 deletion completed in 6.146561142s

• [SLOW TEST:8.248 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:44:13.537: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Oct  4 11:44:13.571: INFO: namespace kubectl-7644
Oct  4 11:44:13.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-7644'
Oct  4 11:44:13.793: INFO: stderr: ""
Oct  4 11:44:13.793: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Oct  4 11:44:14.799: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:44:14.799: INFO: Found 0 / 1
Oct  4 11:44:15.799: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:44:15.799: INFO: Found 1 / 1
Oct  4 11:44:15.799: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Oct  4 11:44:15.803: INFO: Selector matched 1 pods for map[app:redis]
Oct  4 11:44:15.803: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Oct  4 11:44:15.803: INFO: wait on redis-master startup in kubectl-7644 
Oct  4 11:44:15.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 logs redis-master-tvmmz redis-master --namespace=kubectl-7644'
Oct  4 11:44:15.916: INFO: stderr: ""
Oct  4 11:44:15.916: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Oct 11:44:15.100 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Oct 11:44:15.100 # Server started, Redis version 3.2.12\n1:M 04 Oct 11:44:15.100 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Oct 11:44:15.100 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Oct  4 11:44:15.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7644'
Oct  4 11:44:16.044: INFO: stderr: ""
Oct  4 11:44:16.044: INFO: stdout: "service/rm2 exposed\n"
Oct  4 11:44:16.050: INFO: Service rm2 in namespace kubectl-7644 found.
STEP: exposing service
Oct  4 11:44:18.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7644'
Oct  4 11:44:18.147: INFO: stderr: ""
Oct  4 11:44:18.147: INFO: stdout: "service/rm3 exposed\n"
Oct  4 11:44:18.153: INFO: Service rm3 in namespace kubectl-7644 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:44:20.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7644" for this suite.
Oct  4 11:44:38.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:44:38.311: INFO: namespace kubectl-7644 deletion completed in 18.145419549s

• [SLOW TEST:24.774 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:44:38.313: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-ebc1751f-8dde-4278-b265-735a137fb8da
STEP: Creating a pod to test consume configMaps
Oct  4 11:44:38.369: INFO: Waiting up to 5m0s for pod "pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d" in namespace "configmap-7233" to be "success or failure"
Oct  4 11:44:38.377: INFO: Pod "pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.447502ms
Oct  4 11:44:40.382: INFO: Pod "pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013303644s
STEP: Saw pod success
Oct  4 11:44:40.382: INFO: Pod "pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d" satisfied condition "success or failure"
Oct  4 11:44:40.386: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:44:40.418: INFO: Waiting for pod pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d to disappear
Oct  4 11:44:40.424: INFO: Pod pod-configmaps-43ee79f9-3b83-4dd6-9b89-fbc482f6706d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:44:40.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7233" for this suite.
Oct  4 11:44:46.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:44:46.590: INFO: namespace configmap-7233 deletion completed in 6.160044598s

• [SLOW TEST:8.277 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:44:46.591: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-48c90252-8e80-4509-9803-d31b0931b062
STEP: Creating a pod to test consume configMaps
Oct  4 11:44:46.647: INFO: Waiting up to 5m0s for pod "pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9" in namespace "configmap-4426" to be "success or failure"
Oct  4 11:44:46.655: INFO: Pod "pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.828859ms
Oct  4 11:44:48.660: INFO: Pod "pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012371346s
STEP: Saw pod success
Oct  4 11:44:48.660: INFO: Pod "pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9" satisfied condition "success or failure"
Oct  4 11:44:48.670: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9 container configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 11:44:48.701: INFO: Waiting for pod pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9 to disappear
Oct  4 11:44:48.704: INFO: Pod pod-configmaps-dce0d76c-7149-419b-a23d-9af2382882f9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:44:48.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4426" for this suite.
Oct  4 11:44:54.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:44:54.855: INFO: namespace configmap-4426 deletion completed in 6.145748455s

• [SLOW TEST:8.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:44:54.856: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-rbxd
STEP: Creating a pod to test atomic-volume-subpath
Oct  4 11:44:54.919: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rbxd" in namespace "subpath-9154" to be "success or failure"
Oct  4 11:44:54.931: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.095088ms
Oct  4 11:44:56.935: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016430245s
Oct  4 11:44:58.940: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 4.021009258s
Oct  4 11:45:00.945: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 6.025951691s
Oct  4 11:45:02.950: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 8.030973308s
Oct  4 11:45:04.954: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 10.03559168s
Oct  4 11:45:06.962: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 12.043094542s
Oct  4 11:45:08.967: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 14.048056947s
Oct  4 11:45:10.972: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 16.052976727s
Oct  4 11:45:12.976: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 18.057291707s
Oct  4 11:45:14.984: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 20.06527307s
Oct  4 11:45:16.991: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Running", Reason="", readiness=true. Elapsed: 22.071960116s
Oct  4 11:45:18.996: INFO: Pod "pod-subpath-test-configmap-rbxd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.076875313s
STEP: Saw pod success
Oct  4 11:45:18.996: INFO: Pod "pod-subpath-test-configmap-rbxd" satisfied condition "success or failure"
Oct  4 11:45:19.000: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-configmap-rbxd container test-container-subpath-configmap-rbxd: <nil>
STEP: delete the pod
Oct  4 11:45:19.029: INFO: Waiting for pod pod-subpath-test-configmap-rbxd to disappear
Oct  4 11:45:19.033: INFO: Pod pod-subpath-test-configmap-rbxd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rbxd
Oct  4 11:45:19.033: INFO: Deleting pod "pod-subpath-test-configmap-rbxd" in namespace "subpath-9154"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:45:19.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9154" for this suite.
Oct  4 11:45:25.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:45:25.197: INFO: namespace subpath-9154 deletion completed in 6.153677663s

• [SLOW TEST:30.341 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:45:25.197: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-h49rr in namespace proxy-5510
I1004 11:45:25.255938      15 runners.go:180] Created replication controller with name: proxy-service-h49rr, namespace: proxy-5510, replica count: 1
I1004 11:45:26.307161      15 runners.go:180] proxy-service-h49rr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1004 11:45:27.307338      15 runners.go:180] proxy-service-h49rr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1004 11:45:28.307543      15 runners.go:180] proxy-service-h49rr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1004 11:45:29.307761      15 runners.go:180] proxy-service-h49rr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1004 11:45:30.307920      15 runners.go:180] proxy-service-h49rr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct  4 11:45:30.312: INFO: setup took 5.081132746s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Oct  4 11:45:30.322: INFO: (0) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 9.864727ms)
Oct  4 11:45:30.323: INFO: (0) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 10.020283ms)
Oct  4 11:45:30.323: INFO: (0) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.08592ms)
Oct  4 11:45:30.323: INFO: (0) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 10.626287ms)
Oct  4 11:45:30.323: INFO: (0) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.233117ms)
Oct  4 11:45:30.328: INFO: (0) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 15.263805ms)
Oct  4 11:45:30.329: INFO: (0) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 16.298983ms)
Oct  4 11:45:30.329: INFO: (0) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 16.315323ms)
Oct  4 11:45:30.340: INFO: (0) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 26.683236ms)
Oct  4 11:45:30.341: INFO: (0) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 28.40796ms)
Oct  4 11:45:30.341: INFO: (0) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 28.101785ms)
Oct  4 11:45:30.341: INFO: (0) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 28.217977ms)
Oct  4 11:45:30.341: INFO: (0) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 28.614931ms)
Oct  4 11:45:30.342: INFO: (0) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 29.184321ms)
Oct  4 11:45:30.342: INFO: (0) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 29.642811ms)
Oct  4 11:45:30.343: INFO: (0) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 30.322492ms)
Oct  4 11:45:30.352: INFO: (1) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 8.953388ms)
Oct  4 11:45:30.352: INFO: (1) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 8.79029ms)
Oct  4 11:45:30.352: INFO: (1) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 8.879762ms)
Oct  4 11:45:30.352: INFO: (1) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 9.23802ms)
Oct  4 11:45:30.353: INFO: (1) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 9.28554ms)
Oct  4 11:45:30.355: INFO: (1) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 12.166656ms)
Oct  4 11:45:30.356: INFO: (1) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 12.148932ms)
Oct  4 11:45:30.356: INFO: (1) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 12.118168ms)
Oct  4 11:45:30.356: INFO: (1) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 12.529582ms)
Oct  4 11:45:30.356: INFO: (1) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.721194ms)
Oct  4 11:45:30.356: INFO: (1) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 12.737458ms)
Oct  4 11:45:30.356: INFO: (1) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 12.911005ms)
Oct  4 11:45:30.358: INFO: (1) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 14.312165ms)
Oct  4 11:45:30.359: INFO: (1) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 15.916777ms)
Oct  4 11:45:30.360: INFO: (1) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 16.175193ms)
Oct  4 11:45:30.360: INFO: (1) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 16.396616ms)
Oct  4 11:45:30.365: INFO: (2) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 4.979891ms)
Oct  4 11:45:30.369: INFO: (2) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 8.561766ms)
Oct  4 11:45:30.369: INFO: (2) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 8.792561ms)
Oct  4 11:45:30.370: INFO: (2) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 9.621376ms)
Oct  4 11:45:30.373: INFO: (2) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 12.43513ms)
Oct  4 11:45:30.374: INFO: (2) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 13.697565ms)
Oct  4 11:45:30.374: INFO: (2) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 13.258284ms)
Oct  4 11:45:30.374: INFO: (2) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 13.348041ms)
Oct  4 11:45:30.374: INFO: (2) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 13.931904ms)
Oct  4 11:45:30.374: INFO: (2) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 13.649619ms)
Oct  4 11:45:30.375: INFO: (2) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 14.175ms)
Oct  4 11:45:30.376: INFO: (2) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 15.078535ms)
Oct  4 11:45:30.376: INFO: (2) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 15.399413ms)
Oct  4 11:45:30.376: INFO: (2) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 16.268694ms)
Oct  4 11:45:30.376: INFO: (2) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.148289ms)
Oct  4 11:45:30.377: INFO: (2) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 16.172501ms)
Oct  4 11:45:30.386: INFO: (3) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 9.272614ms)
Oct  4 11:45:30.390: INFO: (3) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.911612ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 13.386029ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 13.376717ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 13.432041ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 14.195675ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 14.363375ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 13.597882ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 13.91729ms)
Oct  4 11:45:30.391: INFO: (3) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 13.760108ms)
Oct  4 11:45:30.392: INFO: (3) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 14.332504ms)
Oct  4 11:45:30.392: INFO: (3) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 15.232454ms)
Oct  4 11:45:30.396: INFO: (3) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 18.400629ms)
Oct  4 11:45:30.398: INFO: (3) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 20.115778ms)
Oct  4 11:45:30.398: INFO: (3) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 20.169117ms)
Oct  4 11:45:30.398: INFO: (3) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 20.45229ms)
Oct  4 11:45:30.411: INFO: (4) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.985473ms)
Oct  4 11:45:30.411: INFO: (4) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 13.074258ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 13.364166ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 13.437759ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 14.196835ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 13.895587ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 13.772965ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 14.298965ms)
Oct  4 11:45:30.412: INFO: (4) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 14.044258ms)
Oct  4 11:45:30.413: INFO: (4) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 14.480237ms)
Oct  4 11:45:30.414: INFO: (4) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 15.84043ms)
Oct  4 11:45:30.417: INFO: (4) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 18.680745ms)
Oct  4 11:45:30.417: INFO: (4) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 18.889813ms)
Oct  4 11:45:30.417: INFO: (4) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 19.116474ms)
Oct  4 11:45:30.418: INFO: (4) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 19.103124ms)
Oct  4 11:45:30.418: INFO: (4) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 19.08904ms)
Oct  4 11:45:30.425: INFO: (5) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 6.833389ms)
Oct  4 11:45:30.429: INFO: (5) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 10.873424ms)
Oct  4 11:45:30.429: INFO: (5) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.873817ms)
Oct  4 11:45:30.429: INFO: (5) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.164417ms)
Oct  4 11:45:30.430: INFO: (5) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.234273ms)
Oct  4 11:45:30.430: INFO: (5) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 11.323444ms)
Oct  4 11:45:30.430: INFO: (5) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 12.340075ms)
Oct  4 11:45:30.430: INFO: (5) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 11.960033ms)
Oct  4 11:45:30.430: INFO: (5) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 11.741081ms)
Oct  4 11:45:30.431: INFO: (5) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 11.965717ms)
Oct  4 11:45:30.434: INFO: (5) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 15.349898ms)
Oct  4 11:45:30.436: INFO: (5) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 18.197857ms)
Oct  4 11:45:30.436: INFO: (5) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 18.445458ms)
Oct  4 11:45:30.437: INFO: (5) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 18.241749ms)
Oct  4 11:45:30.437: INFO: (5) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 18.091825ms)
Oct  4 11:45:30.437: INFO: (5) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 18.671134ms)
Oct  4 11:45:30.444: INFO: (6) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 6.92747ms)
Oct  4 11:45:30.449: INFO: (6) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 11.57378ms)
Oct  4 11:45:30.450: INFO: (6) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 11.859744ms)
Oct  4 11:45:30.450: INFO: (6) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 12.001348ms)
Oct  4 11:45:30.450: INFO: (6) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 12.531268ms)
Oct  4 11:45:30.450: INFO: (6) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 12.983792ms)
Oct  4 11:45:30.450: INFO: (6) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 12.910785ms)
Oct  4 11:45:30.450: INFO: (6) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 12.747253ms)
Oct  4 11:45:30.451: INFO: (6) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 13.103046ms)
Oct  4 11:45:30.451: INFO: (6) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 13.326261ms)
Oct  4 11:45:30.454: INFO: (6) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 16.476532ms)
Oct  4 11:45:30.457: INFO: (6) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 19.122596ms)
Oct  4 11:45:30.457: INFO: (6) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 19.607433ms)
Oct  4 11:45:30.457: INFO: (6) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 20.158344ms)
Oct  4 11:45:30.457: INFO: (6) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 19.596443ms)
Oct  4 11:45:30.458: INFO: (6) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 19.91724ms)
Oct  4 11:45:30.465: INFO: (7) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 7.370806ms)
Oct  4 11:45:30.466: INFO: (7) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 7.953995ms)
Oct  4 11:45:30.471: INFO: (7) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 12.499493ms)
Oct  4 11:45:30.471: INFO: (7) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 12.868686ms)
Oct  4 11:45:30.471: INFO: (7) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 12.993227ms)
Oct  4 11:45:30.471: INFO: (7) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 13.102781ms)
Oct  4 11:45:30.471: INFO: (7) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 13.296254ms)
Oct  4 11:45:30.472: INFO: (7) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 13.722213ms)
Oct  4 11:45:30.472: INFO: (7) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 14.514639ms)
Oct  4 11:45:30.473: INFO: (7) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 15.050007ms)
Oct  4 11:45:30.473: INFO: (7) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 15.045923ms)
Oct  4 11:45:30.473: INFO: (7) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 15.370164ms)
Oct  4 11:45:30.474: INFO: (7) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 15.616357ms)
Oct  4 11:45:30.474: INFO: (7) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 15.865918ms)
Oct  4 11:45:30.474: INFO: (7) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 16.012696ms)
Oct  4 11:45:30.474: INFO: (7) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 16.090947ms)
Oct  4 11:45:30.482: INFO: (8) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 7.779243ms)
Oct  4 11:45:30.484: INFO: (8) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 9.200839ms)
Oct  4 11:45:30.484: INFO: (8) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 9.585196ms)
Oct  4 11:45:30.484: INFO: (8) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 9.793932ms)
Oct  4 11:45:30.485: INFO: (8) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 10.9632ms)
Oct  4 11:45:30.486: INFO: (8) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 11.209241ms)
Oct  4 11:45:30.486: INFO: (8) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 11.389637ms)
Oct  4 11:45:30.486: INFO: (8) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 11.418007ms)
Oct  4 11:45:30.489: INFO: (8) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 15.148933ms)
Oct  4 11:45:30.490: INFO: (8) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 15.33749ms)
Oct  4 11:45:30.490: INFO: (8) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 15.291609ms)
Oct  4 11:45:30.490: INFO: (8) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 15.695819ms)
Oct  4 11:45:30.490: INFO: (8) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 15.54582ms)
Oct  4 11:45:30.490: INFO: (8) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 16.119615ms)
Oct  4 11:45:30.493: INFO: (8) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 18.467267ms)
Oct  4 11:45:30.495: INFO: (8) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 20.495635ms)
Oct  4 11:45:30.502: INFO: (9) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 6.356018ms)
Oct  4 11:45:30.508: INFO: (9) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 12.099737ms)
Oct  4 11:45:30.509: INFO: (9) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 13.469222ms)
Oct  4 11:45:30.509: INFO: (9) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 13.181526ms)
Oct  4 11:45:30.510: INFO: (9) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 14.058073ms)
Oct  4 11:45:30.510: INFO: (9) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 14.123345ms)
Oct  4 11:45:30.510: INFO: (9) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 14.571757ms)
Oct  4 11:45:30.511: INFO: (9) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 15.636352ms)
Oct  4 11:45:30.511: INFO: (9) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 15.703505ms)
Oct  4 11:45:30.512: INFO: (9) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 15.939496ms)
Oct  4 11:45:30.512: INFO: (9) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.14692ms)
Oct  4 11:45:30.512: INFO: (9) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 16.452081ms)
Oct  4 11:45:30.513: INFO: (9) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 16.860699ms)
Oct  4 11:45:30.513: INFO: (9) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 17.920764ms)
Oct  4 11:45:30.513: INFO: (9) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 17.268824ms)
Oct  4 11:45:30.514: INFO: (9) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 17.331146ms)
Oct  4 11:45:30.520: INFO: (10) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 5.7934ms)
Oct  4 11:45:30.521: INFO: (10) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 6.761127ms)
Oct  4 11:45:30.525: INFO: (10) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.372107ms)
Oct  4 11:45:30.525: INFO: (10) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 10.504989ms)
Oct  4 11:45:30.525: INFO: (10) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 10.656206ms)
Oct  4 11:45:30.525: INFO: (10) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.10211ms)
Oct  4 11:45:30.525: INFO: (10) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 11.383588ms)
Oct  4 11:45:30.526: INFO: (10) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 11.604286ms)
Oct  4 11:45:30.526: INFO: (10) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 12.286484ms)
Oct  4 11:45:30.527: INFO: (10) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.219893ms)
Oct  4 11:45:30.528: INFO: (10) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 14.303855ms)
Oct  4 11:45:30.531: INFO: (10) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 15.961202ms)
Oct  4 11:45:30.531: INFO: (10) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 16.949027ms)
Oct  4 11:45:30.531: INFO: (10) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 16.418137ms)
Oct  4 11:45:30.531: INFO: (10) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.912596ms)
Oct  4 11:45:30.531: INFO: (10) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 17.316345ms)
Oct  4 11:45:30.538: INFO: (11) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 6.339954ms)
Oct  4 11:45:30.542: INFO: (11) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 10.014258ms)
Oct  4 11:45:30.542: INFO: (11) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 10.389919ms)
Oct  4 11:45:30.543: INFO: (11) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 10.640683ms)
Oct  4 11:45:30.543: INFO: (11) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.916151ms)
Oct  4 11:45:30.543: INFO: (11) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.242358ms)
Oct  4 11:45:30.543: INFO: (11) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 10.905939ms)
Oct  4 11:45:30.543: INFO: (11) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 11.165998ms)
Oct  4 11:45:30.545: INFO: (11) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 13.080147ms)
Oct  4 11:45:30.545: INFO: (11) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 13.753466ms)
Oct  4 11:45:30.546: INFO: (11) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 13.373766ms)
Oct  4 11:45:30.546: INFO: (11) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 14.413159ms)
Oct  4 11:45:30.548: INFO: (11) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.243059ms)
Oct  4 11:45:30.550: INFO: (11) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 18.628964ms)
Oct  4 11:45:30.551: INFO: (11) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 18.579937ms)
Oct  4 11:45:30.551: INFO: (11) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 18.482606ms)
Oct  4 11:45:30.557: INFO: (12) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 5.828216ms)
Oct  4 11:45:30.559: INFO: (12) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 7.72105ms)
Oct  4 11:45:30.559: INFO: (12) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 7.514327ms)
Oct  4 11:45:30.560: INFO: (12) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 8.453078ms)
Oct  4 11:45:30.560: INFO: (12) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 7.89991ms)
Oct  4 11:45:30.560: INFO: (12) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 8.17053ms)
Oct  4 11:45:30.560: INFO: (12) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 9.034764ms)
Oct  4 11:45:30.563: INFO: (12) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 11.389773ms)
Oct  4 11:45:30.564: INFO: (12) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 11.348275ms)
Oct  4 11:45:30.564: INFO: (12) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 11.558134ms)
Oct  4 11:45:30.564: INFO: (12) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 12.609425ms)
Oct  4 11:45:30.565: INFO: (12) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 14.32114ms)
Oct  4 11:45:30.566: INFO: (12) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 14.280101ms)
Oct  4 11:45:30.567: INFO: (12) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 14.664799ms)
Oct  4 11:45:30.567: INFO: (12) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 14.850611ms)
Oct  4 11:45:30.567: INFO: (12) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 16.260101ms)
Oct  4 11:45:30.574: INFO: (13) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 6.364812ms)
Oct  4 11:45:30.577: INFO: (13) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 9.35327ms)
Oct  4 11:45:30.581: INFO: (13) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.903149ms)
Oct  4 11:45:30.581: INFO: (13) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 14.128382ms)
Oct  4 11:45:30.581: INFO: (13) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 13.452546ms)
Oct  4 11:45:30.582: INFO: (13) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 13.80034ms)
Oct  4 11:45:30.582: INFO: (13) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 13.774593ms)
Oct  4 11:45:30.582: INFO: (13) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 14.630022ms)
Oct  4 11:45:30.582: INFO: (13) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 14.566539ms)
Oct  4 11:45:30.582: INFO: (13) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 14.698353ms)
Oct  4 11:45:30.582: INFO: (13) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 14.262956ms)
Oct  4 11:45:30.583: INFO: (13) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 14.86228ms)
Oct  4 11:45:30.584: INFO: (13) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 16.631763ms)
Oct  4 11:45:30.585: INFO: (13) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 16.849695ms)
Oct  4 11:45:30.585: INFO: (13) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.903178ms)
Oct  4 11:45:30.585: INFO: (13) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 17.179638ms)
Oct  4 11:45:30.590: INFO: (14) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 4.965673ms)
Oct  4 11:45:30.597: INFO: (14) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 11.423738ms)
Oct  4 11:45:30.597: INFO: (14) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 11.58777ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 12.357821ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 12.510744ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 12.917653ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.589464ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 12.883745ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 12.875927ms)
Oct  4 11:45:30.598: INFO: (14) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 13.121646ms)
Oct  4 11:45:30.599: INFO: (14) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 13.043351ms)
Oct  4 11:45:30.601: INFO: (14) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 15.468292ms)
Oct  4 11:45:30.601: INFO: (14) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 15.767958ms)
Oct  4 11:45:30.601: INFO: (14) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 15.6867ms)
Oct  4 11:45:30.602: INFO: (14) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.339811ms)
Oct  4 11:45:30.602: INFO: (14) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 16.128174ms)
Oct  4 11:45:30.607: INFO: (15) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 5.606488ms)
Oct  4 11:45:30.610: INFO: (15) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 7.357014ms)
Oct  4 11:45:30.610: INFO: (15) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 7.486616ms)
Oct  4 11:45:30.613: INFO: (15) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 10.983365ms)
Oct  4 11:45:30.614: INFO: (15) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.650496ms)
Oct  4 11:45:30.614: INFO: (15) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 10.602638ms)
Oct  4 11:45:30.614: INFO: (15) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.032167ms)
Oct  4 11:45:30.614: INFO: (15) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 11.043314ms)
Oct  4 11:45:30.614: INFO: (15) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 11.006441ms)
Oct  4 11:45:30.614: INFO: (15) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.080757ms)
Oct  4 11:45:30.615: INFO: (15) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 12.743977ms)
Oct  4 11:45:30.615: INFO: (15) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 13.235598ms)
Oct  4 11:45:30.621: INFO: (15) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 17.860402ms)
Oct  4 11:45:30.621: INFO: (15) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 17.974788ms)
Oct  4 11:45:30.621: INFO: (15) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 18.017924ms)
Oct  4 11:45:30.621: INFO: (15) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 18.415525ms)
Oct  4 11:45:30.628: INFO: (16) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 7.238673ms)
Oct  4 11:45:30.631: INFO: (16) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 9.954863ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 10.282425ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 10.351678ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.867987ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 10.719529ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 10.708786ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 10.691143ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 11.208912ms)
Oct  4 11:45:30.632: INFO: (16) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 11.340482ms)
Oct  4 11:45:30.636: INFO: (16) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 15.112217ms)
Oct  4 11:45:30.636: INFO: (16) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 15.205632ms)
Oct  4 11:45:30.637: INFO: (16) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 15.58624ms)
Oct  4 11:45:30.638: INFO: (16) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.775589ms)
Oct  4 11:45:30.639: INFO: (16) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 17.24675ms)
Oct  4 11:45:30.639: INFO: (16) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 17.185179ms)
Oct  4 11:45:30.647: INFO: (17) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 7.929229ms)
Oct  4 11:45:30.651: INFO: (17) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 11.354159ms)
Oct  4 11:45:30.651: INFO: (17) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 11.38207ms)
Oct  4 11:45:30.651: INFO: (17) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 12.130951ms)
Oct  4 11:45:30.652: INFO: (17) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 12.172542ms)
Oct  4 11:45:30.652: INFO: (17) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 12.031312ms)
Oct  4 11:45:30.652: INFO: (17) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 12.449527ms)
Oct  4 11:45:30.652: INFO: (17) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 12.573574ms)
Oct  4 11:45:30.652: INFO: (17) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 12.684916ms)
Oct  4 11:45:30.652: INFO: (17) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 12.536338ms)
Oct  4 11:45:30.653: INFO: (17) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 14.210432ms)
Oct  4 11:45:30.656: INFO: (17) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 16.847535ms)
Oct  4 11:45:30.656: INFO: (17) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 16.744156ms)
Oct  4 11:45:30.657: INFO: (17) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 17.683952ms)
Oct  4 11:45:30.657: INFO: (17) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 17.659599ms)
Oct  4 11:45:30.657: INFO: (17) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 17.693927ms)
Oct  4 11:45:30.663: INFO: (18) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 5.812669ms)
Oct  4 11:45:30.666: INFO: (18) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 8.132821ms)
Oct  4 11:45:30.666: INFO: (18) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 7.979546ms)
Oct  4 11:45:30.666: INFO: (18) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 8.642607ms)
Oct  4 11:45:30.666: INFO: (18) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 8.694993ms)
Oct  4 11:45:30.666: INFO: (18) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 8.623952ms)
Oct  4 11:45:30.669: INFO: (18) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 10.880425ms)
Oct  4 11:45:30.669: INFO: (18) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 11.389692ms)
Oct  4 11:45:30.669: INFO: (18) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 11.293437ms)
Oct  4 11:45:30.669: INFO: (18) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 11.514203ms)
Oct  4 11:45:30.670: INFO: (18) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 13.128413ms)
Oct  4 11:45:30.670: INFO: (18) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 13.006019ms)
Oct  4 11:45:30.672: INFO: (18) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 14.181182ms)
Oct  4 11:45:30.674: INFO: (18) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 16.220781ms)
Oct  4 11:45:30.674: INFO: (18) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 16.506151ms)
Oct  4 11:45:30.674: INFO: (18) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 16.426787ms)
Oct  4 11:45:30.682: INFO: (19) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh/proxy/rewriteme">test</a> (200; 7.62071ms)
Oct  4 11:45:30.683: INFO: (19) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:460/proxy/: tls baz (200; 7.661281ms)
Oct  4 11:45:30.683: INFO: (19) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:443/proxy/tlsrewritem... (200; 7.871707ms)
Oct  4 11:45:30.683: INFO: (19) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">... (200; 7.711876ms)
Oct  4 11:45:30.683: INFO: (19) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 7.769973ms)
Oct  4 11:45:30.686: INFO: (19) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 10.304825ms)
Oct  4 11:45:30.686: INFO: (19) /api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/: <a href="/api/v1/namespaces/proxy-5510/pods/proxy-service-h49rr-4jmhh:1080/proxy/rewriteme">test<... (200; 10.370569ms)
Oct  4 11:45:30.686: INFO: (19) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:162/proxy/: bar (200; 10.561881ms)
Oct  4 11:45:30.686: INFO: (19) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname2/proxy/: tls qux (200; 11.894609ms)
Oct  4 11:45:30.686: INFO: (19) /api/v1/namespaces/proxy-5510/pods/https:proxy-service-h49rr-4jmhh:462/proxy/: tls qux (200; 10.708695ms)
Oct  4 11:45:30.686: INFO: (19) /api/v1/namespaces/proxy-5510/pods/http:proxy-service-h49rr-4jmhh:160/proxy/: foo (200; 10.943635ms)
Oct  4 11:45:30.691: INFO: (19) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname2/proxy/: bar (200; 15.407556ms)
Oct  4 11:45:30.691: INFO: (19) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname2/proxy/: bar (200; 15.823237ms)
Oct  4 11:45:30.691: INFO: (19) /api/v1/namespaces/proxy-5510/services/proxy-service-h49rr:portname1/proxy/: foo (200; 15.960607ms)
Oct  4 11:45:30.691: INFO: (19) /api/v1/namespaces/proxy-5510/services/https:proxy-service-h49rr:tlsportname1/proxy/: tls baz (200; 16.564493ms)
Oct  4 11:45:30.691: INFO: (19) /api/v1/namespaces/proxy-5510/services/http:proxy-service-h49rr:portname1/proxy/: foo (200; 16.798627ms)
STEP: deleting ReplicationController proxy-service-h49rr in namespace proxy-5510, will wait for the garbage collector to delete the pods
Oct  4 11:45:30.755: INFO: Deleting ReplicationController proxy-service-h49rr took: 9.828857ms
Oct  4 11:45:31.055: INFO: Terminating ReplicationController proxy-service-h49rr pods took: 300.230248ms
[AfterEach] version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:45:42.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5510" for this suite.
Oct  4 11:45:48.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:45:48.300: INFO: namespace proxy-5510 deletion completed in 6.138069867s

• [SLOW TEST:23.103 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:45:48.301: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:45:48.339: INFO: Creating deployment "test-recreate-deployment"
Oct  4 11:45:48.349: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Oct  4 11:45:48.362: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Oct  4 11:45:50.370: INFO: Waiting deployment "test-recreate-deployment" to complete
Oct  4 11:45:50.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786348, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786348, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:45:52.379: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Oct  4 11:45:52.389: INFO: Updating deployment test-recreate-deployment
Oct  4 11:45:52.389: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Oct  4 11:45:52.464: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9096,SelfLink:/apis/apps/v1/namespaces/deployment-9096/deployments/test-recreate-deployment,UID:00f55f3c-700c-4d3b-af50-5b1572006de1,ResourceVersion:27708,Generation:2,CreationTimestamp:2019-10-04 11:45:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-10-04 11:45:52 +0000 UTC 2019-10-04 11:45:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-10-04 11:45:52 +0000 UTC 2019-10-04 11:45:48 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Oct  4 11:45:52.469: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9096,SelfLink:/apis/apps/v1/namespaces/deployment-9096/replicasets/test-recreate-deployment-5c8c9cc69d,UID:cd1c0581-287d-45fd-bc69-7c76cd2539dd,ResourceVersion:27706,Generation:1,CreationTimestamp:2019-10-04 11:45:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 00f55f3c-700c-4d3b-af50-5b1572006de1 0xc002d99e57 0xc002d99e58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:45:52.469: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Oct  4 11:45:52.469: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9096,SelfLink:/apis/apps/v1/namespaces/deployment-9096/replicasets/test-recreate-deployment-6df85df6b9,UID:7d692491-3230-4296-b291-eaaef52fd57c,ResourceVersion:27697,Generation:2,CreationTimestamp:2019-10-04 11:45:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 00f55f3c-700c-4d3b-af50-5b1572006de1 0xc002d99f27 0xc002d99f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:45:52.473: INFO: Pod "test-recreate-deployment-5c8c9cc69d-lxnd4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-lxnd4,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9096,SelfLink:/api/v1/namespaces/deployment-9096/pods/test-recreate-deployment-5c8c9cc69d-lxnd4,UID:db117e3c-1526-4312-a514-d893a0426afa,ResourceVersion:27709,Generation:0,CreationTimestamp:2019-10-04 11:45:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d cd1c0581-287d-45fd-bc69-7c76cd2539dd 0xc002986847 0xc002986848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vpbs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vpbs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-vpbs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029868c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029868e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:45:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:45:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:45:52 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.18,PodIP:,StartTime:2019-10-04 11:45:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:45:52.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9096" for this suite.
Oct  4 11:45:58.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:45:58.629: INFO: namespace deployment-9096 deletion completed in 6.149295102s

• [SLOW TEST:10.328 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:45:58.629: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8df8f238-41a1-4074-a2bd-f03524532b11
STEP: Creating a pod to test consume secrets
Oct  4 11:45:58.682: INFO: Waiting up to 5m0s for pod "pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560" in namespace "secrets-6139" to be "success or failure"
Oct  4 11:45:58.687: INFO: Pod "pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560": Phase="Pending", Reason="", readiness=false. Elapsed: 4.783699ms
Oct  4 11:46:00.692: INFO: Pod "pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009949119s
Oct  4 11:46:02.697: INFO: Pod "pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014534497s
STEP: Saw pod success
Oct  4 11:46:02.697: INFO: Pod "pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560" satisfied condition "success or failure"
Oct  4 11:46:02.701: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560 container secret-env-test: <nil>
STEP: delete the pod
Oct  4 11:46:02.729: INFO: Waiting for pod pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560 to disappear
Oct  4 11:46:02.734: INFO: Pod pod-secrets-4e7c415b-6dc9-4703-bce3-2419f182d560 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:46:02.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6139" for this suite.
Oct  4 11:46:08.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:46:08.881: INFO: namespace secrets-6139 deletion completed in 6.142254297s

• [SLOW TEST:10.252 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:46:08.883: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:46:08.948: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Oct  4 11:46:08.964: INFO: Number of nodes with available pods: 0
Oct  4 11:46:08.964: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Oct  4 11:46:08.991: INFO: Number of nodes with available pods: 0
Oct  4 11:46:08.991: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:09.995: INFO: Number of nodes with available pods: 0
Oct  4 11:46:09.995: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:10.996: INFO: Number of nodes with available pods: 0
Oct  4 11:46:10.996: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:11.996: INFO: Number of nodes with available pods: 1
Oct  4 11:46:11.996: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Oct  4 11:46:12.015: INFO: Number of nodes with available pods: 1
Oct  4 11:46:12.015: INFO: Number of running nodes: 0, number of available pods: 1
Oct  4 11:46:13.021: INFO: Number of nodes with available pods: 0
Oct  4 11:46:13.021: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Oct  4 11:46:13.044: INFO: Number of nodes with available pods: 0
Oct  4 11:46:13.044: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:14.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:14.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:15.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:15.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:16.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:16.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:17.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:17.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:18.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:18.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:19.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:19.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:20.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:20.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:21.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:21.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:22.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:22.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:23.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:23.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:24.049: INFO: Number of nodes with available pods: 0
Oct  4 11:46:24.049: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:46:25.049: INFO: Number of nodes with available pods: 1
Oct  4 11:46:25.049: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8714, will wait for the garbage collector to delete the pods
Oct  4 11:46:25.121: INFO: Deleting DaemonSet.extensions daemon-set took: 10.976144ms
Oct  4 11:46:25.422: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.389854ms
Oct  4 11:46:32.128: INFO: Number of nodes with available pods: 0
Oct  4 11:46:32.128: INFO: Number of running nodes: 0, number of available pods: 0
Oct  4 11:46:32.131: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8714/daemonsets","resourceVersion":"27948"},"items":null}

Oct  4 11:46:32.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8714/pods","resourceVersion":"27948"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:46:32.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8714" for this suite.
Oct  4 11:46:38.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:46:38.313: INFO: namespace daemonsets-8714 deletion completed in 6.146239599s

• [SLOW TEST:29.430 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:46:38.313: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Oct  4 11:46:42.380: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-245756238 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Oct  4 11:46:52.459: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:46:52.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4304" for this suite.
Oct  4 11:46:58.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:46:58.629: INFO: namespace pods-4304 deletion completed in 6.16132546s

• [SLOW TEST:20.316 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:46:58.631: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-9718cced-bb10-46c2-89ae-5f8c8b882e0c in namespace container-probe-6709
Oct  4 11:47:02.690: INFO: Started pod liveness-9718cced-bb10-46c2-89ae-5f8c8b882e0c in namespace container-probe-6709
STEP: checking the pod's current state and verifying that restartCount is present
Oct  4 11:47:02.694: INFO: Initial restart count of pod liveness-9718cced-bb10-46c2-89ae-5f8c8b882e0c is 0
Oct  4 11:47:24.757: INFO: Restart count of pod container-probe-6709/liveness-9718cced-bb10-46c2-89ae-5f8c8b882e0c is now 1 (22.063013507s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:47:24.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6709" for this suite.
Oct  4 11:47:30.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:47:30.932: INFO: namespace container-probe-6709 deletion completed in 6.151934587s

• [SLOW TEST:32.301 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:47:30.933: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-cb86d9d2-1306-4726-b8eb-c0cda38d4ba2
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:47:30.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7653" for this suite.
Oct  4 11:47:36.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:47:37.126: INFO: namespace configmap-7653 deletion completed in 6.152472923s

• [SLOW TEST:6.194 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:47:37.129: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-c913a43e-0ddb-4ec9-9e65-b58d4a4accdb in namespace container-probe-318
Oct  4 11:47:41.193: INFO: Started pod busybox-c913a43e-0ddb-4ec9-9e65-b58d4a4accdb in namespace container-probe-318
STEP: checking the pod's current state and verifying that restartCount is present
Oct  4 11:47:41.199: INFO: Initial restart count of pod busybox-c913a43e-0ddb-4ec9-9e65-b58d4a4accdb is 0
Oct  4 11:48:27.319: INFO: Restart count of pod container-probe-318/busybox-c913a43e-0ddb-4ec9-9e65-b58d4a4accdb is now 1 (46.119213481s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:48:27.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-318" for this suite.
Oct  4 11:48:33.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:48:33.483: INFO: namespace container-probe-318 deletion completed in 6.141061959s

• [SLOW TEST:56.355 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:48:33.486: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Oct  4 11:48:33.541: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Oct  4 11:48:33.551: INFO: Waiting for terminating namespaces to be deleted...
Oct  4 11:48:33.555: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-1 before test
Oct  4 11:48:33.563: INFO: calico-node-s4qhf from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.563: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 11:48:33.563: INFO: kube-proxy-xvmz9 from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.563: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 11:48:33.563: INFO: nginx-proxy-lab1-k8s-node-1 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.563: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 11:48:33.563: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-njvm2 from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 11:48:33.563: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct  4 11:48:33.563: INFO: 	Container systemd-logs ready: true, restart count 1
Oct  4 11:48:33.563: INFO: calico-kube-controllers-5b485dc7c7-d4x2r from kube-system started at 2019-10-04 10:22:36 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.563: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Oct  4 11:48:33.563: INFO: nodelocaldns-9rl9p from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.563: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 11:48:33.563: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-2 before test
Oct  4 11:48:33.573: INFO: calico-node-zrmvj from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.573: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 11:48:33.573: INFO: sonobuoy from heptio-sonobuoy started at 2019-10-04 10:37:02 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.573: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Oct  4 11:48:33.573: INFO: nodelocaldns-5kkm7 from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.573: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 11:48:33.573: INFO: nginx-proxy-lab1-k8s-node-2 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.573: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 11:48:33.574: INFO: kube-proxy-bpwhk from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.574: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 11:48:33.574: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-vxftt from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 11:48:33.574: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct  4 11:48:33.574: INFO: 	Container systemd-logs ready: true, restart count 1
Oct  4 11:48:33.574: INFO: 
Logging pods the kubelet thinks is on node lab1-k8s-node-3 before test
Oct  4 11:48:33.583: INFO: nodelocaldns-f9sqh from kube-system started at 2019-10-04 10:23:29 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.583: INFO: 	Container node-cache ready: true, restart count 0
Oct  4 11:48:33.583: INFO: calico-node-cqqg4 from kube-system started at 2019-10-04 10:20:59 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.583: INFO: 	Container calico-node ready: true, restart count 1
Oct  4 11:48:33.583: INFO: nginx-proxy-lab1-k8s-node-3 from kube-system started at 2019-10-04 10:20:06 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.583: INFO: 	Container nginx-proxy ready: true, restart count 0
Oct  4 11:48:33.584: INFO: kube-proxy-nt6kv from kube-system started at 2019-10-04 10:20:12 +0000 UTC (1 container statuses recorded)
Oct  4 11:48:33.584: INFO: 	Container kube-proxy ready: true, restart count 0
Oct  4 11:48:33.584: INFO: sonobuoy-e2e-job-55aaa13fda1d4ddb from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 11:48:33.584: INFO: 	Container e2e ready: true, restart count 0
Oct  4 11:48:33.584: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Oct  4 11:48:33.584: INFO: sonobuoy-systemd-logs-daemon-set-52b773e387094a1e-rsvwx from heptio-sonobuoy started at 2019-10-04 10:37:07 +0000 UTC (2 container statuses recorded)
Oct  4 11:48:33.584: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Oct  4 11:48:33.584: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9f48b7c3-6144-4444-a2c4-67ab0d04041b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9f48b7c3-6144-4444-a2c4-67ab0d04041b off the node lab1-k8s-node-1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9f48b7c3-6144-4444-a2c4-67ab0d04041b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:48:39.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1491" for this suite.
Oct  4 11:48:57.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:48:57.836: INFO: namespace sched-pred-1491 deletion completed in 18.143830092s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:24.350 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:48:57.836: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Oct  4 11:48:57.888: INFO: Waiting up to 5m0s for pod "downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c" in namespace "downward-api-9648" to be "success or failure"
Oct  4 11:48:57.893: INFO: Pod "downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.411126ms
Oct  4 11:48:59.897: INFO: Pod "downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009359049s
Oct  4 11:49:01.902: INFO: Pod "downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014291364s
STEP: Saw pod success
Oct  4 11:49:01.902: INFO: Pod "downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c" satisfied condition "success or failure"
Oct  4 11:49:01.906: INFO: Trying to get logs from node lab1-k8s-node-1 pod downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c container dapi-container: <nil>
STEP: delete the pod
Oct  4 11:49:01.934: INFO: Waiting for pod downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c to disappear
Oct  4 11:49:01.938: INFO: Pod downward-api-f0052990-aeee-45b6-8a47-a472f0b4443c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:49:01.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9648" for this suite.
Oct  4 11:49:07.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:49:08.089: INFO: namespace downward-api-9648 deletion completed in 6.145443201s

• [SLOW TEST:10.252 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:49:08.091: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Oct  4 11:49:08.147: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-295,SelfLink:/api/v1/namespaces/watch-295/configmaps/e2e-watch-test-watch-closed,UID:af33205e-3156-4823-b06c-dd3f89ab1b42,ResourceVersion:28599,Generation:0,CreationTimestamp:2019-10-04 11:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Oct  4 11:49:08.147: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-295,SelfLink:/api/v1/namespaces/watch-295/configmaps/e2e-watch-test-watch-closed,UID:af33205e-3156-4823-b06c-dd3f89ab1b42,ResourceVersion:28600,Generation:0,CreationTimestamp:2019-10-04 11:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Oct  4 11:49:08.167: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-295,SelfLink:/api/v1/namespaces/watch-295/configmaps/e2e-watch-test-watch-closed,UID:af33205e-3156-4823-b06c-dd3f89ab1b42,ResourceVersion:28603,Generation:0,CreationTimestamp:2019-10-04 11:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Oct  4 11:49:08.168: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-295,SelfLink:/api/v1/namespaces/watch-295/configmaps/e2e-watch-test-watch-closed,UID:af33205e-3156-4823-b06c-dd3f89ab1b42,ResourceVersion:28604,Generation:0,CreationTimestamp:2019-10-04 11:49:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:49:08.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-295" for this suite.
Oct  4 11:49:14.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:49:14.317: INFO: namespace watch-295 deletion completed in 6.143669875s

• [SLOW TEST:6.226 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:49:14.317: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:49:14.364: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d" in namespace "downward-api-1985" to be "success or failure"
Oct  4 11:49:14.375: INFO: Pod "downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.367824ms
Oct  4 11:49:16.381: INFO: Pod "downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016683352s
STEP: Saw pod success
Oct  4 11:49:16.381: INFO: Pod "downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d" satisfied condition "success or failure"
Oct  4 11:49:16.387: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d container client-container: <nil>
STEP: delete the pod
Oct  4 11:49:16.416: INFO: Waiting for pod downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d to disappear
Oct  4 11:49:16.420: INFO: Pod downwardapi-volume-f984d680-e55f-418c-b250-5ad35de7785d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:49:16.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1985" for this suite.
Oct  4 11:49:22.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:49:22.576: INFO: namespace downward-api-1985 deletion completed in 6.150590991s

• [SLOW TEST:8.258 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:49:22.580: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3537
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Oct  4 11:49:22.614: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Oct  4 11:49:44.730: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3537 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:49:44.730: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:49:45.909: INFO: Found all expected endpoints: [netserver-0]
Oct  4 11:49:45.914: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.74.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3537 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:49:45.914: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:49:47.082: INFO: Found all expected endpoints: [netserver-1]
Oct  4 11:49:47.088: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.95.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3537 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 11:49:47.088: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 11:49:48.249: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:49:48.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3537" for this suite.
Oct  4 11:50:10.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:50:10.400: INFO: namespace pod-network-test-3537 deletion completed in 22.144909539s

• [SLOW TEST:47.820 seconds]
[sig-network] Networking
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:50:10.402: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-mjb8
STEP: Creating a pod to test atomic-volume-subpath
Oct  4 11:50:10.465: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mjb8" in namespace "subpath-849" to be "success or failure"
Oct  4 11:50:10.470: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.214035ms
Oct  4 11:50:12.474: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008863221s
Oct  4 11:50:14.480: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 4.014085018s
Oct  4 11:50:16.485: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 6.019318677s
Oct  4 11:50:18.489: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 8.023928193s
Oct  4 11:50:20.495: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 10.029399927s
Oct  4 11:50:22.500: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 12.03438155s
Oct  4 11:50:24.504: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 14.038594737s
Oct  4 11:50:26.509: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 16.043533232s
Oct  4 11:50:28.515: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 18.049048353s
Oct  4 11:50:30.519: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 20.05377787s
Oct  4 11:50:32.524: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Running", Reason="", readiness=true. Elapsed: 22.058989506s
Oct  4 11:50:34.533: INFO: Pod "pod-subpath-test-projected-mjb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.067294472s
STEP: Saw pod success
Oct  4 11:50:34.533: INFO: Pod "pod-subpath-test-projected-mjb8" satisfied condition "success or failure"
Oct  4 11:50:34.537: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-subpath-test-projected-mjb8 container test-container-subpath-projected-mjb8: <nil>
STEP: delete the pod
Oct  4 11:50:34.565: INFO: Waiting for pod pod-subpath-test-projected-mjb8 to disappear
Oct  4 11:50:34.569: INFO: Pod pod-subpath-test-projected-mjb8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-mjb8
Oct  4 11:50:34.569: INFO: Deleting pod "pod-subpath-test-projected-mjb8" in namespace "subpath-849"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:50:34.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-849" for this suite.
Oct  4 11:50:40.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:50:40.740: INFO: namespace subpath-849 deletion completed in 6.162868665s

• [SLOW TEST:30.339 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:50:40.740: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:50:42.832: INFO: Waiting up to 5m0s for pod "client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366" in namespace "pods-7274" to be "success or failure"
Oct  4 11:50:42.836: INFO: Pod "client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078812ms
Oct  4 11:50:44.842: INFO: Pod "client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010137592s
STEP: Saw pod success
Oct  4 11:50:44.842: INFO: Pod "client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366" satisfied condition "success or failure"
Oct  4 11:50:44.846: INFO: Trying to get logs from node lab1-k8s-node-1 pod client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366 container env3cont: <nil>
STEP: delete the pod
Oct  4 11:50:44.872: INFO: Waiting for pod client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366 to disappear
Oct  4 11:50:44.876: INFO: Pod client-envvars-6d8a412c-17ca-4ba7-9b12-262fe6583366 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:50:44.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7274" for this suite.
Oct  4 11:51:36.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:51:37.041: INFO: namespace pods-7274 deletion completed in 52.158981434s

• [SLOW TEST:56.300 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:51:37.044: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-92fce241-fdac-402f-b37e-79831ec2a888 in namespace container-probe-8635
Oct  4 11:51:41.109: INFO: Started pod busybox-92fce241-fdac-402f-b37e-79831ec2a888 in namespace container-probe-8635
STEP: checking the pod's current state and verifying that restartCount is present
Oct  4 11:51:41.113: INFO: Initial restart count of pod busybox-92fce241-fdac-402f-b37e-79831ec2a888 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:55:41.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8635" for this suite.
Oct  4 11:55:47.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:55:47.905: INFO: namespace container-probe-8635 deletion completed in 6.15214271s

• [SLOW TEST:250.861 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:55:47.905: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:55:47.952: INFO: Pod name rollover-pod: Found 0 pods out of 1
Oct  4 11:55:52.956: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct  4 11:55:52.957: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Oct  4 11:55:54.961: INFO: Creating deployment "test-rollover-deployment"
Oct  4 11:55:54.973: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Oct  4 11:55:56.983: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Oct  4 11:55:56.991: INFO: Ensure that both replica sets have 1 created replica
Oct  4 11:55:57.002: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Oct  4 11:55:57.012: INFO: Updating deployment test-rollover-deployment
Oct  4 11:55:57.012: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Oct  4 11:55:59.022: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Oct  4 11:55:59.030: INFO: Make sure deployment "test-rollover-deployment" is complete
Oct  4 11:55:59.039: INFO: all replica sets need to contain the pod-template-hash label
Oct  4 11:55:59.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786957, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:56:01.048: INFO: all replica sets need to contain the pod-template-hash label
Oct  4 11:56:01.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786960, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:56:03.049: INFO: all replica sets need to contain the pod-template-hash label
Oct  4 11:56:03.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786960, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:56:05.056: INFO: all replica sets need to contain the pod-template-hash label
Oct  4 11:56:05.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786960, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:56:07.050: INFO: all replica sets need to contain the pod-template-hash label
Oct  4 11:56:07.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786960, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:56:09.048: INFO: all replica sets need to contain the pod-template-hash label
Oct  4 11:56:09.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786960, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705786954, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:56:11.049: INFO: 
Oct  4 11:56:11.049: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Oct  4 11:56:11.061: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-6487,SelfLink:/apis/apps/v1/namespaces/deployment-6487/deployments/test-rollover-deployment,UID:8d6dc346-43fc-4272-91b5-80d66c698002,ResourceVersion:30034,Generation:2,CreationTimestamp:2019-10-04 11:55:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-10-04 11:55:54 +0000 UTC 2019-10-04 11:55:54 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-10-04 11:56:10 +0000 UTC 2019-10-04 11:55:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Oct  4 11:56:11.066: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-6487,SelfLink:/apis/apps/v1/namespaces/deployment-6487/replicasets/test-rollover-deployment-854595fc44,UID:d2cf5a6b-4da3-4ef7-af4a-4336aa307fc2,ResourceVersion:30023,Generation:2,CreationTimestamp:2019-10-04 11:55:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8d6dc346-43fc-4272-91b5-80d66c698002 0xc0036478f7 0xc0036478f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Oct  4 11:56:11.066: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Oct  4 11:56:11.066: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-6487,SelfLink:/apis/apps/v1/namespaces/deployment-6487/replicasets/test-rollover-controller,UID:6628aa71-429d-423f-a19b-38776b583100,ResourceVersion:30033,Generation:2,CreationTimestamp:2019-10-04 11:55:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8d6dc346-43fc-4272-91b5-80d66c698002 0xc003647827 0xc003647828}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:56:11.066: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-6487,SelfLink:/apis/apps/v1/namespaces/deployment-6487/replicasets/test-rollover-deployment-9b8b997cf,UID:a64a7317-0d37-4668-8e60-aefefa594c93,ResourceVersion:29973,Generation:2,CreationTimestamp:2019-10-04 11:55:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 8d6dc346-43fc-4272-91b5-80d66c698002 0xc0036479c0 0xc0036479c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:56:11.071: INFO: Pod "test-rollover-deployment-854595fc44-vmncd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-vmncd,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-6487,SelfLink:/api/v1/namespaces/deployment-6487/pods/test-rollover-deployment-854595fc44-vmncd,UID:ec2f144a-6e5c-44ac-9ab8-9d3beff528b5,ResourceVersion:29995,Generation:0,CreationTimestamp:2019-10-04 11:55:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 d2cf5a6b-4da3-4ef7-af4a-4336aa307fc2 0xc00211a597 0xc00211a598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wmjvq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wmjvq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wmjvq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211a610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211a630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:55:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:55:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:55:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:55:57 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.3,PodIP:10.233.64.54,StartTime:2019-10-04 11:55:57 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-10-04 11:55:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://996df1f1a11ab91242dbff893be0946dd4cffce379e5b5e3e81dc49db4992e95}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:56:11.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6487" for this suite.
Oct  4 11:56:17.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:56:17.219: INFO: namespace deployment-6487 deletion completed in 6.142552314s

• [SLOW TEST:29.314 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:56:17.219: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 11:56:17.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2350'
Oct  4 11:56:17.441: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct  4 11:56:17.441: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Oct  4 11:56:17.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete jobs e2e-test-nginx-job --namespace=kubectl-2350'
Oct  4 11:56:17.540: INFO: stderr: ""
Oct  4 11:56:17.540: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:56:17.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2350" for this suite.
Oct  4 11:56:23.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:56:23.694: INFO: namespace kubectl-2350 deletion completed in 6.147528316s

• [SLOW TEST:6.475 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:56:23.694: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-d44247f5-71f1-4d31-8083-f48f475c102e
STEP: Creating secret with name s-test-opt-upd-ee091bdd-37db-4157-8ff7-5978a18a7cc8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d44247f5-71f1-4d31-8083-f48f475c102e
STEP: Updating secret s-test-opt-upd-ee091bdd-37db-4157-8ff7-5978a18a7cc8
STEP: Creating secret with name s-test-opt-create-91162d97-3cad-4ab6-abe6-5b1cab4f2140
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:56:27.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4600" for this suite.
Oct  4 11:56:49.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:56:50.046: INFO: namespace projected-4600 deletion completed in 22.160989922s

• [SLOW TEST:26.352 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:56:50.046: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-1777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1777 to expose endpoints map[]
Oct  4 11:56:50.110: INFO: successfully validated that service multi-endpoint-test in namespace services-1777 exposes endpoints map[] (4.905925ms elapsed)
STEP: Creating pod pod1 in namespace services-1777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1777 to expose endpoints map[pod1:[100]]
Oct  4 11:56:52.146: INFO: successfully validated that service multi-endpoint-test in namespace services-1777 exposes endpoints map[pod1:[100]] (2.026406019s elapsed)
STEP: Creating pod pod2 in namespace services-1777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1777 to expose endpoints map[pod1:[100] pod2:[101]]
Oct  4 11:56:54.196: INFO: successfully validated that service multi-endpoint-test in namespace services-1777 exposes endpoints map[pod1:[100] pod2:[101]] (2.044100063s elapsed)
STEP: Deleting pod pod1 in namespace services-1777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1777 to expose endpoints map[pod2:[101]]
Oct  4 11:56:54.214: INFO: successfully validated that service multi-endpoint-test in namespace services-1777 exposes endpoints map[pod2:[101]] (10.042101ms elapsed)
STEP: Deleting pod pod2 in namespace services-1777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1777 to expose endpoints map[]
Oct  4 11:56:54.226: INFO: successfully validated that service multi-endpoint-test in namespace services-1777 exposes endpoints map[] (4.738647ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:56:54.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1777" for this suite.
Oct  4 11:57:16.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:57:16.419: INFO: namespace services-1777 deletion completed in 22.152373921s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:26.373 seconds]
[sig-network] Services
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:57:16.420: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Oct  4 11:57:16.497: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:16.497: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:16.497: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:16.502: INFO: Number of nodes with available pods: 0
Oct  4 11:57:16.502: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:57:17.507: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:17.507: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:17.507: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:17.511: INFO: Number of nodes with available pods: 0
Oct  4 11:57:17.511: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:57:18.508: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:18.508: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:18.508: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:18.513: INFO: Number of nodes with available pods: 2
Oct  4 11:57:18.513: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Oct  4 11:57:19.508: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:19.508: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:19.509: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:19.513: INFO: Number of nodes with available pods: 3
Oct  4 11:57:19.513: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Oct  4 11:57:19.536: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:19.537: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:19.537: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:19.544: INFO: Number of nodes with available pods: 2
Oct  4 11:57:19.544: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:57:20.549: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:20.550: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:20.550: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:20.554: INFO: Number of nodes with available pods: 2
Oct  4 11:57:20.554: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 11:57:21.550: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:21.550: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:21.550: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 11:57:21.555: INFO: Number of nodes with available pods: 3
Oct  4 11:57:21.555: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9397, will wait for the garbage collector to delete the pods
Oct  4 11:57:21.628: INFO: Deleting DaemonSet.extensions daemon-set took: 11.375541ms
Oct  4 11:57:21.929: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.186811ms
Oct  4 11:57:32.333: INFO: Number of nodes with available pods: 0
Oct  4 11:57:32.333: INFO: Number of running nodes: 0, number of available pods: 0
Oct  4 11:57:32.338: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9397/daemonsets","resourceVersion":"30530"},"items":null}

Oct  4 11:57:32.342: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9397/pods","resourceVersion":"30530"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:57:32.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9397" for this suite.
Oct  4 11:57:38.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:57:38.513: INFO: namespace daemonsets-9397 deletion completed in 6.150533051s

• [SLOW TEST:22.093 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:57:38.515: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 11:57:38.566: INFO: Waiting up to 5m0s for pod "downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90" in namespace "downward-api-1897" to be "success or failure"
Oct  4 11:57:38.574: INFO: Pod "downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90": Phase="Pending", Reason="", readiness=false. Elapsed: 8.377118ms
Oct  4 11:57:40.580: INFO: Pod "downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013740351s
Oct  4 11:57:42.585: INFO: Pod "downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018681006s
STEP: Saw pod success
Oct  4 11:57:42.585: INFO: Pod "downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90" satisfied condition "success or failure"
Oct  4 11:57:42.589: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90 container client-container: <nil>
STEP: delete the pod
Oct  4 11:57:42.616: INFO: Waiting for pod downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90 to disappear
Oct  4 11:57:42.620: INFO: Pod downwardapi-volume-173bb762-f6d6-4546-97dc-b5c180ec9b90 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:57:42.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1897" for this suite.
Oct  4 11:57:48.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:57:48.781: INFO: namespace downward-api-1897 deletion completed in 6.155023786s

• [SLOW TEST:10.266 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:57:48.783: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 11:57:48.818: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Oct  4 11:57:48.833: INFO: Pod name sample-pod: Found 0 pods out of 1
Oct  4 11:57:53.837: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Oct  4 11:57:53.837: INFO: Creating deployment "test-rolling-update-deployment"
Oct  4 11:57:53.842: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Oct  4 11:57:53.852: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Oct  4 11:57:55.861: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Oct  4 11:57:55.870: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705787073, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705787073, loc:(*time.Location)(0x7ec7a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63705787073, loc:(*time.Location)(0x7ec7a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63705787073, loc:(*time.Location)(0x7ec7a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Oct  4 11:57:57.876: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Oct  4 11:57:57.889: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-7751,SelfLink:/apis/apps/v1/namespaces/deployment-7751/deployments/test-rolling-update-deployment,UID:3e59677e-6b0c-4c71-9750-d6770710534c,ResourceVersion:30715,Generation:1,CreationTimestamp:2019-10-04 11:57:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-10-04 11:57:53 +0000 UTC 2019-10-04 11:57:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-10-04 11:57:55 +0000 UTC 2019-10-04 11:57:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Oct  4 11:57:57.893: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-7751,SelfLink:/apis/apps/v1/namespaces/deployment-7751/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:8cbe775c-2b83-4f3a-9480-0602c93bf18e,ResourceVersion:30704,Generation:1,CreationTimestamp:2019-10-04 11:57:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3e59677e-6b0c-4c71-9750-d6770710534c 0xc003458337 0xc003458338}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Oct  4 11:57:57.893: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Oct  4 11:57:57.893: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-7751,SelfLink:/apis/apps/v1/namespaces/deployment-7751/replicasets/test-rolling-update-controller,UID:916a2e69-f3a3-4439-b39d-76f3c35fc9fd,ResourceVersion:30714,Generation:2,CreationTimestamp:2019-10-04 11:57:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3e59677e-6b0c-4c71-9750-d6770710534c 0xc003458267 0xc003458268}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Oct  4 11:57:57.898: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-gmwqw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-gmwqw,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-7751,SelfLink:/api/v1/namespaces/deployment-7751/pods/test-rolling-update-deployment-79f6b9d75c-gmwqw,UID:5f712acd-b16a-4a4c-8349-2fe27721e9ff,ResourceVersion:30703,Generation:0,CreationTimestamp:2019-10-04 11:57:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 8cbe775c-2b83-4f3a-9480-0602c93bf18e 0xc003458c17 0xc003458c18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xjjbb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xjjbb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xjjbb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:lab1-k8s-node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003458c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003458cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:57:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:57:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:57:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-10-04 11:57:53 +0000 UTC  }],Message:,Reason:,HostIP:10.128.0.9,PodIP:10.233.74.49,StartTime:2019-10-04 11:57:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-10-04 11:57:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://bbea777e698b5c541aeb919b3dc370c87cd5ea78cf289afa1832fcffdb5b77b7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:57:57.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7751" for this suite.
Oct  4 11:58:03.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:58:04.063: INFO: namespace deployment-7751 deletion completed in 6.158816833s

• [SLOW TEST:15.280 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:58:04.063: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Oct  4 11:58:04.114: INFO: Waiting up to 5m0s for pod "downward-api-1862efe1-350f-4459-b5db-9b1ae3076129" in namespace "downward-api-5009" to be "success or failure"
Oct  4 11:58:04.120: INFO: Pod "downward-api-1862efe1-350f-4459-b5db-9b1ae3076129": Phase="Pending", Reason="", readiness=false. Elapsed: 6.390583ms
Oct  4 11:58:06.125: INFO: Pod "downward-api-1862efe1-350f-4459-b5db-9b1ae3076129": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011325086s
STEP: Saw pod success
Oct  4 11:58:06.125: INFO: Pod "downward-api-1862efe1-350f-4459-b5db-9b1ae3076129" satisfied condition "success or failure"
Oct  4 11:58:06.129: INFO: Trying to get logs from node lab1-k8s-node-1 pod downward-api-1862efe1-350f-4459-b5db-9b1ae3076129 container dapi-container: <nil>
STEP: delete the pod
Oct  4 11:58:06.157: INFO: Waiting for pod downward-api-1862efe1-350f-4459-b5db-9b1ae3076129 to disappear
Oct  4 11:58:06.161: INFO: Pod downward-api-1862efe1-350f-4459-b5db-9b1ae3076129 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:58:06.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5009" for this suite.
Oct  4 11:58:12.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:58:12.326: INFO: namespace downward-api-5009 deletion completed in 6.159609721s

• [SLOW TEST:8.263 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:58:12.327: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-0c7c76d6-e2a4-48e7-8849-98bc597943c5
STEP: Creating a pod to test consume secrets
Oct  4 11:58:12.388: INFO: Waiting up to 5m0s for pod "pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b" in namespace "secrets-6441" to be "success or failure"
Oct  4 11:58:12.392: INFO: Pod "pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.15626ms
Oct  4 11:58:14.396: INFO: Pod "pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008342891s
STEP: Saw pod success
Oct  4 11:58:14.396: INFO: Pod "pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b" satisfied condition "success or failure"
Oct  4 11:58:14.400: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 11:58:14.427: INFO: Waiting for pod pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b to disappear
Oct  4 11:58:14.431: INFO: Pod pod-secrets-1150b73e-d284-4086-822d-b533b12a2f9b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:58:14.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6441" for this suite.
Oct  4 11:58:20.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:58:20.590: INFO: namespace secrets-6441 deletion completed in 6.154334729s

• [SLOW TEST:8.264 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:58:20.592: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-2011
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2011
STEP: Deleting pre-stop pod
Oct  4 11:58:31.680: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:58:31.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2011" for this suite.
Oct  4 11:59:09.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:59:09.837: INFO: namespace prestop-2011 deletion completed in 38.142812173s

• [SLOW TEST:49.245 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:59:09.838: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Oct  4 11:59:14.432: INFO: Successfully updated pod "labelsupdate596cad14-17f6-4f3d-9fa5-9db92c573d86"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:59:16.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3990" for this suite.
Oct  4 11:59:38.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 11:59:38.609: INFO: namespace downward-api-3990 deletion completed in 22.147920393s

• [SLOW TEST:28.771 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 11:59:38.610: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Oct  4 11:59:41.193: INFO: Successfully updated pod "pod-update-8dd6d82c-2570-4ccb-aecc-72d78087f4fa"
STEP: verifying the updated pod is in kubernetes
Oct  4 11:59:41.204: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 11:59:41.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5558" for this suite.
Oct  4 12:00:03.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:00:03.368: INFO: namespace pods-5558 deletion completed in 22.155779457s

• [SLOW TEST:24.758 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:00:03.369: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:00:03.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7973" for this suite.
Oct  4 12:00:25.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:00:25.594: INFO: namespace pods-7973 deletion completed in 22.160818968s

• [SLOW TEST:22.225 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:00:25.595: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Oct  4 12:00:25.640: INFO: Waiting up to 5m0s for pod "pod-7f7df405-f34d-408b-a15f-4c063258c9ad" in namespace "emptydir-3110" to be "success or failure"
Oct  4 12:00:25.645: INFO: Pod "pod-7f7df405-f34d-408b-a15f-4c063258c9ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.838589ms
Oct  4 12:00:27.650: INFO: Pod "pod-7f7df405-f34d-408b-a15f-4c063258c9ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010290213s
STEP: Saw pod success
Oct  4 12:00:27.650: INFO: Pod "pod-7f7df405-f34d-408b-a15f-4c063258c9ad" satisfied condition "success or failure"
Oct  4 12:00:27.654: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-7f7df405-f34d-408b-a15f-4c063258c9ad container test-container: <nil>
STEP: delete the pod
Oct  4 12:00:27.681: INFO: Waiting for pod pod-7f7df405-f34d-408b-a15f-4c063258c9ad to disappear
Oct  4 12:00:27.685: INFO: Pod pod-7f7df405-f34d-408b-a15f-4c063258c9ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:00:27.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3110" for this suite.
Oct  4 12:00:33.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:00:33.834: INFO: namespace emptydir-3110 deletion completed in 6.142808131s

• [SLOW TEST:8.240 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:00:33.837: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Oct  4 12:00:33.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5600'
Oct  4 12:00:33.948: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Oct  4 12:00:33.948: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Oct  4 12:00:33.971: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-hwpmm]
Oct  4 12:00:33.971: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-hwpmm" in namespace "kubectl-5600" to be "running and ready"
Oct  4 12:00:33.980: INFO: Pod "e2e-test-nginx-rc-hwpmm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.316427ms
Oct  4 12:00:35.986: INFO: Pod "e2e-test-nginx-rc-hwpmm": Phase="Running", Reason="", readiness=true. Elapsed: 2.014352987s
Oct  4 12:00:35.986: INFO: Pod "e2e-test-nginx-rc-hwpmm" satisfied condition "running and ready"
Oct  4 12:00:35.986: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-hwpmm]
Oct  4 12:00:35.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 logs rc/e2e-test-nginx-rc --namespace=kubectl-5600'
Oct  4 12:00:36.094: INFO: stderr: ""
Oct  4 12:00:36.094: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Oct  4 12:00:36.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete rc e2e-test-nginx-rc --namespace=kubectl-5600'
Oct  4 12:00:36.181: INFO: stderr: ""
Oct  4 12:00:36.181: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:00:36.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5600" for this suite.
Oct  4 12:00:58.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:00:58.333: INFO: namespace kubectl-5600 deletion completed in 22.145806228s

• [SLOW TEST:24.497 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:00:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:00:58.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-36" for this suite.
Oct  4 12:01:20.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:01:20.545: INFO: namespace kubelet-test-36 deletion completed in 22.144302732s

• [SLOW TEST:22.210 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:01:20.545: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Oct  4 12:01:20.592: INFO: Waiting up to 5m0s for pod "var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b" in namespace "var-expansion-3065" to be "success or failure"
Oct  4 12:01:20.596: INFO: Pod "var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.785105ms
Oct  4 12:01:22.601: INFO: Pod "var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00928971s
STEP: Saw pod success
Oct  4 12:01:22.601: INFO: Pod "var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b" satisfied condition "success or failure"
Oct  4 12:01:22.605: INFO: Trying to get logs from node lab1-k8s-node-1 pod var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b container dapi-container: <nil>
STEP: delete the pod
Oct  4 12:01:22.629: INFO: Waiting for pod var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b to disappear
Oct  4 12:01:22.633: INFO: Pod var-expansion-79ce74b3-3d78-4f20-9692-57b44ff3b42b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:01:22.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3065" for this suite.
Oct  4 12:01:28.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:01:28.788: INFO: namespace var-expansion-3065 deletion completed in 6.149812414s

• [SLOW TEST:8.243 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:01:28.788: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Oct  4 12:01:28.835: INFO: Waiting up to 5m0s for pod "client-containers-78a9bbe2-9938-40c6-9072-789f0c894302" in namespace "containers-7974" to be "success or failure"
Oct  4 12:01:28.839: INFO: Pod "client-containers-78a9bbe2-9938-40c6-9072-789f0c894302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.307234ms
Oct  4 12:01:30.844: INFO: Pod "client-containers-78a9bbe2-9938-40c6-9072-789f0c894302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009340534s
STEP: Saw pod success
Oct  4 12:01:30.844: INFO: Pod "client-containers-78a9bbe2-9938-40c6-9072-789f0c894302" satisfied condition "success or failure"
Oct  4 12:01:30.848: INFO: Trying to get logs from node lab1-k8s-node-1 pod client-containers-78a9bbe2-9938-40c6-9072-789f0c894302 container test-container: <nil>
STEP: delete the pod
Oct  4 12:01:30.878: INFO: Waiting for pod client-containers-78a9bbe2-9938-40c6-9072-789f0c894302 to disappear
Oct  4 12:01:30.882: INFO: Pod client-containers-78a9bbe2-9938-40c6-9072-789f0c894302 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:01:30.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7974" for this suite.
Oct  4 12:01:36.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:01:37.058: INFO: namespace containers-7974 deletion completed in 6.170042039s

• [SLOW TEST:8.270 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:01:37.060: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 12:01:37.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919" in namespace "downward-api-1860" to be "success or failure"
Oct  4 12:01:37.123: INFO: Pod "downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919": Phase="Pending", Reason="", readiness=false. Elapsed: 5.520184ms
Oct  4 12:01:39.128: INFO: Pod "downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010327409s
STEP: Saw pod success
Oct  4 12:01:39.128: INFO: Pod "downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919" satisfied condition "success or failure"
Oct  4 12:01:39.132: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919 container client-container: <nil>
STEP: delete the pod
Oct  4 12:01:39.160: INFO: Waiting for pod downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919 to disappear
Oct  4 12:01:39.165: INFO: Pod downwardapi-volume-414c8843-7189-49bb-9693-65e1c924a919 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:01:39.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1860" for this suite.
Oct  4 12:01:45.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:01:45.322: INFO: namespace downward-api-1860 deletion completed in 6.150880979s

• [SLOW TEST:8.262 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:01:45.323: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6395.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6395.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Oct  4 12:01:47.438: INFO: DNS probes using dns-6395/dns-test-d8ee8ca0-c4e2-40ed-94d3-27c203235b72 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:01:47.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6395" for this suite.
Oct  4 12:01:53.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:01:53.609: INFO: namespace dns-6395 deletion completed in 6.149457685s

• [SLOW TEST:8.287 seconds]
[sig-network] DNS
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:01:53.611: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7089
I1004 12:01:53.653738      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7089, replica count: 1
I1004 12:01:54.704117      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1004 12:01:55.704288      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Oct  4 12:01:55.820: INFO: Created: latency-svc-4m7t5
Oct  4 12:01:55.824: INFO: Got endpoints: latency-svc-4m7t5 [20.186833ms]
Oct  4 12:01:55.841: INFO: Created: latency-svc-zr7xk
Oct  4 12:01:55.847: INFO: Got endpoints: latency-svc-zr7xk [22.735321ms]
Oct  4 12:01:55.851: INFO: Created: latency-svc-xrmnz
Oct  4 12:01:55.857: INFO: Got endpoints: latency-svc-xrmnz [32.656609ms]
Oct  4 12:01:55.863: INFO: Created: latency-svc-2mf2h
Oct  4 12:01:55.870: INFO: Got endpoints: latency-svc-2mf2h [44.912438ms]
Oct  4 12:01:55.880: INFO: Created: latency-svc-98gz9
Oct  4 12:01:55.885: INFO: Got endpoints: latency-svc-98gz9 [59.736192ms]
Oct  4 12:01:55.891: INFO: Created: latency-svc-t2nnb
Oct  4 12:01:55.899: INFO: Got endpoints: latency-svc-t2nnb [74.138117ms]
Oct  4 12:01:55.902: INFO: Created: latency-svc-qz684
Oct  4 12:01:55.911: INFO: Got endpoints: latency-svc-qz684 [85.532389ms]
Oct  4 12:01:55.915: INFO: Created: latency-svc-vkg8d
Oct  4 12:01:55.920: INFO: Got endpoints: latency-svc-vkg8d [94.75016ms]
Oct  4 12:01:55.925: INFO: Created: latency-svc-96vcv
Oct  4 12:01:55.928: INFO: Got endpoints: latency-svc-96vcv [103.243093ms]
Oct  4 12:01:55.940: INFO: Created: latency-svc-tcjln
Oct  4 12:01:55.949: INFO: Got endpoints: latency-svc-tcjln [123.73546ms]
Oct  4 12:01:55.954: INFO: Created: latency-svc-64mm7
Oct  4 12:01:55.960: INFO: Got endpoints: latency-svc-64mm7 [134.731316ms]
Oct  4 12:01:55.968: INFO: Created: latency-svc-j8w4r
Oct  4 12:01:55.974: INFO: Got endpoints: latency-svc-j8w4r [148.966258ms]
Oct  4 12:01:55.975: INFO: Created: latency-svc-m7565
Oct  4 12:01:55.981: INFO: Got endpoints: latency-svc-m7565 [155.969991ms]
Oct  4 12:01:55.987: INFO: Created: latency-svc-cn6kz
Oct  4 12:01:55.992: INFO: Got endpoints: latency-svc-cn6kz [166.13123ms]
Oct  4 12:01:55.994: INFO: Created: latency-svc-w2khb
Oct  4 12:01:56.001: INFO: Got endpoints: latency-svc-w2khb [175.329025ms]
Oct  4 12:01:56.005: INFO: Created: latency-svc-7r8gn
Oct  4 12:01:56.015: INFO: Got endpoints: latency-svc-7r8gn [189.627704ms]
Oct  4 12:01:56.016: INFO: Created: latency-svc-pfcgb
Oct  4 12:01:56.024: INFO: Created: latency-svc-9jfkw
Oct  4 12:01:56.024: INFO: Got endpoints: latency-svc-pfcgb [176.596387ms]
Oct  4 12:01:56.032: INFO: Got endpoints: latency-svc-9jfkw [175.156964ms]
Oct  4 12:01:56.035: INFO: Created: latency-svc-kdxtt
Oct  4 12:01:56.041: INFO: Got endpoints: latency-svc-kdxtt [171.317084ms]
Oct  4 12:01:56.044: INFO: Created: latency-svc-d9lhh
Oct  4 12:01:56.049: INFO: Got endpoints: latency-svc-d9lhh [164.218305ms]
Oct  4 12:01:56.053: INFO: Created: latency-svc-pz5pv
Oct  4 12:01:56.059: INFO: Got endpoints: latency-svc-pz5pv [159.28684ms]
Oct  4 12:01:56.065: INFO: Created: latency-svc-ndjtx
Oct  4 12:01:56.067: INFO: Got endpoints: latency-svc-ndjtx [155.942628ms]
Oct  4 12:01:56.070: INFO: Created: latency-svc-lr2vj
Oct  4 12:01:56.077: INFO: Got endpoints: latency-svc-lr2vj [156.721753ms]
Oct  4 12:01:56.079: INFO: Created: latency-svc-4dtlg
Oct  4 12:01:56.086: INFO: Got endpoints: latency-svc-4dtlg [157.104976ms]
Oct  4 12:01:56.090: INFO: Created: latency-svc-rkd74
Oct  4 12:01:56.096: INFO: Got endpoints: latency-svc-rkd74 [147.147795ms]
Oct  4 12:01:56.099: INFO: Created: latency-svc-4snf6
Oct  4 12:01:56.106: INFO: Got endpoints: latency-svc-4snf6 [145.492696ms]
Oct  4 12:01:56.111: INFO: Created: latency-svc-h4rk5
Oct  4 12:01:56.117: INFO: Got endpoints: latency-svc-h4rk5 [142.885069ms]
Oct  4 12:01:56.122: INFO: Created: latency-svc-fhw7b
Oct  4 12:01:56.128: INFO: Got endpoints: latency-svc-fhw7b [146.351049ms]
Oct  4 12:01:56.132: INFO: Created: latency-svc-shsmw
Oct  4 12:01:56.138: INFO: Got endpoints: latency-svc-shsmw [146.397169ms]
Oct  4 12:01:56.142: INFO: Created: latency-svc-rz65d
Oct  4 12:01:56.146: INFO: Got endpoints: latency-svc-rz65d [144.540602ms]
Oct  4 12:01:56.155: INFO: Created: latency-svc-z76dv
Oct  4 12:01:56.164: INFO: Got endpoints: latency-svc-z76dv [148.893578ms]
Oct  4 12:01:56.165: INFO: Created: latency-svc-q2g49
Oct  4 12:01:56.170: INFO: Got endpoints: latency-svc-q2g49 [145.707403ms]
Oct  4 12:01:56.187: INFO: Created: latency-svc-hgzh8
Oct  4 12:01:56.187: INFO: Got endpoints: latency-svc-hgzh8 [154.461118ms]
Oct  4 12:01:56.193: INFO: Created: latency-svc-rd92n
Oct  4 12:01:56.200: INFO: Got endpoints: latency-svc-rd92n [158.996301ms]
Oct  4 12:01:56.204: INFO: Created: latency-svc-t9j7w
Oct  4 12:01:56.211: INFO: Got endpoints: latency-svc-t9j7w [162.0413ms]
Oct  4 12:01:56.216: INFO: Created: latency-svc-zsb5p
Oct  4 12:01:56.220: INFO: Got endpoints: latency-svc-zsb5p [160.932769ms]
Oct  4 12:01:56.229: INFO: Created: latency-svc-22ln4
Oct  4 12:01:56.240: INFO: Got endpoints: latency-svc-22ln4 [172.837567ms]
Oct  4 12:01:56.244: INFO: Created: latency-svc-8q5jh
Oct  4 12:01:56.249: INFO: Got endpoints: latency-svc-8q5jh [172.401069ms]
Oct  4 12:01:56.254: INFO: Created: latency-svc-n7jxc
Oct  4 12:01:56.267: INFO: Created: latency-svc-xswc2
Oct  4 12:01:56.272: INFO: Created: latency-svc-qgmhw
Oct  4 12:01:56.277: INFO: Got endpoints: latency-svc-n7jxc [191.529539ms]
Oct  4 12:01:56.286: INFO: Created: latency-svc-vrl69
Oct  4 12:01:56.295: INFO: Created: latency-svc-n8lkd
Oct  4 12:01:56.303: INFO: Created: latency-svc-fs7br
Oct  4 12:01:56.318: INFO: Created: latency-svc-f495p
Oct  4 12:01:56.330: INFO: Got endpoints: latency-svc-xswc2 [233.178007ms]
Oct  4 12:01:56.331: INFO: Created: latency-svc-7gkb2
Oct  4 12:01:56.341: INFO: Created: latency-svc-nn9jh
Oct  4 12:01:56.351: INFO: Created: latency-svc-tz5zs
Oct  4 12:01:56.360: INFO: Created: latency-svc-kldfr
Oct  4 12:01:56.370: INFO: Created: latency-svc-ljc4c
Oct  4 12:01:56.378: INFO: Got endpoints: latency-svc-qgmhw [270.496927ms]
Oct  4 12:01:56.381: INFO: Created: latency-svc-z647d
Oct  4 12:01:56.389: INFO: Created: latency-svc-x62m9
Oct  4 12:01:56.399: INFO: Created: latency-svc-7swmz
Oct  4 12:01:56.406: INFO: Created: latency-svc-zcxd9
Oct  4 12:01:56.415: INFO: Created: latency-svc-rz46d
Oct  4 12:01:56.422: INFO: Created: latency-svc-4fzf9
Oct  4 12:01:56.424: INFO: Got endpoints: latency-svc-vrl69 [306.440371ms]
Oct  4 12:01:56.438: INFO: Created: latency-svc-qq2rx
Oct  4 12:01:56.478: INFO: Got endpoints: latency-svc-n8lkd [348.324124ms]
Oct  4 12:01:56.492: INFO: Created: latency-svc-jsj2q
Oct  4 12:01:56.526: INFO: Got endpoints: latency-svc-fs7br [387.749751ms]
Oct  4 12:01:56.540: INFO: Created: latency-svc-xt68f
Oct  4 12:01:56.576: INFO: Got endpoints: latency-svc-f495p [428.328346ms]
Oct  4 12:01:56.591: INFO: Created: latency-svc-w5wlb
Oct  4 12:01:56.627: INFO: Got endpoints: latency-svc-7gkb2 [462.539864ms]
Oct  4 12:01:56.643: INFO: Created: latency-svc-478j7
Oct  4 12:01:56.680: INFO: Got endpoints: latency-svc-nn9jh [509.408108ms]
Oct  4 12:01:56.698: INFO: Created: latency-svc-ppgnq
Oct  4 12:01:56.727: INFO: Got endpoints: latency-svc-tz5zs [539.886362ms]
Oct  4 12:01:56.742: INFO: Created: latency-svc-5v9f6
Oct  4 12:01:56.774: INFO: Got endpoints: latency-svc-kldfr [573.657576ms]
Oct  4 12:01:56.790: INFO: Created: latency-svc-5w99g
Oct  4 12:01:56.827: INFO: Got endpoints: latency-svc-ljc4c [615.364777ms]
Oct  4 12:01:56.839: INFO: Created: latency-svc-4jvxt
Oct  4 12:01:56.876: INFO: Got endpoints: latency-svc-z647d [656.054695ms]
Oct  4 12:01:56.890: INFO: Created: latency-svc-99wsk
Oct  4 12:01:56.932: INFO: Got endpoints: latency-svc-x62m9 [692.442866ms]
Oct  4 12:01:56.950: INFO: Created: latency-svc-rt95v
Oct  4 12:01:56.974: INFO: Got endpoints: latency-svc-7swmz [724.607064ms]
Oct  4 12:01:56.992: INFO: Created: latency-svc-fwfn2
Oct  4 12:01:57.025: INFO: Got endpoints: latency-svc-zcxd9 [747.360723ms]
Oct  4 12:01:57.040: INFO: Created: latency-svc-79qmt
Oct  4 12:01:57.075: INFO: Got endpoints: latency-svc-rz46d [745.224376ms]
Oct  4 12:01:57.089: INFO: Created: latency-svc-czbss
Oct  4 12:01:57.134: INFO: Got endpoints: latency-svc-4fzf9 [756.329396ms]
Oct  4 12:01:57.148: INFO: Created: latency-svc-pt8fq
Oct  4 12:01:57.178: INFO: Got endpoints: latency-svc-qq2rx [754.228339ms]
Oct  4 12:01:57.197: INFO: Created: latency-svc-rxvgb
Oct  4 12:01:57.225: INFO: Got endpoints: latency-svc-jsj2q [747.015302ms]
Oct  4 12:01:57.242: INFO: Created: latency-svc-kvnxt
Oct  4 12:01:57.275: INFO: Got endpoints: latency-svc-xt68f [748.197022ms]
Oct  4 12:01:57.289: INFO: Created: latency-svc-64t2b
Oct  4 12:01:57.327: INFO: Got endpoints: latency-svc-w5wlb [751.173837ms]
Oct  4 12:01:57.341: INFO: Created: latency-svc-tvwld
Oct  4 12:01:57.375: INFO: Got endpoints: latency-svc-478j7 [747.403202ms]
Oct  4 12:01:57.387: INFO: Created: latency-svc-cwxvv
Oct  4 12:01:57.426: INFO: Got endpoints: latency-svc-ppgnq [746.128344ms]
Oct  4 12:01:57.441: INFO: Created: latency-svc-xlgln
Oct  4 12:01:57.478: INFO: Got endpoints: latency-svc-5v9f6 [750.867317ms]
Oct  4 12:01:57.494: INFO: Created: latency-svc-xfdfr
Oct  4 12:01:57.526: INFO: Got endpoints: latency-svc-5w99g [751.153097ms]
Oct  4 12:01:57.541: INFO: Created: latency-svc-vjzjk
Oct  4 12:01:57.576: INFO: Got endpoints: latency-svc-4jvxt [748.949801ms]
Oct  4 12:01:57.591: INFO: Created: latency-svc-chwn4
Oct  4 12:01:57.625: INFO: Got endpoints: latency-svc-99wsk [748.528717ms]
Oct  4 12:01:57.639: INFO: Created: latency-svc-mgc4r
Oct  4 12:01:57.675: INFO: Got endpoints: latency-svc-rt95v [742.993327ms]
Oct  4 12:01:57.688: INFO: Created: latency-svc-dgm6r
Oct  4 12:01:57.727: INFO: Got endpoints: latency-svc-fwfn2 [752.456169ms]
Oct  4 12:01:57.740: INFO: Created: latency-svc-j4svh
Oct  4 12:01:57.776: INFO: Got endpoints: latency-svc-79qmt [750.59143ms]
Oct  4 12:01:57.792: INFO: Created: latency-svc-g6flh
Oct  4 12:01:57.825: INFO: Got endpoints: latency-svc-czbss [749.828248ms]
Oct  4 12:01:57.841: INFO: Created: latency-svc-cfmxd
Oct  4 12:01:57.875: INFO: Got endpoints: latency-svc-pt8fq [740.561754ms]
Oct  4 12:01:57.894: INFO: Created: latency-svc-tm2xk
Oct  4 12:01:57.929: INFO: Got endpoints: latency-svc-rxvgb [750.42836ms]
Oct  4 12:01:57.945: INFO: Created: latency-svc-fx4pk
Oct  4 12:01:57.975: INFO: Got endpoints: latency-svc-kvnxt [749.724301ms]
Oct  4 12:01:57.993: INFO: Created: latency-svc-r5zg2
Oct  4 12:01:58.025: INFO: Got endpoints: latency-svc-64t2b [750.132014ms]
Oct  4 12:01:58.039: INFO: Created: latency-svc-ggw9x
Oct  4 12:01:58.074: INFO: Got endpoints: latency-svc-tvwld [746.992357ms]
Oct  4 12:01:58.088: INFO: Created: latency-svc-68zw2
Oct  4 12:01:58.126: INFO: Got endpoints: latency-svc-cwxvv [750.953842ms]
Oct  4 12:01:58.138: INFO: Created: latency-svc-vhql8
Oct  4 12:01:58.174: INFO: Got endpoints: latency-svc-xlgln [747.333869ms]
Oct  4 12:01:58.187: INFO: Created: latency-svc-fw7qq
Oct  4 12:01:58.226: INFO: Got endpoints: latency-svc-xfdfr [748.353943ms]
Oct  4 12:01:58.240: INFO: Created: latency-svc-qqt7f
Oct  4 12:01:58.275: INFO: Got endpoints: latency-svc-vjzjk [748.95948ms]
Oct  4 12:01:58.289: INFO: Created: latency-svc-skxls
Oct  4 12:01:58.324: INFO: Got endpoints: latency-svc-chwn4 [748.214609ms]
Oct  4 12:01:58.339: INFO: Created: latency-svc-x69kv
Oct  4 12:01:58.374: INFO: Got endpoints: latency-svc-mgc4r [749.177155ms]
Oct  4 12:01:58.390: INFO: Created: latency-svc-s5hjs
Oct  4 12:01:58.425: INFO: Got endpoints: latency-svc-dgm6r [749.464089ms]
Oct  4 12:01:58.438: INFO: Created: latency-svc-rbhbt
Oct  4 12:01:58.477: INFO: Got endpoints: latency-svc-j4svh [749.410414ms]
Oct  4 12:01:58.494: INFO: Created: latency-svc-lhdb7
Oct  4 12:01:58.525: INFO: Got endpoints: latency-svc-g6flh [748.154877ms]
Oct  4 12:01:58.538: INFO: Created: latency-svc-w28kj
Oct  4 12:01:58.576: INFO: Got endpoints: latency-svc-cfmxd [750.699317ms]
Oct  4 12:01:58.590: INFO: Created: latency-svc-bl2cx
Oct  4 12:01:58.625: INFO: Got endpoints: latency-svc-tm2xk [749.881387ms]
Oct  4 12:01:58.640: INFO: Created: latency-svc-l7k76
Oct  4 12:01:58.677: INFO: Got endpoints: latency-svc-fx4pk [747.825427ms]
Oct  4 12:01:58.692: INFO: Created: latency-svc-ddt7j
Oct  4 12:01:58.725: INFO: Got endpoints: latency-svc-r5zg2 [749.570638ms]
Oct  4 12:01:58.738: INFO: Created: latency-svc-96xwd
Oct  4 12:01:58.776: INFO: Got endpoints: latency-svc-ggw9x [751.126954ms]
Oct  4 12:01:58.789: INFO: Created: latency-svc-b7pmq
Oct  4 12:01:58.825: INFO: Got endpoints: latency-svc-68zw2 [750.941393ms]
Oct  4 12:01:58.840: INFO: Created: latency-svc-8kn8x
Oct  4 12:01:58.875: INFO: Got endpoints: latency-svc-vhql8 [748.944118ms]
Oct  4 12:01:58.888: INFO: Created: latency-svc-c7ttj
Oct  4 12:01:58.926: INFO: Got endpoints: latency-svc-fw7qq [752.223409ms]
Oct  4 12:01:58.940: INFO: Created: latency-svc-p5tlq
Oct  4 12:01:58.974: INFO: Got endpoints: latency-svc-qqt7f [748.086787ms]
Oct  4 12:01:58.990: INFO: Created: latency-svc-jctb8
Oct  4 12:01:59.025: INFO: Got endpoints: latency-svc-skxls [750.290469ms]
Oct  4 12:01:59.042: INFO: Created: latency-svc-wsnbd
Oct  4 12:01:59.076: INFO: Got endpoints: latency-svc-x69kv [751.484022ms]
Oct  4 12:01:59.090: INFO: Created: latency-svc-ftqgv
Oct  4 12:01:59.128: INFO: Got endpoints: latency-svc-s5hjs [753.137868ms]
Oct  4 12:01:59.142: INFO: Created: latency-svc-6dpt2
Oct  4 12:01:59.178: INFO: Got endpoints: latency-svc-rbhbt [753.264736ms]
Oct  4 12:01:59.195: INFO: Created: latency-svc-46pvz
Oct  4 12:01:59.225: INFO: Got endpoints: latency-svc-lhdb7 [748.260984ms]
Oct  4 12:01:59.239: INFO: Created: latency-svc-knvwq
Oct  4 12:01:59.275: INFO: Got endpoints: latency-svc-w28kj [750.202134ms]
Oct  4 12:01:59.289: INFO: Created: latency-svc-2pm2f
Oct  4 12:01:59.325: INFO: Got endpoints: latency-svc-bl2cx [748.566217ms]
Oct  4 12:01:59.341: INFO: Created: latency-svc-n4z6d
Oct  4 12:01:59.375: INFO: Got endpoints: latency-svc-l7k76 [750.275056ms]
Oct  4 12:01:59.388: INFO: Created: latency-svc-dqbhp
Oct  4 12:01:59.428: INFO: Got endpoints: latency-svc-ddt7j [751.009727ms]
Oct  4 12:01:59.443: INFO: Created: latency-svc-lfnpv
Oct  4 12:01:59.477: INFO: Got endpoints: latency-svc-96xwd [752.297811ms]
Oct  4 12:01:59.494: INFO: Created: latency-svc-zqw4j
Oct  4 12:01:59.526: INFO: Got endpoints: latency-svc-b7pmq [749.256969ms]
Oct  4 12:01:59.541: INFO: Created: latency-svc-qzrgn
Oct  4 12:01:59.574: INFO: Got endpoints: latency-svc-8kn8x [748.779352ms]
Oct  4 12:01:59.588: INFO: Created: latency-svc-zmwhv
Oct  4 12:01:59.625: INFO: Got endpoints: latency-svc-c7ttj [750.423213ms]
Oct  4 12:01:59.641: INFO: Created: latency-svc-2mwx2
Oct  4 12:01:59.675: INFO: Got endpoints: latency-svc-p5tlq [748.399665ms]
Oct  4 12:01:59.689: INFO: Created: latency-svc-7d8pj
Oct  4 12:01:59.726: INFO: Got endpoints: latency-svc-jctb8 [751.952342ms]
Oct  4 12:01:59.742: INFO: Created: latency-svc-4vxq2
Oct  4 12:01:59.776: INFO: Got endpoints: latency-svc-wsnbd [750.037013ms]
Oct  4 12:01:59.789: INFO: Created: latency-svc-czd8h
Oct  4 12:01:59.825: INFO: Got endpoints: latency-svc-ftqgv [749.106781ms]
Oct  4 12:01:59.841: INFO: Created: latency-svc-tbbfn
Oct  4 12:01:59.878: INFO: Got endpoints: latency-svc-6dpt2 [750.487626ms]
Oct  4 12:01:59.892: INFO: Created: latency-svc-98dwz
Oct  4 12:01:59.925: INFO: Got endpoints: latency-svc-46pvz [746.496447ms]
Oct  4 12:01:59.940: INFO: Created: latency-svc-bxk49
Oct  4 12:01:59.974: INFO: Got endpoints: latency-svc-knvwq [748.75874ms]
Oct  4 12:01:59.991: INFO: Created: latency-svc-ptj56
Oct  4 12:02:00.027: INFO: Got endpoints: latency-svc-2pm2f [751.448698ms]
Oct  4 12:02:00.040: INFO: Created: latency-svc-5mf2s
Oct  4 12:02:00.076: INFO: Got endpoints: latency-svc-n4z6d [750.883767ms]
Oct  4 12:02:00.090: INFO: Created: latency-svc-rt58x
Oct  4 12:02:00.126: INFO: Got endpoints: latency-svc-dqbhp [750.214145ms]
Oct  4 12:02:00.139: INFO: Created: latency-svc-tb5n6
Oct  4 12:02:00.175: INFO: Got endpoints: latency-svc-lfnpv [746.819107ms]
Oct  4 12:02:00.189: INFO: Created: latency-svc-k5tq5
Oct  4 12:02:00.225: INFO: Got endpoints: latency-svc-zqw4j [747.621717ms]
Oct  4 12:02:00.242: INFO: Created: latency-svc-67xl5
Oct  4 12:02:00.276: INFO: Got endpoints: latency-svc-qzrgn [750.419544ms]
Oct  4 12:02:00.291: INFO: Created: latency-svc-zwhrf
Oct  4 12:02:00.324: INFO: Got endpoints: latency-svc-zmwhv [749.976046ms]
Oct  4 12:02:00.340: INFO: Created: latency-svc-slkvz
Oct  4 12:02:00.378: INFO: Got endpoints: latency-svc-2mwx2 [753.067855ms]
Oct  4 12:02:00.396: INFO: Created: latency-svc-lsd5f
Oct  4 12:02:00.425: INFO: Got endpoints: latency-svc-7d8pj [750.289726ms]
Oct  4 12:02:00.442: INFO: Created: latency-svc-k74qd
Oct  4 12:02:00.475: INFO: Got endpoints: latency-svc-4vxq2 [747.525624ms]
Oct  4 12:02:00.487: INFO: Created: latency-svc-hd2bq
Oct  4 12:02:00.526: INFO: Got endpoints: latency-svc-czd8h [750.905393ms]
Oct  4 12:02:00.540: INFO: Created: latency-svc-mhcnt
Oct  4 12:02:00.575: INFO: Got endpoints: latency-svc-tbbfn [749.493286ms]
Oct  4 12:02:00.590: INFO: Created: latency-svc-wdx59
Oct  4 12:02:00.625: INFO: Got endpoints: latency-svc-98dwz [746.290855ms]
Oct  4 12:02:00.638: INFO: Created: latency-svc-xqz82
Oct  4 12:02:00.677: INFO: Got endpoints: latency-svc-bxk49 [752.469647ms]
Oct  4 12:02:00.691: INFO: Created: latency-svc-rnknp
Oct  4 12:02:00.724: INFO: Got endpoints: latency-svc-ptj56 [750.605286ms]
Oct  4 12:02:00.739: INFO: Created: latency-svc-42wn9
Oct  4 12:02:00.775: INFO: Got endpoints: latency-svc-5mf2s [748.27418ms]
Oct  4 12:02:00.788: INFO: Created: latency-svc-bsw9j
Oct  4 12:02:00.826: INFO: Got endpoints: latency-svc-rt58x [750.300618ms]
Oct  4 12:02:00.843: INFO: Created: latency-svc-h4s5j
Oct  4 12:02:00.876: INFO: Got endpoints: latency-svc-tb5n6 [750.073579ms]
Oct  4 12:02:00.889: INFO: Created: latency-svc-wzql2
Oct  4 12:02:00.927: INFO: Got endpoints: latency-svc-k5tq5 [751.798782ms]
Oct  4 12:02:00.943: INFO: Created: latency-svc-r46l5
Oct  4 12:02:00.978: INFO: Got endpoints: latency-svc-67xl5 [753.531513ms]
Oct  4 12:02:00.998: INFO: Created: latency-svc-hkgn9
Oct  4 12:02:01.025: INFO: Got endpoints: latency-svc-zwhrf [749.085139ms]
Oct  4 12:02:01.042: INFO: Created: latency-svc-xpsgz
Oct  4 12:02:01.076: INFO: Got endpoints: latency-svc-slkvz [750.965367ms]
Oct  4 12:02:01.089: INFO: Created: latency-svc-7bfm5
Oct  4 12:02:01.129: INFO: Got endpoints: latency-svc-lsd5f [747.237034ms]
Oct  4 12:02:01.145: INFO: Created: latency-svc-dfwpr
Oct  4 12:02:01.176: INFO: Got endpoints: latency-svc-k74qd [750.512884ms]
Oct  4 12:02:01.194: INFO: Created: latency-svc-xgsj9
Oct  4 12:02:01.226: INFO: Got endpoints: latency-svc-hd2bq [750.431831ms]
Oct  4 12:02:01.245: INFO: Created: latency-svc-76cfc
Oct  4 12:02:01.274: INFO: Got endpoints: latency-svc-mhcnt [747.70741ms]
Oct  4 12:02:01.288: INFO: Created: latency-svc-qgf28
Oct  4 12:02:01.330: INFO: Got endpoints: latency-svc-wdx59 [754.562885ms]
Oct  4 12:02:01.344: INFO: Created: latency-svc-x9rvh
Oct  4 12:02:01.375: INFO: Got endpoints: latency-svc-xqz82 [749.889686ms]
Oct  4 12:02:01.390: INFO: Created: latency-svc-666j8
Oct  4 12:02:01.427: INFO: Got endpoints: latency-svc-rnknp [749.098886ms]
Oct  4 12:02:01.439: INFO: Created: latency-svc-8lf47
Oct  4 12:02:01.475: INFO: Got endpoints: latency-svc-42wn9 [749.863977ms]
Oct  4 12:02:01.494: INFO: Created: latency-svc-7qjdd
Oct  4 12:02:01.526: INFO: Got endpoints: latency-svc-bsw9j [750.908117ms]
Oct  4 12:02:01.540: INFO: Created: latency-svc-2qdd7
Oct  4 12:02:01.574: INFO: Got endpoints: latency-svc-h4s5j [748.229387ms]
Oct  4 12:02:01.590: INFO: Created: latency-svc-cx5k9
Oct  4 12:02:01.625: INFO: Got endpoints: latency-svc-wzql2 [749.00886ms]
Oct  4 12:02:01.639: INFO: Created: latency-svc-jzr9b
Oct  4 12:02:01.674: INFO: Got endpoints: latency-svc-r46l5 [746.79517ms]
Oct  4 12:02:01.689: INFO: Created: latency-svc-q7wpv
Oct  4 12:02:01.725: INFO: Got endpoints: latency-svc-hkgn9 [746.037501ms]
Oct  4 12:02:01.738: INFO: Created: latency-svc-phlmg
Oct  4 12:02:01.776: INFO: Got endpoints: latency-svc-xpsgz [750.280795ms]
Oct  4 12:02:01.788: INFO: Created: latency-svc-9ctgp
Oct  4 12:02:01.826: INFO: Got endpoints: latency-svc-7bfm5 [750.743095ms]
Oct  4 12:02:01.840: INFO: Created: latency-svc-h44sx
Oct  4 12:02:01.876: INFO: Got endpoints: latency-svc-dfwpr [746.54669ms]
Oct  4 12:02:01.891: INFO: Created: latency-svc-wm8pv
Oct  4 12:02:01.926: INFO: Got endpoints: latency-svc-xgsj9 [750.381257ms]
Oct  4 12:02:01.945: INFO: Created: latency-svc-cj2v4
Oct  4 12:02:01.982: INFO: Got endpoints: latency-svc-76cfc [756.523831ms]
Oct  4 12:02:02.003: INFO: Created: latency-svc-smmpf
Oct  4 12:02:02.025: INFO: Got endpoints: latency-svc-qgf28 [750.537992ms]
Oct  4 12:02:02.039: INFO: Created: latency-svc-cgrq9
Oct  4 12:02:02.075: INFO: Got endpoints: latency-svc-x9rvh [744.952234ms]
Oct  4 12:02:02.089: INFO: Created: latency-svc-sgqsl
Oct  4 12:02:02.125: INFO: Got endpoints: latency-svc-666j8 [750.349112ms]
Oct  4 12:02:02.141: INFO: Created: latency-svc-bnb5v
Oct  4 12:02:02.177: INFO: Got endpoints: latency-svc-8lf47 [749.986094ms]
Oct  4 12:02:02.190: INFO: Created: latency-svc-fzktj
Oct  4 12:02:02.230: INFO: Got endpoints: latency-svc-7qjdd [754.771192ms]
Oct  4 12:02:02.243: INFO: Created: latency-svc-kvlk6
Oct  4 12:02:02.275: INFO: Got endpoints: latency-svc-2qdd7 [748.786003ms]
Oct  4 12:02:02.290: INFO: Created: latency-svc-9sdft
Oct  4 12:02:02.326: INFO: Got endpoints: latency-svc-cx5k9 [750.075711ms]
Oct  4 12:02:02.338: INFO: Created: latency-svc-jknjf
Oct  4 12:02:02.375: INFO: Got endpoints: latency-svc-jzr9b [750.308736ms]
Oct  4 12:02:02.389: INFO: Created: latency-svc-chskg
Oct  4 12:02:02.424: INFO: Got endpoints: latency-svc-q7wpv [749.998585ms]
Oct  4 12:02:02.440: INFO: Created: latency-svc-vldr4
Oct  4 12:02:02.474: INFO: Got endpoints: latency-svc-phlmg [749.376358ms]
Oct  4 12:02:02.487: INFO: Created: latency-svc-vdztv
Oct  4 12:02:02.525: INFO: Got endpoints: latency-svc-9ctgp [748.900484ms]
Oct  4 12:02:02.539: INFO: Created: latency-svc-7fmdf
Oct  4 12:02:02.577: INFO: Got endpoints: latency-svc-h44sx [750.793543ms]
Oct  4 12:02:02.589: INFO: Created: latency-svc-sxrjg
Oct  4 12:02:02.627: INFO: Got endpoints: latency-svc-wm8pv [750.789965ms]
Oct  4 12:02:02.642: INFO: Created: latency-svc-64nc6
Oct  4 12:02:02.676: INFO: Got endpoints: latency-svc-cj2v4 [749.165018ms]
Oct  4 12:02:02.689: INFO: Created: latency-svc-mn5mt
Oct  4 12:02:02.725: INFO: Got endpoints: latency-svc-smmpf [742.538197ms]
Oct  4 12:02:02.738: INFO: Created: latency-svc-hzckp
Oct  4 12:02:02.776: INFO: Got endpoints: latency-svc-cgrq9 [750.75572ms]
Oct  4 12:02:02.791: INFO: Created: latency-svc-vgjwb
Oct  4 12:02:02.826: INFO: Got endpoints: latency-svc-sgqsl [751.084414ms]
Oct  4 12:02:02.841: INFO: Created: latency-svc-bbcr7
Oct  4 12:02:02.876: INFO: Got endpoints: latency-svc-bnb5v [750.673057ms]
Oct  4 12:02:02.891: INFO: Created: latency-svc-qx7kf
Oct  4 12:02:02.926: INFO: Got endpoints: latency-svc-fzktj [748.77324ms]
Oct  4 12:02:02.943: INFO: Created: latency-svc-f66jq
Oct  4 12:02:02.974: INFO: Got endpoints: latency-svc-kvlk6 [744.141621ms]
Oct  4 12:02:02.987: INFO: Created: latency-svc-ththn
Oct  4 12:02:03.026: INFO: Got endpoints: latency-svc-9sdft [751.429828ms]
Oct  4 12:02:03.041: INFO: Created: latency-svc-4vsd9
Oct  4 12:02:03.074: INFO: Got endpoints: latency-svc-jknjf [748.646852ms]
Oct  4 12:02:03.088: INFO: Created: latency-svc-fptkt
Oct  4 12:02:03.124: INFO: Got endpoints: latency-svc-chskg [748.699575ms]
Oct  4 12:02:03.139: INFO: Created: latency-svc-nxwrg
Oct  4 12:02:03.175: INFO: Got endpoints: latency-svc-vldr4 [750.665618ms]
Oct  4 12:02:03.189: INFO: Created: latency-svc-jvbg6
Oct  4 12:02:03.225: INFO: Got endpoints: latency-svc-vdztv [750.816582ms]
Oct  4 12:02:03.246: INFO: Created: latency-svc-9mkfp
Oct  4 12:02:03.276: INFO: Got endpoints: latency-svc-7fmdf [750.868474ms]
Oct  4 12:02:03.289: INFO: Created: latency-svc-hr62h
Oct  4 12:02:03.331: INFO: Got endpoints: latency-svc-sxrjg [754.327875ms]
Oct  4 12:02:03.347: INFO: Created: latency-svc-ltq7j
Oct  4 12:02:03.374: INFO: Got endpoints: latency-svc-64nc6 [747.135192ms]
Oct  4 12:02:03.389: INFO: Created: latency-svc-hx4cs
Oct  4 12:02:03.426: INFO: Got endpoints: latency-svc-mn5mt [749.798989ms]
Oct  4 12:02:03.440: INFO: Created: latency-svc-n4dv8
Oct  4 12:02:03.477: INFO: Got endpoints: latency-svc-hzckp [752.219283ms]
Oct  4 12:02:03.494: INFO: Created: latency-svc-fd4gj
Oct  4 12:02:03.525: INFO: Got endpoints: latency-svc-vgjwb [748.852735ms]
Oct  4 12:02:03.539: INFO: Created: latency-svc-bwwqc
Oct  4 12:02:03.577: INFO: Got endpoints: latency-svc-bbcr7 [750.08808ms]
Oct  4 12:02:03.591: INFO: Created: latency-svc-kwlvb
Oct  4 12:02:03.628: INFO: Got endpoints: latency-svc-qx7kf [752.056767ms]
Oct  4 12:02:03.652: INFO: Created: latency-svc-gfff6
Oct  4 12:02:03.708: INFO: Got endpoints: latency-svc-f66jq [781.719206ms]
Oct  4 12:02:03.734: INFO: Got endpoints: latency-svc-ththn [759.738203ms]
Oct  4 12:02:03.781: INFO: Got endpoints: latency-svc-4vsd9 [754.194537ms]
Oct  4 12:02:03.835: INFO: Got endpoints: latency-svc-fptkt [760.78817ms]
Oct  4 12:02:03.884: INFO: Got endpoints: latency-svc-nxwrg [759.157331ms]
Oct  4 12:02:03.933: INFO: Got endpoints: latency-svc-jvbg6 [757.985506ms]
Oct  4 12:02:03.976: INFO: Got endpoints: latency-svc-9mkfp [750.405959ms]
Oct  4 12:02:04.034: INFO: Got endpoints: latency-svc-hr62h [757.802015ms]
Oct  4 12:02:04.084: INFO: Got endpoints: latency-svc-ltq7j [752.859638ms]
Oct  4 12:02:04.134: INFO: Got endpoints: latency-svc-hx4cs [760.150008ms]
Oct  4 12:02:04.183: INFO: Got endpoints: latency-svc-n4dv8 [757.149091ms]
Oct  4 12:02:04.248: INFO: Got endpoints: latency-svc-fd4gj [770.649683ms]
Oct  4 12:02:04.276: INFO: Got endpoints: latency-svc-bwwqc [751.438339ms]
Oct  4 12:02:04.325: INFO: Got endpoints: latency-svc-kwlvb [748.310982ms]
Oct  4 12:02:04.376: INFO: Got endpoints: latency-svc-gfff6 [747.418403ms]
Oct  4 12:02:04.376: INFO: Latencies: [22.735321ms 32.656609ms 44.912438ms 59.736192ms 74.138117ms 85.532389ms 94.75016ms 103.243093ms 123.73546ms 134.731316ms 142.885069ms 144.540602ms 145.492696ms 145.707403ms 146.351049ms 146.397169ms 147.147795ms 148.893578ms 148.966258ms 154.461118ms 155.942628ms 155.969991ms 156.721753ms 157.104976ms 158.996301ms 159.28684ms 160.932769ms 162.0413ms 164.218305ms 166.13123ms 171.317084ms 172.401069ms 172.837567ms 175.156964ms 175.329025ms 176.596387ms 189.627704ms 191.529539ms 233.178007ms 270.496927ms 306.440371ms 348.324124ms 387.749751ms 428.328346ms 462.539864ms 509.408108ms 539.886362ms 573.657576ms 615.364777ms 656.054695ms 692.442866ms 724.607064ms 740.561754ms 742.538197ms 742.993327ms 744.141621ms 744.952234ms 745.224376ms 746.037501ms 746.128344ms 746.290855ms 746.496447ms 746.54669ms 746.79517ms 746.819107ms 746.992357ms 747.015302ms 747.135192ms 747.237034ms 747.333869ms 747.360723ms 747.403202ms 747.418403ms 747.525624ms 747.621717ms 747.70741ms 747.825427ms 748.086787ms 748.154877ms 748.197022ms 748.214609ms 748.229387ms 748.260984ms 748.27418ms 748.310982ms 748.353943ms 748.399665ms 748.528717ms 748.566217ms 748.646852ms 748.699575ms 748.75874ms 748.77324ms 748.779352ms 748.786003ms 748.852735ms 748.900484ms 748.944118ms 748.949801ms 748.95948ms 749.00886ms 749.085139ms 749.098886ms 749.106781ms 749.165018ms 749.177155ms 749.256969ms 749.376358ms 749.410414ms 749.464089ms 749.493286ms 749.570638ms 749.724301ms 749.798989ms 749.828248ms 749.863977ms 749.881387ms 749.889686ms 749.976046ms 749.986094ms 749.998585ms 750.037013ms 750.073579ms 750.075711ms 750.08808ms 750.132014ms 750.202134ms 750.214145ms 750.275056ms 750.280795ms 750.289726ms 750.290469ms 750.300618ms 750.308736ms 750.349112ms 750.381257ms 750.405959ms 750.419544ms 750.423213ms 750.42836ms 750.431831ms 750.487626ms 750.512884ms 750.537992ms 750.59143ms 750.605286ms 750.665618ms 750.673057ms 750.699317ms 750.743095ms 750.75572ms 750.789965ms 750.793543ms 750.816582ms 750.867317ms 750.868474ms 750.883767ms 750.905393ms 750.908117ms 750.941393ms 750.953842ms 750.965367ms 751.009727ms 751.084414ms 751.126954ms 751.153097ms 751.173837ms 751.429828ms 751.438339ms 751.448698ms 751.484022ms 751.798782ms 751.952342ms 752.056767ms 752.219283ms 752.223409ms 752.297811ms 752.456169ms 752.469647ms 752.859638ms 753.067855ms 753.137868ms 753.264736ms 753.531513ms 754.194537ms 754.228339ms 754.327875ms 754.562885ms 754.771192ms 756.329396ms 756.523831ms 757.149091ms 757.802015ms 757.985506ms 759.157331ms 759.738203ms 760.150008ms 760.78817ms 770.649683ms 781.719206ms]
Oct  4 12:02:04.376: INFO: 50 %ile: 749.00886ms
Oct  4 12:02:04.377: INFO: 90 %ile: 753.067855ms
Oct  4 12:02:04.377: INFO: 99 %ile: 770.649683ms
Oct  4 12:02:04.377: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:02:04.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7089" for this suite.
Oct  4 12:02:28.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:02:28.535: INFO: namespace svc-latency-7089 deletion completed in 24.151922968s

• [SLOW TEST:34.924 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:02:28.536: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-18963335-643f-4f0e-93ee-499f8fb60a0d
STEP: Creating a pod to test consume configMaps
Oct  4 12:02:28.590: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1" in namespace "projected-901" to be "success or failure"
Oct  4 12:02:28.596: INFO: Pod "pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.604772ms
Oct  4 12:02:30.600: INFO: Pod "pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010434982s
Oct  4 12:02:32.606: INFO: Pod "pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015575875s
STEP: Saw pod success
Oct  4 12:02:32.606: INFO: Pod "pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1" satisfied condition "success or failure"
Oct  4 12:02:32.610: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Oct  4 12:02:32.635: INFO: Waiting for pod pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1 to disappear
Oct  4 12:02:32.639: INFO: Pod pod-projected-configmaps-0904b895-504d-41f6-98eb-3f49bf2af4f1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:02:32.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-901" for this suite.
Oct  4 12:02:38.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:02:38.789: INFO: namespace projected-901 deletion completed in 6.143998707s

• [SLOW TEST:10.254 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:02:38.794: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Oct  4 12:02:44.879: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:44.879: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:45.043: INFO: Exec stderr: ""
Oct  4 12:02:45.043: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:45.043: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:45.219: INFO: Exec stderr: ""
Oct  4 12:02:45.219: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:45.219: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:45.381: INFO: Exec stderr: ""
Oct  4 12:02:45.381: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:45.381: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:45.572: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Oct  4 12:02:45.572: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:45.573: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:45.756: INFO: Exec stderr: ""
Oct  4 12:02:45.756: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:45.756: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:45.948: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Oct  4 12:02:45.948: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:45.948: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:46.153: INFO: Exec stderr: ""
Oct  4 12:02:46.153: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:46.153: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:46.367: INFO: Exec stderr: ""
Oct  4 12:02:46.367: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:46.367: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:46.567: INFO: Exec stderr: ""
Oct  4 12:02:46.567: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4725 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Oct  4 12:02:46.567: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
Oct  4 12:02:46.762: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:02:46.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4725" for this suite.
Oct  4 12:03:38.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:03:38.924: INFO: namespace e2e-kubelet-etc-hosts-4725 deletion completed in 52.154047056s

• [SLOW TEST:60.130 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:03:38.924: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Oct  4 12:03:38.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3" in namespace "downward-api-2565" to be "success or failure"
Oct  4 12:03:38.987: INFO: Pod "downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.95924ms
Oct  4 12:03:40.992: INFO: Pod "downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013978925s
STEP: Saw pod success
Oct  4 12:03:40.992: INFO: Pod "downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3" satisfied condition "success or failure"
Oct  4 12:03:40.997: INFO: Trying to get logs from node lab1-k8s-node-1 pod downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3 container client-container: <nil>
STEP: delete the pod
Oct  4 12:03:41.027: INFO: Waiting for pod downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3 to disappear
Oct  4 12:03:41.031: INFO: Pod downwardapi-volume-954d401c-88dd-4a07-82b0-870a9d4f6cb3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:03:41.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2565" for this suite.
Oct  4 12:03:47.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:03:47.192: INFO: namespace downward-api-2565 deletion completed in 6.154733827s

• [SLOW TEST:8.268 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:03:47.192: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-48f6fe87-f566-46fd-a66d-55d27e8db10e
STEP: Creating a pod to test consume secrets
Oct  4 12:03:47.279: INFO: Waiting up to 5m0s for pod "pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7" in namespace "secrets-7338" to be "success or failure"
Oct  4 12:03:47.283: INFO: Pod "pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173646ms
Oct  4 12:03:49.288: INFO: Pod "pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009045878s
STEP: Saw pod success
Oct  4 12:03:49.288: INFO: Pod "pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7" satisfied condition "success or failure"
Oct  4 12:03:49.292: INFO: Trying to get logs from node lab1-k8s-node-1 pod pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7 container secret-volume-test: <nil>
STEP: delete the pod
Oct  4 12:03:49.324: INFO: Waiting for pod pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7 to disappear
Oct  4 12:03:49.330: INFO: Pod pod-secrets-5140d5b3-83e8-4b52-8a74-f9ff6e147ba7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:03:49.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7338" for this suite.
Oct  4 12:03:55.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:03:55.492: INFO: namespace secrets-7338 deletion completed in 6.156890862s
STEP: Destroying namespace "secret-namespace-295" for this suite.
Oct  4 12:04:01.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:04:01.643: INFO: namespace secret-namespace-295 deletion completed in 6.150806135s

• [SLOW TEST:14.451 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:04:01.643: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1004 12:04:41.720737      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Oct  4 12:04:41.720: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:04:41.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1079" for this suite.
Oct  4 12:04:47.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:04:47.878: INFO: namespace gc-1079 deletion completed in 6.153441582s

• [SLOW TEST:46.235 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:04:47.880: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Oct  4 12:04:47.941: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Oct  4 12:04:47.956: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:47.956: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:47.957: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:47.966: INFO: Number of nodes with available pods: 0
Oct  4 12:04:47.967: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 12:04:48.972: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:48.972: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:48.972: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:48.976: INFO: Number of nodes with available pods: 0
Oct  4 12:04:48.976: INFO: Node lab1-k8s-node-1 is running more than one daemon pod
Oct  4 12:04:49.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:49.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:49.973: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:49.978: INFO: Number of nodes with available pods: 3
Oct  4 12:04:49.978: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Oct  4 12:04:50.016: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:50.016: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:50.016: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:50.022: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:50.022: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:50.022: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:51.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:51.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:51.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:51.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:51.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:51.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:52.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:52.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:52.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:52.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:52.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:52.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:53.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:53.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:53.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:53.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:53.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:53.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:53.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:54.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:54.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:54.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:54.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:54.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:54.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:54.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:55.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:55.027: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:55.027: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:55.027: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:55.032: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:55.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:55.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:56.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:56.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:56.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:56.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:56.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:56.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:56.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:57.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:57.027: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:57.027: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:57.027: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:57.032: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:57.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:57.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:58.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:58.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:58.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:58.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:58.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:58.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:58.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:59.037: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:59.037: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:59.037: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:04:59.037: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:04:59.043: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:59.043: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:04:59.043: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:00.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:00.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:00.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:00.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:05:00.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:00.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:00.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:01.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:01.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:01.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:01.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:05:01.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:01.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:01.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:02.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:02.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:02.028: INFO: Wrong image for pod: daemon-set-z7qsd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:02.028: INFO: Pod daemon-set-z7qsd is not available
Oct  4 12:05:02.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:02.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:02.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:03.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:03.027: INFO: Pod daemon-set-jkdkh is not available
Oct  4 12:05:03.027: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:03.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:03.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:03.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:04.049: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:04.049: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:04.069: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:04.069: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:04.069: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:05.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:05.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:05.028: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:05.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:05.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:05.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:06.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:06.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:06.028: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:06.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:06.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:06.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:07.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:07.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:07.028: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:07.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:07.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:07.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:08.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:08.027: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:08.027: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:08.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:08.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:08.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:09.029: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:09.029: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:09.029: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:09.035: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:09.035: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:09.035: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:10.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:10.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:10.028: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:10.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:10.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:10.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:11.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:11.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:11.028: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:11.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:11.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:11.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:12.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:12.028: INFO: Wrong image for pod: daemon-set-pzd7b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:12.028: INFO: Pod daemon-set-pzd7b is not available
Oct  4 12:05:12.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:12.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:12.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:13.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:13.028: INFO: Pod daemon-set-cn8h5 is not available
Oct  4 12:05:13.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:13.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:13.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:14.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:14.028: INFO: Pod daemon-set-cn8h5 is not available
Oct  4 12:05:14.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:14.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:14.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:15.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:15.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:15.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:15.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:15.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:16.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:16.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:16.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:16.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:16.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:17.027: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:17.027: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:17.032: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:17.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:17.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:18.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:18.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:18.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:18.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:18.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:19.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:19.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:19.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:19.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:19.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:20.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:20.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:20.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:20.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:20.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:21.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:21.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:21.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:21.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:21.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:22.028: INFO: Wrong image for pod: daemon-set-9jpgj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Oct  4 12:05:22.028: INFO: Pod daemon-set-9jpgj is not available
Oct  4 12:05:22.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:22.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:22.034: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:23.028: INFO: Pod daemon-set-zjq8s is not available
Oct  4 12:05:23.032: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:23.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:23.033: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Oct  4 12:05:23.037: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:23.037: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:23.037: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:23.041: INFO: Number of nodes with available pods: 2
Oct  4 12:05:23.041: INFO: Node lab1-k8s-node-2 is running more than one daemon pod
Oct  4 12:05:24.047: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:24.047: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:24.047: INFO: DaemonSet pods can't tolerate node lab1-k8s-master-3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Oct  4 12:05:24.052: INFO: Number of nodes with available pods: 3
Oct  4 12:05:24.052: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4092, will wait for the garbage collector to delete the pods
Oct  4 12:05:24.138: INFO: Deleting DaemonSet.extensions daemon-set took: 12.758157ms
Oct  4 12:05:24.438: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.223106ms
Oct  4 12:05:32.143: INFO: Number of nodes with available pods: 0
Oct  4 12:05:32.143: INFO: Number of running nodes: 0, number of available pods: 0
Oct  4 12:05:32.147: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4092/daemonsets","resourceVersion":"34287"},"items":null}

Oct  4 12:05:32.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4092/pods","resourceVersion":"34287"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:05:32.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4092" for this suite.
Oct  4 12:05:38.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:05:38.310: INFO: namespace daemonsets-4092 deletion completed in 6.139570837s

• [SLOW TEST:50.431 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:05:38.311: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Oct  4 12:05:38.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 create -f - --namespace=kubectl-7153'
Oct  4 12:05:38.562: INFO: stderr: ""
Oct  4 12:05:38.562: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Oct  4 12:05:38.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7153'
Oct  4 12:05:38.649: INFO: stderr: ""
Oct  4 12:05:38.649: INFO: stdout: "update-demo-nautilus-dprc7 update-demo-nautilus-zdtgn "
Oct  4 12:05:38.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-dprc7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7153'
Oct  4 12:05:38.732: INFO: stderr: ""
Oct  4 12:05:38.732: INFO: stdout: ""
Oct  4 12:05:38.732: INFO: update-demo-nautilus-dprc7 is created but not running
Oct  4 12:05:43.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7153'
Oct  4 12:05:43.805: INFO: stderr: ""
Oct  4 12:05:43.805: INFO: stdout: "update-demo-nautilus-dprc7 update-demo-nautilus-zdtgn "
Oct  4 12:05:43.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-dprc7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7153'
Oct  4 12:05:43.875: INFO: stderr: ""
Oct  4 12:05:43.875: INFO: stdout: "true"
Oct  4 12:05:43.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-dprc7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7153'
Oct  4 12:05:43.954: INFO: stderr: ""
Oct  4 12:05:43.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 12:05:43.954: INFO: validating pod update-demo-nautilus-dprc7
Oct  4 12:05:43.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 12:05:43.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 12:05:43.962: INFO: update-demo-nautilus-dprc7 is verified up and running
Oct  4 12:05:43.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-zdtgn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7153'
Oct  4 12:05:44.040: INFO: stderr: ""
Oct  4 12:05:44.040: INFO: stdout: "true"
Oct  4 12:05:44.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods update-demo-nautilus-zdtgn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7153'
Oct  4 12:05:44.121: INFO: stderr: ""
Oct  4 12:05:44.121: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Oct  4 12:05:44.121: INFO: validating pod update-demo-nautilus-zdtgn
Oct  4 12:05:44.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Oct  4 12:05:44.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Oct  4 12:05:44.129: INFO: update-demo-nautilus-zdtgn is verified up and running
STEP: using delete to clean up resources
Oct  4 12:05:44.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 delete --grace-period=0 --force -f - --namespace=kubectl-7153'
Oct  4 12:05:44.214: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Oct  4 12:05:44.214: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Oct  4 12:05:44.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7153'
Oct  4 12:05:44.317: INFO: stderr: "No resources found.\n"
Oct  4 12:05:44.317: INFO: stdout: ""
Oct  4 12:05:44.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 get pods -l name=update-demo --namespace=kubectl-7153 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Oct  4 12:05:44.410: INFO: stderr: ""
Oct  4 12:05:44.410: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:05:44.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7153" for this suite.
Oct  4 12:06:06.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:06:06.571: INFO: namespace kubectl-7153 deletion completed in 22.155406379s

• [SLOW TEST:28.260 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:06:06.572: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Oct  4 12:06:09.646: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:06:09.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1539" for this suite.
Oct  4 12:06:15.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:06:15.836: INFO: namespace container-runtime-1539 deletion completed in 6.164172677s

• [SLOW TEST:9.265 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:06:15.837: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-4b4fe7d4-4019-472f-ab75-b757b6d523e0
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-4b4fe7d4-4019-472f-ab75-b757b6d523e0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:06:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3718" for this suite.
Oct  4 12:06:41.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:06:42.113: INFO: namespace configmap-3718 deletion completed in 22.156797149s

• [SLOW TEST:26.275 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Oct  4 12:06:42.113: INFO: >>> kubeConfig: /tmp/kubeconfig-245756238
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Oct  4 12:06:44.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-245756238 exec pod-sharedvolume-10f4be02-d897-4cb9-907e-5e82130a43ec -c busybox-main-container --namespace=emptydir-7261 -- cat /usr/share/volumeshare/shareddata.txt'
Oct  4 12:06:44.515: INFO: stderr: ""
Oct  4 12:06:44.515: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Oct  4 12:06:44.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7261" for this suite.
Oct  4 12:06:50.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Oct  4 12:06:50.668: INFO: namespace emptydir-7261 deletion completed in 6.146519322s

• [SLOW TEST:8.554 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.3-beta.0.68+2d3c76f9091b6b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSOct  4 12:06:50.669: INFO: Running AfterSuite actions on all nodes
Oct  4 12:06:50.669: INFO: Running AfterSuite actions on node 1
Oct  4 12:06:50.669: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5363.340 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h29m25.186184081s
Test Suite Passed
