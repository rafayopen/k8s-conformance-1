I0221 19:52:42.316326      17 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-389596720
I0221 19:52:42.316428      17 e2e.go:243] Starting e2e run "b63ebd86-e1d8-42d8-b54b-73290741da8d" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1582314760 - Will randomize all specs
Will run 215 of 4412 specs

Feb 21 19:52:42.663: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 19:52:42.669: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 21 19:52:42.714: INFO: Condition Ready of node ip-172-20-17-149.us-east-2.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2020-02-21 19:52:38 +0000 UTC}]. Failure
Feb 21 19:52:42.714: INFO: Unschedulable nodes:
Feb 21 19:52:42.714: INFO: -> ip-172-20-17-149.us-east-2.compute.internal Ready=false Network=false Taints=[{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2020-02-21 19:52:38 +0000 UTC}]
Feb 21 19:52:42.714: INFO: ================================
Feb 21 19:53:12.728: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 21 19:53:12.767: INFO: The status of Pod calico-node-r6hl2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:12.767: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 21 19:53:12.767: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:12.767: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:12.767: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:12.767: INFO: 
Feb 21 19:53:14.793: INFO: The status of Pod calico-node-r6hl2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:14.793: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Feb 21 19:53:14.793: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:14.793: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:14.793: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:14.793: INFO: 
Feb 21 19:53:16.791: INFO: The status of Pod calico-node-r6hl2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:16.791: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Feb 21 19:53:16.791: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:16.791: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:16.791: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:16.791: INFO: 
Feb 21 19:53:18.793: INFO: The status of Pod calico-node-r6hl2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:18.793: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Feb 21 19:53:18.793: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:18.793: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:18.793: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:18.793: INFO: 
Feb 21 19:53:20.791: INFO: The status of Pod calico-node-r6hl2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:20.791: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
Feb 21 19:53:20.791: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:20.791: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:20.791: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:20.792: INFO: 
Feb 21 19:53:22.790: INFO: The status of Pod calico-node-r6hl2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:22.790: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Feb 21 19:53:22.790: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:22.790: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:22.790: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:22.790: INFO: 
Feb 21 19:53:24.810: INFO: The status of Pod calico-node-r6hl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:24.811: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (12 seconds elapsed)
Feb 21 19:53:24.811: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:24.811: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:24.811: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:24.811: INFO: 
Feb 21 19:53:26.794: INFO: The status of Pod calico-node-r6hl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:26.794: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (14 seconds elapsed)
Feb 21 19:53:26.794: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:26.794: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:26.794: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:26.795: INFO: 
Feb 21 19:53:28.793: INFO: The status of Pod calico-node-r6hl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:28.793: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (16 seconds elapsed)
Feb 21 19:53:28.793: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:28.793: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:28.793: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:28.794: INFO: 
Feb 21 19:53:30.793: INFO: The status of Pod calico-node-r6hl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:30.793: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (18 seconds elapsed)
Feb 21 19:53:30.793: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:30.793: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:30.793: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:30.793: INFO: 
Feb 21 19:53:32.791: INFO: The status of Pod calico-node-r6hl2 is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Feb 21 19:53:32.791: INFO: 37 / 38 pods in namespace 'kube-system' are running and ready (20 seconds elapsed)
Feb 21 19:53:32.791: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:32.791: INFO: POD                NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 19:53:32.791: INFO: calico-node-r6hl2  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:53:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC ContainersNotReady containers with unready status: [calico-node]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 19:52:37 +0000 UTC  }]
Feb 21 19:53:32.791: INFO: 
Feb 21 19:53:34.790: INFO: 38 / 38 pods in namespace 'kube-system' are running and ready (22 seconds elapsed)
Feb 21 19:53:34.790: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 21 19:53:34.790: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 21 19:53:34.800: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 21 19:53:34.800: INFO: e2e test version: v1.15.7
Feb 21 19:53:34.801: INFO: kube-apiserver version: v1.15.7
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:53:34.802: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-lifecycle-hook
Feb 21 19:53:34.851: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 21 19:53:34.866: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6538
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 21 19:53:45.044: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:45.048: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:47.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:47.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:49.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:49.052: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:51.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:51.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:53.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:53.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:55.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:55.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:57.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:57.054: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:53:59.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:53:59.052: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:54:01.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:54:01.052: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:54:03.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:54:03.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:54:05.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:54:05.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:54:07.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:54:07.053: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 21 19:54:09.048: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 21 19:54:09.052: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:54:09.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6538" for this suite.
Feb 21 19:54:33.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:54:33.576: INFO: namespace container-lifecycle-hook-6538 deletion completed in 24.496039112s

• [SLOW TEST:58.774 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:54:33.576: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:54:55.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7133" for this suite.
Feb 21 19:55:01.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:55:01.451: INFO: namespace container-runtime-7133 deletion completed in 6.364467097s

• [SLOW TEST:27.875 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:55:01.452: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 19:55:01.654: INFO: (0) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 17.29374ms)
Feb 21 19:55:01.659: INFO: (1) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.986223ms)
Feb 21 19:55:01.667: INFO: (2) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.293596ms)
Feb 21 19:55:01.674: INFO: (3) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.02646ms)
Feb 21 19:55:01.681: INFO: (4) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.227545ms)
Feb 21 19:55:01.686: INFO: (5) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.148293ms)
Feb 21 19:55:01.692: INFO: (6) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.749529ms)
Feb 21 19:55:01.702: INFO: (7) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.886687ms)
Feb 21 19:55:01.707: INFO: (8) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.867446ms)
Feb 21 19:55:01.713: INFO: (9) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.624446ms)
Feb 21 19:55:01.718: INFO: (10) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.709954ms)
Feb 21 19:55:01.723: INFO: (11) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.662052ms)
Feb 21 19:55:01.727: INFO: (12) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.658415ms)
Feb 21 19:55:01.732: INFO: (13) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.617777ms)
Feb 21 19:55:01.737: INFO: (14) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.648986ms)
Feb 21 19:55:01.742: INFO: (15) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.366837ms)
Feb 21 19:55:01.747: INFO: (16) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.210409ms)
Feb 21 19:55:01.753: INFO: (17) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.346146ms)
Feb 21 19:55:01.758: INFO: (18) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.016037ms)
Feb 21 19:55:01.763: INFO: (19) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.840293ms)
[AfterEach] version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:55:01.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9293" for this suite.
Feb 21 19:55:07.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:55:07.980: INFO: namespace proxy-9293 deletion completed in 6.212754961s

• [SLOW TEST:6.528 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:55:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 21 19:55:08.160: INFO: Waiting up to 5m0s for pod "downward-api-919de454-bc39-492e-b96d-b156646a6447" in namespace "downward-api-4540" to be "success or failure"
Feb 21 19:55:08.165: INFO: Pod "downward-api-919de454-bc39-492e-b96d-b156646a6447": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019608ms
Feb 21 19:55:10.169: INFO: Pod "downward-api-919de454-bc39-492e-b96d-b156646a6447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009392156s
STEP: Saw pod success
Feb 21 19:55:10.169: INFO: Pod "downward-api-919de454-bc39-492e-b96d-b156646a6447" satisfied condition "success or failure"
Feb 21 19:55:10.173: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downward-api-919de454-bc39-492e-b96d-b156646a6447 container dapi-container: <nil>
STEP: delete the pod
Feb 21 19:55:10.201: INFO: Waiting for pod downward-api-919de454-bc39-492e-b96d-b156646a6447 to disappear
Feb 21 19:55:10.205: INFO: Pod downward-api-919de454-bc39-492e-b96d-b156646a6447 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:55:10.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4540" for this suite.
Feb 21 19:55:16.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:55:16.427: INFO: namespace downward-api-4540 deletion completed in 6.216905308s

• [SLOW TEST:8.446 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:55:16.427: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 21 19:55:19.165: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4851 pod-service-account-174d32ac-0c19-42d7-9bee-21b945ce3e85 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 21 19:55:19.607: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4851 pod-service-account-174d32ac-0c19-42d7-9bee-21b945ce3e85 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 21 19:55:19.799: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4851 pod-service-account-174d32ac-0c19-42d7-9bee-21b945ce3e85 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:55:19.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4851" for this suite.
Feb 21 19:55:26.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:55:26.191: INFO: namespace svcaccounts-4851 deletion completed in 6.19916136s

• [SLOW TEST:9.764 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:55:26.191: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-865ae4ea-1374-41e0-bd3b-3bd60a5ec24a
STEP: Creating a pod to test consume configMaps
Feb 21 19:55:26.460: INFO: Waiting up to 5m0s for pod "pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7" in namespace "configmap-7321" to be "success or failure"
Feb 21 19:55:26.466: INFO: Pod "pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558074ms
Feb 21 19:55:28.470: INFO: Pod "pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009940469s
Feb 21 19:55:30.475: INFO: Pod "pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014654567s
STEP: Saw pod success
Feb 21 19:55:30.475: INFO: Pod "pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7" satisfied condition "success or failure"
Feb 21 19:55:30.479: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 19:55:30.505: INFO: Waiting for pod pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7 to disappear
Feb 21 19:55:30.508: INFO: Pod pod-configmaps-245c5cf7-fa9b-40ad-a36e-7f333013c2c7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:55:30.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7321" for this suite.
Feb 21 19:55:36.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:55:36.842: INFO: namespace configmap-7321 deletion completed in 6.328139095s

• [SLOW TEST:10.650 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:55:36.842: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 19:55:37.099: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6d88608d-769d-4469-ab5a-1fb6b676a920", Controller:(*bool)(0xc000ba781e), BlockOwnerDeletion:(*bool)(0xc000ba781f)}}
Feb 21 19:55:37.106: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b34499dc-5136-4e8d-84e5-8bd6f81463d4", Controller:(*bool)(0xc0016c185e), BlockOwnerDeletion:(*bool)(0xc0016c185f)}}
Feb 21 19:55:37.113: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b68a4bff-5a1b-4632-9239-2a3aa724a913", Controller:(*bool)(0xc000ba79de), BlockOwnerDeletion:(*bool)(0xc000ba79df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:55:42.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1107" for this suite.
Feb 21 19:55:48.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:55:48.441: INFO: namespace gc-1107 deletion completed in 6.308334687s

• [SLOW TEST:11.599 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:55:48.442: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 21 19:55:48.605: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5235,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 19:55:48.605: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5235,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 21 19:55:58.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5275,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 21 19:55:58.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5275,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 21 19:56:08.626: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5334,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 19:56:08.626: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5334,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 21 19:56:18.637: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5381,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 19:56:18.637: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-a,UID:caddfe5f-7bc8-4c6a-bb72-a352302160de,ResourceVersion:5381,Generation:0,CreationTimestamp:2020-02-21 19:55:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 21 19:56:28.646: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-b,UID:6c6ba40c-0d52-4f89-88c5-b25bf8b38f83,ResourceVersion:5420,Generation:0,CreationTimestamp:2020-02-21 19:56:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 19:56:28.646: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-b,UID:6c6ba40c-0d52-4f89-88c5-b25bf8b38f83,ResourceVersion:5420,Generation:0,CreationTimestamp:2020-02-21 19:56:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 21 19:56:38.708: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-b,UID:6c6ba40c-0d52-4f89-88c5-b25bf8b38f83,ResourceVersion:5459,Generation:0,CreationTimestamp:2020-02-21 19:56:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 19:56:38.708: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6737,SelfLink:/api/v1/namespaces/watch-6737/configmaps/e2e-watch-test-configmap-b,UID:6c6ba40c-0d52-4f89-88c5-b25bf8b38f83,ResourceVersion:5459,Generation:0,CreationTimestamp:2020-02-21 19:56:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:56:48.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6737" for this suite.
Feb 21 19:56:54.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:56:54.937: INFO: namespace watch-6737 deletion completed in 6.222475475s

• [SLOW TEST:66.496 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:56:54.938: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:56:57.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2263" for this suite.
Feb 21 19:57:39.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:57:39.342: INFO: namespace kubelet-test-2263 deletion completed in 42.205141475s

• [SLOW TEST:44.405 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:57:39.343: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6772
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Feb 21 19:57:39.498: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-389596720 proxy --unix-socket=/tmp/kubectl-proxy-unix677206479/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:57:39.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6772" for this suite.
Feb 21 19:57:45.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:57:45.780: INFO: namespace kubectl-6772 deletion completed in 6.224140611s

• [SLOW TEST:6.437 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:57:45.780: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2grs7 in namespace proxy-1749
I0221 19:57:45.972114      17 runners.go:180] Created replication controller with name: proxy-service-2grs7, namespace: proxy-1749, replica count: 1
I0221 19:57:47.023256      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 19:57:48.023486      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 19:57:49.023745      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:50.024074      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:51.024287      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:52.024505      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:53.024771      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:54.024999      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:55.025260      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:56.025480      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0221 19:57:57.025772      17 runners.go:180] proxy-service-2grs7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 19:57:57.030: INFO: setup took 11.086211642s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 21 19:57:57.045: INFO: (0) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 14.861131ms)
Feb 21 19:57:57.045: INFO: (0) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 14.77152ms)
Feb 21 19:57:57.045: INFO: (0) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 15.05779ms)
Feb 21 19:57:57.045: INFO: (0) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 14.748627ms)
Feb 21 19:57:57.045: INFO: (0) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 15.037692ms)
Feb 21 19:57:57.045: INFO: (0) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 15.260359ms)
Feb 21 19:57:57.046: INFO: (0) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 15.346144ms)
Feb 21 19:57:57.046: INFO: (0) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 15.528054ms)
Feb 21 19:57:57.046: INFO: (0) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 15.776088ms)
Feb 21 19:57:57.046: INFO: (0) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 16.027697ms)
Feb 21 19:57:57.046: INFO: (0) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 16.09133ms)
Feb 21 19:57:57.051: INFO: (0) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 21.180654ms)
Feb 21 19:57:57.052: INFO: (0) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 21.61954ms)
Feb 21 19:57:57.053: INFO: (0) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 22.860577ms)
Feb 21 19:57:57.053: INFO: (0) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 22.840916ms)
Feb 21 19:57:57.054: INFO: (0) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 23.681477ms)
Feb 21 19:57:57.065: INFO: (1) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.856421ms)
Feb 21 19:57:57.066: INFO: (1) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 11.360633ms)
Feb 21 19:57:57.066: INFO: (1) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.112436ms)
Feb 21 19:57:57.066: INFO: (1) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 11.187212ms)
Feb 21 19:57:57.067: INFO: (1) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 12.553221ms)
Feb 21 19:57:57.068: INFO: (1) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 13.464827ms)
Feb 21 19:57:57.069: INFO: (1) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 14.142695ms)
Feb 21 19:57:57.070: INFO: (1) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 14.785252ms)
Feb 21 19:57:57.070: INFO: (1) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 15.801259ms)
Feb 21 19:57:57.070: INFO: (1) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 15.836991ms)
Feb 21 19:57:57.070: INFO: (1) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 15.789944ms)
Feb 21 19:57:57.071: INFO: (1) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 15.935561ms)
Feb 21 19:57:57.071: INFO: (1) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 17.163562ms)
Feb 21 19:57:57.071: INFO: (1) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 16.447654ms)
Feb 21 19:57:57.072: INFO: (1) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 16.467972ms)
Feb 21 19:57:57.072: INFO: (1) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 16.791765ms)
Feb 21 19:57:57.084: INFO: (2) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 12.144901ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.571847ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 11.829672ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 12.599781ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 11.94809ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 11.897007ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.122738ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 12.404333ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 12.957198ms)
Feb 21 19:57:57.085: INFO: (2) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 12.608915ms)
Feb 21 19:57:57.086: INFO: (2) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 13.887004ms)
Feb 21 19:57:57.088: INFO: (2) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 15.035561ms)
Feb 21 19:57:57.100: INFO: (2) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 27.612624ms)
Feb 21 19:57:57.100: INFO: (2) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 27.265913ms)
Feb 21 19:57:57.100: INFO: (2) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 27.246174ms)
Feb 21 19:57:57.100: INFO: (2) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 27.571617ms)
Feb 21 19:57:57.110: INFO: (3) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 9.507475ms)
Feb 21 19:57:57.110: INFO: (3) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.605611ms)
Feb 21 19:57:57.110: INFO: (3) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 9.356266ms)
Feb 21 19:57:57.110: INFO: (3) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 9.435473ms)
Feb 21 19:57:57.110: INFO: (3) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 9.978758ms)
Feb 21 19:57:57.111: INFO: (3) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 9.902916ms)
Feb 21 19:57:57.111: INFO: (3) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 10.261991ms)
Feb 21 19:57:57.111: INFO: (3) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.344741ms)
Feb 21 19:57:57.111: INFO: (3) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.669601ms)
Feb 21 19:57:57.112: INFO: (3) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 11.224646ms)
Feb 21 19:57:57.113: INFO: (3) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 12.18579ms)
Feb 21 19:57:57.114: INFO: (3) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 13.088733ms)
Feb 21 19:57:57.115: INFO: (3) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 14.455787ms)
Feb 21 19:57:57.117: INFO: (3) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 15.95025ms)
Feb 21 19:57:57.117: INFO: (3) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 15.875671ms)
Feb 21 19:57:57.117: INFO: (3) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 16.176582ms)
Feb 21 19:57:57.127: INFO: (4) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 8.612056ms)
Feb 21 19:57:57.127: INFO: (4) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.554539ms)
Feb 21 19:57:57.128: INFO: (4) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 9.934372ms)
Feb 21 19:57:57.128: INFO: (4) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.872634ms)
Feb 21 19:57:57.128: INFO: (4) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 10.148431ms)
Feb 21 19:57:57.129: INFO: (4) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 10.592582ms)
Feb 21 19:57:57.129: INFO: (4) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 11.115726ms)
Feb 21 19:57:57.128: INFO: (4) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.30753ms)
Feb 21 19:57:57.129: INFO: (4) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 10.815654ms)
Feb 21 19:57:57.129: INFO: (4) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 11.732371ms)
Feb 21 19:57:57.129: INFO: (4) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 11.476687ms)
Feb 21 19:57:57.131: INFO: (4) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 13.112609ms)
Feb 21 19:57:57.131: INFO: (4) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.755449ms)
Feb 21 19:57:57.131: INFO: (4) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 13.54664ms)
Feb 21 19:57:57.131: INFO: (4) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 14.264502ms)
Feb 21 19:57:57.132: INFO: (4) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 14.043262ms)
Feb 21 19:57:57.140: INFO: (5) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 8.310543ms)
Feb 21 19:57:57.140: INFO: (5) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 8.635924ms)
Feb 21 19:57:57.142: INFO: (5) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 10.478244ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 10.218357ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.802884ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 10.346381ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 10.573941ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.945878ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.08662ms)
Feb 21 19:57:57.143: INFO: (5) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 10.774414ms)
Feb 21 19:57:57.144: INFO: (5) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 11.53431ms)
Feb 21 19:57:57.144: INFO: (5) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 12.03041ms)
Feb 21 19:57:57.145: INFO: (5) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 12.510427ms)
Feb 21 19:57:57.145: INFO: (5) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 13.060265ms)
Feb 21 19:57:57.145: INFO: (5) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.004345ms)
Feb 21 19:57:57.146: INFO: (5) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 13.469672ms)
Feb 21 19:57:57.156: INFO: (6) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 9.194256ms)
Feb 21 19:57:57.156: INFO: (6) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 9.889899ms)
Feb 21 19:57:57.156: INFO: (6) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 9.638598ms)
Feb 21 19:57:57.156: INFO: (6) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 10.158137ms)
Feb 21 19:57:57.156: INFO: (6) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.001171ms)
Feb 21 19:57:57.156: INFO: (6) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 10.29122ms)
Feb 21 19:57:57.157: INFO: (6) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 9.985258ms)
Feb 21 19:57:57.157: INFO: (6) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 10.160316ms)
Feb 21 19:57:57.157: INFO: (6) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.230317ms)
Feb 21 19:57:57.157: INFO: (6) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.357279ms)
Feb 21 19:57:57.158: INFO: (6) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.430489ms)
Feb 21 19:57:57.159: INFO: (6) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 12.795094ms)
Feb 21 19:57:57.160: INFO: (6) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 13.373143ms)
Feb 21 19:57:57.160: INFO: (6) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 14.279363ms)
Feb 21 19:57:57.160: INFO: (6) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 14.197031ms)
Feb 21 19:57:57.161: INFO: (6) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 14.885803ms)
Feb 21 19:57:57.172: INFO: (7) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 10.592969ms)
Feb 21 19:57:57.173: INFO: (7) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 11.355821ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 11.758024ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 12.405604ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 12.164238ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 12.202533ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.13751ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.048999ms)
Feb 21 19:57:57.174: INFO: (7) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.503193ms)
Feb 21 19:57:57.175: INFO: (7) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 13.418316ms)
Feb 21 19:57:57.175: INFO: (7) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 13.781635ms)
Feb 21 19:57:57.176: INFO: (7) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 14.412701ms)
Feb 21 19:57:57.176: INFO: (7) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 15.038095ms)
Feb 21 19:57:57.177: INFO: (7) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 14.903741ms)
Feb 21 19:57:57.177: INFO: (7) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 15.424411ms)
Feb 21 19:57:57.177: INFO: (7) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 15.521013ms)
Feb 21 19:57:57.183: INFO: (8) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 5.885181ms)
Feb 21 19:57:57.185: INFO: (8) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 7.264447ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 12.558109ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 12.402731ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 12.378827ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.511149ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 12.451959ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 12.68971ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 12.745179ms)
Feb 21 19:57:57.190: INFO: (8) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.601767ms)
Feb 21 19:57:57.192: INFO: (8) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 14.132717ms)
Feb 21 19:57:57.192: INFO: (8) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 14.675646ms)
Feb 21 19:57:57.193: INFO: (8) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 15.619768ms)
Feb 21 19:57:57.193: INFO: (8) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 15.594168ms)
Feb 21 19:57:57.194: INFO: (8) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 15.846687ms)
Feb 21 19:57:57.194: INFO: (8) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 16.623666ms)
Feb 21 19:57:57.201: INFO: (9) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 6.548575ms)
Feb 21 19:57:57.207: INFO: (9) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.18185ms)
Feb 21 19:57:57.207: INFO: (9) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 12.178785ms)
Feb 21 19:57:57.207: INFO: (9) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 12.592436ms)
Feb 21 19:57:57.208: INFO: (9) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.467381ms)
Feb 21 19:57:57.208: INFO: (9) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 12.941648ms)
Feb 21 19:57:57.208: INFO: (9) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 12.497186ms)
Feb 21 19:57:57.208: INFO: (9) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 13.212651ms)
Feb 21 19:57:57.208: INFO: (9) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.738323ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 13.307142ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.259404ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 13.587487ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 13.707913ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 13.533894ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 13.474342ms)
Feb 21 19:57:57.209: INFO: (9) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 13.829943ms)
Feb 21 19:57:57.217: INFO: (10) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 7.687862ms)
Feb 21 19:57:57.219: INFO: (10) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 8.710284ms)
Feb 21 19:57:57.219: INFO: (10) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 8.377205ms)
Feb 21 19:57:57.219: INFO: (10) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 8.711783ms)
Feb 21 19:57:57.219: INFO: (10) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 8.952828ms)
Feb 21 19:57:57.219: INFO: (10) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 9.560779ms)
Feb 21 19:57:57.219: INFO: (10) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 9.698553ms)
Feb 21 19:57:57.220: INFO: (10) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 9.077575ms)
Feb 21 19:57:57.220: INFO: (10) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 9.689846ms)
Feb 21 19:57:57.220: INFO: (10) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 10.167655ms)
Feb 21 19:57:57.221: INFO: (10) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 11.04608ms)
Feb 21 19:57:57.223: INFO: (10) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 12.187855ms)
Feb 21 19:57:57.223: INFO: (10) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 12.845938ms)
Feb 21 19:57:57.223: INFO: (10) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 12.680225ms)
Feb 21 19:57:57.223: INFO: (10) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 13.527202ms)
Feb 21 19:57:57.223: INFO: (10) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.209224ms)
Feb 21 19:57:57.232: INFO: (11) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 7.907483ms)
Feb 21 19:57:57.232: INFO: (11) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 8.657013ms)
Feb 21 19:57:57.232: INFO: (11) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 8.365514ms)
Feb 21 19:57:57.233: INFO: (11) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 9.212555ms)
Feb 21 19:57:57.233: INFO: (11) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.263445ms)
Feb 21 19:57:57.233: INFO: (11) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 9.702668ms)
Feb 21 19:57:57.233: INFO: (11) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 9.233245ms)
Feb 21 19:57:57.233: INFO: (11) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 9.377492ms)
Feb 21 19:57:57.235: INFO: (11) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 11.585442ms)
Feb 21 19:57:57.235: INFO: (11) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.93069ms)
Feb 21 19:57:57.235: INFO: (11) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 11.227702ms)
Feb 21 19:57:57.236: INFO: (11) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.155281ms)
Feb 21 19:57:57.237: INFO: (11) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 12.580516ms)
Feb 21 19:57:57.237: INFO: (11) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 12.662455ms)
Feb 21 19:57:57.237: INFO: (11) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 12.97455ms)
Feb 21 19:57:57.238: INFO: (11) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 13.348481ms)
Feb 21 19:57:57.248: INFO: (12) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 9.219632ms)
Feb 21 19:57:57.248: INFO: (12) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.551652ms)
Feb 21 19:57:57.248: INFO: (12) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 9.614714ms)
Feb 21 19:57:57.250: INFO: (12) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 10.84152ms)
Feb 21 19:57:57.250: INFO: (12) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 11.68327ms)
Feb 21 19:57:57.250: INFO: (12) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.479338ms)
Feb 21 19:57:57.250: INFO: (12) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.047399ms)
Feb 21 19:57:57.250: INFO: (12) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 11.665479ms)
Feb 21 19:57:57.251: INFO: (12) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 11.715391ms)
Feb 21 19:57:57.251: INFO: (12) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 12.385696ms)
Feb 21 19:57:57.251: INFO: (12) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 12.728462ms)
Feb 21 19:57:57.252: INFO: (12) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 13.752907ms)
Feb 21 19:57:57.253: INFO: (12) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 14.68453ms)
Feb 21 19:57:57.253: INFO: (12) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 15.728514ms)
Feb 21 19:57:57.254: INFO: (12) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 15.362295ms)
Feb 21 19:57:57.254: INFO: (12) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 15.518322ms)
Feb 21 19:57:57.261: INFO: (13) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 6.938627ms)
Feb 21 19:57:57.264: INFO: (13) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.777123ms)
Feb 21 19:57:57.264: INFO: (13) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 9.68981ms)
Feb 21 19:57:57.264: INFO: (13) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 9.679088ms)
Feb 21 19:57:57.264: INFO: (13) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 9.92333ms)
Feb 21 19:57:57.265: INFO: (13) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 11.612084ms)
Feb 21 19:57:57.266: INFO: (13) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 11.309848ms)
Feb 21 19:57:57.267: INFO: (13) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 12.102648ms)
Feb 21 19:57:57.267: INFO: (13) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 12.007444ms)
Feb 21 19:57:57.267: INFO: (13) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.960996ms)
Feb 21 19:57:57.267: INFO: (13) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 13.052481ms)
Feb 21 19:57:57.267: INFO: (13) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.000084ms)
Feb 21 19:57:57.269: INFO: (13) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 13.871053ms)
Feb 21 19:57:57.270: INFO: (13) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 15.190751ms)
Feb 21 19:57:57.270: INFO: (13) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 15.347102ms)
Feb 21 19:57:57.270: INFO: (13) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 15.620099ms)
Feb 21 19:57:57.295: INFO: (14) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 24.08596ms)
Feb 21 19:57:57.295: INFO: (14) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 24.806508ms)
Feb 21 19:57:57.296: INFO: (14) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 23.979909ms)
Feb 21 19:57:57.295: INFO: (14) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 24.167518ms)
Feb 21 19:57:57.296: INFO: (14) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 24.820376ms)
Feb 21 19:57:57.296: INFO: (14) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 25.423992ms)
Feb 21 19:57:57.296: INFO: (14) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 25.420079ms)
Feb 21 19:57:57.296: INFO: (14) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 25.415495ms)
Feb 21 19:57:57.297: INFO: (14) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 25.880002ms)
Feb 21 19:57:57.297: INFO: (14) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 26.773207ms)
Feb 21 19:57:57.298: INFO: (14) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 26.286582ms)
Feb 21 19:57:57.306: INFO: (14) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 34.797699ms)
Feb 21 19:57:57.307: INFO: (14) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 35.705704ms)
Feb 21 19:57:57.307: INFO: (14) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 35.218212ms)
Feb 21 19:57:57.307: INFO: (14) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 35.710082ms)
Feb 21 19:57:57.307: INFO: (14) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 35.496186ms)
Feb 21 19:57:57.316: INFO: (15) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 9.291168ms)
Feb 21 19:57:57.316: INFO: (15) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 8.962562ms)
Feb 21 19:57:57.317: INFO: (15) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.37888ms)
Feb 21 19:57:57.317: INFO: (15) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 9.361143ms)
Feb 21 19:57:57.317: INFO: (15) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 10.062206ms)
Feb 21 19:57:57.317: INFO: (15) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 9.860205ms)
Feb 21 19:57:57.317: INFO: (15) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 10.227684ms)
Feb 21 19:57:57.318: INFO: (15) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 10.250425ms)
Feb 21 19:57:57.318: INFO: (15) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.282918ms)
Feb 21 19:57:57.318: INFO: (15) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.155437ms)
Feb 21 19:57:57.319: INFO: (15) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 11.794207ms)
Feb 21 19:57:57.320: INFO: (15) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 13.021073ms)
Feb 21 19:57:57.320: INFO: (15) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 13.461861ms)
Feb 21 19:57:57.320: INFO: (15) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 13.102009ms)
Feb 21 19:57:57.321: INFO: (15) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 13.292446ms)
Feb 21 19:57:57.321: INFO: (15) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.737589ms)
Feb 21 19:57:57.332: INFO: (16) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.59566ms)
Feb 21 19:57:57.332: INFO: (16) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.662101ms)
Feb 21 19:57:57.332: INFO: (16) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.539912ms)
Feb 21 19:57:57.332: INFO: (16) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 11.445036ms)
Feb 21 19:57:57.333: INFO: (16) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 11.074626ms)
Feb 21 19:57:57.333: INFO: (16) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.049151ms)
Feb 21 19:57:57.333: INFO: (16) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 11.303662ms)
Feb 21 19:57:57.333: INFO: (16) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 11.465095ms)
Feb 21 19:57:57.333: INFO: (16) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 11.624957ms)
Feb 21 19:57:57.334: INFO: (16) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 12.159738ms)
Feb 21 19:57:57.334: INFO: (16) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 12.50726ms)
Feb 21 19:57:57.336: INFO: (16) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 14.183615ms)
Feb 21 19:57:57.336: INFO: (16) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 14.159446ms)
Feb 21 19:57:57.336: INFO: (16) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 14.824605ms)
Feb 21 19:57:57.336: INFO: (16) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 14.679387ms)
Feb 21 19:57:57.336: INFO: (16) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 14.515843ms)
Feb 21 19:57:57.343: INFO: (17) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 6.84477ms)
Feb 21 19:57:57.343: INFO: (17) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 6.907937ms)
Feb 21 19:57:57.346: INFO: (17) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 9.803877ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 9.940836ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.064365ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.187522ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.298994ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 10.687176ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 10.701565ms)
Feb 21 19:57:57.347: INFO: (17) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 10.829917ms)
Feb 21 19:57:57.350: INFO: (17) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 13.021335ms)
Feb 21 19:57:57.351: INFO: (17) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 14.551815ms)
Feb 21 19:57:57.352: INFO: (17) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 15.846631ms)
Feb 21 19:57:57.352: INFO: (17) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 15.617928ms)
Feb 21 19:57:57.353: INFO: (17) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 16.287076ms)
Feb 21 19:57:57.353: INFO: (17) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 16.412718ms)
Feb 21 19:57:57.360: INFO: (18) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 7.425305ms)
Feb 21 19:57:57.362: INFO: (18) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 8.228573ms)
Feb 21 19:57:57.363: INFO: (18) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 9.529434ms)
Feb 21 19:57:57.363: INFO: (18) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 9.052252ms)
Feb 21 19:57:57.363: INFO: (18) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 9.893165ms)
Feb 21 19:57:57.364: INFO: (18) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.140245ms)
Feb 21 19:57:57.364: INFO: (18) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 10.409287ms)
Feb 21 19:57:57.364: INFO: (18) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 10.074711ms)
Feb 21 19:57:57.364: INFO: (18) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 10.758766ms)
Feb 21 19:57:57.364: INFO: (18) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 10.220664ms)
Feb 21 19:57:57.365: INFO: (18) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 12.28614ms)
Feb 21 19:57:57.367: INFO: (18) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 12.66415ms)
Feb 21 19:57:57.367: INFO: (18) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 13.234559ms)
Feb 21 19:57:57.367: INFO: (18) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 13.354608ms)
Feb 21 19:57:57.367: INFO: (18) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 13.426932ms)
Feb 21 19:57:57.368: INFO: (18) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 13.883887ms)
Feb 21 19:57:57.379: INFO: (19) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:462/proxy/: tls qux (200; 10.577444ms)
Feb 21 19:57:57.379: INFO: (19) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:460/proxy/: tls baz (200; 10.668367ms)
Feb 21 19:57:57.379: INFO: (19) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:162/proxy/: bar (200; 11.055332ms)
Feb 21 19:57:57.379: INFO: (19) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.13877ms)
Feb 21 19:57:57.379: INFO: (19) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:1080/proxy/rewriteme">... (200; 11.620915ms)
Feb 21 19:57:57.380: INFO: (19) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj:1080/proxy/rewriteme">test<... (200; 11.39092ms)
Feb 21 19:57:57.380: INFO: (19) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:160/proxy/: foo (200; 11.467682ms)
Feb 21 19:57:57.380: INFO: (19) /api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/https:proxy-service-2grs7-2phpj:443/proxy/tlsrewritem... (200; 11.806599ms)
Feb 21 19:57:57.381: INFO: (19) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname2/proxy/: bar (200; 12.830953ms)
Feb 21 19:57:57.381: INFO: (19) /api/v1/namespaces/proxy-1749/pods/http:proxy-service-2grs7-2phpj:162/proxy/: bar (200; 12.116427ms)
Feb 21 19:57:57.380: INFO: (19) /api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/: <a href="/api/v1/namespaces/proxy-1749/pods/proxy-service-2grs7-2phpj/proxy/rewriteme">test</a> (200; 11.826717ms)
Feb 21 19:57:57.383: INFO: (19) /api/v1/namespaces/proxy-1749/services/http:proxy-service-2grs7:portname1/proxy/: foo (200; 15.056126ms)
Feb 21 19:57:57.385: INFO: (19) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname1/proxy/: foo (200; 16.653604ms)
Feb 21 19:57:57.385: INFO: (19) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname2/proxy/: tls qux (200; 16.387041ms)
Feb 21 19:57:57.385: INFO: (19) /api/v1/namespaces/proxy-1749/services/proxy-service-2grs7:portname2/proxy/: bar (200; 16.455382ms)
Feb 21 19:57:57.385: INFO: (19) /api/v1/namespaces/proxy-1749/services/https:proxy-service-2grs7:tlsportname1/proxy/: tls baz (200; 16.894634ms)
STEP: deleting ReplicationController proxy-service-2grs7 in namespace proxy-1749, will wait for the garbage collector to delete the pods
Feb 21 19:57:57.450: INFO: Deleting ReplicationController proxy-service-2grs7 took: 10.802037ms
Feb 21 19:57:58.351: INFO: Terminating ReplicationController proxy-service-2grs7 pods took: 900.424951ms
[AfterEach] version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:57:59.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1749" for this suite.
Feb 21 19:58:05.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:58:06.234: INFO: namespace proxy-1749 deletion completed in 6.277619765s

• [SLOW TEST:20.453 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:58:06.235: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4474
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-993d9b5c-bfaa-4d58-b63a-c42863622aca
STEP: Creating secret with name s-test-opt-upd-b2b7104b-3a92-47cc-bb64-a0a84cc541f7
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-993d9b5c-bfaa-4d58-b63a-c42863622aca
STEP: Updating secret s-test-opt-upd-b2b7104b-3a92-47cc-bb64-a0a84cc541f7
STEP: Creating secret with name s-test-opt-create-bb70476a-de5a-41cc-a9d5-fe3a377a3ff1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 19:58:12.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4474" for this suite.
Feb 21 19:58:36.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 19:58:36.869: INFO: namespace projected-4474 deletion completed in 24.283700401s

• [SLOW TEST:30.634 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 19:58:36.869: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4822
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 21 19:58:37.045: INFO: Found 0 stateful pods, waiting for 3
Feb 21 19:58:47.050: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 19:58:47.050: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 19:58:47.050: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 19:58:47.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-4822 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 19:58:47.246: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 19:58:47.246: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 19:58:47.246: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 21 19:58:57.282: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 21 19:59:07.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-4822 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 19:59:07.485: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 19:59:07.485: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 19:59:07.485: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 19:59:27.513: INFO: Waiting for StatefulSet statefulset-4822/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 21 19:59:37.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-4822 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 19:59:37.721: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 19:59:37.721: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 19:59:37.721: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 19:59:47.758: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 21 19:59:57.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-4822 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 19:59:57.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 19:59:57.990: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 19:59:57.990: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 20:00:08.015: INFO: Waiting for StatefulSet statefulset-4822/ss2 to complete update
Feb 21 20:00:08.015: INFO: Waiting for Pod statefulset-4822/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 21 20:00:08.015: INFO: Waiting for Pod statefulset-4822/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 21 20:00:18.025: INFO: Waiting for StatefulSet statefulset-4822/ss2 to complete update
Feb 21 20:00:18.025: INFO: Waiting for Pod statefulset-4822/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 21 20:00:28.024: INFO: Waiting for StatefulSet statefulset-4822/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 21 20:00:38.026: INFO: Deleting all statefulset in ns statefulset-4822
Feb 21 20:00:38.031: INFO: Scaling statefulset ss2 to 0
Feb 21 20:00:58.050: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 20:00:58.054: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:00:58.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4822" for this suite.
Feb 21 20:01:04.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:01:04.452: INFO: namespace statefulset-4822 deletion completed in 6.37542451s

• [SLOW TEST:147.583 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:01:04.453: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 20:01:06.650: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:01:06.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8362" for this suite.
Feb 21 20:01:12.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:01:12.944: INFO: namespace container-runtime-8362 deletion completed in 6.268522173s

• [SLOW TEST:8.492 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:01:12.945: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 20:01:13.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-1666'
Feb 21 20:01:13.190: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 21 20:01:13.190: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Feb 21 20:01:17.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1666'
Feb 21 20:01:17.297: INFO: stderr: ""
Feb 21 20:01:17.297: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:01:17.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1666" for this suite.
Feb 21 20:01:23.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:01:23.643: INFO: namespace kubectl-1666 deletion completed in 6.341292441s

• [SLOW TEST:10.699 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:01:23.644: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:01:23.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856" in namespace "downward-api-6135" to be "success or failure"
Feb 21 20:01:23.839: INFO: Pod "downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856": Phase="Pending", Reason="", readiness=false. Elapsed: 5.264202ms
Feb 21 20:01:25.844: INFO: Pod "downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009993725s
STEP: Saw pod success
Feb 21 20:01:25.844: INFO: Pod "downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856" satisfied condition "success or failure"
Feb 21 20:01:25.848: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856 container client-container: <nil>
STEP: delete the pod
Feb 21 20:01:25.877: INFO: Waiting for pod downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856 to disappear
Feb 21 20:01:25.882: INFO: Pod downwardapi-volume-39b6eb83-35ec-4246-be0f-d4222d4c5856 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:01:25.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6135" for this suite.
Feb 21 20:01:31.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:01:32.288: INFO: namespace downward-api-6135 deletion completed in 6.401493403s

• [SLOW TEST:8.645 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:01:32.289: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Feb 21 20:01:32.463: INFO: Waiting up to 5m0s for pod "var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa" in namespace "var-expansion-6810" to be "success or failure"
Feb 21 20:01:32.468: INFO: Pod "var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.060098ms
Feb 21 20:01:34.472: INFO: Pod "var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009832954s
STEP: Saw pod success
Feb 21 20:01:34.473: INFO: Pod "var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa" satisfied condition "success or failure"
Feb 21 20:01:34.477: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa container dapi-container: <nil>
STEP: delete the pod
Feb 21 20:01:34.501: INFO: Waiting for pod var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa to disappear
Feb 21 20:01:34.504: INFO: Pod var-expansion-76aabfb4-fd96-4d23-9212-32cb28431aaa no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:01:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6810" for this suite.
Feb 21 20:01:40.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:01:40.761: INFO: namespace var-expansion-6810 deletion completed in 6.252151372s

• [SLOW TEST:8.471 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:01:40.761: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3136
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-1af0cba2-fce5-4ccf-bd06-caa8ff410a27
STEP: Creating a pod to test consume configMaps
Feb 21 20:01:40.934: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b" in namespace "projected-3136" to be "success or failure"
Feb 21 20:01:40.938: INFO: Pod "pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579072ms
Feb 21 20:01:42.942: INFO: Pod "pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008258313s
STEP: Saw pod success
Feb 21 20:01:42.942: INFO: Pod "pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b" satisfied condition "success or failure"
Feb 21 20:01:42.948: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:01:42.978: INFO: Waiting for pod pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b to disappear
Feb 21 20:01:42.987: INFO: Pod pod-projected-configmaps-922fb714-c358-469e-bd21-f816b8a2d39b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:01:42.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3136" for this suite.
Feb 21 20:01:49.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:01:49.260: INFO: namespace projected-3136 deletion completed in 6.266612065s

• [SLOW TEST:8.499 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:01:49.261: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 20:01:49.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7730'
Feb 21 20:01:49.591: INFO: stderr: ""
Feb 21 20:01:49.591: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Feb 21 20:01:49.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete pods e2e-test-nginx-pod --namespace=kubectl-7730'
Feb 21 20:01:57.128: INFO: stderr: ""
Feb 21 20:01:57.128: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:01:57.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7730" for this suite.
Feb 21 20:02:03.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:02:03.447: INFO: namespace kubectl-7730 deletion completed in 6.309823295s

• [SLOW TEST:14.186 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:02:03.447: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 21 20:02:07.689: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:07.693: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:09.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:09.697: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:11.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:11.697: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:13.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:13.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:15.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:15.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:17.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:17.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:19.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:19.700: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:21.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:21.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:23.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:23.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:25.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:25.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:27.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:27.697: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:29.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:29.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:31.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:31.698: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:33.698: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:33.702: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:35.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:35.697: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 21 20:02:37.693: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 21 20:02:37.698: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:02:37.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8309" for this suite.
Feb 21 20:03:01.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:03:01.938: INFO: namespace container-lifecycle-hook-8309 deletion completed in 24.235446559s

• [SLOW TEST:58.491 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:03:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-230b18ca-f2a6-4264-b12e-1b20997fda93
STEP: Creating a pod to test consume secrets
Feb 21 20:03:02.138: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e" in namespace "projected-1092" to be "success or failure"
Feb 21 20:03:02.142: INFO: Pod "pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867481ms
Feb 21 20:03:04.146: INFO: Pod "pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008175706s
STEP: Saw pod success
Feb 21 20:03:04.146: INFO: Pod "pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e" satisfied condition "success or failure"
Feb 21 20:03:04.149: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:03:04.176: INFO: Waiting for pod pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e to disappear
Feb 21 20:03:04.180: INFO: Pod pod-projected-secrets-3a1dc985-9b06-49e6-bcf9-d7ab770ddb3e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:03:04.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1092" for this suite.
Feb 21 20:03:10.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:03:10.476: INFO: namespace projected-1092 deletion completed in 6.290714755s

• [SLOW TEST:8.535 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:03:10.476: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-8f1836ec-63c3-473b-b150-d2994d0ff046
STEP: Creating a pod to test consume secrets
Feb 21 20:03:10.674: INFO: Waiting up to 5m0s for pod "pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5" in namespace "secrets-1903" to be "success or failure"
Feb 21 20:03:10.680: INFO: Pod "pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760507ms
Feb 21 20:03:12.687: INFO: Pod "pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01237621s
STEP: Saw pod success
Feb 21 20:03:12.687: INFO: Pod "pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5" satisfied condition "success or failure"
Feb 21 20:03:12.694: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:03:12.724: INFO: Waiting for pod pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5 to disappear
Feb 21 20:03:12.729: INFO: Pod pod-secrets-f445b616-9f12-4a41-a716-18e8ae9902b5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:03:12.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1903" for this suite.
Feb 21 20:03:18.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:03:19.040: INFO: namespace secrets-1903 deletion completed in 6.304129435s

• [SLOW TEST:8.564 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:03:19.041: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:03:19.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4" in namespace "downward-api-5268" to be "success or failure"
Feb 21 20:03:19.262: INFO: Pod "downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228133ms
Feb 21 20:03:21.267: INFO: Pod "downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008603978s
STEP: Saw pod success
Feb 21 20:03:21.267: INFO: Pod "downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4" satisfied condition "success or failure"
Feb 21 20:03:21.271: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4 container client-container: <nil>
STEP: delete the pod
Feb 21 20:03:21.295: INFO: Waiting for pod downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4 to disappear
Feb 21 20:03:21.300: INFO: Pod downwardapi-volume-d6bfd6e5-cd86-4bef-9d0d-820a00219df4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:03:21.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5268" for this suite.
Feb 21 20:03:27.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:03:27.544: INFO: namespace downward-api-5268 deletion completed in 6.239540888s

• [SLOW TEST:8.504 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:03:27.545: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7383
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 20:03:27.696: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 20:03:47.900: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.163:8080/dial?request=hostName&protocol=http&host=100.103.74.146&port=8080&tries=1'] Namespace:pod-network-test-7383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:03:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:03:48.030: INFO: Waiting for endpoints: map[]
Feb 21 20:03:48.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.163:8080/dial?request=hostName&protocol=http&host=100.114.178.210&port=8080&tries=1'] Namespace:pod-network-test-7383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:03:48.038: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:03:48.184: INFO: Waiting for endpoints: map[]
Feb 21 20:03:48.198: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.163:8080/dial?request=hostName&protocol=http&host=100.119.255.81&port=8080&tries=1'] Namespace:pod-network-test-7383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:03:48.198: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:03:48.310: INFO: Waiting for endpoints: map[]
Feb 21 20:03:48.314: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.163:8080/dial?request=hostName&protocol=http&host=100.101.183.162&port=8080&tries=1'] Namespace:pod-network-test-7383 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:03:48.314: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:03:48.413: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:03:48.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7383" for this suite.
Feb 21 20:04:12.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:04:12.620: INFO: namespace pod-network-test-7383 deletion completed in 24.201842597s

• [SLOW TEST:45.076 seconds]
[sig-network] Networking
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:04:12.621: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 21 20:04:12.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-1699'
Feb 21 20:04:13.086: INFO: stderr: ""
Feb 21 20:04:13.086: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 20:04:13.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1699'
Feb 21 20:04:13.163: INFO: stderr: ""
Feb 21 20:04:13.163: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Feb 21 20:04:18.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1699'
Feb 21 20:04:18.242: INFO: stderr: ""
Feb 21 20:04:18.242: INFO: stdout: "update-demo-nautilus-hfpc6 update-demo-nautilus-pskxk "
Feb 21 20:04:18.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-hfpc6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1699'
Feb 21 20:04:18.339: INFO: stderr: ""
Feb 21 20:04:18.339: INFO: stdout: "true"
Feb 21 20:04:18.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-hfpc6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1699'
Feb 21 20:04:18.419: INFO: stderr: ""
Feb 21 20:04:18.419: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 20:04:18.419: INFO: validating pod update-demo-nautilus-hfpc6
Feb 21 20:04:18.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 20:04:18.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 20:04:18.427: INFO: update-demo-nautilus-hfpc6 is verified up and running
Feb 21 20:04:18.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-pskxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1699'
Feb 21 20:04:18.504: INFO: stderr: ""
Feb 21 20:04:18.504: INFO: stdout: "true"
Feb 21 20:04:18.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-pskxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1699'
Feb 21 20:04:18.578: INFO: stderr: ""
Feb 21 20:04:18.578: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 20:04:18.578: INFO: validating pod update-demo-nautilus-pskxk
Feb 21 20:04:18.586: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 20:04:18.586: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 20:04:18.586: INFO: update-demo-nautilus-pskxk is verified up and running
STEP: using delete to clean up resources
Feb 21 20:04:18.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-1699'
Feb 21 20:04:18.669: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:04:18.669: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 21 20:04:18.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1699'
Feb 21 20:04:18.759: INFO: stderr: "No resources found.\n"
Feb 21 20:04:18.759: INFO: stdout: ""
Feb 21 20:04:18.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -l name=update-demo --namespace=kubectl-1699 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 20:04:18.839: INFO: stderr: ""
Feb 21 20:04:18.839: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:04:18.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1699" for this suite.
Feb 21 20:04:42.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:04:43.140: INFO: namespace kubectl-1699 deletion completed in 24.295264269s

• [SLOW TEST:30.520 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:04:43.141: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-24a9ab47-d348-4bd3-874c-56821736631e
STEP: Creating a pod to test consume configMaps
Feb 21 20:04:43.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93" in namespace "projected-2340" to be "success or failure"
Feb 21 20:04:43.368: INFO: Pod "pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930556ms
Feb 21 20:04:45.372: INFO: Pod "pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008217362s
STEP: Saw pod success
Feb 21 20:04:45.372: INFO: Pod "pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93" satisfied condition "success or failure"
Feb 21 20:04:45.375: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:04:45.400: INFO: Waiting for pod pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93 to disappear
Feb 21 20:04:45.403: INFO: Pod pod-projected-configmaps-fab0cc9e-e0b5-4e30-8f9a-4d9bbe3c4e93 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:04:45.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2340" for this suite.
Feb 21 20:04:51.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:04:51.649: INFO: namespace projected-2340 deletion completed in 6.24137643s

• [SLOW TEST:8.509 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:04:51.650: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-60790a4f-2b04-472e-b70a-e2b2db4f6e9b
STEP: Creating a pod to test consume configMaps
Feb 21 20:04:51.854: INFO: Waiting up to 5m0s for pod "pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263" in namespace "configmap-4894" to be "success or failure"
Feb 21 20:04:51.862: INFO: Pod "pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263": Phase="Pending", Reason="", readiness=false. Elapsed: 7.921231ms
Feb 21 20:04:53.868: INFO: Pod "pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014479548s
STEP: Saw pod success
Feb 21 20:04:53.868: INFO: Pod "pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263" satisfied condition "success or failure"
Feb 21 20:04:53.873: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:04:53.902: INFO: Waiting for pod pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263 to disappear
Feb 21 20:04:53.906: INFO: Pod pod-configmaps-37cad7f9-af36-45f7-b005-bd4b6d2f3263 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:04:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4894" for this suite.
Feb 21 20:04:59.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:05:00.234: INFO: namespace configmap-4894 deletion completed in 6.322117155s

• [SLOW TEST:8.584 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:05:00.234: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 21 20:05:00.388: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 20:05:00.399: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 20:05:00.426: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-16-60.us-east-2.compute.internal before test
Feb 21 20:05:00.449: INFO: kube-dns-5fdb85bb5b-fjsw9 from kube-system started at 2020-02-21 19:41:16 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 21 20:05:00.449: INFO: 	Container kubedns ready: true, restart count 0
Feb 21 20:05:00.449: INFO: 	Container sidecar ready: true, restart count 0
Feb 21 20:05:00.449: INFO: nginx-ingress-default-backend-8bb99946f-l74lg from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.449: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 21 20:05:00.449: INFO: basic-demo-548c455c8-9w97q from demo started at 2020-02-21 19:49:39 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container basic-demo ready: true, restart count 0
Feb 21 20:05:00.449: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.449: INFO: vpa-recommender-77cbff9874-f8ccd from kube-system started at 2020-02-21 19:50:23 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container recommender ready: true, restart count 0
Feb 21 20:05:00.449: INFO: goldilocks-dashboard-547bc67bdc-5k6pq from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 20:05:00.449: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-nl44f from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:05:00.449: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:05:00.449: INFO: kube-proxy-ip-172-20-16-60.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:14 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:05:00.449: INFO: calico-node-wl9bf from kube-system started at 2020-02-21 19:40:59 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:05:00.449: INFO: calico-typha-5d656748c8-mtssq from kube-system started at 2020-02-21 19:41:16 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 20:05:00.449: INFO: linkerd-identity-55c9b95464-lg24p from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container identity ready: true, restart count 0
Feb 21 20:05:00.449: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.449: INFO: linkerd-destination-8894499df-fhn96 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.449: INFO: 	Container destination ready: true, restart count 3
Feb 21 20:05:00.449: INFO: 	Container linkerd-proxy ready: true, restart count 4
Feb 21 20:05:00.449: INFO: linkerd-proxy-injector-67fd7b6f8c-gbnlh from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.450: INFO: 	Container proxy-injector ready: true, restart count 0
Feb 21 20:05:00.450: INFO: linkerd-sp-validator-6c76b79bdd-9chl4 from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 20:05:00.450: INFO: cert-manager-75c599cb9b-cmdr6 from cert-manager started at 2020-02-21 19:48:53 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container cert-manager ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.450: INFO: kube-dns-autoscaler-577b4774b5-6978f from kube-system started at 2020-02-21 19:41:16 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container autoscaler ready: true, restart count 0
Feb 21 20:05:00.450: INFO: linkerd-tap-f6665c587-mqw6z from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.450: INFO: 	Container tap ready: true, restart count 0
Feb 21 20:05:00.450: INFO: external-dns-5b6fdf4bbd-xg7zx from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container external-dns ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.450: INFO: goldilocks-vpa-install-smd28 from goldilocks started at 2020-02-21 19:49:40 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container vpa-install ready: false, restart count 0
Feb 21 20:05:00.450: INFO: linkerd-controller-766f7cc6b-ktn5s from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container public-api ready: true, restart count 0
Feb 21 20:05:00.450: INFO: nginx-ingress-controller-6676bbc48c-xhjdt from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 20:05:00.450: INFO: rbac-manager-6497c75d88-n6574 from rbac-manager started at 2020-02-21 19:48:59 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 	Container rbac-manager ready: true, restart count 1
Feb 21 20:05:00.450: INFO: prometheus-operator-prometheus-node-exporter-4w66c from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.450: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:05:00.450: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-17-148.us-east-2.compute.internal before test
Feb 21 20:05:00.482: INFO: linkerd-sp-validator-6c76b79bdd-42xpr from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.482: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.482: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 20:05:00.482: INFO: external-dns-5b6fdf4bbd-5kncn from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.482: INFO: 	Container external-dns ready: true, restart count 0
Feb 21 20:05:00.482: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.482: INFO: prometheus-operator-grafana-769db59dcd-8pr2z from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.482: INFO: 	Container grafana ready: true, restart count 0
Feb 21 20:05:00.482: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Feb 21 20:05:00.482: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.482: INFO: prometheus-operator-prometheus-node-exporter-kkc6f from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.482: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:05:00.482: INFO: goldilocks-dashboard-547bc67bdc-scfmz from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 20:05:00.483: INFO: kube-dns-5fdb85bb5b-dk4sz from kube-system started at 2020-02-21 19:41:56 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container kubedns ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container sidecar ready: true, restart count 0
Feb 21 20:05:00.483: INFO: linkerd-tap-f6665c587-p2qjz from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container tap ready: true, restart count 0
Feb 21 20:05:00.483: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-4nkm6 from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:05:00.483: INFO: linkerd-controller-766f7cc6b-wxnj8 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container public-api ready: true, restart count 0
Feb 21 20:05:00.483: INFO: linkerd-grafana-6848df775f-dcxvr from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container grafana ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 3
Feb 21 20:05:00.483: INFO: linkerd-proxy-injector-67fd7b6f8c-h44bb from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 6
Feb 21 20:05:00.483: INFO: 	Container proxy-injector ready: true, restart count 7
Feb 21 20:05:00.483: INFO: nginx-ingress-controller-6676bbc48c-s48vd from nginx-ingress started at 2020-02-21 19:48:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 20:05:00.483: INFO: metrics-server-7f7cb69697-h977w from metrics-server started at 2020-02-21 19:49:07 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.483: INFO: 	Container metrics-server ready: true, restart count 0
Feb 21 20:05:00.483: INFO: prometheus-operator-operator-74d4bb4cc4-xvw88 from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.483: INFO: 	Container prometheus-operator ready: true, restart count 2
Feb 21 20:05:00.483: INFO: kube-proxy-ip-172-20-17-148.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:36 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.483: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:05:00.483: INFO: calico-node-lcqwl from kube-system started at 2020-02-21 19:41:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:05:00.484: INFO: calico-typha-5d656748c8-25s5c from kube-system started at 2020-02-21 19:41:46 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 20:05:00.484: INFO: linkerd-identity-55c9b95464-cp6f2 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container identity ready: true, restart count 0
Feb 21 20:05:00.484: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.484: INFO: linkerd-destination-8894499df-7w8rg from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:05:00.484: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.484: INFO: cert-manager-cainjector-7c8cff6f8c-nrtbx from cert-manager started at 2020-02-21 19:48:53 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container cainjector ready: true, restart count 1
Feb 21 20:05:00.484: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.484: INFO: basic-demo-548c455c8-f6hmk from demo started at 2020-02-21 19:49:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container basic-demo ready: true, restart count 0
Feb 21 20:05:00.484: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.484: INFO: alertmanager-prometheus-operator-alertmanager-0 from prometheus-operator started at 2020-02-21 19:50:09 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.484: INFO: 	Container alertmanager ready: true, restart count 0
Feb 21 20:05:00.484: INFO: 	Container config-reloader ready: true, restart count 0
Feb 21 20:05:00.484: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.484: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-17-149.us-east-2.compute.internal before test
Feb 21 20:05:00.492: INFO: kube-proxy-ip-172-20-17-149.us-east-2.compute.internal from kube-system started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.492: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:05:00.492: INFO: prometheus-operator-prometheus-node-exporter-lv8jq from prometheus-operator started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.492: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:05:00.492: INFO: calico-node-r6hl2 from kube-system started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.492: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:05:00.492: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-pnrct from sonobuoy started at 2020-02-21 19:52:37 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.492: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:05:00.492: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:05:00.492: INFO: prometheus-prometheus-operator-prometheus-0 from prometheus-operator started at 2020-02-21 19:53:00 +0000 UTC (4 container statuses recorded)
Feb 21 20:05:00.492: INFO: 	Container linkerd-proxy ready: false, restart count 7
Feb 21 20:05:00.492: INFO: 	Container prometheus ready: false, restart count 7
Feb 21 20:05:00.492: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 21 20:05:00.492: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 21 20:05:00.492: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-18-137.us-east-2.compute.internal before test
Feb 21 20:05:00.524: INFO: linkerd-destination-8894499df-jbvgn from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.524: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:05:00.524: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.524: INFO: linkerd-sp-validator-6c76b79bdd-nf5gb from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.524: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.524: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 20:05:00.524: INFO: linkerd-tap-f6665c587-bwb9s from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.524: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.524: INFO: 	Container tap ready: true, restart count 0
Feb 21 20:05:00.524: INFO: nginx-ingress-default-backend-8bb99946f-2mkfz from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.524: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.524: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 21 20:05:00.525: INFO: cluster-autoscaler-aws-cluster-autoscaler-5ff6c85c6d-sjlzh from cluster-autoscaler started at 2020-02-21 19:49:03 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container aws-cluster-autoscaler ready: true, restart count 3
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.525: INFO: basic-demo-548c455c8-6s29j from demo started at 2020-02-21 19:49:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container basic-demo ready: true, restart count 3
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 4
Feb 21 20:05:00.525: INFO: calico-node-wfjj9 from kube-system started at 2020-02-21 19:41:42 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:05:00.525: INFO: linkerd-identity-55c9b95464-zh22t from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container identity ready: true, restart count 0
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.525: INFO: linkerd-controller-766f7cc6b-zk2jg from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.525: INFO: 	Container public-api ready: true, restart count 0
Feb 21 20:05:00.525: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-rvbkh from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:05:00.525: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:05:00.525: INFO: calico-typha-5d656748c8-pc9zv from kube-system started at 2020-02-21 19:42:09 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 20:05:00.525: INFO: linkerd-proxy-injector-67fd7b6f8c-t9vtj from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.525: INFO: 	Container proxy-injector ready: true, restart count 0
Feb 21 20:05:00.525: INFO: nginx-ingress-controller-6676bbc48c-m69kk from nginx-ingress started at 2020-02-21 19:48:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:05:00.525: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 20:05:00.525: INFO: external-dns-5b6fdf4bbd-ssg7b from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container external-dns ready: true, restart count 1
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:05:00.525: INFO: kube-proxy-ip-172-20-18-137.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:17 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:05:00.525: INFO: linkerd-web-6fc4b84756-m5b28 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 3
Feb 21 20:05:00.525: INFO: 	Container web ready: true, restart count 1
Feb 21 20:05:00.525: INFO: linkerd-prometheus-799db67749-n279t from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.525: INFO: 	Container prometheus ready: true, restart count 0
Feb 21 20:05:00.525: INFO: prometheus-operator-kube-state-metrics-5d46566c59-6jn4w from prometheus-operator started at 2020-02-21 19:49:36 +0000 UTC (2 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container kube-state-metrics ready: true, restart count 2
Feb 21 20:05:00.525: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:05:00.525: INFO: prometheus-operator-prometheus-node-exporter-bcqpd from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:05:00.525: INFO: goldilocks-controller-6d64cc65c7-5fng8 from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 20:05:00.525: INFO: sonobuoy from sonobuoy started at 2020-02-21 19:52:20 +0000 UTC (1 container statuses recorded)
Feb 21 20:05:00.525: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8db43edb-181f-4d0c-b072-75ef205421d1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8db43edb-181f-4d0c-b072-75ef205421d1 off the node ip-172-20-17-149.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8db43edb-181f-4d0c-b072-75ef205421d1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:05:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3170" for this suite.
Feb 21 20:05:18.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:05:19.004: INFO: namespace sched-pred-3170 deletion completed in 12.363544172s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:18.769 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:05:19.004: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:05:19.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8" in namespace "projected-9103" to be "success or failure"
Feb 21 20:05:19.268: INFO: Pod "downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.559047ms
Feb 21 20:05:21.272: INFO: Pod "downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012005516s
STEP: Saw pod success
Feb 21 20:05:21.273: INFO: Pod "downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8" satisfied condition "success or failure"
Feb 21 20:05:21.276: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8 container client-container: <nil>
STEP: delete the pod
Feb 21 20:05:21.304: INFO: Waiting for pod downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8 to disappear
Feb 21 20:05:21.309: INFO: Pod downwardapi-volume-20d55d19-a30d-4d1f-98f2-cf512285acc8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:05:21.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9103" for this suite.
Feb 21 20:05:27.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:05:27.528: INFO: namespace projected-9103 deletion completed in 6.214329557s

• [SLOW TEST:8.524 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:05:27.529: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6320
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 in namespace container-probe-6320
Feb 21 20:05:31.711: INFO: Started pod liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 in namespace container-probe-6320
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 20:05:31.716: INFO: Initial restart count of pod liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 is 0
Feb 21 20:05:47.766: INFO: Restart count of pod container-probe-6320/liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 is now 1 (16.050369732s elapsed)
Feb 21 20:06:05.817: INFO: Restart count of pod container-probe-6320/liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 is now 2 (34.100857074s elapsed)
Feb 21 20:06:25.868: INFO: Restart count of pod container-probe-6320/liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 is now 3 (54.151808053s elapsed)
Feb 21 20:06:45.917: INFO: Restart count of pod container-probe-6320/liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 is now 4 (1m14.20089629s elapsed)
Feb 21 20:08:02.106: INFO: Restart count of pod container-probe-6320/liveness-491b3fea-3ab5-4730-b4ff-c861acb43078 is now 5 (2m30.390400021s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:08:02.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6320" for this suite.
Feb 21 20:08:08.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:08:08.381: INFO: namespace container-probe-6320 deletion completed in 6.255975275s

• [SLOW TEST:160.852 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:08:08.381: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2726
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-xnfn
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 20:08:08.659: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xnfn" in namespace "subpath-2726" to be "success or failure"
Feb 21 20:08:08.663: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.893843ms
Feb 21 20:08:10.668: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 2.008293554s
Feb 21 20:08:12.672: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 4.012770272s
Feb 21 20:08:14.677: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 6.017574835s
Feb 21 20:08:16.682: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 8.022258548s
Feb 21 20:08:18.689: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 10.029461299s
Feb 21 20:08:20.693: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 12.033959791s
Feb 21 20:08:22.698: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 14.038473503s
Feb 21 20:08:24.709: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 16.049302659s
Feb 21 20:08:26.713: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 18.053616851s
Feb 21 20:08:28.717: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Running", Reason="", readiness=true. Elapsed: 20.058084023s
Feb 21 20:08:30.722: INFO: Pod "pod-subpath-test-secret-xnfn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.062663674s
STEP: Saw pod success
Feb 21 20:08:30.722: INFO: Pod "pod-subpath-test-secret-xnfn" satisfied condition "success or failure"
Feb 21 20:08:30.726: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-subpath-test-secret-xnfn container test-container-subpath-secret-xnfn: <nil>
STEP: delete the pod
Feb 21 20:08:30.759: INFO: Waiting for pod pod-subpath-test-secret-xnfn to disappear
Feb 21 20:08:30.765: INFO: Pod pod-subpath-test-secret-xnfn no longer exists
STEP: Deleting pod pod-subpath-test-secret-xnfn
Feb 21 20:08:30.765: INFO: Deleting pod "pod-subpath-test-secret-xnfn" in namespace "subpath-2726"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:08:30.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2726" for this suite.
Feb 21 20:08:36.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:08:37.040: INFO: namespace subpath-2726 deletion completed in 6.26572024s

• [SLOW TEST:28.658 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:08:37.041: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:08:37.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 version'
Feb 21 20:08:37.255: INFO: stderr: ""
Feb 21 20:08:37.255: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.7\", GitCommit:\"6c143d35bb11d74970e7bc0b6c45b6bfdffc0bd4\", GitTreeState:\"clean\", BuildDate:\"2019-12-11T12:42:56Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.7\", GitCommit:\"6c143d35bb11d74970e7bc0b6c45b6bfdffc0bd4\", GitTreeState:\"clean\", BuildDate:\"2019-12-11T12:34:17Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:08:37.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9505" for this suite.
Feb 21 20:08:43.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:08:43.543: INFO: namespace kubectl-9505 deletion completed in 6.282101062s

• [SLOW TEST:6.502 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:08:43.543: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Feb 21 20:08:45.726: INFO: Pod pod-hostip-05df55a5-f448-4e80-a4fe-efc09bf9ae00 has hostIP: 172.20.17.149
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:08:45.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7412" for this suite.
Feb 21 20:09:07.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:09:07.943: INFO: namespace pods-7412 deletion completed in 22.211909164s

• [SLOW TEST:24.400 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:09:07.943: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5904
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-48a69563-80b4-478c-b31f-f42fd8309aa5
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:09:12.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5904" for this suite.
Feb 21 20:09:36.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:09:36.446: INFO: namespace configmap-5904 deletion completed in 24.27251501s

• [SLOW TEST:28.503 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:09:36.447: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1974
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1974
STEP: Creating statefulset with conflicting port in namespace statefulset-1974
STEP: Waiting until pod test-pod will start running in namespace statefulset-1974
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1974
Feb 21 20:09:40.679: INFO: Observed stateful pod in namespace: statefulset-1974, name: ss-0, uid: 91678171-0358-43b7-9d41-49d077de1bf5, status phase: Pending. Waiting for statefulset controller to delete.
Feb 21 20:09:41.074: INFO: Observed stateful pod in namespace: statefulset-1974, name: ss-0, uid: 91678171-0358-43b7-9d41-49d077de1bf5, status phase: Failed. Waiting for statefulset controller to delete.
Feb 21 20:09:41.085: INFO: Observed stateful pod in namespace: statefulset-1974, name: ss-0, uid: 91678171-0358-43b7-9d41-49d077de1bf5, status phase: Failed. Waiting for statefulset controller to delete.
Feb 21 20:09:41.094: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1974
STEP: Removing pod with conflicting port in namespace statefulset-1974
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1974 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 21 20:09:45.124: INFO: Deleting all statefulset in ns statefulset-1974
Feb 21 20:09:45.128: INFO: Scaling statefulset ss to 0
Feb 21 20:09:55.146: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 20:09:55.151: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:09:55.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1974" for this suite.
Feb 21 20:10:01.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:10:01.472: INFO: namespace statefulset-1974 deletion completed in 6.298627493s

• [SLOW TEST:25.025 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:10:01.472: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-745
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 20:10:01.681: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 20:10:24.047: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.119.255.82 8081 | grep -v '^\s*$'] Namespace:pod-network-test-745 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:10:24.047: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:10:25.138: INFO: Found all expected endpoints: [netserver-0]
Feb 21 20:10:25.143: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.114.178.211 8081 | grep -v '^\s*$'] Namespace:pod-network-test-745 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:10:25.143: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:10:26.230: INFO: Found all expected endpoints: [netserver-1]
Feb 21 20:10:26.234: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.103.74.149 8081 | grep -v '^\s*$'] Namespace:pod-network-test-745 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:10:26.235: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:10:27.348: INFO: Found all expected endpoints: [netserver-2]
Feb 21 20:10:27.351: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.183.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-745 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:10:27.352: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:10:28.449: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:10:28.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-745" for this suite.
Feb 21 20:10:52.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:10:52.826: INFO: namespace pod-network-test-745 deletion completed in 24.369301726s

• [SLOW TEST:51.354 seconds]
[sig-network] Networking
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:10:52.827: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-586
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 21 20:10:55.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec pod-sharedvolume-6c34b39a-747f-457f-81f7-2ea4ff96684c -c busybox-main-container --namespace=emptydir-586 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 21 20:10:55.403: INFO: stderr: ""
Feb 21 20:10:55.403: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:10:55.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-586" for this suite.
Feb 21 20:11:01.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:11:01.661: INFO: namespace emptydir-586 deletion completed in 6.252069719s

• [SLOW TEST:8.835 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:11:01.661: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Feb 21 20:11:01.883: INFO: Waiting up to 5m0s for pod "var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707" in namespace "var-expansion-8301" to be "success or failure"
Feb 21 20:11:01.894: INFO: Pod "var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707": Phase="Pending", Reason="", readiness=false. Elapsed: 11.460973ms
Feb 21 20:11:03.899: INFO: Pod "var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016054155s
STEP: Saw pod success
Feb 21 20:11:03.899: INFO: Pod "var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707" satisfied condition "success or failure"
Feb 21 20:11:03.902: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707 container dapi-container: <nil>
STEP: delete the pod
Feb 21 20:11:03.927: INFO: Waiting for pod var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707 to disappear
Feb 21 20:11:03.938: INFO: Pod var-expansion-9e95b03f-989e-4b7f-ad5d-6d3c6ad78707 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:11:03.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8301" for this suite.
Feb 21 20:11:09.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:11:10.243: INFO: namespace var-expansion-8301 deletion completed in 6.299992852s

• [SLOW TEST:8.581 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:11:10.243: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 21 20:11:14.476: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-59c6052c-c52c-470f-814e-4f625c5e4345,GenerateName:,Namespace:events-1443,SelfLink:/api/v1/namespaces/events-1443/pods/send-events-59c6052c-c52c-470f-814e-4f625c5e4345,UID:18024a92-9be7-4ebc-9976-d90ca24b5f29,ResourceVersion:10063,Generation:0,CreationTimestamp:2020-02-21 20:11:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 410979861,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.179/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cnqt7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cnqt7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-cnqt7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003137f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003137f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:11:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:11:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:11:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:11:10 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.179,StartTime:2020-02-21 20:11:10 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-02-21 20:11:12 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://97ffd38c7a75765e53be774084be67d56e95ae39fe55db4aecc559b6b4c951dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 21 20:11:16.485: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 21 20:11:18.490: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:11:18.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1443" for this suite.
Feb 21 20:11:58.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:11:58.838: INFO: namespace events-1443 deletion completed in 40.330994981s

• [SLOW TEST:48.595 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:11:58.839: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 21 20:11:59.067: INFO: Waiting up to 5m0s for pod "pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139" in namespace "emptydir-4896" to be "success or failure"
Feb 21 20:11:59.071: INFO: Pod "pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139": Phase="Pending", Reason="", readiness=false. Elapsed: 3.885315ms
Feb 21 20:12:01.076: INFO: Pod "pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008422395s
STEP: Saw pod success
Feb 21 20:12:01.076: INFO: Pod "pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139" satisfied condition "success or failure"
Feb 21 20:12:01.079: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139 container test-container: <nil>
STEP: delete the pod
Feb 21 20:12:01.113: INFO: Waiting for pod pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139 to disappear
Feb 21 20:12:01.117: INFO: Pod pod-fb7e7f6a-3d3a-456e-92f5-0f935a72e139 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:12:01.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4896" for this suite.
Feb 21 20:12:07.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:12:07.349: INFO: namespace emptydir-4896 deletion completed in 6.227606014s

• [SLOW TEST:8.511 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:12:07.350: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8769
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ade677c6-730c-462b-b15b-b2d4df401a1d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ade677c6-730c-462b-b15b-b2d4df401a1d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:12:11.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8769" for this suite.
Feb 21 20:12:35.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:12:35.937: INFO: namespace projected-8769 deletion completed in 24.30225575s

• [SLOW TEST:28.587 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:12:35.937: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:12:36.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6499" for this suite.
Feb 21 20:12:42.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:12:42.353: INFO: namespace services-6499 deletion completed in 6.250506344s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.416 seconds]
[sig-network] Services
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:12:42.354: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8852
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-8852/secret-test-c14e09b0-8c85-4298-9858-e1469f1682bd
STEP: Creating a pod to test consume secrets
Feb 21 20:12:42.538: INFO: Waiting up to 5m0s for pod "pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e" in namespace "secrets-8852" to be "success or failure"
Feb 21 20:12:42.543: INFO: Pod "pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882474ms
Feb 21 20:12:44.548: INFO: Pod "pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009526311s
STEP: Saw pod success
Feb 21 20:12:44.548: INFO: Pod "pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e" satisfied condition "success or failure"
Feb 21 20:12:44.551: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e container env-test: <nil>
STEP: delete the pod
Feb 21 20:12:44.577: INFO: Waiting for pod pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e to disappear
Feb 21 20:12:44.581: INFO: Pod pod-configmaps-39bc1244-0270-4956-b3ae-091a12abd99e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:12:44.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8852" for this suite.
Feb 21 20:12:50.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:12:50.838: INFO: namespace secrets-8852 deletion completed in 6.25212531s

• [SLOW TEST:8.484 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:12:50.838: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-366
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 21 20:12:51.053: INFO: Waiting up to 5m0s for pod "downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06" in namespace "downward-api-366" to be "success or failure"
Feb 21 20:12:51.057: INFO: Pod "downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11013ms
Feb 21 20:12:53.062: INFO: Pod "downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008884456s
STEP: Saw pod success
Feb 21 20:12:53.062: INFO: Pod "downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06" satisfied condition "success or failure"
Feb 21 20:12:53.066: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06 container dapi-container: <nil>
STEP: delete the pod
Feb 21 20:12:53.092: INFO: Waiting for pod downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06 to disappear
Feb 21 20:12:53.096: INFO: Pod downward-api-aeabd34d-4125-4beb-b4b9-befc0d0d0d06 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:12:53.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-366" for this suite.
Feb 21 20:12:59.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:12:59.448: INFO: namespace downward-api-366 deletion completed in 6.347629482s

• [SLOW TEST:8.610 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:12:59.449: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:13:59.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7190" for this suite.
Feb 21 20:14:23.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:14:23.955: INFO: namespace container-probe-7190 deletion completed in 24.285058904s

• [SLOW TEST:84.506 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:14:23.956: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 21 20:14:24.133: INFO: Waiting up to 5m0s for pod "pod-787ac630-67bb-4564-b3e3-a034f87f2756" in namespace "emptydir-1895" to be "success or failure"
Feb 21 20:14:24.138: INFO: Pod "pod-787ac630-67bb-4564-b3e3-a034f87f2756": Phase="Pending", Reason="", readiness=false. Elapsed: 5.038383ms
Feb 21 20:14:26.143: INFO: Pod "pod-787ac630-67bb-4564-b3e3-a034f87f2756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010095833s
STEP: Saw pod success
Feb 21 20:14:26.143: INFO: Pod "pod-787ac630-67bb-4564-b3e3-a034f87f2756" satisfied condition "success or failure"
Feb 21 20:14:26.147: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-787ac630-67bb-4564-b3e3-a034f87f2756 container test-container: <nil>
STEP: delete the pod
Feb 21 20:14:26.175: INFO: Waiting for pod pod-787ac630-67bb-4564-b3e3-a034f87f2756 to disappear
Feb 21 20:14:26.179: INFO: Pod pod-787ac630-67bb-4564-b3e3-a034f87f2756 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:14:26.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1895" for this suite.
Feb 21 20:14:32.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:14:32.452: INFO: namespace emptydir-1895 deletion completed in 6.265327263s

• [SLOW TEST:8.496 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:14:32.452: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Feb 21 20:14:32.627: INFO: Waiting up to 5m0s for pod "client-containers-988f86c7-f845-4731-a580-ca3041b91fd8" in namespace "containers-8489" to be "success or failure"
Feb 21 20:14:32.636: INFO: Pod "client-containers-988f86c7-f845-4731-a580-ca3041b91fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.890693ms
Feb 21 20:14:34.641: INFO: Pod "client-containers-988f86c7-f845-4731-a580-ca3041b91fd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013649415s
Feb 21 20:14:36.646: INFO: Pod "client-containers-988f86c7-f845-4731-a580-ca3041b91fd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018066567s
STEP: Saw pod success
Feb 21 20:14:36.646: INFO: Pod "client-containers-988f86c7-f845-4731-a580-ca3041b91fd8" satisfied condition "success or failure"
Feb 21 20:14:36.649: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod client-containers-988f86c7-f845-4731-a580-ca3041b91fd8 container test-container: <nil>
STEP: delete the pod
Feb 21 20:14:36.676: INFO: Waiting for pod client-containers-988f86c7-f845-4731-a580-ca3041b91fd8 to disappear
Feb 21 20:14:36.680: INFO: Pod client-containers-988f86c7-f845-4731-a580-ca3041b91fd8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:14:36.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8489" for this suite.
Feb 21 20:14:42.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:14:42.961: INFO: namespace containers-8489 deletion completed in 6.276624901s

• [SLOW TEST:10.509 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:14:42.962: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-61
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-b5c1cda9-6059-42df-839e-08bf3369bdef
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:14:43.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-61" for this suite.
Feb 21 20:14:49.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:14:49.369: INFO: namespace secrets-61 deletion completed in 6.244825208s

• [SLOW TEST:6.408 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:14:49.369: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Feb 21 20:14:49.533: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-389596720 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:14:49.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3923" for this suite.
Feb 21 20:14:55.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:14:55.875: INFO: namespace kubectl-3923 deletion completed in 6.275568628s

• [SLOW TEST:6.505 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:14:55.875: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 21 20:14:56.044: INFO: Waiting up to 5m0s for pod "pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef" in namespace "emptydir-8060" to be "success or failure"
Feb 21 20:14:56.049: INFO: Pod "pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752717ms
Feb 21 20:14:58.054: INFO: Pod "pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009369408s
STEP: Saw pod success
Feb 21 20:14:58.054: INFO: Pod "pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef" satisfied condition "success or failure"
Feb 21 20:14:58.058: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef container test-container: <nil>
STEP: delete the pod
Feb 21 20:14:58.083: INFO: Waiting for pod pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef to disappear
Feb 21 20:14:58.087: INFO: Pod pod-ab9fbada-5c3c-4c37-913c-bd08e33168ef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:14:58.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8060" for this suite.
Feb 21 20:15:04.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:15:04.322: INFO: namespace emptydir-8060 deletion completed in 6.229950969s

• [SLOW TEST:8.447 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:15:04.322: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 21 20:15:35.028: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0221 20:15:35.028724      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:15:35.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7222" for this suite.
Feb 21 20:15:41.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:15:41.241: INFO: namespace gc-7222 deletion completed in 6.207445595s

• [SLOW TEST:36.919 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:15:41.241: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 21 20:15:41.727: INFO: Pod name wrapped-volume-race-48d4ebd9-1e61-4d61-b898-48a8458a1c28: Found 0 pods out of 5
Feb 21 20:15:46.735: INFO: Pod name wrapped-volume-race-48d4ebd9-1e61-4d61-b898-48a8458a1c28: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-48d4ebd9-1e61-4d61-b898-48a8458a1c28 in namespace emptydir-wrapper-658, will wait for the garbage collector to delete the pods
Feb 21 20:15:58.829: INFO: Deleting ReplicationController wrapped-volume-race-48d4ebd9-1e61-4d61-b898-48a8458a1c28 took: 10.5132ms
Feb 21 20:15:58.929: INFO: Terminating ReplicationController wrapped-volume-race-48d4ebd9-1e61-4d61-b898-48a8458a1c28 pods took: 100.530581ms
STEP: Creating RC which spawns configmap-volume pods
Feb 21 20:16:35.160: INFO: Pod name wrapped-volume-race-97727f92-45ff-4348-b807-e9ba909c68f3: Found 0 pods out of 5
Feb 21 20:16:40.167: INFO: Pod name wrapped-volume-race-97727f92-45ff-4348-b807-e9ba909c68f3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-97727f92-45ff-4348-b807-e9ba909c68f3 in namespace emptydir-wrapper-658, will wait for the garbage collector to delete the pods
Feb 21 20:16:50.257: INFO: Deleting ReplicationController wrapped-volume-race-97727f92-45ff-4348-b807-e9ba909c68f3 took: 10.432559ms
Feb 21 20:16:50.357: INFO: Terminating ReplicationController wrapped-volume-race-97727f92-45ff-4348-b807-e9ba909c68f3 pods took: 100.404691ms
STEP: Creating RC which spawns configmap-volume pods
Feb 21 20:17:34.581: INFO: Pod name wrapped-volume-race-19a64f7d-7fcd-4134-a21a-a61a6e0150d1: Found 0 pods out of 5
Feb 21 20:17:39.588: INFO: Pod name wrapped-volume-race-19a64f7d-7fcd-4134-a21a-a61a6e0150d1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-19a64f7d-7fcd-4134-a21a-a61a6e0150d1 in namespace emptydir-wrapper-658, will wait for the garbage collector to delete the pods
Feb 21 20:17:49.679: INFO: Deleting ReplicationController wrapped-volume-race-19a64f7d-7fcd-4134-a21a-a61a6e0150d1 took: 12.281837ms
Feb 21 20:17:49.780: INFO: Terminating ReplicationController wrapped-volume-race-19a64f7d-7fcd-4134-a21a-a61a6e0150d1 pods took: 100.326551ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:18:35.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-658" for this suite.
Feb 21 20:18:43.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:18:43.367: INFO: namespace emptydir-wrapper-658 deletion completed in 8.319756711s

• [SLOW TEST:182.126 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:18:43.368: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-8ed0c287-36fd-4a30-a709-89c0d2a94121 in namespace container-probe-6706
Feb 21 20:18:45.571: INFO: Started pod busybox-8ed0c287-36fd-4a30-a709-89c0d2a94121 in namespace container-probe-6706
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 20:18:45.574: INFO: Initial restart count of pod busybox-8ed0c287-36fd-4a30-a709-89c0d2a94121 is 0
Feb 21 20:19:33.703: INFO: Restart count of pod container-probe-6706/busybox-8ed0c287-36fd-4a30-a709-89c0d2a94121 is now 1 (48.128076216s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:19:33.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6706" for this suite.
Feb 21 20:19:39.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:19:39.956: INFO: namespace container-probe-6706 deletion completed in 6.231855993s

• [SLOW TEST:56.588 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:19:39.956: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 21 20:19:40.114: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Feb 21 20:19:40.996: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 21 20:19:43.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913180, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 20:19:45.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913180, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 20:19:47.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913180, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 20:19:49.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913181, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717913180, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 20:19:53.514: INFO: Waited 2.415398339s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:19:54.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7369" for this suite.
Feb 21 20:20:00.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:20:00.898: INFO: namespace aggregator-7369 deletion completed in 6.338828621s

• [SLOW TEST:20.942 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:20:00.899: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-pmp8
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 20:20:01.081: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pmp8" in namespace "subpath-109" to be "success or failure"
Feb 21 20:20:01.087: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.445705ms
Feb 21 20:20:03.093: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 2.012135592s
Feb 21 20:20:05.098: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 4.01696288s
Feb 21 20:20:07.102: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 6.021559826s
Feb 21 20:20:09.107: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 8.026327219s
Feb 21 20:20:11.112: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 10.030787279s
Feb 21 20:20:13.116: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 12.035246974s
Feb 21 20:20:15.121: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 14.039756265s
Feb 21 20:20:17.125: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 16.044003742s
Feb 21 20:20:19.129: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 18.048373718s
Feb 21 20:20:21.134: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Running", Reason="", readiness=true. Elapsed: 20.053275454s
Feb 21 20:20:23.139: INFO: Pod "pod-subpath-test-projected-pmp8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.057891473s
STEP: Saw pod success
Feb 21 20:20:23.139: INFO: Pod "pod-subpath-test-projected-pmp8" satisfied condition "success or failure"
Feb 21 20:20:23.143: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-subpath-test-projected-pmp8 container test-container-subpath-projected-pmp8: <nil>
STEP: delete the pod
Feb 21 20:20:23.170: INFO: Waiting for pod pod-subpath-test-projected-pmp8 to disappear
Feb 21 20:20:23.174: INFO: Pod pod-subpath-test-projected-pmp8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-pmp8
Feb 21 20:20:23.174: INFO: Deleting pod "pod-subpath-test-projected-pmp8" in namespace "subpath-109"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:20:23.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-109" for this suite.
Feb 21 20:20:29.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:20:29.407: INFO: namespace subpath-109 deletion completed in 6.220280782s

• [SLOW TEST:28.508 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:20:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-55e79a2b-0698-4a55-8c59-79f340e30670
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:20:29.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9984" for this suite.
Feb 21 20:20:35.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:20:35.868: INFO: namespace configmap-9984 deletion completed in 6.297781232s

• [SLOW TEST:6.460 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:20:35.869: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-29dc90d3-9212-4eb6-a32f-835e693b944c
STEP: Creating a pod to test consume configMaps
Feb 21 20:20:36.059: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7" in namespace "projected-2507" to be "success or failure"
Feb 21 20:20:36.063: INFO: Pod "pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2834ms
Feb 21 20:20:38.068: INFO: Pod "pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009062394s
STEP: Saw pod success
Feb 21 20:20:38.068: INFO: Pod "pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7" satisfied condition "success or failure"
Feb 21 20:20:38.072: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:20:38.106: INFO: Waiting for pod pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7 to disappear
Feb 21 20:20:38.110: INFO: Pod pod-projected-configmaps-c0847ae0-cbd1-4a59-afd6-14bc115828b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:20:38.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2507" for this suite.
Feb 21 20:20:44.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:20:44.350: INFO: namespace projected-2507 deletion completed in 6.234933233s

• [SLOW TEST:8.482 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:20:44.351: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:20:44.558: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3" in namespace "projected-9524" to be "success or failure"
Feb 21 20:20:44.562: INFO: Pod "downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907738ms
Feb 21 20:20:46.566: INFO: Pod "downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00811624s
STEP: Saw pod success
Feb 21 20:20:46.566: INFO: Pod "downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3" satisfied condition "success or failure"
Feb 21 20:20:46.570: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3 container client-container: <nil>
STEP: delete the pod
Feb 21 20:20:46.599: INFO: Waiting for pod downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3 to disappear
Feb 21 20:20:46.605: INFO: Pod downwardapi-volume-4e093f25-4d0b-4bd1-8bd2-dd0b7b618cc3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:20:46.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9524" for this suite.
Feb 21 20:20:52.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:20:52.837: INFO: namespace projected-9524 deletion completed in 6.227863843s

• [SLOW TEST:8.486 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:20:52.838: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-7420c5cf-8843-49ec-826d-49d8873de6d5
STEP: Creating a pod to test consume secrets
Feb 21 20:20:53.008: INFO: Waiting up to 5m0s for pod "pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e" in namespace "secrets-3453" to be "success or failure"
Feb 21 20:20:53.017: INFO: Pod "pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.9146ms
Feb 21 20:20:55.022: INFO: Pod "pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013465565s
STEP: Saw pod success
Feb 21 20:20:55.022: INFO: Pod "pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e" satisfied condition "success or failure"
Feb 21 20:20:55.025: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:20:55.051: INFO: Waiting for pod pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e to disappear
Feb 21 20:20:55.055: INFO: Pod pod-secrets-2579b0bb-f43b-4d64-ab63-cf8aa176f00e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:20:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3453" for this suite.
Feb 21 20:21:01.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:21:01.455: INFO: namespace secrets-3453 deletion completed in 6.394695504s

• [SLOW TEST:8.617 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:21:01.458: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:21:01.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3" in namespace "downward-api-4276" to be "success or failure"
Feb 21 20:21:01.652: INFO: Pod "downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.14747ms
Feb 21 20:21:03.657: INFO: Pod "downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011363098s
STEP: Saw pod success
Feb 21 20:21:03.657: INFO: Pod "downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3" satisfied condition "success or failure"
Feb 21 20:21:03.661: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3 container client-container: <nil>
STEP: delete the pod
Feb 21 20:21:03.686: INFO: Waiting for pod downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3 to disappear
Feb 21 20:21:03.690: INFO: Pod downwardapi-volume-6a94159a-c3df-4a6c-b7e3-d739990f69b3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:21:03.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4276" for this suite.
Feb 21 20:21:09.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:21:09.934: INFO: namespace downward-api-4276 deletion completed in 6.239570228s

• [SLOW TEST:8.477 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:21:09.935: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:21:10.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3637" for this suite.
Feb 21 20:21:16.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:21:16.410: INFO: namespace kubelet-test-3637 deletion completed in 6.229379649s

• [SLOW TEST:6.475 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:21:16.410: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Feb 21 20:21:16.583: INFO: Waiting up to 5m0s for pod "client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7" in namespace "containers-3135" to be "success or failure"
Feb 21 20:21:16.593: INFO: Pod "client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.277642ms
Feb 21 20:21:18.601: INFO: Pod "client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017917433s
STEP: Saw pod success
Feb 21 20:21:18.601: INFO: Pod "client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7" satisfied condition "success or failure"
Feb 21 20:21:18.605: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7 container test-container: <nil>
STEP: delete the pod
Feb 21 20:21:18.633: INFO: Waiting for pod client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7 to disappear
Feb 21 20:21:18.636: INFO: Pod client-containers-cbe9243a-a6ac-40ea-a4fc-d1143ea86cc7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:21:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3135" for this suite.
Feb 21 20:21:24.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:21:24.891: INFO: namespace containers-3135 deletion completed in 6.249426495s

• [SLOW TEST:8.481 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:21:24.892: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:21:49.076: INFO: Container started at 2020-02-21 20:21:25 +0000 UTC, pod became ready at 2020-02-21 20:21:47 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:21:49.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3207" for this suite.
Feb 21 20:22:11.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:22:11.289: INFO: namespace container-probe-3207 deletion completed in 22.20812294s

• [SLOW TEST:46.398 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:22:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:22:11.563: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6" in namespace "downward-api-7508" to be "success or failure"
Feb 21 20:22:11.568: INFO: Pod "downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536596ms
Feb 21 20:22:13.572: INFO: Pod "downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008466231s
STEP: Saw pod success
Feb 21 20:22:13.572: INFO: Pod "downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6" satisfied condition "success or failure"
Feb 21 20:22:13.576: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6 container client-container: <nil>
STEP: delete the pod
Feb 21 20:22:13.602: INFO: Waiting for pod downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6 to disappear
Feb 21 20:22:13.606: INFO: Pod downwardapi-volume-7df8d43f-6f71-4b43-a091-ac8ba13bc4c6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:22:13.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7508" for this suite.
Feb 21 20:22:19.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:22:19.838: INFO: namespace downward-api-7508 deletion completed in 6.227339695s

• [SLOW TEST:8.549 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:22:19.839: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3841
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-920cd704-8803-49b7-883e-1b5ed8f6f8c7
STEP: Creating configMap with name cm-test-opt-upd-d00bba03-fd9b-4116-b7b1-03bcedabcdbc
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-920cd704-8803-49b7-883e-1b5ed8f6f8c7
STEP: Updating configmap cm-test-opt-upd-d00bba03-fd9b-4116-b7b1-03bcedabcdbc
STEP: Creating configMap with name cm-test-opt-create-5f07b05b-75a4-43d7-baf3-8b56b8b4f74f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:23:30.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3841" for this suite.
Feb 21 20:23:54.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:23:54.954: INFO: namespace projected-3841 deletion completed in 24.311543609s

• [SLOW TEST:95.116 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:23:54.955: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 21 20:23:55.128: INFO: namespace kubectl-8592
Feb 21 20:23:55.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-8592'
Feb 21 20:23:55.641: INFO: stderr: ""
Feb 21 20:23:55.641: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 21 20:23:56.647: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 20:23:56.647: INFO: Found 0 / 1
Feb 21 20:23:57.646: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 20:23:57.646: INFO: Found 0 / 1
Feb 21 20:23:58.646: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 20:23:58.646: INFO: Found 1 / 1
Feb 21 20:23:58.646: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 20:23:58.650: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 20:23:58.650: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 20:23:58.650: INFO: wait on redis-master startup in kubectl-8592 
Feb 21 20:23:58.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-dwv55 redis-master --namespace=kubectl-8592'
Feb 21 20:23:58.739: INFO: stderr: ""
Feb 21 20:23:58.739: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Feb 20:23:57.725 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Feb 20:23:57.725 # Server started, Redis version 3.2.12\n1:M 21 Feb 20:23:57.725 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Feb 20:23:57.726 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 21 20:23:58.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8592'
Feb 21 20:23:58.847: INFO: stderr: ""
Feb 21 20:23:58.847: INFO: stdout: "service/rm2 exposed\n"
Feb 21 20:23:58.851: INFO: Service rm2 in namespace kubectl-8592 found.
STEP: exposing service
Feb 21 20:24:00.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8592'
Feb 21 20:24:00.965: INFO: stderr: ""
Feb 21 20:24:00.965: INFO: stdout: "service/rm3 exposed\n"
Feb 21 20:24:00.972: INFO: Service rm3 in namespace kubectl-8592 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:24:02.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8592" for this suite.
Feb 21 20:24:27.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:24:27.258: INFO: namespace kubectl-8592 deletion completed in 24.272502885s

• [SLOW TEST:32.303 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:24:27.259: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:24:27.433: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c" in namespace "projected-7785" to be "success or failure"
Feb 21 20:24:27.440: INFO: Pod "downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.148121ms
Feb 21 20:24:29.444: INFO: Pod "downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009788219s
STEP: Saw pod success
Feb 21 20:24:29.444: INFO: Pod "downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c" satisfied condition "success or failure"
Feb 21 20:24:29.448: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c container client-container: <nil>
STEP: delete the pod
Feb 21 20:24:29.477: INFO: Waiting for pod downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c to disappear
Feb 21 20:24:29.482: INFO: Pod downwardapi-volume-b949830e-91d2-4e17-8297-12feb1e7627c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:24:29.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7785" for this suite.
Feb 21 20:24:35.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:24:35.743: INFO: namespace projected-7785 deletion completed in 6.256619901s

• [SLOW TEST:8.484 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:24:35.744: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-700e92b2-8c82-4b54-9d04-2361dce0dc09
STEP: Creating a pod to test consume configMaps
Feb 21 20:24:35.960: INFO: Waiting up to 5m0s for pod "pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc" in namespace "configmap-7468" to be "success or failure"
Feb 21 20:24:35.964: INFO: Pod "pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.684139ms
Feb 21 20:24:37.968: INFO: Pod "pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008379133s
STEP: Saw pod success
Feb 21 20:24:37.968: INFO: Pod "pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc" satisfied condition "success or failure"
Feb 21 20:24:37.972: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:24:37.998: INFO: Waiting for pod pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc to disappear
Feb 21 20:24:38.002: INFO: Pod pod-configmaps-244ac776-fca8-438e-935c-6875280b17cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:24:38.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7468" for this suite.
Feb 21 20:24:44.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:24:44.244: INFO: namespace configmap-7468 deletion completed in 6.237572047s

• [SLOW TEST:8.500 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:24:44.244: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6424
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-90c5af3a-141f-4e5b-8299-aa46453e36a8 in namespace container-probe-6424
Feb 21 20:24:46.428: INFO: Started pod test-webserver-90c5af3a-141f-4e5b-8299-aa46453e36a8 in namespace container-probe-6424
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 20:24:46.432: INFO: Initial restart count of pod test-webserver-90c5af3a-141f-4e5b-8299-aa46453e36a8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:28:47.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6424" for this suite.
Feb 21 20:28:53.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:28:53.273: INFO: namespace container-probe-6424 deletion completed in 6.22059892s

• [SLOW TEST:249.029 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:28:53.274: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 21 20:28:55.986: INFO: Successfully updated pod "annotationupdateb13035fe-8466-4fa7-a135-aaf5ba186772"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:29:00.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5539" for this suite.
Feb 21 20:29:24.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:29:24.239: INFO: namespace projected-5539 deletion completed in 24.21578598s

• [SLOW TEST:30.965 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:29:24.240: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-krj5
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 20:29:24.415: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-krj5" in namespace "subpath-8129" to be "success or failure"
Feb 21 20:29:24.423: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348663ms
Feb 21 20:29:26.429: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013711301s
Feb 21 20:29:28.433: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 4.018083317s
Feb 21 20:29:30.438: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 6.022643584s
Feb 21 20:29:32.443: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 8.027678595s
Feb 21 20:29:34.447: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 10.032150309s
Feb 21 20:29:36.452: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 12.03694518s
Feb 21 20:29:38.457: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 14.041890609s
Feb 21 20:29:40.461: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 16.046347721s
Feb 21 20:29:42.466: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 18.051170204s
Feb 21 20:29:44.471: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Running", Reason="", readiness=true. Elapsed: 20.055694335s
Feb 21 20:29:46.475: INFO: Pod "pod-subpath-test-downwardapi-krj5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.060202758s
STEP: Saw pod success
Feb 21 20:29:46.475: INFO: Pod "pod-subpath-test-downwardapi-krj5" satisfied condition "success or failure"
Feb 21 20:29:46.479: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-subpath-test-downwardapi-krj5 container test-container-subpath-downwardapi-krj5: <nil>
STEP: delete the pod
Feb 21 20:29:46.503: INFO: Waiting for pod pod-subpath-test-downwardapi-krj5 to disappear
Feb 21 20:29:46.507: INFO: Pod pod-subpath-test-downwardapi-krj5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-krj5
Feb 21 20:29:46.507: INFO: Deleting pod "pod-subpath-test-downwardapi-krj5" in namespace "subpath-8129"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:29:46.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8129" for this suite.
Feb 21 20:29:52.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:29:52.734: INFO: namespace subpath-8129 deletion completed in 6.218841271s

• [SLOW TEST:28.495 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:29:52.735: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 20:29:52.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-716'
Feb 21 20:29:53.000: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 21 20:29:53.000: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 21 20:29:53.017: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 21 20:29:53.018: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 21 20:29:53.035: INFO: scanned /root for discovery docs: <nil>
Feb 21 20:29:53.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-716'
Feb 21 20:30:08.836: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 21 20:30:08.836: INFO: stdout: "Created e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2\nScaling up e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 21 20:30:08.836: INFO: stdout: "Created e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2\nScaling up e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 21 20:30:08.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-716'
Feb 21 20:30:08.934: INFO: stderr: ""
Feb 21 20:30:08.934: INFO: stdout: "e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2-qkchw "
Feb 21 20:30:08.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2-qkchw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-716'
Feb 21 20:30:09.030: INFO: stderr: ""
Feb 21 20:30:09.030: INFO: stdout: "true"
Feb 21 20:30:09.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2-qkchw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-716'
Feb 21 20:30:09.105: INFO: stderr: ""
Feb 21 20:30:09.105: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 21 20:30:09.105: INFO: e2e-test-nginx-rc-33706cc992170198438c4b2ffd8667c2-qkchw is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Feb 21 20:30:09.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete rc e2e-test-nginx-rc --namespace=kubectl-716'
Feb 21 20:30:09.195: INFO: stderr: ""
Feb 21 20:30:09.195: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:30:09.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-716" for this suite.
Feb 21 20:30:33.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:30:33.445: INFO: namespace kubectl-716 deletion completed in 24.242418171s

• [SLOW TEST:40.710 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:30:33.446: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Feb 21 20:30:33.615: INFO: Waiting up to 5m0s for pod "var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113" in namespace "var-expansion-2796" to be "success or failure"
Feb 21 20:30:33.623: INFO: Pod "var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113": Phase="Pending", Reason="", readiness=false. Elapsed: 7.78917ms
Feb 21 20:30:35.628: INFO: Pod "var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012659096s
STEP: Saw pod success
Feb 21 20:30:35.628: INFO: Pod "var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113" satisfied condition "success or failure"
Feb 21 20:30:35.632: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113 container dapi-container: <nil>
STEP: delete the pod
Feb 21 20:30:35.659: INFO: Waiting for pod var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113 to disappear
Feb 21 20:30:35.663: INFO: Pod var-expansion-5e7b6a95-2d1c-4c94-bfbd-de90278f6113 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:30:35.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2796" for this suite.
Feb 21 20:30:41.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:30:41.928: INFO: namespace var-expansion-2796 deletion completed in 6.259979726s

• [SLOW TEST:8.482 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:30:41.928: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 20:30:42.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9181'
Feb 21 20:30:42.183: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 21 20:30:42.183: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 21 20:30:42.193: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rpv9g]
Feb 21 20:30:42.194: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rpv9g" in namespace "kubectl-9181" to be "running and ready"
Feb 21 20:30:42.204: INFO: Pod "e2e-test-nginx-rc-rpv9g": Phase="Pending", Reason="", readiness=false. Elapsed: 10.051834ms
Feb 21 20:30:44.208: INFO: Pod "e2e-test-nginx-rc-rpv9g": Phase="Running", Reason="", readiness=true. Elapsed: 2.014684821s
Feb 21 20:30:44.208: INFO: Pod "e2e-test-nginx-rc-rpv9g" satisfied condition "running and ready"
Feb 21 20:30:44.208: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rpv9g]
Feb 21 20:30:44.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs rc/e2e-test-nginx-rc --namespace=kubectl-9181'
Feb 21 20:30:44.308: INFO: stderr: ""
Feb 21 20:30:44.308: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Feb 21 20:30:44.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete rc e2e-test-nginx-rc --namespace=kubectl-9181'
Feb 21 20:30:44.396: INFO: stderr: ""
Feb 21 20:30:44.396: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:30:44.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9181" for this suite.
Feb 21 20:31:08.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:31:08.664: INFO: namespace kubectl-9181 deletion completed in 24.260805511s

• [SLOW TEST:26.736 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:31:08.665: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Feb 21 20:31:08.842: INFO: Waiting up to 5m0s for pod "client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4" in namespace "containers-171" to be "success or failure"
Feb 21 20:31:08.845: INFO: Pod "client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.76527ms
Feb 21 20:31:10.852: INFO: Pod "client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010450506s
STEP: Saw pod success
Feb 21 20:31:10.852: INFO: Pod "client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4" satisfied condition "success or failure"
Feb 21 20:31:10.856: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4 container test-container: <nil>
STEP: delete the pod
Feb 21 20:31:10.887: INFO: Waiting for pod client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4 to disappear
Feb 21 20:31:10.891: INFO: Pod client-containers-bd572c5b-230e-4ec2-aa68-492ecd7d53e4 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:31:10.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-171" for this suite.
Feb 21 20:31:16.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:31:17.102: INFO: namespace containers-171 deletion completed in 6.205806972s

• [SLOW TEST:8.437 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:31:17.102: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Feb 21 20:31:17.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-5655'
Feb 21 20:31:17.526: INFO: stderr: ""
Feb 21 20:31:17.526: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 20:31:17.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5655'
Feb 21 20:31:17.633: INFO: stderr: ""
Feb 21 20:31:17.633: INFO: stdout: "update-demo-nautilus-9nskw update-demo-nautilus-ssclb "
Feb 21 20:31:17.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-9nskw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:17.704: INFO: stderr: ""
Feb 21 20:31:17.704: INFO: stdout: ""
Feb 21 20:31:17.704: INFO: update-demo-nautilus-9nskw is created but not running
Feb 21 20:31:22.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5655'
Feb 21 20:31:22.784: INFO: stderr: ""
Feb 21 20:31:22.784: INFO: stdout: "update-demo-nautilus-9nskw update-demo-nautilus-ssclb "
Feb 21 20:31:22.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-9nskw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:22.863: INFO: stderr: ""
Feb 21 20:31:22.863: INFO: stdout: "true"
Feb 21 20:31:22.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-9nskw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:22.936: INFO: stderr: ""
Feb 21 20:31:22.936: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 20:31:22.936: INFO: validating pod update-demo-nautilus-9nskw
Feb 21 20:31:22.946: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 20:31:22.946: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 20:31:22.946: INFO: update-demo-nautilus-9nskw is verified up and running
Feb 21 20:31:22.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-ssclb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:23.032: INFO: stderr: ""
Feb 21 20:31:23.032: INFO: stdout: "true"
Feb 21 20:31:23.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-ssclb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:23.106: INFO: stderr: ""
Feb 21 20:31:23.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 20:31:23.106: INFO: validating pod update-demo-nautilus-ssclb
Feb 21 20:31:23.113: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 20:31:23.113: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 20:31:23.113: INFO: update-demo-nautilus-ssclb is verified up and running
STEP: rolling-update to new replication controller
Feb 21 20:31:23.115: INFO: scanned /root for discovery docs: <nil>
Feb 21 20:31:23.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5655'
Feb 21 20:31:45.766: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 21 20:31:45.766: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 20:31:45.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5655'
Feb 21 20:31:45.852: INFO: stderr: ""
Feb 21 20:31:45.852: INFO: stdout: "update-demo-kitten-ptr96 update-demo-kitten-xzqxc update-demo-nautilus-ssclb "
STEP: Replicas for name=update-demo: expected=2 actual=3
Feb 21 20:31:50.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5655'
Feb 21 20:31:50.927: INFO: stderr: ""
Feb 21 20:31:50.927: INFO: stdout: "update-demo-kitten-ptr96 update-demo-kitten-xzqxc "
Feb 21 20:31:50.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-kitten-ptr96 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:51.022: INFO: stderr: ""
Feb 21 20:31:51.022: INFO: stdout: "true"
Feb 21 20:31:51.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-kitten-ptr96 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:51.098: INFO: stderr: ""
Feb 21 20:31:51.098: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 21 20:31:51.098: INFO: validating pod update-demo-kitten-ptr96
Feb 21 20:31:51.109: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 21 20:31:51.109: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 21 20:31:51.109: INFO: update-demo-kitten-ptr96 is verified up and running
Feb 21 20:31:51.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-kitten-xzqxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:51.194: INFO: stderr: ""
Feb 21 20:31:51.194: INFO: stdout: "true"
Feb 21 20:31:51.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-kitten-xzqxc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5655'
Feb 21 20:31:51.264: INFO: stderr: ""
Feb 21 20:31:51.264: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 21 20:31:51.264: INFO: validating pod update-demo-kitten-xzqxc
Feb 21 20:31:51.271: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 21 20:31:51.271: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 21 20:31:51.272: INFO: update-demo-kitten-xzqxc is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:31:51.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5655" for this suite.
Feb 21 20:32:15.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:32:15.537: INFO: namespace kubectl-5655 deletion completed in 24.260434614s

• [SLOW TEST:58.435 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:32:15.538: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 21 20:32:25.843: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0221 20:32:25.843660      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:32:25.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8097" for this suite.
Feb 21 20:32:33.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:32:34.147: INFO: namespace gc-8097 deletion completed in 8.293975906s

• [SLOW TEST:18.609 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:32:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:32:34.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2" in namespace "downward-api-5223" to be "success or failure"
Feb 21 20:32:34.365: INFO: Pod "downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705786ms
Feb 21 20:32:36.369: INFO: Pod "downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009014642s
STEP: Saw pod success
Feb 21 20:32:36.369: INFO: Pod "downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2" satisfied condition "success or failure"
Feb 21 20:32:36.373: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2 container client-container: <nil>
STEP: delete the pod
Feb 21 20:32:36.400: INFO: Waiting for pod downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2 to disappear
Feb 21 20:32:36.404: INFO: Pod downwardapi-volume-97d55b98-db3d-431b-bcad-19895a437cd2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:32:36.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5223" for this suite.
Feb 21 20:32:42.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:32:42.605: INFO: namespace downward-api-5223 deletion completed in 6.195516389s

• [SLOW TEST:8.458 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:32:42.606: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 21 20:32:42.858: INFO: Waiting up to 5m0s for pod "pod-e6a1978c-7762-42ed-a670-ffdce8265834" in namespace "emptydir-3449" to be "success or failure"
Feb 21 20:32:42.861: INFO: Pod "pod-e6a1978c-7762-42ed-a670-ffdce8265834": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718329ms
Feb 21 20:32:44.866: INFO: Pod "pod-e6a1978c-7762-42ed-a670-ffdce8265834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008057472s
STEP: Saw pod success
Feb 21 20:32:44.866: INFO: Pod "pod-e6a1978c-7762-42ed-a670-ffdce8265834" satisfied condition "success or failure"
Feb 21 20:32:44.869: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-e6a1978c-7762-42ed-a670-ffdce8265834 container test-container: <nil>
STEP: delete the pod
Feb 21 20:32:44.895: INFO: Waiting for pod pod-e6a1978c-7762-42ed-a670-ffdce8265834 to disappear
Feb 21 20:32:44.899: INFO: Pod pod-e6a1978c-7762-42ed-a670-ffdce8265834 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:32:44.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3449" for this suite.
Feb 21 20:32:50.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:32:51.147: INFO: namespace emptydir-3449 deletion completed in 6.243054023s

• [SLOW TEST:8.541 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:32:51.148: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-6062d573-1fd3-493f-9c5d-b9bf14340cfe
STEP: Creating a pod to test consume configMaps
Feb 21 20:32:51.359: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854" in namespace "projected-665" to be "success or failure"
Feb 21 20:32:51.363: INFO: Pod "pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.418095ms
Feb 21 20:32:53.368: INFO: Pod "pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009466023s
STEP: Saw pod success
Feb 21 20:32:53.368: INFO: Pod "pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854" satisfied condition "success or failure"
Feb 21 20:32:53.372: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:32:53.399: INFO: Waiting for pod pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854 to disappear
Feb 21 20:32:53.403: INFO: Pod pod-projected-configmaps-09a3c287-63ac-4096-80fa-2c76a9586854 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:32:53.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-665" for this suite.
Feb 21 20:32:59.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:32:59.643: INFO: namespace projected-665 deletion completed in 6.235381911s

• [SLOW TEST:8.495 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:32:59.643: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 21 20:32:59.812: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8556,SelfLink:/api/v1/namespaces/watch-8556/configmaps/e2e-watch-test-watch-closed,UID:4ac1e097-7bcf-44be-b5d1-faf6a3ece238,ResourceVersion:16808,Generation:0,CreationTimestamp:2020-02-21 20:32:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 20:32:59.813: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8556,SelfLink:/api/v1/namespaces/watch-8556/configmaps/e2e-watch-test-watch-closed,UID:4ac1e097-7bcf-44be-b5d1-faf6a3ece238,ResourceVersion:16809,Generation:0,CreationTimestamp:2020-02-21 20:32:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 21 20:32:59.832: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8556,SelfLink:/api/v1/namespaces/watch-8556/configmaps/e2e-watch-test-watch-closed,UID:4ac1e097-7bcf-44be-b5d1-faf6a3ece238,ResourceVersion:16810,Generation:0,CreationTimestamp:2020-02-21 20:32:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 20:32:59.832: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8556,SelfLink:/api/v1/namespaces/watch-8556/configmaps/e2e-watch-test-watch-closed,UID:4ac1e097-7bcf-44be-b5d1-faf6a3ece238,ResourceVersion:16811,Generation:0,CreationTimestamp:2020-02-21 20:32:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:32:59.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8556" for this suite.
Feb 21 20:33:05.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:33:06.142: INFO: namespace watch-8556 deletion completed in 6.304867391s

• [SLOW TEST:6.498 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:33:06.142: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:33:06.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95" in namespace "projected-8007" to be "success or failure"
Feb 21 20:33:06.366: INFO: Pod "downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.746931ms
Feb 21 20:33:08.370: INFO: Pod "downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00826079s
STEP: Saw pod success
Feb 21 20:33:08.370: INFO: Pod "downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95" satisfied condition "success or failure"
Feb 21 20:33:08.374: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95 container client-container: <nil>
STEP: delete the pod
Feb 21 20:33:08.402: INFO: Waiting for pod downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95 to disappear
Feb 21 20:33:08.406: INFO: Pod downwardapi-volume-736e5f11-0fb4-41bf-a8a6-99815446aa95 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:33:08.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8007" for this suite.
Feb 21 20:33:14.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:33:14.616: INFO: namespace projected-8007 deletion completed in 6.205027512s

• [SLOW TEST:8.473 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:33:14.616: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 21 20:33:54.812: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0221 20:33:54.812097      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 21 20:33:54.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3996" for this suite.
Feb 21 20:34:02.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:34:03.058: INFO: namespace gc-3996 deletion completed in 8.241506024s

• [SLOW TEST:48.442 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:34:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1774
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-059dd498-cd52-41d3-b0a3-b5490c7e1665
STEP: Creating a pod to test consume secrets
Feb 21 20:34:03.260: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09" in namespace "projected-1774" to be "success or failure"
Feb 21 20:34:03.264: INFO: Pod "pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09": Phase="Pending", Reason="", readiness=false. Elapsed: 4.128695ms
Feb 21 20:34:05.269: INFO: Pod "pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008798933s
STEP: Saw pod success
Feb 21 20:34:05.269: INFO: Pod "pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09" satisfied condition "success or failure"
Feb 21 20:34:05.272: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:34:05.301: INFO: Waiting for pod pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09 to disappear
Feb 21 20:34:05.304: INFO: Pod pod-projected-secrets-a04b743b-d680-43e8-be6d-2b85f47e3f09 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:34:05.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1774" for this suite.
Feb 21 20:34:11.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:34:11.550: INFO: namespace projected-1774 deletion completed in 6.240956651s

• [SLOW TEST:8.490 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:34:11.551: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6364
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 21 20:34:14.256: INFO: Successfully updated pod "pod-update-6231915f-751b-4467-ab61-1c9b817c00e3"
STEP: verifying the updated pod is in kubernetes
Feb 21 20:34:14.264: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:34:14.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6364" for this suite.
Feb 21 20:34:36.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:34:36.543: INFO: namespace pods-6364 deletion completed in 22.273766828s

• [SLOW TEST:24.993 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:34:36.544: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5782
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-d1e241d8-103b-4e47-bba2-5986837367d9
STEP: Creating a pod to test consume secrets
Feb 21 20:34:36.773: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766" in namespace "projected-5782" to be "success or failure"
Feb 21 20:34:36.777: INFO: Pod "pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034899ms
Feb 21 20:34:38.782: INFO: Pod "pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008924764s
STEP: Saw pod success
Feb 21 20:34:38.782: INFO: Pod "pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766" satisfied condition "success or failure"
Feb 21 20:34:38.785: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:34:38.812: INFO: Waiting for pod pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766 to disappear
Feb 21 20:34:38.816: INFO: Pod pod-projected-secrets-f1b39b0a-ca8f-4b4b-892c-74ced761b766 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:34:38.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5782" for this suite.
Feb 21 20:34:44.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:34:45.063: INFO: namespace projected-5782 deletion completed in 6.240959735s

• [SLOW TEST:8.519 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:34:45.063: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:34:49.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4127" for this suite.
Feb 21 20:34:55.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:34:55.532: INFO: namespace kubelet-test-4127 deletion completed in 6.288191556s

• [SLOW TEST:10.469 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:34:55.532: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 21 20:34:55.763: INFO: Waiting up to 5m0s for pod "pod-f7211a87-7f19-42a8-bd66-6c65542ed549" in namespace "emptydir-4165" to be "success or failure"
Feb 21 20:34:55.767: INFO: Pod "pod-f7211a87-7f19-42a8-bd66-6c65542ed549": Phase="Pending", Reason="", readiness=false. Elapsed: 4.781657ms
Feb 21 20:34:57.772: INFO: Pod "pod-f7211a87-7f19-42a8-bd66-6c65542ed549": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009615809s
STEP: Saw pod success
Feb 21 20:34:57.772: INFO: Pod "pod-f7211a87-7f19-42a8-bd66-6c65542ed549" satisfied condition "success or failure"
Feb 21 20:34:57.776: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-f7211a87-7f19-42a8-bd66-6c65542ed549 container test-container: <nil>
STEP: delete the pod
Feb 21 20:34:57.800: INFO: Waiting for pod pod-f7211a87-7f19-42a8-bd66-6c65542ed549 to disappear
Feb 21 20:34:57.804: INFO: Pod pod-f7211a87-7f19-42a8-bd66-6c65542ed549 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:34:57.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4165" for this suite.
Feb 21 20:35:03.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:35:04.025: INFO: namespace emptydir-4165 deletion completed in 6.215792809s

• [SLOW TEST:8.492 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:35:04.025: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 20:35:04.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9" in namespace "downward-api-7036" to be "success or failure"
Feb 21 20:35:04.253: INFO: Pod "downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256768ms
Feb 21 20:35:06.258: INFO: Pod "downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008882993s
STEP: Saw pod success
Feb 21 20:35:06.258: INFO: Pod "downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9" satisfied condition "success or failure"
Feb 21 20:35:06.262: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9 container client-container: <nil>
STEP: delete the pod
Feb 21 20:35:06.287: INFO: Waiting for pod downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9 to disappear
Feb 21 20:35:06.291: INFO: Pod downwardapi-volume-be72395a-6733-4c5b-9404-cbaaf3c56de9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:35:06.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7036" for this suite.
Feb 21 20:35:12.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:35:12.544: INFO: namespace downward-api-7036 deletion completed in 6.247398616s

• [SLOW TEST:8.519 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:35:12.545: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 21 20:35:12.760: INFO: Waiting up to 5m0s for pod "pod-098ffa44-429c-42ae-902d-c7cf08bf49cd" in namespace "emptydir-7237" to be "success or failure"
Feb 21 20:35:12.764: INFO: Pod "pod-098ffa44-429c-42ae-902d-c7cf08bf49cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990959ms
Feb 21 20:35:14.768: INFO: Pod "pod-098ffa44-429c-42ae-902d-c7cf08bf49cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008415613s
STEP: Saw pod success
Feb 21 20:35:14.768: INFO: Pod "pod-098ffa44-429c-42ae-902d-c7cf08bf49cd" satisfied condition "success or failure"
Feb 21 20:35:14.772: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-098ffa44-429c-42ae-902d-c7cf08bf49cd container test-container: <nil>
STEP: delete the pod
Feb 21 20:35:14.803: INFO: Waiting for pod pod-098ffa44-429c-42ae-902d-c7cf08bf49cd to disappear
Feb 21 20:35:14.807: INFO: Pod pod-098ffa44-429c-42ae-902d-c7cf08bf49cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:35:14.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7237" for this suite.
Feb 21 20:35:20.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:35:21.023: INFO: namespace emptydir-7237 deletion completed in 6.210513097s

• [SLOW TEST:8.478 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:35:21.023: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 20:35:21.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5238'
Feb 21 20:35:21.495: INFO: stderr: ""
Feb 21 20:35:21.495: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 21 20:35:26.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pod e2e-test-nginx-pod --namespace=kubectl-5238 -o json'
Feb 21 20:35:26.618: INFO: stderr: ""
Feb 21 20:35:26.618: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.101.183.182/32\"\n        },\n        \"creationTimestamp\": \"2020-02-21T20:35:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5238\",\n        \"resourceVersion\": \"17746\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5238/pods/e2e-test-nginx-pod\",\n        \"uid\": \"c928f497-dc0e-4925-934b-cb09c96e4f25\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-89gf9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-20-17-149.us-east-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-89gf9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-89gf9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-21T20:35:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-21T20:35:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-21T20:35:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-21T20:35:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://f61a13e701c06f592585d8fb4704a3e936c65a2f8011cb0c8d7b8891cc8a0b50\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-21T20:35:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.20.17.149\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.101.183.182\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-21T20:35:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 21 20:35:26.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 replace -f - --namespace=kubectl-5238'
Feb 21 20:35:26.845: INFO: stderr: ""
Feb 21 20:35:26.845: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Feb 21 20:35:26.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete pods e2e-test-nginx-pod --namespace=kubectl-5238'
Feb 21 20:35:37.123: INFO: stderr: ""
Feb 21 20:35:37.123: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:35:37.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5238" for this suite.
Feb 21 20:35:43.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:35:43.368: INFO: namespace kubectl-5238 deletion completed in 6.23876291s

• [SLOW TEST:22.344 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:35:43.368: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-a4d7ff69-a051-462f-a957-f26c87884f95
STEP: Creating a pod to test consume configMaps
Feb 21 20:35:43.549: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa" in namespace "configmap-9652" to be "success or failure"
Feb 21 20:35:43.553: INFO: Pod "pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.92252ms
Feb 21 20:35:45.557: INFO: Pod "pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008489369s
STEP: Saw pod success
Feb 21 20:35:45.557: INFO: Pod "pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa" satisfied condition "success or failure"
Feb 21 20:35:45.561: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:35:45.588: INFO: Waiting for pod pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa to disappear
Feb 21 20:35:45.593: INFO: Pod pod-configmaps-ca8dd32c-0014-4d8a-9072-87a4ca1337fa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:35:45.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9652" for this suite.
Feb 21 20:35:51.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:35:51.840: INFO: namespace configmap-9652 deletion completed in 6.242713027s

• [SLOW TEST:8.472 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:35:51.841: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 21 20:35:54.038: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-389596720 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 21 20:35:59.119: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:35:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7764" for this suite.
Feb 21 20:36:05.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:36:05.352: INFO: namespace pods-7764 deletion completed in 6.224301995s

• [SLOW TEST:13.511 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:36:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:36:05.512: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:36:07.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6803" for this suite.
Feb 21 20:36:47.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:36:47.955: INFO: namespace pods-6803 deletion completed in 40.309125517s

• [SLOW TEST:42.603 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:36:47.956: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Feb 21 20:36:48.122: INFO: Waiting up to 5m0s for pod "client-containers-36828813-83e2-468d-ae76-39c7fe85d120" in namespace "containers-9284" to be "success or failure"
Feb 21 20:36:48.126: INFO: Pod "client-containers-36828813-83e2-468d-ae76-39c7fe85d120": Phase="Pending", Reason="", readiness=false. Elapsed: 3.555417ms
Feb 21 20:36:50.130: INFO: Pod "client-containers-36828813-83e2-468d-ae76-39c7fe85d120": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007540644s
STEP: Saw pod success
Feb 21 20:36:50.130: INFO: Pod "client-containers-36828813-83e2-468d-ae76-39c7fe85d120" satisfied condition "success or failure"
Feb 21 20:36:50.133: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod client-containers-36828813-83e2-468d-ae76-39c7fe85d120 container test-container: <nil>
STEP: delete the pod
Feb 21 20:36:50.158: INFO: Waiting for pod client-containers-36828813-83e2-468d-ae76-39c7fe85d120 to disappear
Feb 21 20:36:50.164: INFO: Pod client-containers-36828813-83e2-468d-ae76-39c7fe85d120 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:36:50.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9284" for this suite.
Feb 21 20:36:56.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:36:56.369: INFO: namespace containers-9284 deletion completed in 6.200638617s

• [SLOW TEST:8.413 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:36:56.370: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9907
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 20:36:56.524: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 20:37:18.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.189:8080/dial?request=hostName&protocol=udp&host=100.101.183.187&port=8081&tries=1'] Namespace:pod-network-test-9907 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:37:18.693: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:37:18.800: INFO: Waiting for endpoints: map[]
Feb 21 20:37:18.805: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.189:8080/dial?request=hostName&protocol=udp&host=100.119.255.85&port=8081&tries=1'] Namespace:pod-network-test-9907 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:37:18.805: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:37:18.905: INFO: Waiting for endpoints: map[]
Feb 21 20:37:18.909: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.189:8080/dial?request=hostName&protocol=udp&host=100.114.178.212&port=8081&tries=1'] Namespace:pod-network-test-9907 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:37:18.909: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:37:19.003: INFO: Waiting for endpoints: map[]
Feb 21 20:37:19.008: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.183.189:8080/dial?request=hostName&protocol=udp&host=100.103.74.166&port=8081&tries=1'] Namespace:pod-network-test-9907 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:37:19.008: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:37:19.107: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:37:19.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9907" for this suite.
Feb 21 20:37:43.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:37:43.344: INFO: namespace pod-network-test-9907 deletion completed in 24.231731934s

• [SLOW TEST:46.974 seconds]
[sig-network] Networking
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:37:43.344: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 21 20:37:44.579: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:37:44.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0221 20:37:44.578996      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-713" for this suite.
Feb 21 20:37:50.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:37:50.782: INFO: namespace gc-713 deletion completed in 6.198034841s

• [SLOW TEST:7.438 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:37:50.782: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3479
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:37:50.974: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 20:37:50.991: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:50.991: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:50.991: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:50.995: INFO: Number of nodes with available pods: 0
Feb 21 20:37:50.995: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:37:52.003: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:52.003: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:52.003: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:52.007: INFO: Number of nodes with available pods: 0
Feb 21 20:37:52.007: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:37:53.002: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:53.002: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:53.002: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:53.006: INFO: Number of nodes with available pods: 3
Feb 21 20:37:53.006: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:37:54.002: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:54.002: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:54.002: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:54.006: INFO: Number of nodes with available pods: 3
Feb 21 20:37:54.006: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:37:55.002: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:55.002: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:55.002: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:55.006: INFO: Number of nodes with available pods: 4
Feb 21 20:37:55.006: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 21 20:37:55.045: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:55.045: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:55.045: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:55.045: INFO: Wrong image for pod: daemon-set-tpqgh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:55.051: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:55.051: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:55.051: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:56.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:56.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:56.056: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:56.056: INFO: Wrong image for pod: daemon-set-tpqgh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:56.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:56.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:56.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:57.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:57.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:57.056: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:57.056: INFO: Wrong image for pod: daemon-set-tpqgh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:57.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:57.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:57.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:58.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:58.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:58.056: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:58.056: INFO: Wrong image for pod: daemon-set-tpqgh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:58.056: INFO: Pod daemon-set-tpqgh is not available
Feb 21 20:37:58.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:58.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:58.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:59.058: INFO: Pod daemon-set-557jj is not available
Feb 21 20:37:59.058: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:59.058: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:59.058: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:37:59.064: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:59.064: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:37:59.065: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:00.074: INFO: Pod daemon-set-557jj is not available
Feb 21 20:38:00.074: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:00.074: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:00.074: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:00.080: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:00.080: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:00.080: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:01.059: INFO: Pod daemon-set-557jj is not available
Feb 21 20:38:01.059: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:01.059: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:01.059: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:01.064: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:01.064: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:01.064: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:02.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:02.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:02.056: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:02.064: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:02.064: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:02.064: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:03.057: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:03.057: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:03.057: INFO: Wrong image for pod: daemon-set-lqnvg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:03.057: INFO: Pod daemon-set-lqnvg is not available
Feb 21 20:38:03.065: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:03.065: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:03.065: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:04.056: INFO: Pod daemon-set-62n7d is not available
Feb 21 20:38:04.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:04.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:04.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:04.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:04.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:05.056: INFO: Pod daemon-set-62n7d is not available
Feb 21 20:38:05.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:05.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:05.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:05.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:05.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:06.058: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:06.058: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:06.058: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:06.065: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:06.065: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:06.065: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:07.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:07.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:07.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:07.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:07.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:07.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:08.057: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:08.057: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:08.057: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:08.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:08.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:08.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:09.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:09.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:09.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:09.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:09.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:09.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:10.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:10.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:10.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:10.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:10.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:10.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:11.057: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:11.057: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:11.057: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:11.063: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:11.063: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:11.063: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:12.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:12.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:12.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:12.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:12.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:12.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:13.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:13.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:13.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:13.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:13.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:13.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:14.057: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:14.057: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:14.057: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:14.066: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:14.066: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:14.067: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:15.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:15.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:15.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:15.062: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:15.062: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:15.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:16.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:16.056: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:16.056: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:16.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:16.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:16.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:17.057: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:17.057: INFO: Wrong image for pod: daemon-set-jzn92. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:17.058: INFO: Pod daemon-set-jzn92 is not available
Feb 21 20:38:17.063: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:17.063: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:17.063: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:18.058: INFO: Pod daemon-set-968f6 is not available
Feb 21 20:38:18.058: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:18.065: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:18.065: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:18.065: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:19.056: INFO: Pod daemon-set-968f6 is not available
Feb 21 20:38:19.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:19.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:19.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:19.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:20.056: INFO: Pod daemon-set-968f6 is not available
Feb 21 20:38:20.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:20.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:20.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:20.062: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:21.056: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:21.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:21.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:21.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:22.059: INFO: Wrong image for pod: daemon-set-dtmb6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 21 20:38:22.059: INFO: Pod daemon-set-dtmb6 is not available
Feb 21 20:38:22.064: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:22.064: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:22.064: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:23.056: INFO: Pod daemon-set-ssg2z is not available
Feb 21 20:38:23.061: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:23.061: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:23.061: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 21 20:38:23.069: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:23.069: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:23.069: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:23.073: INFO: Number of nodes with available pods: 3
Feb 21 20:38:23.073: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:38:24.079: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:24.079: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:24.079: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:24.084: INFO: Number of nodes with available pods: 3
Feb 21 20:38:24.084: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:38:25.079: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:25.079: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:25.079: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:25.086: INFO: Number of nodes with available pods: 3
Feb 21 20:38:25.086: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:38:26.078: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:26.078: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:26.079: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:38:26.083: INFO: Number of nodes with available pods: 4
Feb 21 20:38:26.083: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3479, will wait for the garbage collector to delete the pods
Feb 21 20:38:26.165: INFO: Deleting DaemonSet.extensions daemon-set took: 10.295638ms
Feb 21 20:38:27.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000289297s
Feb 21 20:38:37.871: INFO: Number of nodes with available pods: 0
Feb 21 20:38:37.871: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 20:38:37.875: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3479/daemonsets","resourceVersion":"18868"},"items":null}

Feb 21 20:38:37.878: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3479/pods","resourceVersion":"18868"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:38:37.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3479" for this suite.
Feb 21 20:38:43.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:38:44.174: INFO: namespace daemonsets-3479 deletion completed in 6.270076085s

• [SLOW TEST:53.392 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:38:44.175: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9441
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9441
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 21 20:38:44.355: INFO: Found 0 stateful pods, waiting for 3
Feb 21 20:38:54.361: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 20:38:54.361: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 20:38:54.361: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 21 20:38:54.395: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 21 20:39:04.438: INFO: Updating stateful set ss2
Feb 21 20:39:04.450: INFO: Waiting for Pod statefulset-9441/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 21 20:39:14.459: INFO: Waiting for Pod statefulset-9441/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Feb 21 20:39:24.500: INFO: Found 1 stateful pods, waiting for 3
Feb 21 20:39:34.505: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 20:39:34.505: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 20:39:34.505: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 21 20:39:34.535: INFO: Updating stateful set ss2
Feb 21 20:39:34.544: INFO: Waiting for Pod statefulset-9441/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 21 20:39:44.553: INFO: Waiting for Pod statefulset-9441/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 21 20:39:54.575: INFO: Updating stateful set ss2
Feb 21 20:39:54.583: INFO: Waiting for StatefulSet statefulset-9441/ss2 to complete update
Feb 21 20:39:54.583: INFO: Waiting for Pod statefulset-9441/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 21 20:40:04.592: INFO: Waiting for StatefulSet statefulset-9441/ss2 to complete update
Feb 21 20:40:04.592: INFO: Waiting for Pod statefulset-9441/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 21 20:40:14.592: INFO: Deleting all statefulset in ns statefulset-9441
Feb 21 20:40:14.596: INFO: Scaling statefulset ss2 to 0
Feb 21 20:40:54.621: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 20:40:54.626: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:40:54.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9441" for this suite.
Feb 21 20:41:00.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:41:00.952: INFO: namespace statefulset-9441 deletion completed in 6.303030992s

• [SLOW TEST:136.777 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:41:00.953: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 20:41:01.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5094'
Feb 21 20:41:01.244: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 21 20:41:01.244: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Feb 21 20:41:01.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete jobs e2e-test-nginx-job --namespace=kubectl-5094'
Feb 21 20:41:01.334: INFO: stderr: ""
Feb 21 20:41:01.334: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:41:01.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5094" for this suite.
Feb 21 20:41:07.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:41:07.567: INFO: namespace kubectl-5094 deletion completed in 6.227693757s

• [SLOW TEST:6.614 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:41:07.568: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-e36fc5ec-822a-49fb-a683-ba6efada3efa in namespace container-probe-3809
Feb 21 20:41:09.752: INFO: Started pod busybox-e36fc5ec-822a-49fb-a683-ba6efada3efa in namespace container-probe-3809
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 20:41:09.756: INFO: Initial restart count of pod busybox-e36fc5ec-822a-49fb-a683-ba6efada3efa is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:45:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3809" for this suite.
Feb 21 20:45:16.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:45:16.765: INFO: namespace container-probe-3809 deletion completed in 6.322340355s

• [SLOW TEST:249.197 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:45:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-02e33b8c-b5df-49b3-8633-17aaa002d28d
STEP: Creating a pod to test consume secrets
Feb 21 20:45:16.944: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37" in namespace "projected-6421" to be "success or failure"
Feb 21 20:45:16.949: INFO: Pod "pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080372ms
Feb 21 20:45:18.954: INFO: Pod "pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009971882s
STEP: Saw pod success
Feb 21 20:45:18.954: INFO: Pod "pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37" satisfied condition "success or failure"
Feb 21 20:45:18.958: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:45:18.987: INFO: Waiting for pod pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37 to disappear
Feb 21 20:45:18.992: INFO: Pod pod-projected-secrets-e630c7f9-7281-47b8-9fae-c0429649ec37 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:45:18.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6421" for this suite.
Feb 21 20:45:25.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:45:25.209: INFO: namespace projected-6421 deletion completed in 6.211381578s

• [SLOW TEST:8.444 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:45:25.209: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 21 20:45:27.923: INFO: Successfully updated pod "annotationupdate86f60b22-48c6-4193-b287-a8798cb3b293"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:45:31.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2137" for this suite.
Feb 21 20:45:55.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:45:56.174: INFO: namespace downward-api-2137 deletion completed in 24.21520485s

• [SLOW TEST:30.964 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:45:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:45:56.327: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 21 20:45:56.338: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 21 20:46:01.344: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 20:46:01.344: INFO: Creating deployment "test-rolling-update-deployment"
Feb 21 20:46:01.354: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 21 20:46:01.363: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 21 20:46:03.375: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 21 20:46:03.379: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 21 20:46:03.396: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9669,SelfLink:/apis/apps/v1/namespaces/deployment-9669/deployments/test-rolling-update-deployment,UID:623fa300-abf2-429b-97e7-6102df4416c8,ResourceVersion:20937,Generation:1,CreationTimestamp:2020-02-21 20:46:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-21 20:46:01 +0000 UTC 2020-02-21 20:46:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-21 20:46:03 +0000 UTC 2020-02-21 20:46:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 20:46:03.401: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-9669,SelfLink:/apis/apps/v1/namespaces/deployment-9669/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:f20e5fba-2c17-43fd-a78d-e05d24c60c54,ResourceVersion:20930,Generation:1,CreationTimestamp:2020-02-21 20:46:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 623fa300-abf2-429b-97e7-6102df4416c8 0xc001810b47 0xc001810b48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 21 20:46:03.401: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 21 20:46:03.401: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9669,SelfLink:/apis/apps/v1/namespaces/deployment-9669/replicasets/test-rolling-update-controller,UID:4a258f38-bc54-4928-9de4-cb54cba28d61,ResourceVersion:20936,Generation:2,CreationTimestamp:2020-02-21 20:45:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 623fa300-abf2-429b-97e7-6102df4416c8 0xc001810a77 0xc001810a78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 20:46:03.406: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-4z2ct" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-4z2ct,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/test-rolling-update-deployment-79f6b9d75c-4z2ct,UID:da0c4e52-d441-412e-9e33-99fae5e78f25,ResourceVersion:20929,Generation:0,CreationTimestamp:2020-02-21 20:46:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.141/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c f20e5fba-2c17-43fd-a78d-e05d24c60c54 0xc001811437 0xc001811438}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8g8x4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8g8x4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8g8x4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0018114a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0018114c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:46:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:46:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:46:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:46:01 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.141,StartTime:2020-02-21 20:46:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-21 20:46:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://74a27eddbd3a9376449b2045364a32efb730e3224aee3431410fc37a0eeeca35}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:46:03.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9669" for this suite.
Feb 21 20:46:09.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:46:09.640: INFO: namespace deployment-9669 deletion completed in 6.229119135s

• [SLOW TEST:13.466 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:46:09.640: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Feb 21 20:46:09.792: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 21 20:46:09.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6480'
Feb 21 20:46:10.268: INFO: stderr: ""
Feb 21 20:46:10.268: INFO: stdout: "service/redis-slave created\n"
Feb 21 20:46:10.268: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 21 20:46:10.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6480'
Feb 21 20:46:10.617: INFO: stderr: ""
Feb 21 20:46:10.617: INFO: stdout: "service/redis-master created\n"
Feb 21 20:46:10.617: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 21 20:46:10.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6480'
Feb 21 20:46:11.008: INFO: stderr: ""
Feb 21 20:46:11.008: INFO: stdout: "service/frontend created\n"
Feb 21 20:46:11.008: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 21 20:46:11.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6480'
Feb 21 20:46:11.295: INFO: stderr: ""
Feb 21 20:46:11.295: INFO: stdout: "deployment.apps/frontend created\n"
Feb 21 20:46:11.295: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 21 20:46:11.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6480'
Feb 21 20:46:11.677: INFO: stderr: ""
Feb 21 20:46:11.677: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 21 20:46:11.678: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 21 20:46:11.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6480'
Feb 21 20:46:11.898: INFO: stderr: ""
Feb 21 20:46:11.898: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 21 20:46:11.898: INFO: Waiting for all frontend pods to be Running.
Feb 21 20:46:31.950: INFO: Waiting for frontend to serve content.
Feb 21 20:46:31.966: INFO: Trying to add a new entry to the guestbook.
Feb 21 20:46:31.983: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 21 20:46:31.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-6480'
Feb 21 20:46:32.117: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:46:32.117: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 20:46:32.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-6480'
Feb 21 20:46:32.258: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:46:32.258: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 20:46:32.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-6480'
Feb 21 20:46:32.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:46:32.379: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 20:46:32.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-6480'
Feb 21 20:46:32.471: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:46:32.471: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 20:46:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-6480'
Feb 21 20:46:32.555: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:46:32.555: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 21 20:46:32.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-6480'
Feb 21 20:46:32.641: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:46:32.641: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:46:32.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6480" for this suite.
Feb 21 20:47:12.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:47:13.052: INFO: namespace kubectl-6480 deletion completed in 40.406085756s

• [SLOW TEST:63.412 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:47:13.053: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Feb 21 20:47:13.758: INFO: created pod pod-service-account-defaultsa
Feb 21 20:47:13.758: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 21 20:47:13.768: INFO: created pod pod-service-account-mountsa
Feb 21 20:47:13.768: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 21 20:47:13.862: INFO: created pod pod-service-account-nomountsa
Feb 21 20:47:13.863: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 21 20:47:13.873: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 21 20:47:13.873: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 21 20:47:13.885: INFO: created pod pod-service-account-mountsa-mountspec
Feb 21 20:47:13.885: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 21 20:47:13.897: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 21 20:47:13.897: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 21 20:47:13.906: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 21 20:47:13.906: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 21 20:47:13.919: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 21 20:47:13.919: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 21 20:47:13.931: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 21 20:47:13.931: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:47:13.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1871" for this suite.
Feb 21 20:47:37.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:47:38.250: INFO: namespace svcaccounts-1871 deletion completed in 24.31171469s

• [SLOW TEST:25.197 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:47:38.250: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 21 20:47:38.423: INFO: Waiting up to 5m0s for pod "pod-d98a6249-c7a0-4494-82ce-7a9836b08587" in namespace "emptydir-6576" to be "success or failure"
Feb 21 20:47:38.429: INFO: Pod "pod-d98a6249-c7a0-4494-82ce-7a9836b08587": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041337ms
Feb 21 20:47:40.434: INFO: Pod "pod-d98a6249-c7a0-4494-82ce-7a9836b08587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009984012s
STEP: Saw pod success
Feb 21 20:47:40.434: INFO: Pod "pod-d98a6249-c7a0-4494-82ce-7a9836b08587" satisfied condition "success or failure"
Feb 21 20:47:40.439: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-d98a6249-c7a0-4494-82ce-7a9836b08587 container test-container: <nil>
STEP: delete the pod
Feb 21 20:47:40.462: INFO: Waiting for pod pod-d98a6249-c7a0-4494-82ce-7a9836b08587 to disappear
Feb 21 20:47:40.466: INFO: Pod pod-d98a6249-c7a0-4494-82ce-7a9836b08587 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:47:40.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6576" for this suite.
Feb 21 20:47:46.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:47:46.731: INFO: namespace emptydir-6576 deletion completed in 6.258026702s

• [SLOW TEST:8.481 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:47:46.732: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 21 20:47:46.884: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 20:47:46.893: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 20:47:46.899: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-16-60.us-east-2.compute.internal before test
Feb 21 20:47:46.914: INFO: linkerd-controller-766f7cc6b-ktn5s from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container public-api ready: true, restart count 0
Feb 21 20:47:46.914: INFO: nginx-ingress-controller-6676bbc48c-xhjdt from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 20:47:46.914: INFO: kube-dns-5fdb85bb5b-fjsw9 from kube-system started at 2020-02-21 19:41:16 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container kubedns ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container sidecar ready: true, restart count 0
Feb 21 20:47:46.914: INFO: basic-demo-548c455c8-9w97q from demo started at 2020-02-21 19:49:39 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container basic-demo ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.914: INFO: vpa-recommender-77cbff9874-f8ccd from kube-system started at 2020-02-21 19:50:23 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container recommender ready: true, restart count 0
Feb 21 20:47:46.914: INFO: calico-typha-5d656748c8-mtssq from kube-system started at 2020-02-21 19:41:16 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 20:47:46.914: INFO: linkerd-identity-55c9b95464-lg24p from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container identity ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.914: INFO: linkerd-proxy-injector-67fd7b6f8c-gbnlh from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.914: INFO: 	Container proxy-injector ready: true, restart count 0
Feb 21 20:47:46.914: INFO: cert-manager-75c599cb9b-cmdr6 from cert-manager started at 2020-02-21 19:48:53 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container cert-manager ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.914: INFO: kube-dns-autoscaler-577b4774b5-6978f from kube-system started at 2020-02-21 19:41:16 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container autoscaler ready: true, restart count 0
Feb 21 20:47:46.914: INFO: linkerd-tap-f6665c587-mqw6z from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.914: INFO: 	Container tap ready: true, restart count 0
Feb 21 20:47:46.914: INFO: goldilocks-vpa-install-smd28 from goldilocks started at 2020-02-21 19:49:40 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container vpa-install ready: false, restart count 0
Feb 21 20:47:46.914: INFO: rbac-manager-6497c75d88-n6574 from rbac-manager started at 2020-02-21 19:48:59 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.914: INFO: 	Container rbac-manager ready: true, restart count 1
Feb 21 20:47:46.914: INFO: prometheus-operator-prometheus-node-exporter-4w66c from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.914: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:47:46.915: INFO: nginx-ingress-default-backend-8bb99946f-l74lg from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.915: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 21 20:47:46.915: INFO: goldilocks-dashboard-547bc67bdc-5k6pq from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 20:47:46.915: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-nl44f from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:47:46.915: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:47:46.915: INFO: kube-proxy-ip-172-20-16-60.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:14 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:47:46.915: INFO: calico-node-wl9bf from kube-system started at 2020-02-21 19:40:59 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:47:46.915: INFO: linkerd-destination-8894499df-fhn96 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container destination ready: true, restart count 3
Feb 21 20:47:46.915: INFO: 	Container linkerd-proxy ready: true, restart count 4
Feb 21 20:47:46.915: INFO: linkerd-sp-validator-6c76b79bdd-9chl4 from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.915: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 20:47:46.915: INFO: external-dns-5b6fdf4bbd-xg7zx from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.915: INFO: 	Container external-dns ready: true, restart count 0
Feb 21 20:47:46.915: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.915: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-17-148.us-east-2.compute.internal before test
Feb 21 20:47:46.932: INFO: external-dns-5b6fdf4bbd-5kncn from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container external-dns ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: prometheus-operator-grafana-769db59dcd-8pr2z from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container grafana ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.932: INFO: prometheus-operator-prometheus-node-exporter-kkc6f from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:47:46.932: INFO: goldilocks-dashboard-547bc67bdc-scfmz from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 20:47:46.932: INFO: linkerd-sp-validator-6c76b79bdd-42xpr from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.932: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 20:47:46.932: INFO: linkerd-tap-f6665c587-p2qjz from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container tap ready: true, restart count 0
Feb 21 20:47:46.932: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-4nkm6 from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:47:46.932: INFO: kube-dns-5fdb85bb5b-dk4sz from kube-system started at 2020-02-21 19:41:56 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container kubedns ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container sidecar ready: true, restart count 0
Feb 21 20:47:46.932: INFO: linkerd-grafana-6848df775f-dcxvr from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container grafana ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 3
Feb 21 20:47:46.932: INFO: linkerd-proxy-injector-67fd7b6f8c-h44bb from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 6
Feb 21 20:47:46.932: INFO: 	Container proxy-injector ready: true, restart count 7
Feb 21 20:47:46.932: INFO: nginx-ingress-controller-6676bbc48c-s48vd from nginx-ingress started at 2020-02-21 19:48:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 20:47:46.932: INFO: metrics-server-7f7cb69697-h977w from metrics-server started at 2020-02-21 19:49:07 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container metrics-server ready: true, restart count 0
Feb 21 20:47:46.932: INFO: prometheus-operator-operator-74d4bb4cc4-xvw88 from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.932: INFO: 	Container prometheus-operator ready: true, restart count 2
Feb 21 20:47:46.932: INFO: linkerd-controller-766f7cc6b-wxnj8 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container public-api ready: true, restart count 0
Feb 21 20:47:46.932: INFO: calico-node-lcqwl from kube-system started at 2020-02-21 19:41:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:47:46.932: INFO: calico-typha-5d656748c8-25s5c from kube-system started at 2020-02-21 19:41:46 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 20:47:46.932: INFO: linkerd-identity-55c9b95464-cp6f2 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container identity ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.932: INFO: linkerd-destination-8894499df-7w8rg from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: cert-manager-cainjector-7c8cff6f8c-nrtbx from cert-manager started at 2020-02-21 19:48:53 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container cainjector ready: true, restart count 1
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: basic-demo-548c455c8-f6hmk from demo started at 2020-02-21 19:49:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container basic-demo ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.932: INFO: alertmanager-prometheus-operator-alertmanager-0 from prometheus-operator started at 2020-02-21 19:50:09 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container alertmanager ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container config-reloader ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: kube-proxy-ip-172-20-17-148.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:36 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.932: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:47:46.932: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-17-149.us-east-2.compute.internal before test
Feb 21 20:47:46.947: INFO: calico-node-r6hl2 from kube-system started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.947: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:47:46.947: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-pnrct from sonobuoy started at 2020-02-21 19:52:37 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:47:46.947: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 20:47:46.947: INFO: kube-proxy-ip-172-20-17-149.us-east-2.compute.internal from kube-system started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.947: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:47:46.947: INFO: prometheus-prometheus-operator-prometheus-0 from prometheus-operator started at 2020-02-21 19:53:00 +0000 UTC (4 container statuses recorded)
Feb 21 20:47:46.947: INFO: 	Container linkerd-proxy ready: true, restart count 14
Feb 21 20:47:46.947: INFO: 	Container prometheus ready: true, restart count 20
Feb 21 20:47:46.947: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 21 20:47:46.947: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 21 20:47:46.947: INFO: prometheus-operator-prometheus-node-exporter-lv8jq from prometheus-operator started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.947: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:47:46.947: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-18-137.us-east-2.compute.internal before test
Feb 21 20:47:46.964: INFO: calico-typha-5d656748c8-pc9zv from kube-system started at 2020-02-21 19:42:09 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 20:47:46.964: INFO: linkerd-proxy-injector-67fd7b6f8c-t9vtj from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.964: INFO: 	Container proxy-injector ready: true, restart count 0
Feb 21 20:47:46.964: INFO: nginx-ingress-controller-6676bbc48c-m69kk from nginx-ingress started at 2020-02-21 19:48:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.964: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 20:47:46.964: INFO: external-dns-5b6fdf4bbd-ssg7b from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container external-dns ready: true, restart count 1
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.964: INFO: kube-proxy-ip-172-20-18-137.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:17 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 20:47:46.964: INFO: linkerd-web-6fc4b84756-m5b28 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 3
Feb 21 20:47:46.964: INFO: 	Container web ready: true, restart count 1
Feb 21 20:47:46.964: INFO: linkerd-prometheus-799db67749-n279t from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.964: INFO: 	Container prometheus ready: true, restart count 0
Feb 21 20:47:46.964: INFO: prometheus-operator-kube-state-metrics-5d46566c59-6jn4w from prometheus-operator started at 2020-02-21 19:49:36 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container kube-state-metrics ready: true, restart count 2
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.964: INFO: prometheus-operator-prometheus-node-exporter-bcqpd from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 20:47:46.964: INFO: goldilocks-controller-6d64cc65c7-5fng8 from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 20:47:46.964: INFO: sonobuoy from sonobuoy started at 2020-02-21 19:52:20 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 21 20:47:46.964: INFO: linkerd-destination-8894499df-jbvgn from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.964: INFO: linkerd-sp-validator-6c76b79bdd-nf5gb from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 20:47:46.964: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 20:47:46.964: INFO: linkerd-tap-f6665c587-bwb9s from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.964: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.964: INFO: 	Container tap ready: true, restart count 0
Feb 21 20:47:46.964: INFO: nginx-ingress-default-backend-8bb99946f-2mkfz from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.965: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 21 20:47:46.965: INFO: cluster-autoscaler-aws-cluster-autoscaler-5ff6c85c6d-sjlzh from cluster-autoscaler started at 2020-02-21 19:49:03 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container aws-cluster-autoscaler ready: true, restart count 3
Feb 21 20:47:46.965: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.965: INFO: basic-demo-548c455c8-6s29j from demo started at 2020-02-21 19:49:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container basic-demo ready: true, restart count 3
Feb 21 20:47:46.965: INFO: 	Container linkerd-proxy ready: true, restart count 4
Feb 21 20:47:46.965: INFO: calico-node-wfjj9 from kube-system started at 2020-02-21 19:41:42 +0000 UTC (1 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 20:47:46.965: INFO: linkerd-identity-55c9b95464-zh22t from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container identity ready: true, restart count 0
Feb 21 20:47:46.965: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 20:47:46.965: INFO: linkerd-controller-766f7cc6b-zk2jg from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container destination ready: true, restart count 0
Feb 21 20:47:46.965: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 20:47:46.965: INFO: 	Container public-api ready: true, restart count 0
Feb 21 20:47:46.965: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-rvbkh from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 20:47:46.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 21 20:47:46.965: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f585e604b6f821], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:47:48.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8906" for this suite.
Feb 21 20:47:54.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:47:54.440: INFO: namespace sched-pred-8906 deletion completed in 6.371453563s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.708 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:47:54.441: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2889
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-2889
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2889
Feb 21 20:47:54.622: INFO: Found 0 stateful pods, waiting for 1
Feb 21 20:48:04.628: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 21 20:48:04.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 20:48:04.825: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 20:48:04.825: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 20:48:04.825: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 20:48:04.830: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 21 20:48:14.835: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 20:48:14.835: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 20:48:14.852: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:14.852: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:14.852: INFO: 
Feb 21 20:48:14.852: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 21 20:48:15.860: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995698943s
Feb 21 20:48:16.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98746826s
Feb 21 20:48:17.869: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982696982s
Feb 21 20:48:18.874: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978054851s
Feb 21 20:48:19.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973157871s
Feb 21 20:48:20.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.968368445s
Feb 21 20:48:21.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963400648s
Feb 21 20:48:22.895: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958455305s
Feb 21 20:48:23.900: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.553173ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2889
Feb 21 20:48:24.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:48:25.111: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 20:48:25.111: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 20:48:25.111: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 20:48:25.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:48:25.284: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 21 20:48:25.284: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 20:48:25.284: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 20:48:25.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:48:25.494: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 21 20:48:25.494: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 20:48:25.494: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 20:48:25.499: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 20:48:25.499: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 20:48:25.499: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 21 20:48:25.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 20:48:25.689: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 20:48:25.689: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 20:48:25.689: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 20:48:25.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 20:48:25.864: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 20:48:25.864: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 20:48:25.864: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 20:48:25.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 20:48:26.054: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 20:48:26.055: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 20:48:26.055: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 20:48:26.055: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 20:48:26.060: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 21 20:48:36.069: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 20:48:36.069: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 20:48:36.069: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 20:48:36.085: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:36.085: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:36.085: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:36.085: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:36.085: INFO: 
Feb 21 20:48:36.085: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:37.090: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:37.090: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:37.090: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:37.090: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:37.090: INFO: 
Feb 21 20:48:37.090: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:38.095: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:38.095: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:38.095: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:38.095: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:38.095: INFO: 
Feb 21 20:48:38.095: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:39.101: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:39.101: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:39.101: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:39.101: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:39.101: INFO: 
Feb 21 20:48:39.101: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:40.106: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:40.106: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:40.106: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:40.106: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:40.106: INFO: 
Feb 21 20:48:40.106: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:41.112: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:41.112: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:41.112: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:41.112: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:41.112: INFO: 
Feb 21 20:48:41.112: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:42.117: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:42.117: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:42.117: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:42.117: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:42.117: INFO: 
Feb 21 20:48:42.117: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:43.122: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:43.122: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:43.122: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:43.122: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:43.122: INFO: 
Feb 21 20:48:43.122: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:44.127: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:44.127: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:44.127: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:44.127: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:44.127: INFO: 
Feb 21 20:48:44.127: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 21 20:48:45.132: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Feb 21 20:48:45.133: INFO: ss-0  ip-172-20-17-149.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:47:54 +0000 UTC  }]
Feb 21 20:48:45.133: INFO: ss-1  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:45.133: INFO: ss-2  ip-172-20-17-149.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 20:48:14 +0000 UTC  }]
Feb 21 20:48:45.133: INFO: 
Feb 21 20:48:45.133: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2889
Feb 21 20:48:46.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:48:46.251: INFO: rc: 1
Feb 21 20:48:46.251: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00371a480 exit status 1 <nil> <nil> true [0xc00228ed40 0xc00228eda8 0xc00228ee10] [0xc00228ed40 0xc00228eda8 0xc00228ee10] [0xc00228ed98 0xc00228ee08] [0xba6c10 0xba6c10] 0xc002cd0d20 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Feb 21 20:48:56.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:48:56.327: INFO: rc: 1
Feb 21 20:48:56.327: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efdd70 exit status 1 <nil> <nil> true [0xc00276e638 0xc00276e678 0xc00276e690] [0xc00276e638 0xc00276e678 0xc00276e690] [0xc00276e670 0xc00276e688] [0xba6c10 0xba6c10] 0xc002a47800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:49:06.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:49:06.409: INFO: rc: 1
Feb 21 20:49:06.409: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00371a810 exit status 1 <nil> <nil> true [0xc00228ee78 0xc00228ef28 0xc00228efa8] [0xc00228ee78 0xc00228ef28 0xc00228efa8] [0xc00228eef8 0xc00228ef78] [0xba6c10 0xba6c10] 0xc002cd1140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:49:16.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:49:16.488: INFO: rc: 1
Feb 21 20:49:16.488: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00371aba0 exit status 1 <nil> <nil> true [0xc00228efb8 0xc00228f080 0xc00228f138] [0xc00228efb8 0xc00228f080 0xc00228f138] [0xc00228f040 0xc00228f100] [0xba6c10 0xba6c10] 0xc002cd14a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:49:26.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:49:26.566: INFO: rc: 1
Feb 21 20:49:26.566: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c80150 exit status 1 <nil> <nil> true [0xc00276e698 0xc00276e6b0 0xc00276e6f8] [0xc00276e698 0xc00276e6b0 0xc00276e6f8] [0xc00276e6a8 0xc00276e6e0] [0xba6c10 0xba6c10] 0xc002a47bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:49:36.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:49:36.648: INFO: rc: 1
Feb 21 20:49:36.648: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c804e0 exit status 1 <nil> <nil> true [0xc00276e708 0xc00276e730 0xc00276e748] [0xc00276e708 0xc00276e730 0xc00276e748] [0xc00276e728 0xc00276e740] [0xba6c10 0xba6c10] 0xc002a47f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:49:46.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:49:46.728: INFO: rc: 1
Feb 21 20:49:46.728: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c80870 exit status 1 <nil> <nil> true [0xc00276e758 0xc00276e770 0xc00276e788] [0xc00276e758 0xc00276e770 0xc00276e788] [0xc00276e768 0xc00276e780] [0xba6c10 0xba6c10] 0xc002dae2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:49:56.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:49:56.814: INFO: rc: 1
Feb 21 20:49:56.815: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00371af60 exit status 1 <nil> <nil> true [0xc00228f158 0xc00228f1a0 0xc00228f258] [0xc00228f158 0xc00228f1a0 0xc00228f258] [0xc00228f180 0xc00228f240] [0xba6c10 0xba6c10] 0xc002cd1860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:50:06.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:50:06.900: INFO: rc: 1
Feb 21 20:50:06.900: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00371b2c0 exit status 1 <nil> <nil> true [0xc00228f278 0xc00228f358 0xc00228f460] [0xc00228f278 0xc00228f358 0xc00228f460] [0xc00228f2e8 0xc00228f3e0] [0xba6c10 0xba6c10] 0xc002cd1bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:50:16.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:50:17.014: INFO: rc: 1
Feb 21 20:50:17.014: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c80c30 exit status 1 <nil> <nil> true [0xc00276e798 0xc00276e7b0 0xc00276e7c8] [0xc00276e798 0xc00276e7b0 0xc00276e7c8] [0xc00276e7a8 0xc00276e7c0] [0xba6c10 0xba6c10] 0xc002dae660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:50:27.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:50:27.097: INFO: rc: 1
Feb 21 20:50:27.097: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efc360 exit status 1 <nil> <nil> true [0xc00228e010 0xc00228e080 0xc00228e0f8] [0xc00228e010 0xc00228e080 0xc00228e0f8] [0xc00228e048 0xc00228e0b0] [0xba6c10 0xba6c10] 0xc002a46840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:50:37.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:50:40.246: INFO: rc: 1
Feb 21 20:50:40.246: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efc720 exit status 1 <nil> <nil> true [0xc00228e118 0xc00228e168 0xc00228e1b0] [0xc00228e118 0xc00228e168 0xc00228e1b0] [0xc00228e148 0xc00228e1a0] [0xba6c10 0xba6c10] 0xc002a46c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:50:50.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:50:50.325: INFO: rc: 1
Feb 21 20:50:50.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e06360 exit status 1 <nil> <nil> true [0xc00276e000 0xc00276e020 0xc00276e048] [0xc00276e000 0xc00276e020 0xc00276e048] [0xc00276e018 0xc00276e038] [0xba6c10 0xba6c10] 0xc0032522a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:51:00.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:51:00.426: INFO: rc: 1
Feb 21 20:51:00.426: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efcab0 exit status 1 <nil> <nil> true [0xc00228e1e0 0xc00228e228 0xc00228e270] [0xc00228e1e0 0xc00228e228 0xc00228e270] [0xc00228e220 0xc00228e260] [0xba6c10 0xba6c10] 0xc002a46f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:51:10.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:51:10.517: INFO: rc: 1
Feb 21 20:51:10.518: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efce40 exit status 1 <nil> <nil> true [0xc00228e278 0xc00228e2b0 0xc00228e338] [0xc00228e278 0xc00228e2b0 0xc00228e338] [0xc00228e298 0xc00228e310] [0xba6c10 0xba6c10] 0xc002a47380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:51:20.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:51:20.598: INFO: rc: 1
Feb 21 20:51:20.598: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efd1d0 exit status 1 <nil> <nil> true [0xc00228e390 0xc00228e438 0xc00228e5d8] [0xc00228e390 0xc00228e438 0xc00228e5d8] [0xc00228e3e8 0xc00228e5a0] [0xba6c10 0xba6c10] 0xc002a476e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:51:30.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:51:30.680: INFO: rc: 1
Feb 21 20:51:30.680: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efd530 exit status 1 <nil> <nil> true [0xc00228e608 0xc00228e6a8 0xc00228e718] [0xc00228e608 0xc00228e6a8 0xc00228e718] [0xc00228e650 0xc00228e700] [0xba6c10 0xba6c10] 0xc002a47aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:51:40.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:51:40.764: INFO: rc: 1
Feb 21 20:51:40.764: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8a030 exit status 1 <nil> <nil> true [0xc00276e058 0xc00276e078 0xc00276e090] [0xc00276e058 0xc00276e078 0xc00276e090] [0xc00276e070 0xc00276e088] [0xba6c10 0xba6c10] 0xc0032527e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:51:50.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:51:50.836: INFO: rc: 1
Feb 21 20:51:50.836: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8a3c0 exit status 1 <nil> <nil> true [0xc00276e0a0 0xc00276e0c8 0xc00276e0e0] [0xc00276e0a0 0xc00276e0c8 0xc00276e0e0] [0xc00276e0c0 0xc00276e0d8] [0xba6c10 0xba6c10] 0xc003252b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:52:00.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:52:00.911: INFO: rc: 1
Feb 21 20:52:00.911: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8a750 exit status 1 <nil> <nil> true [0xc00276e0e8 0xc00276e110 0xc00276e138] [0xc00276e0e8 0xc00276e110 0xc00276e138] [0xc00276e100 0xc00276e130] [0xba6c10 0xba6c10] 0xc003252ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:52:10.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:52:11.002: INFO: rc: 1
Feb 21 20:52:11.002: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8aae0 exit status 1 <nil> <nil> true [0xc00276e140 0xc00276e170 0xc00276e190] [0xc00276e140 0xc00276e170 0xc00276e190] [0xc00276e160 0xc00276e188] [0xba6c10 0xba6c10] 0xc003253200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:52:21.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:52:21.083: INFO: rc: 1
Feb 21 20:52:21.083: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e06330 exit status 1 <nil> <nil> true [0xc00228e020 0xc00228e0a0 0xc00228e118] [0xc00228e020 0xc00228e0a0 0xc00228e118] [0xc00228e080 0xc00228e0f8] [0xba6c10 0xba6c10] 0xc002a46840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:52:31.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:52:31.167: INFO: rc: 1
Feb 21 20:52:31.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efc330 exit status 1 <nil> <nil> true [0xc00276e000 0xc00276e020 0xc00276e048] [0xc00276e000 0xc00276e020 0xc00276e048] [0xc00276e018 0xc00276e038] [0xba6c10 0xba6c10] 0xc0032522a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:52:41.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:52:41.251: INFO: rc: 1
Feb 21 20:52:41.251: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efc750 exit status 1 <nil> <nil> true [0xc00276e058 0xc00276e078 0xc00276e090] [0xc00276e058 0xc00276e078 0xc00276e090] [0xc00276e070 0xc00276e088] [0xba6c10 0xba6c10] 0xc0032527e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:52:51.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:52:51.325: INFO: rc: 1
Feb 21 20:52:51.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8a000 exit status 1 <nil> <nil> true [0xc00228e138 0xc00228e188 0xc00228e1e0] [0xc00228e138 0xc00228e188 0xc00228e1e0] [0xc00228e168 0xc00228e1b0] [0xba6c10 0xba6c10] 0xc002a46c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:53:01.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:53:01.399: INFO: rc: 1
Feb 21 20:53:01.399: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efcb10 exit status 1 <nil> <nil> true [0xc00276e0a0 0xc00276e0c8 0xc00276e0e0] [0xc00276e0a0 0xc00276e0c8 0xc00276e0e0] [0xc00276e0c0 0xc00276e0d8] [0xba6c10 0xba6c10] 0xc003252b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:53:11.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:53:11.526: INFO: rc: 1
Feb 21 20:53:11.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8a3f0 exit status 1 <nil> <nil> true [0xc00228e218 0xc00228e240 0xc00228e278] [0xc00228e218 0xc00228e240 0xc00228e278] [0xc00228e228 0xc00228e270] [0xba6c10 0xba6c10] 0xc002a46f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:53:21.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:53:21.605: INFO: rc: 1
Feb 21 20:53:21.605: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efced0 exit status 1 <nil> <nil> true [0xc00276e0e8 0xc00276e110 0xc00276e138] [0xc00276e0e8 0xc00276e110 0xc00276e138] [0xc00276e100 0xc00276e130] [0xba6c10 0xba6c10] 0xc003252ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:53:31.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:53:31.683: INFO: rc: 1
Feb 21 20:53:31.683: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002efd290 exit status 1 <nil> <nil> true [0xc00276e140 0xc00276e170 0xc00276e190] [0xc00276e140 0xc00276e170 0xc00276e190] [0xc00276e160 0xc00276e188] [0xba6c10 0xba6c10] 0xc003253200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:53:41.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:53:41.753: INFO: rc: 1
Feb 21 20:53:41.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002e8a7e0 exit status 1 <nil> <nil> true [0xc00228e288 0xc00228e2e0 0xc00228e390] [0xc00228e288 0xc00228e2e0 0xc00228e390] [0xc00228e2b0 0xc00228e338] [0xba6c10 0xba6c10] 0xc002a47380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 21 20:53:51.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-2889 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 20:53:51.827: INFO: rc: 1
Feb 21 20:53:51.827: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 21 20:53:51.827: INFO: Scaling statefulset ss to 0
Feb 21 20:53:51.842: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 21 20:53:51.846: INFO: Deleting all statefulset in ns statefulset-2889
Feb 21 20:53:51.851: INFO: Scaling statefulset ss to 0
Feb 21 20:53:51.869: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 20:53:51.875: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:53:51.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2889" for this suite.
Feb 21 20:53:57.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:53:58.287: INFO: namespace statefulset-2889 deletion completed in 6.385308619s

• [SLOW TEST:363.847 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:53:58.288: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:53:58.486: INFO: Creating ReplicaSet my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2
Feb 21 20:53:58.499: INFO: Pod name my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2: Found 0 pods out of 1
Feb 21 20:54:03.505: INFO: Pod name my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2: Found 1 pods out of 1
Feb 21 20:54:03.505: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2" is running
Feb 21 20:54:03.510: INFO: Pod "my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2-6bbzx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 20:53:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 20:54:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 20:54:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 20:53:58 +0000 UTC Reason: Message:}])
Feb 21 20:54:03.512: INFO: Trying to dial the pod
Feb 21 20:54:08.532: INFO: Controller my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2: Got expected result from replica 1 [my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2-6bbzx]: "my-hostname-basic-44ba52c9-7278-43c0-863d-d136dd5016d2-6bbzx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:54:08.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9256" for this suite.
Feb 21 20:54:14.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:54:14.815: INFO: namespace replicaset-9256 deletion completed in 6.275837534s

• [SLOW TEST:16.527 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:54:14.816: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-650e82a9-0afa-41b6-9729-1b3ad1d59576
STEP: Creating a pod to test consume secrets
Feb 21 20:54:15.059: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2" in namespace "projected-9606" to be "success or failure"
Feb 21 20:54:15.070: INFO: Pod "pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992478ms
Feb 21 20:54:17.076: INFO: Pod "pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016476192s
STEP: Saw pod success
Feb 21 20:54:17.076: INFO: Pod "pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2" satisfied condition "success or failure"
Feb 21 20:54:17.080: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:54:17.115: INFO: Waiting for pod pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2 to disappear
Feb 21 20:54:17.119: INFO: Pod pod-projected-secrets-afad7313-4c39-4cdd-8e0a-ba14d82fc3b2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:54:17.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9606" for this suite.
Feb 21 20:54:23.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:54:23.534: INFO: namespace projected-9606 deletion completed in 6.40972565s

• [SLOW TEST:8.718 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:54:23.534: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-19fa8b8a-ea5b-4cd2-8f29-8ecf03396fb0
STEP: Creating a pod to test consume configMaps
Feb 21 20:54:23.702: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e" in namespace "configmap-9999" to be "success or failure"
Feb 21 20:54:23.708: INFO: Pod "pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.659812ms
Feb 21 20:54:25.714: INFO: Pod "pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011574392s
STEP: Saw pod success
Feb 21 20:54:25.714: INFO: Pod "pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e" satisfied condition "success or failure"
Feb 21 20:54:25.718: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:54:25.750: INFO: Waiting for pod pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e to disappear
Feb 21 20:54:25.755: INFO: Pod pod-configmaps-9d27f918-f27c-43be-b9dc-8fe725fd7f1e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:54:25.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9999" for this suite.
Feb 21 20:54:31.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:54:32.116: INFO: namespace configmap-9999 deletion completed in 6.354610991s

• [SLOW TEST:8.582 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:54:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b58faed3-91a4-45f5-857e-889020d30e7e
STEP: Creating a pod to test consume configMaps
Feb 21 20:54:32.360: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8" in namespace "projected-2425" to be "success or failure"
Feb 21 20:54:32.366: INFO: Pod "pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.190434ms
Feb 21 20:54:34.371: INFO: Pod "pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011812666s
STEP: Saw pod success
Feb 21 20:54:34.372: INFO: Pod "pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8" satisfied condition "success or failure"
Feb 21 20:54:34.378: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:54:34.408: INFO: Waiting for pod pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8 to disappear
Feb 21 20:54:34.413: INFO: Pod pod-projected-configmaps-168de6ff-8baa-4c3d-a4f6-b85f243786e8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:54:34.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2425" for this suite.
Feb 21 20:54:40.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:54:40.694: INFO: namespace projected-2425 deletion completed in 6.275580017s

• [SLOW TEST:8.577 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:54:40.695: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-6b8c924a-fc67-497d-bbde-c87974db6502
STEP: Creating a pod to test consume configMaps
Feb 21 20:54:40.864: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381" in namespace "projected-6083" to be "success or failure"
Feb 21 20:54:40.869: INFO: Pod "pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381": Phase="Pending", Reason="", readiness=false. Elapsed: 5.097893ms
Feb 21 20:54:42.875: INFO: Pod "pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01047305s
STEP: Saw pod success
Feb 21 20:54:42.875: INFO: Pod "pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381" satisfied condition "success or failure"
Feb 21 20:54:42.879: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 20:54:42.910: INFO: Waiting for pod pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381 to disappear
Feb 21 20:54:42.914: INFO: Pod pod-projected-configmaps-eb3ccbc5-da9c-4275-99c3-ba86bd5b5381 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:54:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6083" for this suite.
Feb 21 20:54:48.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:54:49.200: INFO: namespace projected-6083 deletion completed in 6.279689949s

• [SLOW TEST:8.505 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:54:49.201: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:54:49.439: INFO: Create a RollingUpdate DaemonSet
Feb 21 20:54:49.448: INFO: Check that daemon pods launch on every node of the cluster
Feb 21 20:54:49.458: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:49.458: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:49.458: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:49.464: INFO: Number of nodes with available pods: 0
Feb 21 20:54:49.464: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:54:50.472: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:50.472: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:50.472: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:50.477: INFO: Number of nodes with available pods: 0
Feb 21 20:54:50.477: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:54:51.474: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:51.474: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:51.474: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:51.480: INFO: Number of nodes with available pods: 4
Feb 21 20:54:51.481: INFO: Number of running nodes: 4, number of available pods: 4
Feb 21 20:54:51.481: INFO: Update the DaemonSet to trigger a rollout
Feb 21 20:54:51.493: INFO: Updating DaemonSet daemon-set
Feb 21 20:54:55.523: INFO: Roll back the DaemonSet before rollout is complete
Feb 21 20:54:55.536: INFO: Updating DaemonSet daemon-set
Feb 21 20:54:55.536: INFO: Make sure DaemonSet rollback is complete
Feb 21 20:54:55.544: INFO: Wrong image for pod: daemon-set-dqqfn. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 21 20:54:55.544: INFO: Pod daemon-set-dqqfn is not available
Feb 21 20:54:55.556: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:55.556: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:55.556: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:56.562: INFO: Wrong image for pod: daemon-set-dqqfn. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 21 20:54:56.562: INFO: Pod daemon-set-dqqfn is not available
Feb 21 20:54:56.568: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:56.568: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:56.568: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:57.562: INFO: Pod daemon-set-6dvwq is not available
Feb 21 20:54:57.568: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:57.568: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:54:57.568: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4695, will wait for the garbage collector to delete the pods
Feb 21 20:54:57.656: INFO: Deleting DaemonSet.extensions daemon-set took: 18.665847ms
Feb 21 20:54:58.557: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.403931ms
Feb 21 20:55:07.863: INFO: Number of nodes with available pods: 0
Feb 21 20:55:07.863: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 20:55:07.867: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4695/daemonsets","resourceVersion":"23740"},"items":null}

Feb 21 20:55:07.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4695/pods","resourceVersion":"23740"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:55:07.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4695" for this suite.
Feb 21 20:55:13.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:55:14.164: INFO: namespace daemonsets-4695 deletion completed in 6.259909483s

• [SLOW TEST:24.963 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:55:14.165: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 96.5.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.5.96_udp@PTR;check="$$(dig +tcp +noall +answer +search 96.5.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.5.96_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 96.5.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.5.96_udp@PTR;check="$$(dig +tcp +noall +answer +search 96.5.65.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.65.5.96_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 20:55:24.464: INFO: Unable to read jessie_udp@dns-test-service.dns-6600.svc.cluster.local from pod dns-6600/dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b: the server could not find the requested resource (get pods dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b)
Feb 21 20:55:24.469: INFO: Unable to read jessie_tcp@dns-test-service.dns-6600.svc.cluster.local from pod dns-6600/dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b: the server could not find the requested resource (get pods dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b)
Feb 21 20:55:24.475: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local from pod dns-6600/dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b: the server could not find the requested resource (get pods dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b)
Feb 21 20:55:24.481: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local from pod dns-6600/dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b: the server could not find the requested resource (get pods dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b)
Feb 21 20:55:24.515: INFO: Lookups using dns-6600/dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b failed for: [jessie_udp@dns-test-service.dns-6600.svc.cluster.local jessie_tcp@dns-test-service.dns-6600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6600.svc.cluster.local]

Feb 21 20:55:29.646: INFO: DNS probes using dns-6600/dns-test-2cedb06b-418a-4953-badb-b8e0a512db4b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:55:29.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6600" for this suite.
Feb 21 20:55:35.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:55:36.098: INFO: namespace dns-6600 deletion completed in 6.325227155s

• [SLOW TEST:21.933 seconds]
[sig-network] DNS
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:55:36.099: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 21 20:55:46.290: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0221 20:55:46.290487      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:55:46.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1050" for this suite.
Feb 21 20:55:52.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:55:52.545: INFO: namespace gc-1050 deletion completed in 6.249202307s

• [SLOW TEST:16.446 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:55:52.546: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 21 20:55:58.815: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:58.815: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:58.909: INFO: Exec stderr: ""
Feb 21 20:55:58.910: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:58.910: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.025: INFO: Exec stderr: ""
Feb 21 20:55:59.025: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.025: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.119: INFO: Exec stderr: ""
Feb 21 20:55:59.119: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.119: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.208: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 21 20:55:59.208: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.208: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.321: INFO: Exec stderr: ""
Feb 21 20:55:59.321: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.321: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.417: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 21 20:55:59.417: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.417: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.508: INFO: Exec stderr: ""
Feb 21 20:55:59.508: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.508: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.606: INFO: Exec stderr: ""
Feb 21 20:55:59.606: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.606: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:55:59.759: INFO: Exec stderr: ""
Feb 21 20:55:59.759: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1506 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 20:55:59.759: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 20:56:00.007: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:56:00.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1506" for this suite.
Feb 21 20:56:40.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:56:40.302: INFO: namespace e2e-kubelet-etc-hosts-1506 deletion completed in 40.281612705s

• [SLOW TEST:47.757 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:56:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 21 20:56:44.527: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:44.532: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:46.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:46.539: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:48.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:48.538: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:50.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:50.538: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:52.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:52.537: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:54.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:54.538: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:56.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:56.538: INFO: Pod pod-with-prestop-http-hook still exists
Feb 21 20:56:58.533: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 21 20:56:58.540: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:56:58.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8162" for this suite.
Feb 21 20:57:20.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:57:21.086: INFO: namespace container-lifecycle-hook-8162 deletion completed in 22.525230625s

• [SLOW TEST:40.784 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:57:21.086: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:57:23.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2049" for this suite.
Feb 21 20:58:11.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:58:11.615: INFO: namespace kubelet-test-2049 deletion completed in 48.323802761s

• [SLOW TEST:50.529 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:58:11.616: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-7a70c3fe-44b9-4131-b4fa-7999ad657c77
STEP: Creating a pod to test consume secrets
Feb 21 20:58:11.860: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979" in namespace "projected-3169" to be "success or failure"
Feb 21 20:58:11.868: INFO: Pod "pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470272ms
Feb 21 20:58:13.874: INFO: Pod "pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014328343s
STEP: Saw pod success
Feb 21 20:58:13.874: INFO: Pod "pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979" satisfied condition "success or failure"
Feb 21 20:58:13.879: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 21 20:58:13.908: INFO: Waiting for pod pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979 to disappear
Feb 21 20:58:13.912: INFO: Pod pod-projected-secrets-5521755d-d56b-43c3-ba47-999780068979 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:58:13.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3169" for this suite.
Feb 21 20:58:19.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:58:20.190: INFO: namespace projected-3169 deletion completed in 6.268628302s

• [SLOW TEST:8.575 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:58:20.190: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-1177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-1177
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1177
STEP: Deleting pre-stop pod
Feb 21 20:58:31.490: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:58:31.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1177" for this suite.
Feb 21 20:59:09.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:59:09.895: INFO: namespace prestop-1177 deletion completed in 38.385131899s

• [SLOW TEST:49.705 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:59:09.895: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Feb 21 20:59:10.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-8817'
Feb 21 20:59:10.544: INFO: stderr: ""
Feb 21 20:59:10.544: INFO: stdout: "pod/pause created\n"
Feb 21 20:59:10.544: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 21 20:59:10.544: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8817" to be "running and ready"
Feb 21 20:59:10.550: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.638712ms
Feb 21 20:59:12.557: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.013021062s
Feb 21 20:59:12.557: INFO: Pod "pause" satisfied condition "running and ready"
Feb 21 20:59:12.557: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 21 20:59:12.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 label pods pause testing-label=testing-label-value --namespace=kubectl-8817'
Feb 21 20:59:12.646: INFO: stderr: ""
Feb 21 20:59:12.646: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 21 20:59:12.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pod pause -L testing-label --namespace=kubectl-8817'
Feb 21 20:59:12.720: INFO: stderr: ""
Feb 21 20:59:12.720: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 21 20:59:12.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 label pods pause testing-label- --namespace=kubectl-8817'
Feb 21 20:59:12.810: INFO: stderr: ""
Feb 21 20:59:12.810: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 21 20:59:12.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pod pause -L testing-label --namespace=kubectl-8817'
Feb 21 20:59:12.885: INFO: stderr: ""
Feb 21 20:59:12.885: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Feb 21 20:59:12.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-8817'
Feb 21 20:59:12.976: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 20:59:12.976: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 21 20:59:12.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get rc,svc -l name=pause --no-headers --namespace=kubectl-8817'
Feb 21 20:59:13.058: INFO: stderr: "No resources found.\n"
Feb 21 20:59:13.058: INFO: stdout: ""
Feb 21 20:59:13.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -l name=pause --namespace=kubectl-8817 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 20:59:13.130: INFO: stderr: ""
Feb 21 20:59:13.130: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:59:13.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8817" for this suite.
Feb 21 20:59:19.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:59:19.394: INFO: namespace kubectl-8817 deletion completed in 6.257593172s

• [SLOW TEST:9.499 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:59:19.394: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6642
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 20:59:19.541: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:59:25.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6642" for this suite.
Feb 21 20:59:31.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 20:59:31.880: INFO: namespace custom-resource-definition-6642 deletion completed in 6.268641333s

• [SLOW TEST:12.486 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 20:59:31.881: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 20:59:32.077: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:32.077: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:32.078: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:32.082: INFO: Number of nodes with available pods: 0
Feb 21 20:59:32.082: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:33.088: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:33.088: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:33.089: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:33.094: INFO: Number of nodes with available pods: 0
Feb 21 20:59:33.094: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:34.088: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:34.088: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:34.089: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:34.094: INFO: Number of nodes with available pods: 4
Feb 21 20:59:34.094: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 21 20:59:34.120: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:34.120: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:34.120: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:34.125: INFO: Number of nodes with available pods: 3
Feb 21 20:59:34.125: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:35.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:35.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:35.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:35.137: INFO: Number of nodes with available pods: 3
Feb 21 20:59:35.137: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:36.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:36.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:36.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:36.137: INFO: Number of nodes with available pods: 3
Feb 21 20:59:36.137: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:37.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:37.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:37.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:37.141: INFO: Number of nodes with available pods: 3
Feb 21 20:59:37.141: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:38.133: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:38.133: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:38.133: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:38.138: INFO: Number of nodes with available pods: 3
Feb 21 20:59:38.138: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:39.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:39.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:39.133: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:39.138: INFO: Number of nodes with available pods: 3
Feb 21 20:59:39.138: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:40.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:40.133: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:40.133: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:40.138: INFO: Number of nodes with available pods: 3
Feb 21 20:59:40.138: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:41.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:41.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:41.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:41.137: INFO: Number of nodes with available pods: 3
Feb 21 20:59:41.137: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:42.133: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:42.133: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:42.133: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:42.141: INFO: Number of nodes with available pods: 3
Feb 21 20:59:42.141: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:43.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:43.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:43.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:43.137: INFO: Number of nodes with available pods: 3
Feb 21 20:59:43.138: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:44.134: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:44.134: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:44.134: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:44.139: INFO: Number of nodes with available pods: 3
Feb 21 20:59:44.139: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:45.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:45.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:45.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:45.140: INFO: Number of nodes with available pods: 3
Feb 21 20:59:45.140: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:46.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:46.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:46.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:46.138: INFO: Number of nodes with available pods: 3
Feb 21 20:59:46.138: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:47.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:47.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:47.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:47.139: INFO: Number of nodes with available pods: 3
Feb 21 20:59:47.139: INFO: Node ip-172-20-17-148.us-east-2.compute.internal is running more than one daemon pod
Feb 21 20:59:48.132: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:48.132: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:48.132: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 20:59:48.137: INFO: Number of nodes with available pods: 4
Feb 21 20:59:48.137: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3666, will wait for the garbage collector to delete the pods
Feb 21 20:59:48.210: INFO: Deleting DaemonSet.extensions daemon-set took: 15.241604ms
Feb 21 20:59:48.311: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.755386ms
Feb 21 20:59:57.218: INFO: Number of nodes with available pods: 0
Feb 21 20:59:57.218: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 20:59:57.223: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3666/daemonsets","resourceVersion":"25248"},"items":null}

Feb 21 20:59:57.227: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3666/pods","resourceVersion":"25248"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 20:59:57.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3666" for this suite.
Feb 21 21:00:03.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:00:03.602: INFO: namespace daemonsets-3666 deletion completed in 6.34486477s

• [SLOW TEST:31.721 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:00:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-493b625f-15ff-40c9-92d9-42701b68b8cb
STEP: Creating a pod to test consume secrets
Feb 21 21:00:03.778: INFO: Waiting up to 5m0s for pod "pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e" in namespace "secrets-1463" to be "success or failure"
Feb 21 21:00:03.787: INFO: Pod "pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.864782ms
Feb 21 21:00:05.797: INFO: Pod "pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018521085s
STEP: Saw pod success
Feb 21 21:00:05.797: INFO: Pod "pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e" satisfied condition "success or failure"
Feb 21 21:00:05.802: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e container secret-env-test: <nil>
STEP: delete the pod
Feb 21 21:00:05.838: INFO: Waiting for pod pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e to disappear
Feb 21 21:00:05.843: INFO: Pod pod-secrets-d9ecd08d-cb1d-4040-a8db-dcd45823d28e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:00:05.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1463" for this suite.
Feb 21 21:00:11.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:00:12.165: INFO: namespace secrets-1463 deletion completed in 6.317239312s

• [SLOW TEST:8.563 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:00:12.166: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-4fj9
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 21:00:12.356: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4fj9" in namespace "subpath-9498" to be "success or failure"
Feb 21 21:00:12.362: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.054607ms
Feb 21 21:00:14.369: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011917937s
Feb 21 21:00:16.374: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017477001s
Feb 21 21:00:18.381: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 6.023723746s
Feb 21 21:00:20.387: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 8.030326079s
Feb 21 21:00:22.393: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 10.036081332s
Feb 21 21:00:24.399: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 12.041792134s
Feb 21 21:00:26.404: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 14.047366443s
Feb 21 21:00:28.411: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 16.053570296s
Feb 21 21:00:30.417: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 18.05981445s
Feb 21 21:00:32.422: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Running", Reason="", readiness=true. Elapsed: 20.065156015s
Feb 21 21:00:34.427: INFO: Pod "pod-subpath-test-configmap-4fj9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.070370323s
STEP: Saw pod success
Feb 21 21:00:34.427: INFO: Pod "pod-subpath-test-configmap-4fj9" satisfied condition "success or failure"
Feb 21 21:00:34.432: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-subpath-test-configmap-4fj9 container test-container-subpath-configmap-4fj9: <nil>
STEP: delete the pod
Feb 21 21:00:34.462: INFO: Waiting for pod pod-subpath-test-configmap-4fj9 to disappear
Feb 21 21:00:34.467: INFO: Pod pod-subpath-test-configmap-4fj9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4fj9
Feb 21 21:00:34.467: INFO: Deleting pod "pod-subpath-test-configmap-4fj9" in namespace "subpath-9498"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:00:34.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9498" for this suite.
Feb 21 21:00:40.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:00:40.782: INFO: namespace subpath-9498 deletion completed in 6.304150774s

• [SLOW TEST:28.616 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:00:40.783: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 21 21:00:40.949: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:00:44.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6562" for this suite.
Feb 21 21:00:50.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:00:50.518: INFO: namespace init-container-6562 deletion completed in 6.265461555s

• [SLOW TEST:9.735 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:00:50.518: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 21 21:00:50.758: INFO: Waiting up to 5m0s for pod "downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8" in namespace "downward-api-9769" to be "success or failure"
Feb 21 21:00:50.768: INFO: Pod "downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.943131ms
Feb 21 21:00:52.773: INFO: Pod "downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014200952s
STEP: Saw pod success
Feb 21 21:00:52.773: INFO: Pod "downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8" satisfied condition "success or failure"
Feb 21 21:00:52.778: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8 container dapi-container: <nil>
STEP: delete the pod
Feb 21 21:00:52.808: INFO: Waiting for pod downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8 to disappear
Feb 21 21:00:52.812: INFO: Pod downward-api-4bced534-f765-4b87-a35a-f5eccccf3cb8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:00:52.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9769" for this suite.
Feb 21 21:00:58.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:00:59.092: INFO: namespace downward-api-9769 deletion completed in 6.273471926s

• [SLOW TEST:8.573 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:00:59.092: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 21 21:00:59.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4714'
Feb 21 21:00:59.449: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 21 21:00:59.449: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Feb 21 21:00:59.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4714'
Feb 21 21:00:59.543: INFO: stderr: ""
Feb 21 21:00:59.543: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:00:59.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4714" for this suite.
Feb 21 21:01:21.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:01:21.843: INFO: namespace kubectl-4714 deletion completed in 22.251954866s

• [SLOW TEST:22.751 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:01:21.844: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7289
I0221 21:01:22.001241      17 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7289, replica count: 1
I0221 21:01:23.051778      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0221 21:01:24.052005      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 21 21:01:24.174: INFO: Created: latency-svc-lzmx6
Feb 21 21:01:24.179: INFO: Got endpoints: latency-svc-lzmx6 [26.464868ms]
Feb 21 21:01:24.202: INFO: Created: latency-svc-m57qc
Feb 21 21:01:24.210: INFO: Got endpoints: latency-svc-m57qc [30.184074ms]
Feb 21 21:01:24.220: INFO: Created: latency-svc-8v9nv
Feb 21 21:01:24.223: INFO: Got endpoints: latency-svc-8v9nv [43.079665ms]
Feb 21 21:01:24.236: INFO: Created: latency-svc-bfvng
Feb 21 21:01:24.243: INFO: Got endpoints: latency-svc-bfvng [62.89043ms]
Feb 21 21:01:24.257: INFO: Created: latency-svc-vkcsz
Feb 21 21:01:24.260: INFO: Got endpoints: latency-svc-vkcsz [80.100134ms]
Feb 21 21:01:24.300: INFO: Created: latency-svc-cz627
Feb 21 21:01:24.309: INFO: Got endpoints: latency-svc-cz627 [128.476937ms]
Feb 21 21:01:24.334: INFO: Created: latency-svc-rstrs
Feb 21 21:01:24.338: INFO: Got endpoints: latency-svc-rstrs [157.277888ms]
Feb 21 21:01:24.351: INFO: Created: latency-svc-zqtmb
Feb 21 21:01:24.351: INFO: Got endpoints: latency-svc-zqtmb [169.574166ms]
Feb 21 21:01:24.361: INFO: Created: latency-svc-fxrpg
Feb 21 21:01:24.367: INFO: Got endpoints: latency-svc-fxrpg [186.889094ms]
Feb 21 21:01:24.380: INFO: Created: latency-svc-4h6gc
Feb 21 21:01:24.394: INFO: Got endpoints: latency-svc-4h6gc [213.253904ms]
Feb 21 21:01:24.401: INFO: Created: latency-svc-48cg8
Feb 21 21:01:24.408: INFO: Got endpoints: latency-svc-48cg8 [226.811739ms]
Feb 21 21:01:24.420: INFO: Created: latency-svc-qhpzx
Feb 21 21:01:24.430: INFO: Got endpoints: latency-svc-qhpzx [248.926471ms]
Feb 21 21:01:24.439: INFO: Created: latency-svc-2m22z
Feb 21 21:01:24.443: INFO: Got endpoints: latency-svc-2m22z [261.066881ms]
Feb 21 21:01:24.453: INFO: Created: latency-svc-2kt2b
Feb 21 21:01:24.459: INFO: Got endpoints: latency-svc-2kt2b [278.055463ms]
Feb 21 21:01:24.469: INFO: Created: latency-svc-s4dvj
Feb 21 21:01:24.472: INFO: Got endpoints: latency-svc-s4dvj [290.868268ms]
Feb 21 21:01:24.486: INFO: Created: latency-svc-48nzm
Feb 21 21:01:24.490: INFO: Got endpoints: latency-svc-48nzm [308.679343ms]
Feb 21 21:01:24.502: INFO: Created: latency-svc-g8blc
Feb 21 21:01:24.511: INFO: Got endpoints: latency-svc-g8blc [300.989631ms]
Feb 21 21:01:24.520: INFO: Created: latency-svc-5s299
Feb 21 21:01:24.524: INFO: Got endpoints: latency-svc-5s299 [301.094049ms]
Feb 21 21:01:24.538: INFO: Created: latency-svc-mcc8z
Feb 21 21:01:24.544: INFO: Got endpoints: latency-svc-mcc8z [300.85406ms]
Feb 21 21:01:24.557: INFO: Created: latency-svc-wt9fg
Feb 21 21:01:24.562: INFO: Got endpoints: latency-svc-wt9fg [301.873622ms]
Feb 21 21:01:24.575: INFO: Created: latency-svc-cjkmf
Feb 21 21:01:24.580: INFO: Got endpoints: latency-svc-cjkmf [271.789267ms]
Feb 21 21:01:24.595: INFO: Created: latency-svc-wp7bh
Feb 21 21:01:24.605: INFO: Got endpoints: latency-svc-wp7bh [267.073374ms]
Feb 21 21:01:24.608: INFO: Created: latency-svc-rm65p
Feb 21 21:01:24.615: INFO: Got endpoints: latency-svc-rm65p [263.315792ms]
Feb 21 21:01:24.632: INFO: Created: latency-svc-nw4vt
Feb 21 21:01:24.637: INFO: Got endpoints: latency-svc-nw4vt [269.331136ms]
Feb 21 21:01:24.653: INFO: Created: latency-svc-cnp6n
Feb 21 21:01:24.656: INFO: Got endpoints: latency-svc-cnp6n [262.21159ms]
Feb 21 21:01:24.664: INFO: Created: latency-svc-p7ks8
Feb 21 21:01:24.669: INFO: Got endpoints: latency-svc-p7ks8 [261.737703ms]
Feb 21 21:01:24.678: INFO: Created: latency-svc-p8d8s
Feb 21 21:01:24.682: INFO: Got endpoints: latency-svc-p8d8s [252.085505ms]
Feb 21 21:01:24.691: INFO: Created: latency-svc-z5gqz
Feb 21 21:01:24.704: INFO: Got endpoints: latency-svc-z5gqz [261.329433ms]
Feb 21 21:01:24.712: INFO: Created: latency-svc-sfjlf
Feb 21 21:01:24.718: INFO: Got endpoints: latency-svc-sfjlf [258.618082ms]
Feb 21 21:01:24.729: INFO: Created: latency-svc-w4z74
Feb 21 21:01:24.737: INFO: Got endpoints: latency-svc-w4z74 [264.378328ms]
Feb 21 21:01:24.759: INFO: Created: latency-svc-lfv86
Feb 21 21:01:24.760: INFO: Got endpoints: latency-svc-lfv86 [41.424674ms]
Feb 21 21:01:24.772: INFO: Created: latency-svc-ds6l2
Feb 21 21:01:24.777: INFO: Got endpoints: latency-svc-ds6l2 [287.083597ms]
Feb 21 21:01:24.790: INFO: Created: latency-svc-28jjw
Feb 21 21:01:24.803: INFO: Got endpoints: latency-svc-28jjw [291.982675ms]
Feb 21 21:01:24.814: INFO: Created: latency-svc-b6jdk
Feb 21 21:01:24.820: INFO: Got endpoints: latency-svc-b6jdk [295.840379ms]
Feb 21 21:01:24.833: INFO: Created: latency-svc-zxk7l
Feb 21 21:01:24.833: INFO: Got endpoints: latency-svc-zxk7l [289.724653ms]
Feb 21 21:01:24.851: INFO: Created: latency-svc-2nh2n
Feb 21 21:01:24.855: INFO: Got endpoints: latency-svc-2nh2n [292.766355ms]
Feb 21 21:01:24.860: INFO: Created: latency-svc-wpsb9
Feb 21 21:01:24.864: INFO: Got endpoints: latency-svc-wpsb9 [283.48231ms]
Feb 21 21:01:24.877: INFO: Created: latency-svc-27v2w
Feb 21 21:01:24.883: INFO: Got endpoints: latency-svc-27v2w [278.156718ms]
Feb 21 21:01:24.892: INFO: Created: latency-svc-btbqt
Feb 21 21:01:24.898: INFO: Got endpoints: latency-svc-btbqt [282.868109ms]
Feb 21 21:01:24.910: INFO: Created: latency-svc-jvx5w
Feb 21 21:01:24.914: INFO: Got endpoints: latency-svc-jvx5w [277.57436ms]
Feb 21 21:01:24.933: INFO: Created: latency-svc-qnr8k
Feb 21 21:01:24.933: INFO: Got endpoints: latency-svc-qnr8k [277.062451ms]
Feb 21 21:01:24.947: INFO: Created: latency-svc-725wl
Feb 21 21:01:24.964: INFO: Got endpoints: latency-svc-725wl [288.979524ms]
Feb 21 21:01:24.964: INFO: Created: latency-svc-ksgff
Feb 21 21:01:24.975: INFO: Got endpoints: latency-svc-ksgff [293.154907ms]
Feb 21 21:01:24.984: INFO: Created: latency-svc-dgqzp
Feb 21 21:01:24.997: INFO: Got endpoints: latency-svc-dgqzp [291.508966ms]
Feb 21 21:01:24.998: INFO: Created: latency-svc-d2prg
Feb 21 21:01:25.010: INFO: Got endpoints: latency-svc-d2prg [272.465041ms]
Feb 21 21:01:25.019: INFO: Created: latency-svc-tlnrr
Feb 21 21:01:25.026: INFO: Got endpoints: latency-svc-tlnrr [266.039996ms]
Feb 21 21:01:25.032: INFO: Created: latency-svc-8m5cp
Feb 21 21:01:25.147: INFO: Got endpoints: latency-svc-8m5cp [370.200481ms]
Feb 21 21:01:25.154: INFO: Created: latency-svc-f9pv4
Feb 21 21:01:25.162: INFO: Got endpoints: latency-svc-f9pv4 [358.854452ms]
Feb 21 21:01:25.174: INFO: Created: latency-svc-jcdnn
Feb 21 21:01:25.182: INFO: Got endpoints: latency-svc-jcdnn [361.524882ms]
Feb 21 21:01:25.192: INFO: Created: latency-svc-nx2kx
Feb 21 21:01:25.194: INFO: Got endpoints: latency-svc-nx2kx [361.090396ms]
Feb 21 21:01:25.210: INFO: Created: latency-svc-rptlr
Feb 21 21:01:25.230: INFO: Got endpoints: latency-svc-rptlr [375.361546ms]
Feb 21 21:01:25.231: INFO: Created: latency-svc-pzmfv
Feb 21 21:01:25.241: INFO: Created: latency-svc-t9vkz
Feb 21 21:01:25.258: INFO: Created: latency-svc-4qbtx
Feb 21 21:01:25.296: INFO: Created: latency-svc-rbxtk
Feb 21 21:01:25.300: INFO: Got endpoints: latency-svc-pzmfv [435.757206ms]
Feb 21 21:01:25.311: INFO: Created: latency-svc-lqvbs
Feb 21 21:01:25.332: INFO: Got endpoints: latency-svc-t9vkz [448.610431ms]
Feb 21 21:01:25.336: INFO: Created: latency-svc-xktd5
Feb 21 21:01:25.359: INFO: Created: latency-svc-2dzwd
Feb 21 21:01:25.385: INFO: Got endpoints: latency-svc-4qbtx [486.715ms]
Feb 21 21:01:25.385: INFO: Created: latency-svc-tmmzk
Feb 21 21:01:25.428: INFO: Got endpoints: latency-svc-rbxtk [513.199177ms]
Feb 21 21:01:25.429: INFO: Created: latency-svc-4s56c
Feb 21 21:01:25.467: INFO: Created: latency-svc-ms6ls
Feb 21 21:01:25.486: INFO: Got endpoints: latency-svc-lqvbs [552.639647ms]
Feb 21 21:01:25.502: INFO: Created: latency-svc-xbtbw
Feb 21 21:01:25.546: INFO: Got endpoints: latency-svc-xktd5 [582.188307ms]
Feb 21 21:01:25.546: INFO: Created: latency-svc-9gqnm
Feb 21 21:01:25.563: INFO: Created: latency-svc-p2npv
Feb 21 21:01:25.582: INFO: Got endpoints: latency-svc-2dzwd [606.58427ms]
Feb 21 21:01:25.590: INFO: Created: latency-svc-nqgpr
Feb 21 21:01:25.630: INFO: Got endpoints: latency-svc-tmmzk [633.311423ms]
Feb 21 21:01:25.640: INFO: Created: latency-svc-ddxs4
Feb 21 21:01:25.651: INFO: Created: latency-svc-vwpw5
Feb 21 21:01:25.679: INFO: Created: latency-svc-bvx9z
Feb 21 21:01:25.680: INFO: Got endpoints: latency-svc-4s56c [670.275722ms]
Feb 21 21:01:25.689: INFO: Created: latency-svc-w2dgh
Feb 21 21:01:25.707: INFO: Created: latency-svc-98sdh
Feb 21 21:01:25.719: INFO: Created: latency-svc-cwdcp
Feb 21 21:01:25.728: INFO: Got endpoints: latency-svc-ms6ls [702.517058ms]
Feb 21 21:01:25.734: INFO: Created: latency-svc-52krd
Feb 21 21:01:25.758: INFO: Created: latency-svc-rcpd4
Feb 21 21:01:25.773: INFO: Created: latency-svc-p8lh6
Feb 21 21:01:25.781: INFO: Got endpoints: latency-svc-xbtbw [634.092195ms]
Feb 21 21:01:25.792: INFO: Created: latency-svc-ccmpq
Feb 21 21:01:25.812: INFO: Created: latency-svc-x4bt4
Feb 21 21:01:25.817: INFO: Created: latency-svc-b57mg
Feb 21 21:01:25.827: INFO: Got endpoints: latency-svc-9gqnm [665.327155ms]
Feb 21 21:01:25.864: INFO: Created: latency-svc-g4v9w
Feb 21 21:01:25.877: INFO: Got endpoints: latency-svc-p2npv [695.216439ms]
Feb 21 21:01:25.916: INFO: Created: latency-svc-gs76x
Feb 21 21:01:25.928: INFO: Got endpoints: latency-svc-nqgpr [733.920437ms]
Feb 21 21:01:25.948: INFO: Created: latency-svc-xgz2j
Feb 21 21:01:25.981: INFO: Got endpoints: latency-svc-ddxs4 [750.016699ms]
Feb 21 21:01:26.002: INFO: Created: latency-svc-6zckr
Feb 21 21:01:26.032: INFO: Got endpoints: latency-svc-vwpw5 [731.826197ms]
Feb 21 21:01:26.052: INFO: Created: latency-svc-82wpw
Feb 21 21:01:26.077: INFO: Got endpoints: latency-svc-bvx9z [744.880929ms]
Feb 21 21:01:26.096: INFO: Created: latency-svc-9n9l2
Feb 21 21:01:26.129: INFO: Got endpoints: latency-svc-w2dgh [744.069521ms]
Feb 21 21:01:26.147: INFO: Created: latency-svc-jf62t
Feb 21 21:01:26.181: INFO: Got endpoints: latency-svc-98sdh [752.715293ms]
Feb 21 21:01:26.218: INFO: Created: latency-svc-92nfm
Feb 21 21:01:26.227: INFO: Got endpoints: latency-svc-cwdcp [741.359865ms]
Feb 21 21:01:26.249: INFO: Created: latency-svc-vxvf9
Feb 21 21:01:26.278: INFO: Got endpoints: latency-svc-52krd [732.581594ms]
Feb 21 21:01:26.299: INFO: Created: latency-svc-4bx7j
Feb 21 21:01:26.328: INFO: Got endpoints: latency-svc-rcpd4 [746.483285ms]
Feb 21 21:01:26.351: INFO: Created: latency-svc-hjvv4
Feb 21 21:01:26.378: INFO: Got endpoints: latency-svc-p8lh6 [747.501729ms]
Feb 21 21:01:26.397: INFO: Created: latency-svc-xnnq9
Feb 21 21:01:26.430: INFO: Got endpoints: latency-svc-ccmpq [749.425386ms]
Feb 21 21:01:26.453: INFO: Created: latency-svc-gft6k
Feb 21 21:01:26.478: INFO: Got endpoints: latency-svc-x4bt4 [749.808182ms]
Feb 21 21:01:26.498: INFO: Created: latency-svc-kq2rj
Feb 21 21:01:26.528: INFO: Got endpoints: latency-svc-b57mg [746.93079ms]
Feb 21 21:01:26.549: INFO: Created: latency-svc-xcfkq
Feb 21 21:01:26.578: INFO: Got endpoints: latency-svc-g4v9w [750.598312ms]
Feb 21 21:01:26.600: INFO: Created: latency-svc-5lr5j
Feb 21 21:01:26.634: INFO: Got endpoints: latency-svc-gs76x [756.592476ms]
Feb 21 21:01:26.675: INFO: Created: latency-svc-xl74x
Feb 21 21:01:26.676: INFO: Got endpoints: latency-svc-xgz2j [747.629656ms]
Feb 21 21:01:26.695: INFO: Created: latency-svc-tfjcg
Feb 21 21:01:26.738: INFO: Got endpoints: latency-svc-6zckr [757.04407ms]
Feb 21 21:01:26.761: INFO: Created: latency-svc-8wlcg
Feb 21 21:01:26.778: INFO: Got endpoints: latency-svc-82wpw [746.50954ms]
Feb 21 21:01:26.808: INFO: Created: latency-svc-t9wk7
Feb 21 21:01:26.828: INFO: Got endpoints: latency-svc-9n9l2 [750.662166ms]
Feb 21 21:01:26.851: INFO: Created: latency-svc-pfxck
Feb 21 21:01:26.877: INFO: Got endpoints: latency-svc-jf62t [748.100178ms]
Feb 21 21:01:26.896: INFO: Created: latency-svc-w6thg
Feb 21 21:01:26.927: INFO: Got endpoints: latency-svc-92nfm [730.427843ms]
Feb 21 21:01:26.947: INFO: Created: latency-svc-54fcb
Feb 21 21:01:26.980: INFO: Got endpoints: latency-svc-vxvf9 [752.948203ms]
Feb 21 21:01:27.017: INFO: Created: latency-svc-t5mv2
Feb 21 21:01:27.028: INFO: Got endpoints: latency-svc-4bx7j [749.823012ms]
Feb 21 21:01:27.050: INFO: Created: latency-svc-zss4d
Feb 21 21:01:27.081: INFO: Got endpoints: latency-svc-hjvv4 [752.727619ms]
Feb 21 21:01:27.104: INFO: Created: latency-svc-86zjc
Feb 21 21:01:27.128: INFO: Got endpoints: latency-svc-xnnq9 [749.589569ms]
Feb 21 21:01:27.147: INFO: Created: latency-svc-xdpng
Feb 21 21:01:27.185: INFO: Got endpoints: latency-svc-gft6k [754.537437ms]
Feb 21 21:01:27.205: INFO: Created: latency-svc-gmprb
Feb 21 21:01:27.227: INFO: Got endpoints: latency-svc-kq2rj [748.592351ms]
Feb 21 21:01:27.253: INFO: Created: latency-svc-vj2bc
Feb 21 21:01:27.281: INFO: Got endpoints: latency-svc-xcfkq [752.296266ms]
Feb 21 21:01:27.301: INFO: Created: latency-svc-p7lhx
Feb 21 21:01:27.331: INFO: Got endpoints: latency-svc-5lr5j [752.863859ms]
Feb 21 21:01:27.352: INFO: Created: latency-svc-jtfhv
Feb 21 21:01:27.377: INFO: Got endpoints: latency-svc-xl74x [743.143561ms]
Feb 21 21:01:27.399: INFO: Created: latency-svc-7rj5w
Feb 21 21:01:27.428: INFO: Got endpoints: latency-svc-tfjcg [751.435751ms]
Feb 21 21:01:27.452: INFO: Created: latency-svc-vglns
Feb 21 21:01:27.478: INFO: Got endpoints: latency-svc-8wlcg [740.595269ms]
Feb 21 21:01:27.500: INFO: Created: latency-svc-qrvzb
Feb 21 21:01:27.528: INFO: Got endpoints: latency-svc-t9wk7 [749.448411ms]
Feb 21 21:01:27.555: INFO: Created: latency-svc-f4b8q
Feb 21 21:01:27.578: INFO: Got endpoints: latency-svc-pfxck [749.865223ms]
Feb 21 21:01:27.600: INFO: Created: latency-svc-shr2l
Feb 21 21:01:27.629: INFO: Got endpoints: latency-svc-w6thg [751.624027ms]
Feb 21 21:01:27.658: INFO: Created: latency-svc-ssld8
Feb 21 21:01:27.677: INFO: Got endpoints: latency-svc-54fcb [749.355614ms]
Feb 21 21:01:27.702: INFO: Created: latency-svc-dzwgn
Feb 21 21:01:27.733: INFO: Got endpoints: latency-svc-t5mv2 [752.516788ms]
Feb 21 21:01:27.759: INFO: Created: latency-svc-gjvg2
Feb 21 21:01:27.777: INFO: Got endpoints: latency-svc-zss4d [749.112378ms]
Feb 21 21:01:27.798: INFO: Created: latency-svc-wnhbg
Feb 21 21:01:27.827: INFO: Got endpoints: latency-svc-86zjc [746.020988ms]
Feb 21 21:01:27.848: INFO: Created: latency-svc-82x95
Feb 21 21:01:27.877: INFO: Got endpoints: latency-svc-xdpng [749.641627ms]
Feb 21 21:01:27.897: INFO: Created: latency-svc-9cmsv
Feb 21 21:01:27.930: INFO: Got endpoints: latency-svc-gmprb [745.219518ms]
Feb 21 21:01:27.951: INFO: Created: latency-svc-zkxtv
Feb 21 21:01:27.981: INFO: Got endpoints: latency-svc-vj2bc [753.899811ms]
Feb 21 21:01:28.001: INFO: Created: latency-svc-rgdqz
Feb 21 21:01:28.028: INFO: Got endpoints: latency-svc-p7lhx [747.239535ms]
Feb 21 21:01:28.051: INFO: Created: latency-svc-78wvf
Feb 21 21:01:28.079: INFO: Got endpoints: latency-svc-jtfhv [747.776407ms]
Feb 21 21:01:28.102: INFO: Created: latency-svc-b8t8d
Feb 21 21:01:28.128: INFO: Got endpoints: latency-svc-7rj5w [750.897255ms]
Feb 21 21:01:28.154: INFO: Created: latency-svc-7frk4
Feb 21 21:01:28.182: INFO: Got endpoints: latency-svc-vglns [753.775449ms]
Feb 21 21:01:28.202: INFO: Created: latency-svc-k5f5x
Feb 21 21:01:28.229: INFO: Got endpoints: latency-svc-qrvzb [750.19753ms]
Feb 21 21:01:28.266: INFO: Created: latency-svc-gc8j9
Feb 21 21:01:28.277: INFO: Got endpoints: latency-svc-f4b8q [749.09604ms]
Feb 21 21:01:28.303: INFO: Created: latency-svc-j6lsd
Feb 21 21:01:28.330: INFO: Got endpoints: latency-svc-shr2l [752.126477ms]
Feb 21 21:01:28.354: INFO: Created: latency-svc-hpd8z
Feb 21 21:01:28.378: INFO: Got endpoints: latency-svc-ssld8 [749.148151ms]
Feb 21 21:01:28.401: INFO: Created: latency-svc-26dvk
Feb 21 21:01:28.427: INFO: Got endpoints: latency-svc-dzwgn [750.824402ms]
Feb 21 21:01:28.449: INFO: Created: latency-svc-dlrsz
Feb 21 21:01:28.478: INFO: Got endpoints: latency-svc-gjvg2 [744.587725ms]
Feb 21 21:01:28.500: INFO: Created: latency-svc-69sbc
Feb 21 21:01:28.528: INFO: Got endpoints: latency-svc-wnhbg [750.472216ms]
Feb 21 21:01:28.554: INFO: Created: latency-svc-sngtl
Feb 21 21:01:28.578: INFO: Got endpoints: latency-svc-82x95 [750.481601ms]
Feb 21 21:01:28.598: INFO: Created: latency-svc-ghtnf
Feb 21 21:01:28.630: INFO: Got endpoints: latency-svc-9cmsv [752.46211ms]
Feb 21 21:01:28.653: INFO: Created: latency-svc-cx6gh
Feb 21 21:01:28.678: INFO: Got endpoints: latency-svc-zkxtv [747.19979ms]
Feb 21 21:01:28.724: INFO: Created: latency-svc-kjxf7
Feb 21 21:01:28.729: INFO: Got endpoints: latency-svc-rgdqz [747.416215ms]
Feb 21 21:01:28.753: INFO: Created: latency-svc-bg2kw
Feb 21 21:01:28.778: INFO: Got endpoints: latency-svc-78wvf [749.594211ms]
Feb 21 21:01:28.804: INFO: Created: latency-svc-pm8jd
Feb 21 21:01:28.828: INFO: Got endpoints: latency-svc-b8t8d [749.06246ms]
Feb 21 21:01:28.852: INFO: Created: latency-svc-7dlbq
Feb 21 21:01:28.878: INFO: Got endpoints: latency-svc-7frk4 [749.609857ms]
Feb 21 21:01:28.898: INFO: Created: latency-svc-rvmt7
Feb 21 21:01:28.930: INFO: Got endpoints: latency-svc-k5f5x [748.041653ms]
Feb 21 21:01:28.951: INFO: Created: latency-svc-tt46v
Feb 21 21:01:28.983: INFO: Got endpoints: latency-svc-gc8j9 [754.320862ms]
Feb 21 21:01:29.006: INFO: Created: latency-svc-8rsqs
Feb 21 21:01:29.031: INFO: Got endpoints: latency-svc-j6lsd [754.205761ms]
Feb 21 21:01:29.055: INFO: Created: latency-svc-xmsm6
Feb 21 21:01:29.084: INFO: Got endpoints: latency-svc-hpd8z [751.576092ms]
Feb 21 21:01:29.105: INFO: Created: latency-svc-pgf9z
Feb 21 21:01:29.128: INFO: Got endpoints: latency-svc-26dvk [750.079893ms]
Feb 21 21:01:29.152: INFO: Created: latency-svc-zx7tl
Feb 21 21:01:29.179: INFO: Got endpoints: latency-svc-dlrsz [751.687232ms]
Feb 21 21:01:29.213: INFO: Created: latency-svc-szxjj
Feb 21 21:01:29.228: INFO: Got endpoints: latency-svc-69sbc [750.154425ms]
Feb 21 21:01:29.257: INFO: Created: latency-svc-hlbbd
Feb 21 21:01:29.280: INFO: Got endpoints: latency-svc-sngtl [751.70698ms]
Feb 21 21:01:29.336: INFO: Got endpoints: latency-svc-ghtnf [757.979927ms]
Feb 21 21:01:29.355: INFO: Created: latency-svc-6wz7c
Feb 21 21:01:29.375: INFO: Created: latency-svc-x8bnm
Feb 21 21:01:29.382: INFO: Got endpoints: latency-svc-cx6gh [752.018256ms]
Feb 21 21:01:29.406: INFO: Created: latency-svc-f4gvk
Feb 21 21:01:29.430: INFO: Got endpoints: latency-svc-kjxf7 [752.443982ms]
Feb 21 21:01:29.449: INFO: Created: latency-svc-72q5c
Feb 21 21:01:29.483: INFO: Got endpoints: latency-svc-bg2kw [753.480871ms]
Feb 21 21:01:29.506: INFO: Created: latency-svc-mq7tl
Feb 21 21:01:29.528: INFO: Got endpoints: latency-svc-pm8jd [749.733975ms]
Feb 21 21:01:29.558: INFO: Created: latency-svc-q5bqh
Feb 21 21:01:29.582: INFO: Got endpoints: latency-svc-7dlbq [753.757953ms]
Feb 21 21:01:29.627: INFO: Created: latency-svc-4dwt6
Feb 21 21:01:29.629: INFO: Got endpoints: latency-svc-rvmt7 [751.318938ms]
Feb 21 21:01:29.658: INFO: Created: latency-svc-zbmqk
Feb 21 21:01:29.678: INFO: Got endpoints: latency-svc-tt46v [748.273879ms]
Feb 21 21:01:29.709: INFO: Created: latency-svc-nglzp
Feb 21 21:01:29.731: INFO: Got endpoints: latency-svc-8rsqs [747.567431ms]
Feb 21 21:01:29.770: INFO: Created: latency-svc-5wpmj
Feb 21 21:01:29.784: INFO: Got endpoints: latency-svc-xmsm6 [751.983013ms]
Feb 21 21:01:29.818: INFO: Created: latency-svc-lgf8h
Feb 21 21:01:29.828: INFO: Got endpoints: latency-svc-pgf9z [744.006512ms]
Feb 21 21:01:29.862: INFO: Created: latency-svc-twkf4
Feb 21 21:01:29.878: INFO: Got endpoints: latency-svc-zx7tl [749.639578ms]
Feb 21 21:01:29.905: INFO: Created: latency-svc-mng9m
Feb 21 21:01:29.929: INFO: Got endpoints: latency-svc-szxjj [749.433173ms]
Feb 21 21:01:29.987: INFO: Got endpoints: latency-svc-hlbbd [758.522391ms]
Feb 21 21:01:29.998: INFO: Created: latency-svc-rp4nt
Feb 21 21:01:30.036: INFO: Got endpoints: latency-svc-6wz7c [755.985391ms]
Feb 21 21:01:30.049: INFO: Created: latency-svc-7jcqj
Feb 21 21:01:30.065: INFO: Created: latency-svc-lgvbf
Feb 21 21:01:30.082: INFO: Got endpoints: latency-svc-x8bnm [745.696629ms]
Feb 21 21:01:30.114: INFO: Created: latency-svc-kd4g9
Feb 21 21:01:30.130: INFO: Got endpoints: latency-svc-f4gvk [748.271ms]
Feb 21 21:01:30.151: INFO: Created: latency-svc-xkb82
Feb 21 21:01:30.185: INFO: Got endpoints: latency-svc-72q5c [754.28343ms]
Feb 21 21:01:30.212: INFO: Created: latency-svc-kjnqz
Feb 21 21:01:30.228: INFO: Got endpoints: latency-svc-mq7tl [745.346906ms]
Feb 21 21:01:30.250: INFO: Created: latency-svc-7n885
Feb 21 21:01:30.279: INFO: Got endpoints: latency-svc-q5bqh [750.974477ms]
Feb 21 21:01:30.299: INFO: Created: latency-svc-l4h5p
Feb 21 21:01:30.328: INFO: Got endpoints: latency-svc-4dwt6 [746.414746ms]
Feb 21 21:01:30.353: INFO: Created: latency-svc-zgwqp
Feb 21 21:01:30.379: INFO: Got endpoints: latency-svc-zbmqk [749.343281ms]
Feb 21 21:01:30.410: INFO: Created: latency-svc-wzm9b
Feb 21 21:01:30.432: INFO: Got endpoints: latency-svc-nglzp [753.594637ms]
Feb 21 21:01:30.462: INFO: Created: latency-svc-rgqvg
Feb 21 21:01:30.479: INFO: Got endpoints: latency-svc-5wpmj [748.675104ms]
Feb 21 21:01:30.504: INFO: Created: latency-svc-lkv7d
Feb 21 21:01:30.530: INFO: Got endpoints: latency-svc-lgf8h [746.161891ms]
Feb 21 21:01:30.552: INFO: Created: latency-svc-rmz9m
Feb 21 21:01:30.578: INFO: Got endpoints: latency-svc-twkf4 [750.817936ms]
Feb 21 21:01:30.597: INFO: Created: latency-svc-phpbt
Feb 21 21:01:30.634: INFO: Got endpoints: latency-svc-mng9m [755.40569ms]
Feb 21 21:01:30.660: INFO: Created: latency-svc-hl5qn
Feb 21 21:01:30.677: INFO: Got endpoints: latency-svc-rp4nt [745.49514ms]
Feb 21 21:01:30.698: INFO: Created: latency-svc-79psx
Feb 21 21:01:30.728: INFO: Got endpoints: latency-svc-7jcqj [707.893753ms]
Feb 21 21:01:30.763: INFO: Created: latency-svc-n78nj
Feb 21 21:01:30.778: INFO: Got endpoints: latency-svc-lgvbf [741.908642ms]
Feb 21 21:01:30.807: INFO: Created: latency-svc-v29ps
Feb 21 21:01:30.843: INFO: Got endpoints: latency-svc-kd4g9 [761.451401ms]
Feb 21 21:01:30.870: INFO: Created: latency-svc-j4ljl
Feb 21 21:01:30.877: INFO: Got endpoints: latency-svc-xkb82 [746.2136ms]
Feb 21 21:01:30.899: INFO: Created: latency-svc-6kwpr
Feb 21 21:01:30.929: INFO: Got endpoints: latency-svc-kjnqz [744.128901ms]
Feb 21 21:01:30.965: INFO: Created: latency-svc-nrmzq
Feb 21 21:01:30.990: INFO: Got endpoints: latency-svc-7n885 [761.976639ms]
Feb 21 21:01:31.010: INFO: Created: latency-svc-zjckg
Feb 21 21:01:31.028: INFO: Got endpoints: latency-svc-l4h5p [748.897472ms]
Feb 21 21:01:31.049: INFO: Created: latency-svc-x49vk
Feb 21 21:01:31.086: INFO: Got endpoints: latency-svc-zgwqp [757.184569ms]
Feb 21 21:01:31.207: INFO: Created: latency-svc-pmv4s
Feb 21 21:01:31.208: INFO: Got endpoints: latency-svc-rgqvg [775.735714ms]
Feb 21 21:01:31.208: INFO: Got endpoints: latency-svc-wzm9b [829.183391ms]
Feb 21 21:01:31.245: INFO: Created: latency-svc-snstw
Feb 21 21:01:31.246: INFO: Got endpoints: latency-svc-lkv7d [766.274399ms]
Feb 21 21:01:31.272: INFO: Created: latency-svc-wmtc5
Feb 21 21:01:31.287: INFO: Created: latency-svc-vs7pk
Feb 21 21:01:31.287: INFO: Got endpoints: latency-svc-rmz9m [756.757503ms]
Feb 21 21:01:31.333: INFO: Created: latency-svc-hlphg
Feb 21 21:01:31.333: INFO: Got endpoints: latency-svc-phpbt [754.31433ms]
Feb 21 21:01:31.370: INFO: Created: latency-svc-f8shk
Feb 21 21:01:31.381: INFO: Got endpoints: latency-svc-hl5qn [747.102989ms]
Feb 21 21:01:31.410: INFO: Created: latency-svc-vdjcc
Feb 21 21:01:31.430: INFO: Got endpoints: latency-svc-79psx [752.887617ms]
Feb 21 21:01:31.455: INFO: Created: latency-svc-m5cpp
Feb 21 21:01:31.479: INFO: Got endpoints: latency-svc-n78nj [750.997148ms]
Feb 21 21:01:31.506: INFO: Created: latency-svc-wfs9d
Feb 21 21:01:31.534: INFO: Got endpoints: latency-svc-v29ps [756.510227ms]
Feb 21 21:01:31.563: INFO: Created: latency-svc-tcwl2
Feb 21 21:01:31.580: INFO: Got endpoints: latency-svc-j4ljl [736.560287ms]
Feb 21 21:01:31.603: INFO: Created: latency-svc-tcmct
Feb 21 21:01:31.628: INFO: Got endpoints: latency-svc-6kwpr [750.860428ms]
Feb 21 21:01:31.654: INFO: Created: latency-svc-fkq2s
Feb 21 21:01:31.677: INFO: Got endpoints: latency-svc-nrmzq [748.174427ms]
Feb 21 21:01:31.714: INFO: Created: latency-svc-8jqfv
Feb 21 21:01:31.727: INFO: Got endpoints: latency-svc-zjckg [737.25528ms]
Feb 21 21:01:31.749: INFO: Created: latency-svc-vc9mp
Feb 21 21:01:31.778: INFO: Got endpoints: latency-svc-x49vk [749.785942ms]
Feb 21 21:01:31.797: INFO: Created: latency-svc-6dbrw
Feb 21 21:01:31.827: INFO: Got endpoints: latency-svc-pmv4s [741.559027ms]
Feb 21 21:01:31.847: INFO: Created: latency-svc-lkfqp
Feb 21 21:01:31.887: INFO: Got endpoints: latency-svc-snstw [678.784994ms]
Feb 21 21:01:31.915: INFO: Created: latency-svc-gj5fw
Feb 21 21:01:31.929: INFO: Got endpoints: latency-svc-wmtc5 [720.299614ms]
Feb 21 21:01:31.962: INFO: Created: latency-svc-xq4zg
Feb 21 21:01:32.002: INFO: Got endpoints: latency-svc-vs7pk [755.780316ms]
Feb 21 21:01:32.040: INFO: Got endpoints: latency-svc-hlphg [753.24795ms]
Feb 21 21:01:32.041: INFO: Created: latency-svc-c2cqh
Feb 21 21:01:32.080: INFO: Got endpoints: latency-svc-f8shk [746.748086ms]
Feb 21 21:01:32.150: INFO: Got endpoints: latency-svc-vdjcc [768.927473ms]
Feb 21 21:01:32.177: INFO: Got endpoints: latency-svc-m5cpp [746.707597ms]
Feb 21 21:01:32.229: INFO: Got endpoints: latency-svc-wfs9d [749.87658ms]
Feb 21 21:01:32.278: INFO: Got endpoints: latency-svc-tcwl2 [739.759371ms]
Feb 21 21:01:32.329: INFO: Got endpoints: latency-svc-tcmct [748.652075ms]
Feb 21 21:01:32.380: INFO: Got endpoints: latency-svc-fkq2s [751.869326ms]
Feb 21 21:01:32.431: INFO: Got endpoints: latency-svc-8jqfv [753.489489ms]
Feb 21 21:01:32.479: INFO: Got endpoints: latency-svc-vc9mp [751.26574ms]
Feb 21 21:01:32.530: INFO: Got endpoints: latency-svc-6dbrw [752.299165ms]
Feb 21 21:01:32.582: INFO: Got endpoints: latency-svc-lkfqp [754.482612ms]
Feb 21 21:01:32.631: INFO: Got endpoints: latency-svc-gj5fw [744.380289ms]
Feb 21 21:01:32.677: INFO: Got endpoints: latency-svc-xq4zg [748.80757ms]
Feb 21 21:01:32.729: INFO: Got endpoints: latency-svc-c2cqh [727.054721ms]
Feb 21 21:01:32.729: INFO: Latencies: [30.184074ms 41.424674ms 43.079665ms 62.89043ms 80.100134ms 128.476937ms 157.277888ms 169.574166ms 186.889094ms 213.253904ms 226.811739ms 248.926471ms 252.085505ms 258.618082ms 261.066881ms 261.329433ms 261.737703ms 262.21159ms 263.315792ms 264.378328ms 266.039996ms 267.073374ms 269.331136ms 271.789267ms 272.465041ms 277.062451ms 277.57436ms 278.055463ms 278.156718ms 282.868109ms 283.48231ms 287.083597ms 288.979524ms 289.724653ms 290.868268ms 291.508966ms 291.982675ms 292.766355ms 293.154907ms 295.840379ms 300.85406ms 300.989631ms 301.094049ms 301.873622ms 308.679343ms 358.854452ms 361.090396ms 361.524882ms 370.200481ms 375.361546ms 435.757206ms 448.610431ms 486.715ms 513.199177ms 552.639647ms 582.188307ms 606.58427ms 633.311423ms 634.092195ms 665.327155ms 670.275722ms 678.784994ms 695.216439ms 702.517058ms 707.893753ms 720.299614ms 727.054721ms 730.427843ms 731.826197ms 732.581594ms 733.920437ms 736.560287ms 737.25528ms 739.759371ms 740.595269ms 741.359865ms 741.559027ms 741.908642ms 743.143561ms 744.006512ms 744.069521ms 744.128901ms 744.380289ms 744.587725ms 744.880929ms 745.219518ms 745.346906ms 745.49514ms 745.696629ms 746.020988ms 746.161891ms 746.2136ms 746.414746ms 746.483285ms 746.50954ms 746.707597ms 746.748086ms 746.93079ms 747.102989ms 747.19979ms 747.239535ms 747.416215ms 747.501729ms 747.567431ms 747.629656ms 747.776407ms 748.041653ms 748.100178ms 748.174427ms 748.271ms 748.273879ms 748.592351ms 748.652075ms 748.675104ms 748.80757ms 748.897472ms 749.06246ms 749.09604ms 749.112378ms 749.148151ms 749.343281ms 749.355614ms 749.425386ms 749.433173ms 749.448411ms 749.589569ms 749.594211ms 749.609857ms 749.639578ms 749.641627ms 749.733975ms 749.785942ms 749.808182ms 749.823012ms 749.865223ms 749.87658ms 750.016699ms 750.079893ms 750.154425ms 750.19753ms 750.472216ms 750.481601ms 750.598312ms 750.662166ms 750.817936ms 750.824402ms 750.860428ms 750.897255ms 750.974477ms 750.997148ms 751.26574ms 751.318938ms 751.435751ms 751.576092ms 751.624027ms 751.687232ms 751.70698ms 751.869326ms 751.983013ms 752.018256ms 752.126477ms 752.296266ms 752.299165ms 752.443982ms 752.46211ms 752.516788ms 752.715293ms 752.727619ms 752.863859ms 752.887617ms 752.948203ms 753.24795ms 753.480871ms 753.489489ms 753.594637ms 753.757953ms 753.775449ms 753.899811ms 754.205761ms 754.28343ms 754.31433ms 754.320862ms 754.482612ms 754.537437ms 755.40569ms 755.780316ms 755.985391ms 756.510227ms 756.592476ms 756.757503ms 757.04407ms 757.184569ms 757.979927ms 758.522391ms 761.451401ms 761.976639ms 766.274399ms 768.927473ms 775.735714ms 829.183391ms]
Feb 21 21:01:32.730: INFO: 50 %ile: 747.239535ms
Feb 21 21:01:32.730: INFO: 90 %ile: 754.31433ms
Feb 21 21:01:32.730: INFO: 99 %ile: 775.735714ms
Feb 21 21:01:32.730: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:01:32.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7289" for this suite.
Feb 21 21:01:58.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:01:59.102: INFO: namespace svc-latency-7289 deletion completed in 26.362797473s

• [SLOW TEST:37.258 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:01:59.102: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:02:04.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5575" for this suite.
Feb 21 21:02:10.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:02:11.217: INFO: namespace watch-5575 deletion completed in 6.406260245s

• [SLOW TEST:12.115 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:02:11.217: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Feb 21 21:02:11.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 api-versions'
Feb 21 21:02:11.548: INFO: stderr: ""
Feb 21 21:02:11.548: INFO: stdout: "acme.cert-manager.io/v1alpha2\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling.k8s.io/v1beta1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert-manager.io/v1alpha2\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nlinkerd.io/v1alpha1\nlinkerd.io/v1alpha2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrbacmanager.reactiveops.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsplit.smi-spec.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntap.linkerd.io/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:02:11.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2264" for this suite.
Feb 21 21:02:17.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:02:17.896: INFO: namespace kubectl-2264 deletion completed in 6.340627318s

• [SLOW TEST:6.679 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:02:17.897: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-584
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-584
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-584
Feb 21 21:02:18.129: INFO: Found 0 stateful pods, waiting for 1
Feb 21 21:02:28.139: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 21 21:02:28.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 21:02:28.328: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 21:02:28.328: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 21:02:28.328: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 21:02:28.333: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 21 21:02:38.339: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 21:02:38.339: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 21:02:38.361: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998578s
Feb 21 21:02:39.366: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994058281s
Feb 21 21:02:40.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98838221s
Feb 21 21:02:41.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982376855s
Feb 21 21:02:42.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.973940132s
Feb 21 21:02:43.393: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969023076s
Feb 21 21:02:44.399: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.961076066s
Feb 21 21:02:45.406: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.955403216s
Feb 21 21:02:46.412: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.94867248s
Feb 21 21:02:47.417: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.695895ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-584
Feb 21 21:02:48.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 21:02:48.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 21:02:48.610: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 21:02:48.610: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 21:02:48.615: INFO: Found 1 stateful pods, waiting for 3
Feb 21 21:02:58.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 21:02:58.621: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 21 21:02:58.621: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 21 21:02:58.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 21:02:58.837: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 21:02:58.837: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 21:02:58.837: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 21:02:58.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 21:02:59.050: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 21:02:59.050: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 21:02:59.050: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 21:02:59.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 21 21:02:59.261: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 21 21:02:59.261: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 21 21:02:59.261: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 21 21:02:59.261: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 21:02:59.266: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 21 21:03:12.312: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 21:03:12.312: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 21:03:12.312: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 21 21:03:12.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998408s
Feb 21 21:03:13.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99469765s
Feb 21 21:03:14.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989122055s
Feb 21 21:03:15.352: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983425186s
Feb 21 21:03:16.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977621824s
Feb 21 21:03:17.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971745316s
Feb 21 21:03:18.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964854476s
Feb 21 21:03:19.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959013123s
Feb 21 21:03:20.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953154482s
Feb 21 21:03:21.392: INFO: Verifying statefulset ss doesn't scale past 3 for another 943.338855ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-584
Feb 21 21:03:22.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 21:03:22.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 21:03:22.586: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 21:03:22.586: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 21:03:22.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 21:03:22.770: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 21:03:22.770: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 21:03:22.770: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 21:03:22.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 exec --namespace=statefulset-584 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 21 21:03:22.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 21 21:03:22.963: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 21 21:03:22.963: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 21 21:03:22.963: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 21 21:03:52.989: INFO: Deleting all statefulset in ns statefulset-584
Feb 21 21:03:52.995: INFO: Scaling statefulset ss to 0
Feb 21 21:03:53.010: INFO: Waiting for statefulset status.replicas updated to 0
Feb 21 21:03:53.014: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:03:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-584" for this suite.
Feb 21 21:03:59.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:03:59.383: INFO: namespace statefulset-584 deletion completed in 6.335429982s

• [SLOW TEST:101.486 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:03:59.383: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:04:01.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3686" for this suite.
Feb 21 21:04:07.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:04:07.964: INFO: namespace emptydir-wrapper-3686 deletion completed in 6.281046714s

• [SLOW TEST:8.581 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:04:07.964: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7641
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:04:08.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f" in namespace "downward-api-7641" to be "success or failure"
Feb 21 21:04:08.171: INFO: Pod "downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.258094ms
Feb 21 21:04:10.177: INFO: Pod "downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011801928s
STEP: Saw pod success
Feb 21 21:04:10.177: INFO: Pod "downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f" satisfied condition "success or failure"
Feb 21 21:04:10.181: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f container client-container: <nil>
STEP: delete the pod
Feb 21 21:04:10.210: INFO: Waiting for pod downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f to disappear
Feb 21 21:04:10.217: INFO: Pod downwardapi-volume-04cb6cab-7202-48b9-bb7a-2fb0ef40d34f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:04:10.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7641" for this suite.
Feb 21 21:04:16.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:04:16.574: INFO: namespace downward-api-7641 deletion completed in 6.351075947s

• [SLOW TEST:8.610 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:04:16.574: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-8367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Feb 21 21:04:16.734: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8367" to be "success or failure"
Feb 21 21:04:16.743: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.255553ms
Feb 21 21:04:18.749: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014261487s
STEP: Saw pod success
Feb 21 21:04:18.749: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 21 21:04:18.754: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 21 21:04:18.812: INFO: Waiting for pod pod-host-path-test to disappear
Feb 21 21:04:18.816: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:04:18.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8367" for this suite.
Feb 21 21:04:24.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:04:25.095: INFO: namespace hostpath-8367 deletion completed in 6.273041422s

• [SLOW TEST:8.521 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:04:25.096: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:04:25.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6532'
Feb 21 21:04:25.494: INFO: stderr: ""
Feb 21 21:04:25.494: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 21 21:04:25.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-6532'
Feb 21 21:04:25.718: INFO: stderr: ""
Feb 21 21:04:25.718: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 21 21:04:26.727: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:04:26.727: INFO: Found 1 / 1
Feb 21 21:04:26.727: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 21:04:26.731: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:04:26.731: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 21:04:26.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 describe pod redis-master-hg9wf --namespace=kubectl-6532'
Feb 21 21:04:26.822: INFO: stderr: ""
Feb 21 21:04:26.823: INFO: stdout: "Name:           redis-master-hg9wf\nNamespace:      kubectl-6532\nPriority:       0\nNode:           ip-172-20-17-149.us-east-2.compute.internal/172.20.17.149\nStart Time:     Fri, 21 Feb 2020 21:04:25 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.101.183.130/32\nStatus:         Running\nIP:             100.101.183.130\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c0a92b96bf0124f3a965760ec5324a993706c05acc34d174be5409fa5b48fd39\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 Feb 2020 21:04:26 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-s4mdj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-s4mdj:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-s4mdj\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                  Message\n  ----    ------     ----  ----                                                  -------\n  Normal  Scheduled  1s    default-scheduler                                     Successfully assigned kubectl-6532/redis-master-hg9wf to ip-172-20-17-149.us-east-2.compute.internal\n  Normal  Pulled     0s    kubelet, ip-172-20-17-149.us-east-2.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    0s    kubelet, ip-172-20-17-149.us-east-2.compute.internal  Created container redis-master\n  Normal  Started    0s    kubelet, ip-172-20-17-149.us-east-2.compute.internal  Started container redis-master\n"
Feb 21 21:04:26.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 describe rc redis-master --namespace=kubectl-6532'
Feb 21 21:04:26.916: INFO: stderr: ""
Feb 21 21:04:26.916: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6532\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  InjectionSkipped  1s    linkerd-proxy-injector  Linkerd sidecar proxy injection skipped: neither the namespace nor the pod have the annotation \"linkerd.io/inject:enabled\"\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: redis-master-hg9wf\n"
Feb 21 21:04:26.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 describe service redis-master --namespace=kubectl-6532'
Feb 21 21:04:27.026: INFO: stderr: ""
Feb 21 21:04:27.026: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6532\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.64.58.250\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.101.183.130:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 21 21:04:27.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 describe node ip-172-20-16-34.us-east-2.compute.internal'
Feb 21 21:04:27.142: INFO: stderr: ""
Feb 21 21:04:27.142: INFO: stdout: "Name:               ip-172-20-16-34.us-east-2.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t2.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-2\n                    failure-domain.beta.kubernetes.io/zone=us-east-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-20-16-34.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    node-role.kubernetes.io/master=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.20.16.34/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.111.197.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 Feb 2020 19:39:28 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 21 Feb 2020 19:39:56 +0000   Fri, 21 Feb 2020 19:39:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 21 Feb 2020 21:04:11 +0000   Fri, 21 Feb 2020 19:39:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 21 Feb 2020 21:04:11 +0000   Fri, 21 Feb 2020 19:39:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 21 Feb 2020 21:04:11 +0000   Fri, 21 Feb 2020 19:39:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 21 Feb 2020 21:04:11 +0000   Fri, 21 Feb 2020 19:39:58 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   172.20.16.34\n  Hostname:     ip-172-20-16-34.us-east-2.compute.internal\n  InternalDNS:  ip-172-20-16-34.us-east-2.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           65989144Ki\n hugepages-2Mi:               0\n memory:                      4049076Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  39\n cpu:                         2\n ephemeral-storage:           60815595010\n hugepages-2Mi:               0\n memory:                      3946676Ki\n pods:                        110\nSystem Info:\n Machine ID:                 d3b6c47977314651a17ceae24e9ecfdb\n System UUID:                EC2E015F-CC36-510C-8253-0415C47D17DE\n Boot ID:                    b220a0c1-07a5-4011-979e-cf9967874f68\n Kernel Version:             4.9.0-11-amd64\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.7\n Kube-Proxy Version:         v1.15.7\nPodCIDR:                     100.96.0.0/24\nProviderID:                  aws:///us-east-2a/i-04277ea5f80b78be3\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                                  ------------  ----------  ---------------  -------------  ---\n  aws-iam-authenticator      aws-iam-authenticator-l57qs                                           10m (0%)      100m (5%)   20Mi (0%)        20Mi (0%)      74m\n  kube-system                calico-kube-controllers-54c96b97b9-mqd2f                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                calico-node-mcmfh                                                     90m (4%)      0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                etcd-manager-events-ip-172-20-16-34.us-east-2.compute.internal        200m (10%)    0 (0%)      100Mi (2%)       0 (0%)         84m\n  kube-system                etcd-manager-main-ip-172-20-16-34.us-east-2.compute.internal          200m (10%)    0 (0%)      100Mi (2%)       0 (0%)         84m\n  kube-system                kube-apiserver-ip-172-20-16-34.us-east-2.compute.internal             150m (7%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                kube-controller-manager-ip-172-20-16-34.us-east-2.compute.internal    100m (5%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                kube-proxy-ip-172-20-16-34.us-east-2.compute.internal                 100m (5%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                kube-scheduler-ip-172-20-16-34.us-east-2.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         84m\n  prometheus-operator        prometheus-operator-prometheus-node-exporter-5xtpq                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-23035895473a4986-dzl4h               0 (0%)        0 (0%)      0 (0%)           0 (0%)         72m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         950m (47%)  100m (5%)\n  memory                      220Mi (5%)  20Mi (0%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Feb 21 21:04:27.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 describe namespace kubectl-6532'
Feb 21 21:04:27.242: INFO: stderr: ""
Feb 21 21:04:27.242: INFO: stdout: "Name:         kubectl-6532\nLabels:       e2e-framework=kubectl\n              e2e-run=b63ebd86-e1d8-42d8-b54b-73290741da8d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:04:27.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6532" for this suite.
Feb 21 21:04:49.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:04:49.587: INFO: namespace kubectl-6532 deletion completed in 22.33897762s

• [SLOW TEST:24.490 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:04:49.587: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:04:49.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42" in namespace "downward-api-5685" to be "success or failure"
Feb 21 21:04:49.756: INFO: Pod "downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42": Phase="Pending", Reason="", readiness=false. Elapsed: 6.496575ms
Feb 21 21:04:51.761: INFO: Pod "downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012207957s
Feb 21 21:04:53.770: INFO: Pod "downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021406705s
STEP: Saw pod success
Feb 21 21:04:53.770: INFO: Pod "downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42" satisfied condition "success or failure"
Feb 21 21:04:53.782: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42 container client-container: <nil>
STEP: delete the pod
Feb 21 21:04:53.812: INFO: Waiting for pod downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42 to disappear
Feb 21 21:04:53.816: INFO: Pod downwardapi-volume-b533b0a0-1420-4f0b-ac35-97256ebdac42 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:04:53.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5685" for this suite.
Feb 21 21:04:59.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:05:00.211: INFO: namespace downward-api-5685 deletion completed in 6.389238825s

• [SLOW TEST:10.624 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:05:00.211: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2707
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2707.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2707.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2707.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2707.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 21:05:04.499: INFO: DNS probes using dns-test-768be27e-cd1f-42ec-bb04-871cf2a2af06 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2707.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2707.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2707.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2707.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 21:05:06.598: INFO: DNS probes using dns-test-174ec5e9-4f04-42eb-9bf4-a518b7d2854e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2707.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2707.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2707.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2707.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 21:05:08.698: INFO: DNS probes using dns-test-e3de772a-867b-4ba5-b6fc-a60e18597a02 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:05:08.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2707" for this suite.
Feb 21 21:05:14.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:05:15.200: INFO: namespace dns-2707 deletion completed in 6.437532872s

• [SLOW TEST:14.989 seconds]
[sig-network] DNS
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:05:15.202: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-b304121d-60e0-4925-a6ec-adbd987bc097
STEP: Creating secret with name secret-projected-all-test-volume-ecfb97e0-0da0-41f9-9557-542a22b232f9
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 21 21:05:15.388: INFO: Waiting up to 5m0s for pod "projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d" in namespace "projected-2929" to be "success or failure"
Feb 21 21:05:15.399: INFO: Pod "projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.974892ms
Feb 21 21:05:17.405: INFO: Pod "projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016826584s
STEP: Saw pod success
Feb 21 21:05:17.405: INFO: Pod "projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d" satisfied condition "success or failure"
Feb 21 21:05:17.409: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 21 21:05:17.442: INFO: Waiting for pod projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d to disappear
Feb 21 21:05:17.446: INFO: Pod projected-volume-3d754b60-dd25-41ec-b057-3373ef1dc26d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:05:17.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2929" for this suite.
Feb 21 21:05:23.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:05:23.731: INFO: namespace projected-2929 deletion completed in 6.27868521s

• [SLOW TEST:8.529 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:05:23.733: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:05:23.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6" in namespace "projected-7071" to be "success or failure"
Feb 21 21:05:23.998: INFO: Pod "downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.178033ms
Feb 21 21:05:26.004: INFO: Pod "downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020013185s
STEP: Saw pod success
Feb 21 21:05:26.004: INFO: Pod "downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6" satisfied condition "success or failure"
Feb 21 21:05:26.009: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6 container client-container: <nil>
STEP: delete the pod
Feb 21 21:05:26.042: INFO: Waiting for pod downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6 to disappear
Feb 21 21:05:26.047: INFO: Pod downwardapi-volume-1cd3f231-0da2-4674-9df0-ef33773f2cf6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:05:26.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7071" for this suite.
Feb 21 21:05:32.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:05:32.381: INFO: namespace projected-7071 deletion completed in 6.325884951s

• [SLOW TEST:8.648 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:05:32.381: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 21 21:05:32.578: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:32.578: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:32.578: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:32.586: INFO: Number of nodes with available pods: 0
Feb 21 21:05:32.586: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:05:33.592: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:33.592: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:33.592: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:33.598: INFO: Number of nodes with available pods: 0
Feb 21 21:05:33.598: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:05:34.592: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:34.592: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:34.594: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:34.598: INFO: Number of nodes with available pods: 4
Feb 21 21:05:34.598: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 21 21:05:34.621: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:34.621: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:34.621: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:34.627: INFO: Number of nodes with available pods: 3
Feb 21 21:05:34.627: INFO: Node ip-172-20-17-149.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:05:35.634: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:35.634: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:35.634: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:35.639: INFO: Number of nodes with available pods: 3
Feb 21 21:05:35.639: INFO: Node ip-172-20-17-149.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:05:36.634: INFO: DaemonSet pods can't tolerate node ip-172-20-16-34.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:36.634: INFO: DaemonSet pods can't tolerate node ip-172-20-17-145.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:36.634: INFO: DaemonSet pods can't tolerate node ip-172-20-18-208.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 21 21:05:36.640: INFO: Number of nodes with available pods: 4
Feb 21 21:05:36.640: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9286, will wait for the garbage collector to delete the pods
Feb 21 21:05:36.720: INFO: Deleting DaemonSet.extensions daemon-set took: 15.772854ms
Feb 21 21:05:36.820: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.274543ms
Feb 21 21:05:47.125: INFO: Number of nodes with available pods: 0
Feb 21 21:05:47.126: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 21:05:47.130: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9286/daemonsets","resourceVersion":"28523"},"items":null}

Feb 21 21:05:47.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9286/pods","resourceVersion":"28523"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:05:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9286" for this suite.
Feb 21 21:05:53.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:05:53.478: INFO: namespace daemonsets-9286 deletion completed in 6.299306166s

• [SLOW TEST:21.097 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:05:53.479: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9204
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-abcb859d-8e32-4529-9560-698f0d3cbead in namespace container-probe-9204
Feb 21 21:05:55.652: INFO: Started pod liveness-abcb859d-8e32-4529-9560-698f0d3cbead in namespace container-probe-9204
STEP: checking the pod's current state and verifying that restartCount is present
Feb 21 21:05:55.659: INFO: Initial restart count of pod liveness-abcb859d-8e32-4529-9560-698f0d3cbead is 0
Feb 21 21:06:19.734: INFO: Restart count of pod container-probe-9204/liveness-abcb859d-8e32-4529-9560-698f0d3cbead is now 1 (24.074780007s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:06:19.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9204" for this suite.
Feb 21 21:06:25.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:06:25.999: INFO: namespace container-probe-9204 deletion completed in 6.240698192s

• [SLOW TEST:32.520 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:06:25.999: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-27
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 21 21:06:26.257: INFO: Waiting up to 5m0s for pod "pod-5c28c175-6901-43cb-9685-3993f2b5bd81" in namespace "emptydir-27" to be "success or failure"
Feb 21 21:06:26.264: INFO: Pod "pod-5c28c175-6901-43cb-9685-3993f2b5bd81": Phase="Pending", Reason="", readiness=false. Elapsed: 6.682344ms
Feb 21 21:06:28.270: INFO: Pod "pod-5c28c175-6901-43cb-9685-3993f2b5bd81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012315015s
STEP: Saw pod success
Feb 21 21:06:28.270: INFO: Pod "pod-5c28c175-6901-43cb-9685-3993f2b5bd81" satisfied condition "success or failure"
Feb 21 21:06:28.276: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-5c28c175-6901-43cb-9685-3993f2b5bd81 container test-container: <nil>
STEP: delete the pod
Feb 21 21:06:28.309: INFO: Waiting for pod pod-5c28c175-6901-43cb-9685-3993f2b5bd81 to disappear
Feb 21 21:06:28.319: INFO: Pod pod-5c28c175-6901-43cb-9685-3993f2b5bd81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:06:28.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-27" for this suite.
Feb 21 21:06:34.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:06:34.582: INFO: namespace emptydir-27 deletion completed in 6.256444892s

• [SLOW TEST:8.583 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:06:34.582: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 21:06:36.818: INFO: DNS probes using dns-4785/dns-test-e4b3a2ba-e26f-4675-8d74-4b42d67d49a7 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:06:36.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4785" for this suite.
Feb 21 21:06:42.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:06:43.210: INFO: namespace dns-4785 deletion completed in 6.361016109s

• [SLOW TEST:8.628 seconds]
[sig-network] DNS
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:06:43.211: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167
Feb 21 21:06:43.368: INFO: Pod name my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167: Found 0 pods out of 1
Feb 21 21:06:48.374: INFO: Pod name my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167: Found 1 pods out of 1
Feb 21 21:06:48.374: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167" are running
Feb 21 21:06:48.379: INFO: Pod "my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167-tgrtw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 21:06:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 21:06:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 21:06:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-21 21:06:43 +0000 UTC Reason: Message:}])
Feb 21 21:06:48.379: INFO: Trying to dial the pod
Feb 21 21:06:53.397: INFO: Controller my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167: Got expected result from replica 1 [my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167-tgrtw]: "my-hostname-basic-15973029-f4df-40ae-adbf-4b28c938b167-tgrtw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:06:53.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6454" for this suite.
Feb 21 21:06:59.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:06:59.889: INFO: namespace replication-controller-6454 deletion completed in 6.485817428s

• [SLOW TEST:16.678 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:06:59.889: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 21 21:07:04.226: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:04.232: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:06.232: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:06.239: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:08.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:08.238: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:10.232: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:10.239: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:12.232: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:12.238: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:14.232: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:14.238: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:16.232: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:16.240: INFO: Pod pod-with-poststart-http-hook still exists
Feb 21 21:07:18.232: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 21 21:07:18.237: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:07:18.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4433" for this suite.
Feb 21 21:07:40.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:07:40.590: INFO: namespace container-lifecycle-hook-4433 deletion completed in 22.346865644s

• [SLOW TEST:40.701 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:07:40.590: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3612
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 21:07:42.785: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:07:42.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3612" for this suite.
Feb 21 21:07:48.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:07:49.085: INFO: namespace container-runtime-3612 deletion completed in 6.266545831s

• [SLOW TEST:8.495 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:07:49.085: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-329
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:07:49.239: INFO: Creating deployment "test-recreate-deployment"
Feb 21 21:07:49.245: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 21 21:07:49.259: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 21 21:07:51.269: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 21 21:07:51.274: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 21 21:07:51.285: INFO: Updating deployment test-recreate-deployment
Feb 21 21:07:51.285: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 21 21:07:51.522: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-329,SelfLink:/apis/apps/v1/namespaces/deployment-329/deployments/test-recreate-deployment,UID:b11b464e-6c9a-4e6c-8be5-98316b765f27,ResourceVersion:29203,Generation:2,CreationTimestamp:2020-02-21 21:07:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-02-21 21:07:51 +0000 UTC 2020-02-21 21:07:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-02-21 21:07:51 +0000 UTC 2020-02-21 21:07:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 21 21:07:51.527: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-329,SelfLink:/apis/apps/v1/namespaces/deployment-329/replicasets/test-recreate-deployment-5c8c9cc69d,UID:39b1f5dc-303f-4eb1-80b0-43143a8b19f4,ResourceVersion:29202,Generation:1,CreationTimestamp:2020-02-21 21:07:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b11b464e-6c9a-4e6c-8be5-98316b765f27 0xc000edf8e7 0xc000edf8e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 21:07:51.527: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 21 21:07:51.527: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-329,SelfLink:/apis/apps/v1/namespaces/deployment-329/replicasets/test-recreate-deployment-6df85df6b9,UID:4f88433f-f839-4162-9bae-aa584fcd2029,ResourceVersion:29191,Generation:2,CreationTimestamp:2020-02-21 21:07:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b11b464e-6c9a-4e6c-8be5-98316b765f27 0xc000edf9f7 0xc000edf9f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 21:07:51.532: INFO: Pod "test-recreate-deployment-5c8c9cc69d-r8j49" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-r8j49,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-329,SelfLink:/api/v1/namespaces/deployment-329/pods/test-recreate-deployment-5c8c9cc69d-r8j49,UID:402e1200-fd53-4b21-8467-73adeb93f081,ResourceVersion:29201,Generation:0,CreationTimestamp:2020-02-21 21:07:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 39b1f5dc-303f-4eb1-80b0-43143a8b19f4 0xc0035da387 0xc0035da388}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wcfdh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wcfdh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wcfdh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035da3f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035da410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:07:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:07:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:07:51 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:07:51 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:,StartTime:2020-02-21 21:07:51 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:07:51.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-329" for this suite.
Feb 21 21:07:57.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:07:57.792: INFO: namespace deployment-329 deletion completed in 6.253160468s

• [SLOW TEST:8.707 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:07:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-043e53b9-8f95-4bab-bb34-f4093b9d1765
STEP: Creating a pod to test consume secrets
Feb 21 21:07:58.012: INFO: Waiting up to 5m0s for pod "pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604" in namespace "secrets-480" to be "success or failure"
Feb 21 21:07:58.018: INFO: Pod "pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604": Phase="Pending", Reason="", readiness=false. Elapsed: 5.468561ms
Feb 21 21:08:00.024: INFO: Pod "pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011580295s
STEP: Saw pod success
Feb 21 21:08:00.024: INFO: Pod "pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604" satisfied condition "success or failure"
Feb 21 21:08:00.029: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 21:08:00.069: INFO: Waiting for pod pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604 to disappear
Feb 21 21:08:00.073: INFO: Pod pod-secrets-9e3817bc-5880-4b18-a239-b92eada4e604 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:08:00.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-480" for this suite.
Feb 21 21:08:06.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:08:06.332: INFO: namespace secrets-480 deletion completed in 6.250934272s

• [SLOW TEST:8.539 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:08:06.333: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 21 21:08:06.511: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2116,SelfLink:/api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-label-changed,UID:16479d1f-febd-4615-878e-8d79246bf4bb,ResourceVersion:29316,Generation:0,CreationTimestamp:2020-02-21 21:08:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 21 21:08:06.511: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2116,SelfLink:/api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-label-changed,UID:16479d1f-febd-4615-878e-8d79246bf4bb,ResourceVersion:29317,Generation:0,CreationTimestamp:2020-02-21 21:08:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 21 21:08:06.511: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2116,SelfLink:/api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-label-changed,UID:16479d1f-febd-4615-878e-8d79246bf4bb,ResourceVersion:29318,Generation:0,CreationTimestamp:2020-02-21 21:08:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 21 21:08:16.558: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2116,SelfLink:/api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-label-changed,UID:16479d1f-febd-4615-878e-8d79246bf4bb,ResourceVersion:29360,Generation:0,CreationTimestamp:2020-02-21 21:08:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 21:08:16.558: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2116,SelfLink:/api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-label-changed,UID:16479d1f-febd-4615-878e-8d79246bf4bb,ResourceVersion:29361,Generation:0,CreationTimestamp:2020-02-21 21:08:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 21 21:08:16.558: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2116,SelfLink:/api/v1/namespaces/watch-2116/configmaps/e2e-watch-test-label-changed,UID:16479d1f-febd-4615-878e-8d79246bf4bb,ResourceVersion:29362,Generation:0,CreationTimestamp:2020-02-21 21:08:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:08:16.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2116" for this suite.
Feb 21 21:08:22.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:08:22.879: INFO: namespace watch-2116 deletion completed in 6.315964899s

• [SLOW TEST:16.546 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:08:22.880: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:08:23.043: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 21 21:08:28.049: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 21:08:28.049: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 21 21:08:30.055: INFO: Creating deployment "test-rollover-deployment"
Feb 21 21:08:30.074: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 21 21:08:32.086: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 21 21:08:32.096: INFO: Ensure that both replica sets have 1 created replica
Feb 21 21:08:32.107: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 21 21:08:32.119: INFO: Updating deployment test-rollover-deployment
Feb 21 21:08:32.120: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 21 21:08:34.132: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 21 21:08:34.141: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 21 21:08:34.150: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 21:08:34.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916112, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 21:08:36.166: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 21:08:36.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916114, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 21:08:38.162: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 21:08:38.162: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916114, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 21:08:40.161: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 21:08:40.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916114, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 21:08:42.160: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 21:08:42.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916114, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 21:08:44.161: INFO: all replica sets need to contain the pod-template-hash label
Feb 21 21:08:44.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916114, loc:(*time.Location)(0x7ed0a00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916110, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 21 21:08:46.161: INFO: 
Feb 21 21:08:46.162: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 21 21:08:46.174: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/deployments/test-rollover-deployment,UID:d5d851c9-9a2b-428e-865a-8424ace44303,ResourceVersion:29537,Generation:2,CreationTimestamp:2020-02-21 21:08:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-21 21:08:30 +0000 UTC 2020-02-21 21:08:30 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-21 21:08:44 +0000 UTC 2020-02-21 21:08:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 21:08:46.179: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/replicasets/test-rollover-deployment-854595fc44,UID:ae81901b-2bbe-4d3a-a641-a4621e19bb9a,ResourceVersion:29530,Generation:2,CreationTimestamp:2020-02-21 21:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d5d851c9-9a2b-428e-865a-8424ace44303 0xc003191ee7 0xc003191ee8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 21 21:08:46.179: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 21 21:08:46.179: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/replicasets/test-rollover-controller,UID:18eb91f0-70a0-4793-91f0-ea170989c9fe,ResourceVersion:29536,Generation:2,CreationTimestamp:2020-02-21 21:08:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d5d851c9-9a2b-428e-865a-8424ace44303 0xc003191e17 0xc003191e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 21:08:46.180: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/replicasets/test-rollover-deployment-9b8b997cf,UID:b50cda3c-5439-4416-b75d-2fb50973cdf6,ResourceVersion:29466,Generation:2,CreationTimestamp:2020-02-21 21:08:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment d5d851c9-9a2b-428e-865a-8424ace44303 0xc003191fb0 0xc003191fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 21:08:46.184: INFO: Pod "test-rollover-deployment-854595fc44-jg77z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-jg77z,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-5275,SelfLink:/api/v1/namespaces/deployment-5275/pods/test-rollover-deployment-854595fc44-jg77z,UID:1375b312-c9b9-4e82-8a0a-ed03f2689f50,ResourceVersion:29488,Generation:0,CreationTimestamp:2020-02-21 21:08:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.153/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 ae81901b-2bbe-4d3a-a641-a4621e19bb9a 0xc002956b87 0xc002956b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4n5fd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4n5fd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-4n5fd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002956bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002956c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:08:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:08:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:08:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:08:32 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.153,StartTime:2020-02-21 21:08:32 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-21 21:08:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://73ac8fb15f679761eaad006dc4e4da73dd7bb541bef1716b0a0d30abb6e924eb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:08:46.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5275" for this suite.
Feb 21 21:08:52.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:08:52.481: INFO: namespace deployment-5275 deletion completed in 6.291157298s

• [SLOW TEST:29.601 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:08:52.482: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 21 21:08:52.646: INFO: Waiting up to 5m0s for pod "pod-88176d08-72e5-4113-8812-a5fdbe46ab0d" in namespace "emptydir-7010" to be "success or failure"
Feb 21 21:08:52.653: INFO: Pod "pod-88176d08-72e5-4113-8812-a5fdbe46ab0d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.567397ms
Feb 21 21:08:54.658: INFO: Pod "pod-88176d08-72e5-4113-8812-a5fdbe46ab0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01201839s
STEP: Saw pod success
Feb 21 21:08:54.658: INFO: Pod "pod-88176d08-72e5-4113-8812-a5fdbe46ab0d" satisfied condition "success or failure"
Feb 21 21:08:54.663: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-88176d08-72e5-4113-8812-a5fdbe46ab0d container test-container: <nil>
STEP: delete the pod
Feb 21 21:08:54.690: INFO: Waiting for pod pod-88176d08-72e5-4113-8812-a5fdbe46ab0d to disappear
Feb 21 21:08:54.694: INFO: Pod pod-88176d08-72e5-4113-8812-a5fdbe46ab0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:08:54.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7010" for this suite.
Feb 21 21:09:00.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:09:00.991: INFO: namespace emptydir-7010 deletion completed in 6.291144153s

• [SLOW TEST:8.509 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:09:00.991: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 21 21:09:07.259: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0221 21:09:07.259859      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:09:07.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9298" for this suite.
Feb 21 21:09:13.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:09:13.535: INFO: namespace gc-9298 deletion completed in 6.270335254s

• [SLOW TEST:12.544 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:09:13.536: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:09:13.775: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 21 21:09:13.787: INFO: Number of nodes with available pods: 0
Feb 21 21:09:13.787: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 21 21:09:13.815: INFO: Number of nodes with available pods: 0
Feb 21 21:09:13.815: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:14.821: INFO: Number of nodes with available pods: 0
Feb 21 21:09:14.821: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:15.821: INFO: Number of nodes with available pods: 1
Feb 21 21:09:15.821: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 21 21:09:15.847: INFO: Number of nodes with available pods: 0
Feb 21 21:09:15.847: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 21 21:09:15.863: INFO: Number of nodes with available pods: 0
Feb 21 21:09:15.863: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:16.868: INFO: Number of nodes with available pods: 0
Feb 21 21:09:16.868: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:17.869: INFO: Number of nodes with available pods: 0
Feb 21 21:09:17.869: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:18.869: INFO: Number of nodes with available pods: 0
Feb 21 21:09:18.869: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:19.868: INFO: Number of nodes with available pods: 0
Feb 21 21:09:19.868: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:20.869: INFO: Number of nodes with available pods: 0
Feb 21 21:09:20.869: INFO: Node ip-172-20-16-60.us-east-2.compute.internal is running more than one daemon pod
Feb 21 21:09:21.868: INFO: Number of nodes with available pods: 1
Feb 21 21:09:21.868: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7429, will wait for the garbage collector to delete the pods
Feb 21 21:09:21.945: INFO: Deleting DaemonSet.extensions daemon-set took: 13.181228ms
Feb 21 21:09:22.046: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.25781ms
Feb 21 21:09:34.451: INFO: Number of nodes with available pods: 0
Feb 21 21:09:34.451: INFO: Number of running nodes: 0, number of available pods: 0
Feb 21 21:09:34.455: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7429/daemonsets","resourceVersion":"29984"},"items":null}

Feb 21 21:09:34.459: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7429/pods","resourceVersion":"29984"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:09:34.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7429" for this suite.
Feb 21 21:09:40.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:09:40.786: INFO: namespace daemonsets-7429 deletion completed in 6.280277305s

• [SLOW TEST:27.251 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:09:40.787: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Feb 21 21:09:40.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-3419'
Feb 21 21:09:41.383: INFO: stderr: ""
Feb 21 21:09:41.383: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Feb 21 21:09:42.389: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:09:42.389: INFO: Found 0 / 1
Feb 21 21:09:43.389: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:09:43.389: INFO: Found 1 / 1
Feb 21 21:09:43.389: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 21 21:09:43.394: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:09:43.394: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 21 21:09:43.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-b96mg redis-master --namespace=kubectl-3419'
Feb 21 21:09:43.485: INFO: stderr: ""
Feb 21 21:09:43.485: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Feb 21:09:42.326 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Feb 21:09:42.326 # Server started, Redis version 3.2.12\n1:M 21 Feb 21:09:42.326 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Feb 21:09:42.326 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 21 21:09:43.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-b96mg redis-master --namespace=kubectl-3419 --tail=1'
Feb 21 21:09:43.578: INFO: stderr: ""
Feb 21 21:09:43.578: INFO: stdout: "1:M 21 Feb 21:09:42.326 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 21 21:09:43.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-b96mg redis-master --namespace=kubectl-3419 --limit-bytes=1'
Feb 21 21:09:43.670: INFO: stderr: ""
Feb 21 21:09:43.670: INFO: stdout: " "
STEP: exposing timestamps
Feb 21 21:09:43.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-b96mg redis-master --namespace=kubectl-3419 --tail=1 --timestamps'
Feb 21 21:09:43.756: INFO: stderr: ""
Feb 21 21:09:43.756: INFO: stdout: "2020-02-21T21:09:42.326807693Z 1:M 21 Feb 21:09:42.326 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 21 21:09:46.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-b96mg redis-master --namespace=kubectl-3419 --since=1s'
Feb 21 21:09:46.353: INFO: stderr: ""
Feb 21 21:09:46.353: INFO: stdout: ""
Feb 21 21:09:46.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 logs redis-master-b96mg redis-master --namespace=kubectl-3419 --since=24h'
Feb 21 21:09:46.446: INFO: stderr: ""
Feb 21 21:09:46.446: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 21 Feb 21:09:42.326 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 21 Feb 21:09:42.326 # Server started, Redis version 3.2.12\n1:M 21 Feb 21:09:42.326 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 21 Feb 21:09:42.326 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Feb 21 21:09:46.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-3419'
Feb 21 21:09:46.529: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 21:09:46.529: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 21 21:09:46.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3419'
Feb 21 21:09:46.614: INFO: stderr: "No resources found.\n"
Feb 21 21:09:46.614: INFO: stdout: ""
Feb 21 21:09:46.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -l name=nginx --namespace=kubectl-3419 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 21:09:46.689: INFO: stderr: ""
Feb 21 21:09:46.689: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:09:46.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3419" for this suite.
Feb 21 21:10:08.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:10:09.015: INFO: namespace kubectl-3419 deletion completed in 22.319842237s

• [SLOW TEST:28.229 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:10:09.016: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7590
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:10:09.192: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8" in namespace "projected-7590" to be "success or failure"
Feb 21 21:10:09.201: INFO: Pod "downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.02019ms
Feb 21 21:10:11.207: INFO: Pod "downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014693948s
STEP: Saw pod success
Feb 21 21:10:11.207: INFO: Pod "downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8" satisfied condition "success or failure"
Feb 21 21:10:11.220: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8 container client-container: <nil>
STEP: delete the pod
Feb 21 21:10:11.254: INFO: Waiting for pod downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8 to disappear
Feb 21 21:10:11.260: INFO: Pod downwardapi-volume-e47956ee-120c-49a6-972f-1623a95018a8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:10:11.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7590" for this suite.
Feb 21 21:10:17.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:10:17.545: INFO: namespace projected-7590 deletion completed in 6.276575035s

• [SLOW TEST:8.529 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:10:17.546: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:10:17.693: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 21 21:10:18.737: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:10:19.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6087" for this suite.
Feb 21 21:10:25.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:10:26.080: INFO: namespace replication-controller-6087 deletion completed in 6.32181687s

• [SLOW TEST:8.534 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:10:26.080: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 21 21:10:26.241: INFO: Waiting up to 5m0s for pod "pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4" in namespace "emptydir-1108" to be "success or failure"
Feb 21 21:10:26.248: INFO: Pod "pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.814808ms
Feb 21 21:10:28.253: INFO: Pod "pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01125073s
STEP: Saw pod success
Feb 21 21:10:28.253: INFO: Pod "pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4" satisfied condition "success or failure"
Feb 21 21:10:28.257: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4 container test-container: <nil>
STEP: delete the pod
Feb 21 21:10:28.292: INFO: Waiting for pod pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4 to disappear
Feb 21 21:10:28.296: INFO: Pod pod-b19d5a7e-7ac9-43a9-b454-7e2a039733d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:10:28.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1108" for this suite.
Feb 21 21:10:34.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:10:34.581: INFO: namespace emptydir-1108 deletion completed in 6.278837775s

• [SLOW TEST:8.501 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:10:34.581: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 21 21:10:34.729: INFO: PodSpec: initContainers in spec.initContainers
Feb 21 21:11:19.506: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-41eefa2c-91f2-4a41-a67a-55e0c710bf38", GenerateName:"", Namespace:"init-container-4434", SelfLink:"/api/v1/namespaces/init-container-4434/pods/pod-init-41eefa2c-91f2-4a41-a67a-55e0c710bf38", UID:"6ec2ed0c-e7d4-4ce2-bd17-b05a454134da", ResourceVersion:"30538", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717916234, loc:(*time.Location)(0x7ed0a00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"729215863"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.101.183.174/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-bcs98", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001aca7c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bcs98", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bcs98", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-bcs98", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001b45648), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-20-17-149.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0021a1a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001b456c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001b456e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001b456e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001b456ec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916234, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916234, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916234, loc:(*time.Location)(0x7ed0a00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717916234, loc:(*time.Location)(0x7ed0a00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.20.17.149", PodIP:"100.101.183.174", StartTime:(*v1.Time)(0xc00276b480), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002bdca10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002bdca80)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e8e3ab22e43fda11e5736cca2848a1a9b48ebff3d518acc954297fc4a70bf560"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00276b520), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00276b4e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:11:19.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4434" for this suite.
Feb 21 21:11:41.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:11:41.784: INFO: namespace init-container-4434 deletion completed in 22.270533282s

• [SLOW TEST:67.203 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:11:41.785: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8698
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-d3e7451f-79d8-4d97-8970-2906c0fb599d
STEP: Creating configMap with name cm-test-opt-upd-260e4bfd-1ff3-43d3-a9ac-bc5847d5a39f
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d3e7451f-79d8-4d97-8970-2906c0fb599d
STEP: Updating configmap cm-test-opt-upd-260e4bfd-1ff3-43d3-a9ac-bc5847d5a39f
STEP: Creating configMap with name cm-test-opt-create-13da5ca2-3d63-4f6a-a4d1-00d68e34ceb9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:11:46.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8698" for this suite.
Feb 21 21:12:08.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:12:08.419: INFO: namespace configmap-8698 deletion completed in 22.297720559s

• [SLOW TEST:26.634 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:12:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:12:08.592: INFO: (0) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.877445ms)
Feb 21 21:12:08.598: INFO: (1) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.654768ms)
Feb 21 21:12:08.604: INFO: (2) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.803011ms)
Feb 21 21:12:08.609: INFO: (3) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.249217ms)
Feb 21 21:12:08.615: INFO: (4) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.712489ms)
Feb 21 21:12:08.622: INFO: (5) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.701064ms)
Feb 21 21:12:08.632: INFO: (6) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.409337ms)
Feb 21 21:12:08.639: INFO: (7) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.093734ms)
Feb 21 21:12:08.645: INFO: (8) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.938836ms)
Feb 21 21:12:08.650: INFO: (9) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.269283ms)
Feb 21 21:12:08.657: INFO: (10) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.404674ms)
Feb 21 21:12:08.663: INFO: (11) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.667816ms)
Feb 21 21:12:08.669: INFO: (12) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.402063ms)
Feb 21 21:12:08.675: INFO: (13) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.284111ms)
Feb 21 21:12:08.681: INFO: (14) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.378248ms)
Feb 21 21:12:08.687: INFO: (15) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.749181ms)
Feb 21 21:12:08.693: INFO: (16) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.374778ms)
Feb 21 21:12:08.698: INFO: (17) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.631015ms)
Feb 21 21:12:08.704: INFO: (18) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.169389ms)
Feb 21 21:12:08.711: INFO: (19) /api/v1/nodes/ip-172-20-16-60.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 7.365998ms)
[AfterEach] version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:12:08.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8555" for this suite.
Feb 21 21:12:14.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:12:15.004: INFO: namespace proxy-8555 deletion completed in 6.282538017s

• [SLOW TEST:6.585 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:12:15.005: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:12:15.189: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 21 21:12:20.197: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 21 21:12:20.197: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 21 21:12:24.240: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3368,SelfLink:/apis/apps/v1/namespaces/deployment-3368/deployments/test-cleanup-deployment,UID:b9a941d4-c872-4a43-ae77-0a3d50c7769f,ResourceVersion:30874,Generation:1,CreationTimestamp:2020-02-21 21:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-21 21:12:20 +0000 UTC 2020-02-21 21:12:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-21 21:12:22 +0000 UTC 2020-02-21 21:12:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 21 21:12:24.245: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-3368,SelfLink:/apis/apps/v1/namespaces/deployment-3368/replicasets/test-cleanup-deployment-55bbcbc84c,UID:94761cdc-ad24-4633-8255-808bbb1ac06d,ResourceVersion:30867,Generation:1,CreationTimestamp:2020-02-21 21:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b9a941d4-c872-4a43-ae77-0a3d50c7769f 0xc001464117 0xc001464118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 21 21:12:24.250: INFO: Pod "test-cleanup-deployment-55bbcbc84c-g29c7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-g29c7,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-3368,SelfLink:/api/v1/namespaces/deployment-3368/pods/test-cleanup-deployment-55bbcbc84c-g29c7,UID:44a19a39-398f-4de1-836b-b595f6803d25,ResourceVersion:30865,Generation:0,CreationTimestamp:2020-02-21 21:12:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.166/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 94761cdc-ad24-4633-8255-808bbb1ac06d 0xc001464eb7 0xc001464eb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rjljx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rjljx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rjljx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001464f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001464f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:12:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:12:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:12:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:12:20 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.166,StartTime:2020-02-21 21:12:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-21 21:12:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://e60c1424029e9638f31805ef76904f75e317b50606d53071e0ee272a63218ab5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:12:24.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3368" for this suite.
Feb 21 21:12:30.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:12:30.586: INFO: namespace deployment-3368 deletion completed in 6.328823155s

• [SLOW TEST:15.581 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:12:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7047
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-71af392d-6db8-4f4b-8447-6af8789ef96a
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-71af392d-6db8-4f4b-8447-6af8789ef96a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:12:34.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7047" for this suite.
Feb 21 21:12:56.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:12:57.262: INFO: namespace configmap-7047 deletion completed in 22.4132703s

• [SLOW TEST:26.676 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:12:57.262: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:12:57.475: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48" in namespace "projected-621" to be "success or failure"
Feb 21 21:12:57.479: INFO: Pod "downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.520202ms
Feb 21 21:12:59.486: INFO: Pod "downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01089129s
STEP: Saw pod success
Feb 21 21:12:59.486: INFO: Pod "downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48" satisfied condition "success or failure"
Feb 21 21:12:59.490: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48 container client-container: <nil>
STEP: delete the pod
Feb 21 21:12:59.523: INFO: Waiting for pod downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48 to disappear
Feb 21 21:12:59.613: INFO: Pod downwardapi-volume-cd90a7ed-62d5-456b-a9c7-8b2f7209ff48 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:12:59.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-621" for this suite.
Feb 21 21:13:05.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:13:05.879: INFO: namespace projected-621 deletion completed in 6.259884935s

• [SLOW TEST:8.617 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:13:05.879: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e488c73d-071c-4145-a34e-814ebea08257
STEP: Creating a pod to test consume configMaps
Feb 21 21:13:06.059: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8" in namespace "projected-8978" to be "success or failure"
Feb 21 21:13:06.065: INFO: Pod "pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.785094ms
Feb 21 21:13:08.071: INFO: Pod "pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011542158s
STEP: Saw pod success
Feb 21 21:13:08.071: INFO: Pod "pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8" satisfied condition "success or failure"
Feb 21 21:13:08.076: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 21:13:08.109: INFO: Waiting for pod pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8 to disappear
Feb 21 21:13:08.114: INFO: Pod pod-projected-configmaps-c567bf6b-c08d-4039-884d-5b6245ac4ce8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:13:08.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8978" for this suite.
Feb 21 21:13:14.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:13:14.469: INFO: namespace projected-8978 deletion completed in 6.347296026s

• [SLOW TEST:8.589 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:13:14.469: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:13:14.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9316" for this suite.
Feb 21 21:13:36.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:13:36.971: INFO: namespace pods-9316 deletion completed in 22.322859343s

• [SLOW TEST:22.502 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:13:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9200/configmap-test-b4b563d2-da9b-4fdc-89b4-e540a912908f
STEP: Creating a pod to test consume configMaps
Feb 21 21:13:37.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186" in namespace "configmap-9200" to be "success or failure"
Feb 21 21:13:37.171: INFO: Pod "pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186": Phase="Pending", Reason="", readiness=false. Elapsed: 5.201389ms
Feb 21 21:13:39.180: INFO: Pod "pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013903761s
STEP: Saw pod success
Feb 21 21:13:39.180: INFO: Pod "pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186" satisfied condition "success or failure"
Feb 21 21:13:39.184: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186 container env-test: <nil>
STEP: delete the pod
Feb 21 21:13:39.212: INFO: Waiting for pod pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186 to disappear
Feb 21 21:13:39.217: INFO: Pod pod-configmaps-9aea7abb-ba52-42c2-958a-aecbea9a0186 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:13:39.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9200" for this suite.
Feb 21 21:13:45.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:13:45.451: INFO: namespace configmap-9200 deletion completed in 6.229212123s

• [SLOW TEST:8.480 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:13:45.452: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 21 21:13:48.159: INFO: Successfully updated pod "labelsupdate4e13d4a2-a5ee-4b86-b719-0111a49b926f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:13:50.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7819" for this suite.
Feb 21 21:14:12.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:14:12.480: INFO: namespace projected-7819 deletion completed in 22.28247823s

• [SLOW TEST:27.028 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:14:12.480: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9237
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:14:14.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9237" for this suite.
Feb 21 21:14:54.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:14:54.981: INFO: namespace kubelet-test-9237 deletion completed in 40.30813645s

• [SLOW TEST:42.501 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:14:54.984: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8024
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8024 to expose endpoints map[]
Feb 21 21:14:55.167: INFO: successfully validated that service endpoint-test2 in namespace services-8024 exposes endpoints map[] (7.439754ms elapsed)
STEP: Creating pod pod1 in namespace services-8024
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8024 to expose endpoints map[pod1:[80]]
Feb 21 21:14:57.248: INFO: successfully validated that service endpoint-test2 in namespace services-8024 exposes endpoints map[pod1:[80]] (2.061601727s elapsed)
STEP: Creating pod pod2 in namespace services-8024
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8024 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 21 21:14:59.307: INFO: successfully validated that service endpoint-test2 in namespace services-8024 exposes endpoints map[pod1:[80] pod2:[80]] (2.046961731s elapsed)
STEP: Deleting pod pod1 in namespace services-8024
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8024 to expose endpoints map[pod2:[80]]
Feb 21 21:14:59.333: INFO: successfully validated that service endpoint-test2 in namespace services-8024 exposes endpoints map[pod2:[80]] (16.988872ms elapsed)
STEP: Deleting pod pod2 in namespace services-8024
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8024 to expose endpoints map[]
Feb 21 21:14:59.355: INFO: successfully validated that service endpoint-test2 in namespace services-8024 exposes endpoints map[] (4.200949ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:14:59.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8024" for this suite.
Feb 21 21:15:21.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:15:21.677: INFO: namespace services-8024 deletion completed in 22.278385597s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:26.693 seconds]
[sig-network] Services
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:15:21.678: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4539
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-d8f4484f-e2da-4188-836b-f477bae03c4e
STEP: Creating a pod to test consume configMaps
Feb 21 21:15:21.895: INFO: Waiting up to 5m0s for pod "pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77" in namespace "configmap-4539" to be "success or failure"
Feb 21 21:15:21.902: INFO: Pod "pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988007ms
Feb 21 21:15:24.034: INFO: Pod "pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.139179477s
STEP: Saw pod success
Feb 21 21:15:24.034: INFO: Pod "pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77" satisfied condition "success or failure"
Feb 21 21:15:24.041: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 21:15:24.100: INFO: Waiting for pod pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77 to disappear
Feb 21 21:15:24.104: INFO: Pod pod-configmaps-4878c02d-b2b3-4435-8ea1-92e96f649e77 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:15:24.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4539" for this suite.
Feb 21 21:15:30.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:15:30.361: INFO: namespace configmap-4539 deletion completed in 6.250515962s

• [SLOW TEST:8.683 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:15:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 21 21:15:30.563: INFO: Waiting up to 5m0s for pod "pod-134419de-764d-4252-879d-8872531d9ac5" in namespace "emptydir-6097" to be "success or failure"
Feb 21 21:15:30.572: INFO: Pod "pod-134419de-764d-4252-879d-8872531d9ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.264846ms
Feb 21 21:15:32.577: INFO: Pod "pod-134419de-764d-4252-879d-8872531d9ac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013524961s
STEP: Saw pod success
Feb 21 21:15:32.577: INFO: Pod "pod-134419de-764d-4252-879d-8872531d9ac5" satisfied condition "success or failure"
Feb 21 21:15:32.583: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-134419de-764d-4252-879d-8872531d9ac5 container test-container: <nil>
STEP: delete the pod
Feb 21 21:15:32.615: INFO: Waiting for pod pod-134419de-764d-4252-879d-8872531d9ac5 to disappear
Feb 21 21:15:32.623: INFO: Pod pod-134419de-764d-4252-879d-8872531d9ac5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:15:32.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6097" for this suite.
Feb 21 21:15:38.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:15:38.881: INFO: namespace emptydir-6097 deletion completed in 6.252347396s

• [SLOW TEST:8.520 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:15:38.882: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:15:39.028: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:15:41.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6120" for this suite.
Feb 21 21:16:33.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:16:33.467: INFO: namespace pods-6120 deletion completed in 52.38518922s

• [SLOW TEST:54.586 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:16:33.468: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a4884f51-fbc8-4a35-b103-3f46a46260e2
STEP: Creating a pod to test consume configMaps
Feb 21 21:16:33.638: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481" in namespace "configmap-1599" to be "success or failure"
Feb 21 21:16:33.645: INFO: Pod "pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481": Phase="Pending", Reason="", readiness=false. Elapsed: 7.108904ms
Feb 21 21:16:35.658: INFO: Pod "pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020074334s
STEP: Saw pod success
Feb 21 21:16:35.658: INFO: Pod "pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481" satisfied condition "success or failure"
Feb 21 21:16:35.664: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 21 21:16:35.698: INFO: Waiting for pod pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481 to disappear
Feb 21 21:16:35.702: INFO: Pod pod-configmaps-d0c7f5ea-630b-414d-b98c-ef57348c1481 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:16:35.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1599" for this suite.
Feb 21 21:16:41.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:16:41.948: INFO: namespace configmap-1599 deletion completed in 6.240102018s

• [SLOW TEST:8.481 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:16:41.951: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 21 21:16:42.126: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 21 21:16:47.133: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:16:47.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4705" for this suite.
Feb 21 21:16:53.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:16:53.493: INFO: namespace replication-controller-4705 deletion completed in 6.333449344s

• [SLOW TEST:11.542 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:16:53.493: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1742
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 21 21:16:56.698: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:16:56.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1742" for this suite.
Feb 21 21:17:18.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:17:19.096: INFO: namespace replicaset-1742 deletion completed in 22.362411418s

• [SLOW TEST:25.604 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:17:19.097: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 21 21:17:19.260: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:17:23.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6145" for this suite.
Feb 21 21:17:45.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:17:45.792: INFO: namespace init-container-6145 deletion completed in 22.283826126s

• [SLOW TEST:26.694 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:17:45.792: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6327, will wait for the garbage collector to delete the pods
Feb 21 21:17:48.020: INFO: Deleting Job.batch foo took: 14.71016ms
Feb 21 21:17:48.121: INFO: Terminating Job.batch foo pods took: 100.233787ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:18:27.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6327" for this suite.
Feb 21 21:18:33.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:18:33.496: INFO: namespace job-6327 deletion completed in 6.262864734s

• [SLOW TEST:47.704 seconds]
[sig-apps] Job
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:18:33.497: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 21 21:18:33.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-8690'
Feb 21 21:18:33.850: INFO: stderr: ""
Feb 21 21:18:33.850: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 21 21:18:34.856: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:18:34.856: INFO: Found 0 / 1
Feb 21 21:18:35.856: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:18:35.856: INFO: Found 1 / 1
Feb 21 21:18:35.856: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 21 21:18:35.860: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:18:35.860: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 21 21:18:35.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 patch pod redis-master-sjxml --namespace=kubectl-8690 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 21 21:18:35.944: INFO: stderr: ""
Feb 21 21:18:35.945: INFO: stdout: "pod/redis-master-sjxml patched\n"
STEP: checking annotations
Feb 21 21:18:35.949: INFO: Selector matched 1 pods for map[app:redis]
Feb 21 21:18:35.949: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:18:35.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8690" for this suite.
Feb 21 21:18:57.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:18:58.263: INFO: namespace kubectl-8690 deletion completed in 22.307808914s

• [SLOW TEST:24.766 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:18:58.263: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 21 21:18:58.459: INFO: Waiting up to 5m0s for pod "pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9" in namespace "emptydir-203" to be "success or failure"
Feb 21 21:18:58.467: INFO: Pod "pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.97049ms
Feb 21 21:19:00.472: INFO: Pod "pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013616795s
STEP: Saw pod success
Feb 21 21:19:00.472: INFO: Pod "pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9" satisfied condition "success or failure"
Feb 21 21:19:00.477: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9 container test-container: <nil>
STEP: delete the pod
Feb 21 21:19:00.506: INFO: Waiting for pod pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9 to disappear
Feb 21 21:19:00.513: INFO: Pod pod-2ad9bbf8-4322-457a-88ab-1fe0fcf0a5e9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:19:00.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-203" for this suite.
Feb 21 21:19:06.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:19:06.777: INFO: namespace emptydir-203 deletion completed in 6.258033502s

• [SLOW TEST:8.514 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:19:06.777: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 21 21:19:06.925: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:19:09.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5408" for this suite.
Feb 21 21:19:15.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:19:16.067: INFO: namespace init-container-5408 deletion completed in 6.289946192s

• [SLOW TEST:9.290 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:19:16.068: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7733
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7426
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:19:22.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-800" for this suite.
Feb 21 21:19:28.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:19:28.889: INFO: namespace namespaces-800 deletion completed in 6.333127005s
STEP: Destroying namespace "nsdeletetest-7733" for this suite.
Feb 21 21:19:28.894: INFO: Namespace nsdeletetest-7733 was already deleted
STEP: Destroying namespace "nsdeletetest-7426" for this suite.
Feb 21 21:19:34.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:19:35.161: INFO: namespace nsdeletetest-7426 deletion completed in 6.26747462s

• [SLOW TEST:19.094 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:19:35.162: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 21 21:19:35.318: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:19:47.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5554" for this suite.
Feb 21 21:19:53.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:19:53.369: INFO: namespace pods-5554 deletion completed in 6.244147959s

• [SLOW TEST:18.207 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:19:53.370: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9963
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-229
STEP: Creating secret with name secret-test-9f882519-43dc-499d-be81-0453a41840a6
STEP: Creating a pod to test consume secrets
Feb 21 21:19:53.694: INFO: Waiting up to 5m0s for pod "pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660" in namespace "secrets-9963" to be "success or failure"
Feb 21 21:19:53.701: INFO: Pod "pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660": Phase="Pending", Reason="", readiness=false. Elapsed: 7.203685ms
Feb 21 21:19:55.707: INFO: Pod "pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012702201s
STEP: Saw pod success
Feb 21 21:19:55.707: INFO: Pod "pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660" satisfied condition "success or failure"
Feb 21 21:19:55.711: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 21:19:55.742: INFO: Waiting for pod pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660 to disappear
Feb 21 21:19:55.746: INFO: Pod pod-secrets-3e802790-f60b-4f32-9085-c62d59d0c660 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:19:55.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9963" for this suite.
Feb 21 21:20:01.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:02.123: INFO: namespace secrets-9963 deletion completed in 6.370987792s
STEP: Destroying namespace "secret-namespace-229" for this suite.
Feb 21 21:20:08.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:08.406: INFO: namespace secret-namespace-229 deletion completed in 6.283401002s

• [SLOW TEST:15.037 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:08.408: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 21 21:20:08.661: INFO: Waiting up to 5m0s for pod "downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f" in namespace "downward-api-1027" to be "success or failure"
Feb 21 21:20:08.668: INFO: Pod "downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.289355ms
Feb 21 21:20:10.674: INFO: Pod "downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012834195s
STEP: Saw pod success
Feb 21 21:20:10.674: INFO: Pod "downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f" satisfied condition "success or failure"
Feb 21 21:20:10.678: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f container dapi-container: <nil>
STEP: delete the pod
Feb 21 21:20:10.714: INFO: Waiting for pod downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f to disappear
Feb 21 21:20:10.718: INFO: Pod downward-api-3d6bf5a3-1025-4967-bf4b-2150d8bbde4f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:20:10.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1027" for this suite.
Feb 21 21:20:16.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:17.091: INFO: namespace downward-api-1027 deletion completed in 6.366594843s

• [SLOW TEST:8.683 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:17.091: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-50f59d9c-f0c1-497d-8511-75c66ccf80d1
STEP: Creating a pod to test consume secrets
Feb 21 21:20:17.318: INFO: Waiting up to 5m0s for pod "pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a" in namespace "secrets-1537" to be "success or failure"
Feb 21 21:20:17.327: INFO: Pod "pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.925709ms
Feb 21 21:20:19.333: INFO: Pod "pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014787702s
STEP: Saw pod success
Feb 21 21:20:19.333: INFO: Pod "pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a" satisfied condition "success or failure"
Feb 21 21:20:19.337: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 21:20:19.367: INFO: Waiting for pod pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a to disappear
Feb 21 21:20:19.373: INFO: Pod pod-secrets-c9f2b8d5-3774-4bd9-8409-781be06cd50a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:20:19.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1537" for this suite.
Feb 21 21:20:25.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:25.683: INFO: namespace secrets-1537 deletion completed in 6.30370891s

• [SLOW TEST:8.592 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:25.684: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-099069dc-9eff-4999-9abc-fb77ec5fc07e
STEP: Creating a pod to test consume secrets
Feb 21 21:20:25.858: INFO: Waiting up to 5m0s for pod "pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a" in namespace "secrets-191" to be "success or failure"
Feb 21 21:20:25.864: INFO: Pod "pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.116255ms
Feb 21 21:20:27.869: INFO: Pod "pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011522756s
STEP: Saw pod success
Feb 21 21:20:27.869: INFO: Pod "pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a" satisfied condition "success or failure"
Feb 21 21:20:27.874: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 21:20:27.904: INFO: Waiting for pod pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a to disappear
Feb 21 21:20:27.908: INFO: Pod pod-secrets-9c145c37-e619-4951-a1d3-41cfaf12322a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:20:27.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-191" for this suite.
Feb 21 21:20:33.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:34.268: INFO: namespace secrets-191 deletion completed in 6.354257384s

• [SLOW TEST:8.584 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:34.268: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:20:34.435: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3" in namespace "projected-8701" to be "success or failure"
Feb 21 21:20:34.442: INFO: Pod "downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.675378ms
Feb 21 21:20:36.452: INFO: Pod "downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017273112s
STEP: Saw pod success
Feb 21 21:20:36.452: INFO: Pod "downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3" satisfied condition "success or failure"
Feb 21 21:20:36.458: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3 container client-container: <nil>
STEP: delete the pod
Feb 21 21:20:36.493: INFO: Waiting for pod downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3 to disappear
Feb 21 21:20:36.499: INFO: Pod downwardapi-volume-1bbf6bb8-69ce-45a9-9908-4d7b5d74bae3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:20:36.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8701" for this suite.
Feb 21 21:20:42.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:42.778: INFO: namespace projected-8701 deletion completed in 6.272137829s

• [SLOW TEST:8.510 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:42.779: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9980
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 21 21:20:42.960: INFO: Waiting up to 5m0s for pod "pod-067540c7-5e8f-434f-b1e1-c1233996f252" in namespace "emptydir-9980" to be "success or failure"
Feb 21 21:20:42.971: INFO: Pod "pod-067540c7-5e8f-434f-b1e1-c1233996f252": Phase="Pending", Reason="", readiness=false. Elapsed: 10.602594ms
Feb 21 21:20:44.980: INFO: Pod "pod-067540c7-5e8f-434f-b1e1-c1233996f252": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019814439s
STEP: Saw pod success
Feb 21 21:20:44.980: INFO: Pod "pod-067540c7-5e8f-434f-b1e1-c1233996f252" satisfied condition "success or failure"
Feb 21 21:20:44.984: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-067540c7-5e8f-434f-b1e1-c1233996f252 container test-container: <nil>
STEP: delete the pod
Feb 21 21:20:45.018: INFO: Waiting for pod pod-067540c7-5e8f-434f-b1e1-c1233996f252 to disappear
Feb 21 21:20:45.024: INFO: Pod pod-067540c7-5e8f-434f-b1e1-c1233996f252 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:20:45.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9980" for this suite.
Feb 21 21:20:51.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:51.286: INFO: namespace emptydir-9980 deletion completed in 6.25571542s

• [SLOW TEST:8.507 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:51.286: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 21 21:20:51.465: INFO: Waiting up to 5m0s for pod "downward-api-80dc0822-0190-428c-914d-e521d6781cc6" in namespace "downward-api-1234" to be "success or failure"
Feb 21 21:20:51.469: INFO: Pod "downward-api-80dc0822-0190-428c-914d-e521d6781cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.751799ms
Feb 21 21:20:53.475: INFO: Pod "downward-api-80dc0822-0190-428c-914d-e521d6781cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010226184s
STEP: Saw pod success
Feb 21 21:20:53.475: INFO: Pod "downward-api-80dc0822-0190-428c-914d-e521d6781cc6" satisfied condition "success or failure"
Feb 21 21:20:53.480: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downward-api-80dc0822-0190-428c-914d-e521d6781cc6 container dapi-container: <nil>
STEP: delete the pod
Feb 21 21:20:53.511: INFO: Waiting for pod downward-api-80dc0822-0190-428c-914d-e521d6781cc6 to disappear
Feb 21 21:20:53.515: INFO: Pod downward-api-80dc0822-0190-428c-914d-e521d6781cc6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:20:53.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1234" for this suite.
Feb 21 21:20:59.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:20:59.852: INFO: namespace downward-api-1234 deletion completed in 6.330546599s

• [SLOW TEST:8.566 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:20:59.852: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:20:59.998: INFO: Creating deployment "nginx-deployment"
Feb 21 21:21:00.010: INFO: Waiting for observed generation 1
Feb 21 21:21:02.024: INFO: Waiting for all required pods to come up
Feb 21 21:21:02.030: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 21 21:21:06.046: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 21 21:21:06.057: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 21 21:21:06.070: INFO: Updating deployment nginx-deployment
Feb 21 21:21:06.070: INFO: Waiting for observed generation 2
Feb 21 21:21:08.080: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 21 21:21:08.085: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 21 21:21:08.090: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 21 21:21:08.103: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 21 21:21:08.103: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 21 21:21:08.108: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 21 21:21:08.120: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 21 21:21:08.120: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 21 21:21:08.135: INFO: Updating deployment nginx-deployment
Feb 21 21:21:08.135: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 21 21:21:08.150: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 21 21:21:10.164: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 21 21:21:10.175: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3038,SelfLink:/apis/apps/v1/namespaces/deployment-3038/deployments/nginx-deployment,UID:b1b88c61-18b8-4700-8d27-25f84b52f942,ResourceVersion:34008,Generation:3,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[{Available False 2020-02-21 21:21:08 +0000 UTC 2020-02-21 21:21:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-02-21 21:21:10 +0000 UTC 2020-02-21 21:21:00 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:10,CollisionCount:nil,},}

Feb 21 21:21:10.180: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-3038,SelfLink:/apis/apps/v1/namespaces/deployment-3038/replicasets/nginx-deployment-55fb7cb77f,UID:941061d6-1504-4cd9-8b79-0aeab8a05654,ResourceVersion:33953,Generation:3,CreationTimestamp:2020-02-21 21:21:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b1b88c61-18b8-4700-8d27-25f84b52f942 0xc0039a5907 0xc0039a5908}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 21 21:21:10.180: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 21 21:21:10.181: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-3038,SelfLink:/apis/apps/v1/namespaces/deployment-3038/replicasets/nginx-deployment-7b8c6f4498,UID:371883a9-1975-44f1-b9fb-e6256781d57d,ResourceVersion:34007,Generation:3,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b1b88c61-18b8-4700-8d27-25f84b52f942 0xc0039a59d7 0xc0039a59d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[],},}
Feb 21 21:21:10.192: INFO: Pod "nginx-deployment-55fb7cb77f-4qh6s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4qh6s,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-4qh6s,UID:1350d308-47fb-42c4-8cdb-4d8faf4705c1,ResourceVersion:34005,Generation:0,CreationTimestamp:2020-02-21 21:21:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.147/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fc367 0xc0037fc368}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fc3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fc3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.147,StartTime:2020-02-21 21:21:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.192: INFO: Pod "nginx-deployment-55fb7cb77f-5g8tq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5g8tq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-5g8tq,UID:0a7435f3-32e0-46e3-8d59-2ca2311eafdb,ResourceVersion:33998,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.119.255.95/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fc4e0 0xc0037fc4e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fc550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fc570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.192: INFO: Pod "nginx-deployment-55fb7cb77f-7glsq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7glsq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-7glsq,UID:db3b2315-c621-479c-ade9-36a3ff6001b5,ResourceVersion:33958,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fc640 0xc0037fc641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fc6b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fc6d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.192: INFO: Pod "nginx-deployment-55fb7cb77f-9qqsx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9qqsx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-9qqsx,UID:a12e9ee0-345d-4bba-be29-bdb94443c095,ResourceVersion:33983,Generation:0,CreationTimestamp:2020-02-21 21:21:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.151/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fc7a0 0xc0037fc7a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fc810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fc830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.151,StartTime:2020-02-21 21:21:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-9snq7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9snq7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-9snq7,UID:282a8dec-f369-48a2-a666-f96662f4b3fe,ResourceVersion:33995,Generation:0,CreationTimestamp:2020-02-21 21:21:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.146/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fc920 0xc0037fc921}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fc990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fc9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.146,StartTime:2020-02-21 21:21:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-d592m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-d592m,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-d592m,UID:3d891f16-451c-4342-8421-d17c1d10ae61,ResourceVersion:33898,Generation:0,CreationTimestamp:2020-02-21 21:21:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.103.74.176/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fcaa0 0xc0037fcaa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fcb10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fcb30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:100.103.74.176,StartTime:2020-02-21 21:21:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-kshtz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kshtz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-kshtz,UID:0051a9bc-8eff-4650-8d36-2900d89233bb,ResourceVersion:33924,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fcc20 0xc0037fcc21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fcc90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fccb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-ntllv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ntllv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-ntllv,UID:897d78e1-7410-4867-82f5-6c6492937967,ResourceVersion:33960,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fcd80 0xc0037fcd81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fcdf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fce10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-qbhpx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qbhpx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-qbhpx,UID:c9ca5fd9-76d9-4b19-8be7-47f81d4cb77c,ResourceVersion:33961,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fcee0 0xc0037fcee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fcf50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fcf70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-qrb4k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qrb4k,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-qrb4k,UID:6971059a-9c72-4425-b018-6ccefd75ff3f,ResourceVersion:33970,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.148/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fd040 0xc0037fd041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.193: INFO: Pod "nginx-deployment-55fb7cb77f-rjw4x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rjw4x,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-rjw4x,UID:da0cbed4-f0c2-4980-b9b6-b4f4521cb543,ResourceVersion:33989,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.114.178.219/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fd1a0 0xc0037fd1a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-148.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.148,PodIP:100.114.178.219,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-55fb7cb77f-vt4gl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vt4gl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-vt4gl,UID:63f81f8b-1825-4a1a-9b59-f533740ddabe,ResourceVersion:33981,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fd320 0xc0037fd321}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-55fb7cb77f-xgn5l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xgn5l,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-55fb7cb77f-xgn5l,UID:b8a576a7-5b9c-4fc0-b2a8-bac7eec8d126,ResourceVersion:33897,Generation:0,CreationTimestamp:2020-02-21 21:21:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.119.255.93/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 941061d6-1504-4cd9-8b79-0aeab8a05654 0xc0037fd480 0xc0037fd481}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd4f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:06 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:100.119.255.93,StartTime:2020-02-21 21:21:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-7b8c6f4498-5b226" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5b226,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-5b226,UID:9a25cd2d-6e37-460e-a605-445549636d5e,ResourceVersion:34012,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.119.255.97/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fd600 0xc0037fd601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-7b8c6f4498-78xlq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-78xlq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-78xlq,UID:5cef2155-bb18-47b0-b005-f827fa8e83da,ResourceVersion:33817,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.141/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fd747 0xc0037fd748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd7b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.141,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b407627b02124508b492c4aca0cb509a0b04a6a104802941994344960b054a2d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-7b8c6f4498-9jv6t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9jv6t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-9jv6t,UID:dfd0897e-d0d4-4045-86c6-6bbb373a68cd,ResourceVersion:33825,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.145/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fd8a7 0xc0037fd8a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fd910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fd930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.145,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d09b685ad8c489472234c650a419f2beae61a938ddfabdd98d00bd2129c10596}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-7b8c6f4498-bwwgq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bwwgq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-bwwgq,UID:8083fd66-5c44-40f7-839b-b7ce7512d59e,ResourceVersion:33807,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.143/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fda07 0xc0037fda08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fda70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fda90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.143,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://597ffb88044a945c2e4ca550def3ae3fede767533903201296ca029cdbb07ca2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.194: INFO: Pod "nginx-deployment-7b8c6f4498-c5ntm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c5ntm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-c5ntm,UID:8da02384-c09b-4ed3-9c87-917d5a26038e,ResourceVersion:33993,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fdb67 0xc0037fdb68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fdbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fdbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-d55mp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d55mp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-d55mp,UID:ee57cd24-e503-4aad-997e-49bb37a1920a,ResourceVersion:33759,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.119.255.92/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fdcb7 0xc0037fdcb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fdd20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fdd40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:100.119.255.92,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:01 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5b77689512a70989a6dc79643cb1cfb098fd0129b3c2af94d6a360316095c573}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-jcdbn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jcdbn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-jcdbn,UID:a9bc0f74-4be9-4b4a-89c7-687a449b9935,ResourceVersion:33978,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fde17 0xc0037fde18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fde80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fdea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-k6frd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-k6frd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-k6frd,UID:f2a7e325-87ab-4fe2-8ddf-ee15ab230b68,ResourceVersion:33984,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.103.74.177/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0037fdf67 0xc0037fdf68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fdfd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fdff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-ks9k4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ks9k4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-ks9k4,UID:ea6c6b28-2b42-4990-893c-156219342b25,ResourceVersion:34002,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.119.255.96/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0039060b7 0xc0039060b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-l6z26" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l6z26,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-l6z26,UID:ce58c814-f1c2-44bd-8247-b32185c716ee,ResourceVersion:33986,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.114.178.220/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906207 0xc003906208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-148.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.148,PodIP:100.114.178.220,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9b49e71e942fbb51ca56c42ced5c6e655f69a4387f0e2be3d106b5c9584986c2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-lskph" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lskph,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-lskph,UID:b0853f66-726c-46b0-a8e7-f2a2420d5f50,ResourceVersion:34006,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.119.255.94/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906367 0xc003906368}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0039063d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0039063f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:100.119.255.94,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://badbcd06357daf0148878adb484c8eefb9c5c8496d813fdf5b822e05e20a9969}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.195: INFO: Pod "nginx-deployment-7b8c6f4498-m8fm5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m8fm5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-m8fm5,UID:600badc7-c918-4872-996c-bfe9574c94d2,ResourceVersion:33803,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.138/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0039064c7 0xc0039064c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.138,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://87eff14e064e6b6a6cea062132938d26649c0856605453dd9839fd80a6f94df7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-mjs65" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mjs65,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-mjs65,UID:fb3ddf42-dc0b-4e9a-9df4-fcd7d93d7350,ResourceVersion:33784,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.144/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906627 0xc003906628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0039066b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.144,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a48992d8584da941f83ad3564d84a53f4a0d04dfe9766e3b82bb92961e47275e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-nb2h2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nb2h2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-nb2h2,UID:2061df23-cb56-435f-9b55-394dee9939ec,ResourceVersion:33959,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906787 0xc003906788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-18-137.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0039067f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.18.137,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-nq2sw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nq2sw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-nq2sw,UID:bc2d0127-9936-4b90-a4b3-14cd707b40ef,ResourceVersion:33966,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc0039068d7 0xc0039068d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-qvl9q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qvl9q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-qvl9q,UID:d44d1380-3faa-4f07-8caf-152300d00520,ResourceVersion:33949,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906a27 0xc003906a28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-vqm6f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vqm6f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-vqm6f,UID:8b60830e-82b0-40fb-a810-d7411e7b0d67,ResourceVersion:33787,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.137/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906b77 0xc003906b78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.137,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:01 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c29a60f3726e9c9cc6f932cf07e95b9bdde9aa691b03ee9140ec53f57e9a7c73}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-x9q8f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x9q8f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-x9q8f,UID:2691627e-b70e-4091-86c4-170aee4e5476,ResourceVersion:33797,Generation:0,CreationTimestamp:2020-02-21 21:21:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.101.183.142/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906cd7 0xc003906cd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-17-149.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:00 +0000 UTC  }],Message:,Reason:,HostIP:172.20.17.149,PodIP:100.101.183.142,StartTime:2020-02-21 21:21:00 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-21 21:21:02 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://24a87c8888539b81275910a80946c43242ec413494a7cc3487d54ed451ac7e43}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-xwmfq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xwmfq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-xwmfq,UID:9af39915-4b70-47d2-967f-52802b171d92,ResourceVersion:33938,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906e37 0xc003906e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003906ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 21 21:21:10.196: INFO: Pod "nginx-deployment-7b8c6f4498-z8j8x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-z8j8x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-3038,SelfLink:/api/v1/namespaces/deployment-3038/pods/nginx-deployment-7b8c6f4498-z8j8x,UID:c22c11c7-c42f-419e-a114-59834ba30832,ResourceVersion:34004,Generation:0,CreationTimestamp:2020-02-21 21:21:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 371883a9-1975-44f1-b9fb-e6256781d57d 0xc003906f87 0xc003906f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mv4jw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mv4jw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mv4jw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-20-16-60.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003906ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003907010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-21 21:21:08 +0000 UTC  }],Message:,Reason:,HostIP:172.20.16.60,PodIP:,StartTime:2020-02-21 21:21:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:21:10.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3038" for this suite.
Feb 21 21:21:18.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:21:18.623: INFO: namespace deployment-3038 deletion completed in 8.413096544s

• [SLOW TEST:18.771 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:21:18.623: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-zpdw
STEP: Creating a pod to test atomic-volume-subpath
Feb 21 21:21:18.875: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zpdw" in namespace "subpath-905" to be "success or failure"
Feb 21 21:21:18.881: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Pending", Reason="", readiness=false. Elapsed: 5.446208ms
Feb 21 21:21:20.886: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01023895s
Feb 21 21:21:22.891: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016012544s
Feb 21 21:21:24.897: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 6.021929209s
Feb 21 21:21:26.902: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 8.027083601s
Feb 21 21:21:28.908: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 10.032888256s
Feb 21 21:21:30.914: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 12.038563785s
Feb 21 21:21:32.920: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 14.044410214s
Feb 21 21:21:34.925: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 16.050069442s
Feb 21 21:21:36.931: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 18.055581528s
Feb 21 21:21:38.936: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 20.060933301s
Feb 21 21:21:40.942: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Running", Reason="", readiness=true. Elapsed: 22.066462277s
Feb 21 21:21:42.948: INFO: Pod "pod-subpath-test-configmap-zpdw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.072879764s
STEP: Saw pod success
Feb 21 21:21:42.948: INFO: Pod "pod-subpath-test-configmap-zpdw" satisfied condition "success or failure"
Feb 21 21:21:42.953: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-subpath-test-configmap-zpdw container test-container-subpath-configmap-zpdw: <nil>
STEP: delete the pod
Feb 21 21:21:42.990: INFO: Waiting for pod pod-subpath-test-configmap-zpdw to disappear
Feb 21 21:21:43.001: INFO: Pod pod-subpath-test-configmap-zpdw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zpdw
Feb 21 21:21:43.001: INFO: Deleting pod "pod-subpath-test-configmap-zpdw" in namespace "subpath-905"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:21:43.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-905" for this suite.
Feb 21 21:21:49.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:21:49.401: INFO: namespace subpath-905 deletion completed in 6.386555179s

• [SLOW TEST:30.778 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:21:49.401: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 21 21:21:52.107: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bacbc6f8-fd21-42c8-a7b1-589c5db22f4c"
Feb 21 21:21:52.107: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bacbc6f8-fd21-42c8-a7b1-589c5db22f4c" in namespace "pods-1504" to be "terminated due to deadline exceeded"
Feb 21 21:21:52.111: INFO: Pod "pod-update-activedeadlineseconds-bacbc6f8-fd21-42c8-a7b1-589c5db22f4c": Phase="Running", Reason="", readiness=true. Elapsed: 4.373167ms
Feb 21 21:21:54.117: INFO: Pod "pod-update-activedeadlineseconds-bacbc6f8-fd21-42c8-a7b1-589c5db22f4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010360527s
Feb 21 21:21:56.122: INFO: Pod "pod-update-activedeadlineseconds-bacbc6f8-fd21-42c8-a7b1-589c5db22f4c": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015795567s
Feb 21 21:21:56.122: INFO: Pod "pod-update-activedeadlineseconds-bacbc6f8-fd21-42c8-a7b1-589c5db22f4c" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:21:56.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1504" for this suite.
Feb 21 21:22:02.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:22:02.390: INFO: namespace pods-1504 deletion completed in 6.262033134s

• [SLOW TEST:12.989 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:22:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:22:02.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47" in namespace "projected-7490" to be "success or failure"
Feb 21 21:22:02.567: INFO: Pod "downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.584097ms
Feb 21 21:22:04.574: INFO: Pod "downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013967892s
STEP: Saw pod success
Feb 21 21:22:04.574: INFO: Pod "downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47" satisfied condition "success or failure"
Feb 21 21:22:04.583: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47 container client-container: <nil>
STEP: delete the pod
Feb 21 21:22:04.615: INFO: Waiting for pod downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47 to disappear
Feb 21 21:22:04.619: INFO: Pod downwardapi-volume-045c51a1-cff3-4150-a017-1e2111ffaa47 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:22:04.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7490" for this suite.
Feb 21 21:22:10.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:22:10.898: INFO: namespace projected-7490 deletion completed in 6.271558166s

• [SLOW TEST:8.508 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:22:10.899: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 21 21:22:13.612: INFO: Successfully updated pod "labelsupdatea7fcd5db-cce9-42fd-807a-15dc437507b3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:22:15.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5425" for this suite.
Feb 21 21:22:37.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:22:37.982: INFO: namespace downward-api-5425 deletion completed in 22.33799464s

• [SLOW TEST:27.084 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:22:37.982: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4105
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 21 21:22:38.138: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 21 21:23:02.505: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.114.178.221:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4105 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 21:23:02.505: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 21:23:02.628: INFO: Found all expected endpoints: [netserver-0]
Feb 21 21:23:02.634: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.119.255.102:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4105 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 21:23:02.634: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 21:23:02.753: INFO: Found all expected endpoints: [netserver-1]
Feb 21 21:23:02.758: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.183.161:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4105 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 21:23:02.758: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 21:23:02.848: INFO: Found all expected endpoints: [netserver-2]
Feb 21 21:23:02.854: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.103.74.186:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4105 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 21 21:23:02.854: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
Feb 21 21:23:02.960: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:23:02.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4105" for this suite.
Feb 21 21:23:26.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:23:27.285: INFO: namespace pod-network-test-4105 deletion completed in 24.318668957s

• [SLOW TEST:49.303 seconds]
[sig-network] Networking
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:23:27.286: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 21 21:23:29.509: INFO: Waiting up to 5m0s for pod "client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03" in namespace "pods-4030" to be "success or failure"
Feb 21 21:23:29.518: INFO: Pod "client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03": Phase="Pending", Reason="", readiness=false. Elapsed: 9.01458ms
Feb 21 21:23:31.524: INFO: Pod "client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014451767s
STEP: Saw pod success
Feb 21 21:23:31.524: INFO: Pod "client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03" satisfied condition "success or failure"
Feb 21 21:23:31.529: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03 container env3cont: <nil>
STEP: delete the pod
Feb 21 21:23:31.562: INFO: Waiting for pod client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03 to disappear
Feb 21 21:23:31.567: INFO: Pod client-envvars-b7ec3342-7a14-4440-8afa-a1afef2e6a03 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:23:31.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4030" for this suite.
Feb 21 21:24:17.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:24:17.969: INFO: namespace pods-4030 deletion completed in 46.395290064s

• [SLOW TEST:50.683 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:24:17.970: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-3528
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3528 to expose endpoints map[]
Feb 21 21:24:18.146: INFO: successfully validated that service multi-endpoint-test in namespace services-3528 exposes endpoints map[] (6.920158ms elapsed)
STEP: Creating pod pod1 in namespace services-3528
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3528 to expose endpoints map[pod1:[100]]
Feb 21 21:24:20.215: INFO: successfully validated that service multi-endpoint-test in namespace services-3528 exposes endpoints map[pod1:[100]] (2.036051283s elapsed)
STEP: Creating pod pod2 in namespace services-3528
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3528 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 21 21:24:22.268: INFO: successfully validated that service multi-endpoint-test in namespace services-3528 exposes endpoints map[pod1:[100] pod2:[101]] (2.042242156s elapsed)
STEP: Deleting pod pod1 in namespace services-3528
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3528 to expose endpoints map[pod2:[101]]
Feb 21 21:24:22.289: INFO: successfully validated that service multi-endpoint-test in namespace services-3528 exposes endpoints map[pod2:[101]] (9.149389ms elapsed)
STEP: Deleting pod pod2 in namespace services-3528
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3528 to expose endpoints map[]
Feb 21 21:24:22.307: INFO: successfully validated that service multi-endpoint-test in namespace services-3528 exposes endpoints map[] (5.891459ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:24:22.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3528" for this suite.
Feb 21 21:24:44.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:24:44.622: INFO: namespace services-3528 deletion completed in 22.269946197s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:26.652 seconds]
[sig-network] Services
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:24:44.622: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8303.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8303.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8303.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8303.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8303.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8303.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 21 21:24:48.975: INFO: DNS probes using dns-8303/dns-test-5efae481-02a7-4599-83b7-f4bfd94df4b8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:24:48.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8303" for this suite.
Feb 21 21:24:55.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:24:55.286: INFO: namespace dns-8303 deletion completed in 6.28255414s

• [SLOW TEST:10.664 seconds]
[sig-network] DNS
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:24:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 21 21:24:55.448: INFO: Waiting up to 5m0s for pod "pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12" in namespace "emptydir-6127" to be "success or failure"
Feb 21 21:24:55.454: INFO: Pod "pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12": Phase="Pending", Reason="", readiness=false. Elapsed: 5.434181ms
Feb 21 21:24:57.459: INFO: Pod "pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010800622s
STEP: Saw pod success
Feb 21 21:24:57.459: INFO: Pod "pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12" satisfied condition "success or failure"
Feb 21 21:24:57.464: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12 container test-container: <nil>
STEP: delete the pod
Feb 21 21:24:57.493: INFO: Waiting for pod pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12 to disappear
Feb 21 21:24:57.497: INFO: Pod pod-6e7b047f-6a75-411d-90c0-dd2149ad7c12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:24:57.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6127" for this suite.
Feb 21 21:25:03.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:25:03.764: INFO: namespace emptydir-6127 deletion completed in 6.261047181s

• [SLOW TEST:8.477 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:25:03.764: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8925/configmap-test-cf9ac44e-a4a2-4c22-b57f-9f0fdff49ee5
STEP: Creating a pod to test consume configMaps
Feb 21 21:25:03.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546" in namespace "configmap-8925" to be "success or failure"
Feb 21 21:25:03.966: INFO: Pod "pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546": Phase="Pending", Reason="", readiness=false. Elapsed: 7.18423ms
Feb 21 21:25:05.971: INFO: Pod "pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012395087s
STEP: Saw pod success
Feb 21 21:25:05.971: INFO: Pod "pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546" satisfied condition "success or failure"
Feb 21 21:25:05.975: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546 container env-test: <nil>
STEP: delete the pod
Feb 21 21:25:06.009: INFO: Waiting for pod pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546 to disappear
Feb 21 21:25:06.013: INFO: Pod pod-configmaps-2cfe33f6-86bb-4cd7-aa85-d1309904f546 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:25:06.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8925" for this suite.
Feb 21 21:25:12.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:25:12.294: INFO: namespace configmap-8925 deletion completed in 6.274871385s

• [SLOW TEST:8.530 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:25:12.295: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5928
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4502
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:25:36.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-421" for this suite.
Feb 21 21:25:42.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:25:43.076: INFO: namespace namespaces-421 deletion completed in 6.263327419s
STEP: Destroying namespace "nsdeletetest-5928" for this suite.
Feb 21 21:25:43.080: INFO: Namespace nsdeletetest-5928 was already deleted
STEP: Destroying namespace "nsdeletetest-4502" for this suite.
Feb 21 21:25:49.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:25:49.323: INFO: namespace nsdeletetest-4502 deletion completed in 6.242082976s

• [SLOW TEST:37.028 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:25:49.323: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Feb 21 21:25:49.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 cluster-info'
Feb 21 21:25:49.768: INFO: stderr: ""
Feb 21 21:25:49.768: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:25:49.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-981" for this suite.
Feb 21 21:25:55.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:25:56.011: INFO: namespace kubectl-981 deletion completed in 6.237254459s

• [SLOW TEST:6.688 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:25:56.012: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2612
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-356b7de2-6710-4b43-9b7e-81111d043393
STEP: Creating secret with name s-test-opt-upd-bae0bf27-500d-47cd-8427-589f59eb90ab
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-356b7de2-6710-4b43-9b7e-81111d043393
STEP: Updating secret s-test-opt-upd-bae0bf27-500d-47cd-8427-589f59eb90ab
STEP: Creating secret with name s-test-opt-create-b1a1ef5a-8e21-49d6-8992-3e183f5fb621
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:26:00.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2612" for this suite.
Feb 21 21:26:22.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:26:22.660: INFO: namespace secrets-2612 deletion completed in 22.262345781s

• [SLOW TEST:26.648 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:26:22.661: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6377
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 21:26:24.854: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:26:24.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6377" for this suite.
Feb 21 21:26:30.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:26:31.137: INFO: namespace container-runtime-6377 deletion completed in 6.25428561s

• [SLOW TEST:8.477 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:26:31.138: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 21 21:26:31.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7052,SelfLink:/api/v1/namespaces/watch-7052/configmaps/e2e-watch-test-resource-version,UID:645ca192-2ad4-475e-a62f-e53bea73f413,ResourceVersion:35981,Generation:0,CreationTimestamp:2020-02-21 21:26:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 21 21:26:31.329: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7052,SelfLink:/api/v1/namespaces/watch-7052/configmaps/e2e-watch-test-resource-version,UID:645ca192-2ad4-475e-a62f-e53bea73f413,ResourceVersion:35982,Generation:0,CreationTimestamp:2020-02-21 21:26:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:26:31.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7052" for this suite.
Feb 21 21:26:37.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:26:37.585: INFO: namespace watch-7052 deletion completed in 6.25016027s

• [SLOW TEST:6.448 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:26:37.587: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 21 21:26:39.777: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:26:39.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5401" for this suite.
Feb 21 21:26:45.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:26:46.194: INFO: namespace container-runtime-5401 deletion completed in 6.388287724s

• [SLOW TEST:8.607 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:26:46.195: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1506
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 21 21:26:46.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 create -f - --namespace=kubectl-1506'
Feb 21 21:26:46.601: INFO: stderr: ""
Feb 21 21:26:46.602: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 21:26:46.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1506'
Feb 21 21:26:46.702: INFO: stderr: ""
Feb 21 21:26:46.702: INFO: stdout: "update-demo-nautilus-77x55 update-demo-nautilus-8xbnv "
Feb 21 21:26:46.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:46.789: INFO: stderr: ""
Feb 21 21:26:46.789: INFO: stdout: ""
Feb 21 21:26:46.789: INFO: update-demo-nautilus-77x55 is created but not running
Feb 21 21:26:51.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1506'
Feb 21 21:26:51.867: INFO: stderr: ""
Feb 21 21:26:51.867: INFO: stdout: "update-demo-nautilus-77x55 update-demo-nautilus-8xbnv "
Feb 21 21:26:51.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:51.953: INFO: stderr: ""
Feb 21 21:26:51.953: INFO: stdout: "true"
Feb 21 21:26:51.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:52.070: INFO: stderr: ""
Feb 21 21:26:52.070: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 21:26:52.070: INFO: validating pod update-demo-nautilus-77x55
Feb 21 21:26:52.078: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 21:26:52.078: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 21:26:52.078: INFO: update-demo-nautilus-77x55 is verified up and running
Feb 21 21:26:52.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-8xbnv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:52.161: INFO: stderr: ""
Feb 21 21:26:52.161: INFO: stdout: "true"
Feb 21 21:26:52.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-8xbnv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:52.240: INFO: stderr: ""
Feb 21 21:26:52.240: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 21:26:52.240: INFO: validating pod update-demo-nautilus-8xbnv
Feb 21 21:26:52.248: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 21:26:52.248: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 21:26:52.248: INFO: update-demo-nautilus-8xbnv is verified up and running
STEP: scaling down the replication controller
Feb 21 21:26:52.250: INFO: scanned /root for discovery docs: <nil>
Feb 21 21:26:52.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1506'
Feb 21 21:26:53.383: INFO: stderr: ""
Feb 21 21:26:53.383: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 21:26:53.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1506'
Feb 21 21:26:53.460: INFO: stderr: ""
Feb 21 21:26:53.460: INFO: stdout: "update-demo-nautilus-77x55 update-demo-nautilus-8xbnv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 21 21:26:58.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1506'
Feb 21 21:26:58.538: INFO: stderr: ""
Feb 21 21:26:58.538: INFO: stdout: "update-demo-nautilus-77x55 "
Feb 21 21:26:58.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:58.611: INFO: stderr: ""
Feb 21 21:26:58.611: INFO: stdout: "true"
Feb 21 21:26:58.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:58.679: INFO: stderr: ""
Feb 21 21:26:58.679: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 21:26:58.679: INFO: validating pod update-demo-nautilus-77x55
Feb 21 21:26:58.687: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 21:26:58.687: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 21:26:58.687: INFO: update-demo-nautilus-77x55 is verified up and running
STEP: scaling up the replication controller
Feb 21 21:26:58.689: INFO: scanned /root for discovery docs: <nil>
Feb 21 21:26:58.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1506'
Feb 21 21:26:59.805: INFO: stderr: ""
Feb 21 21:26:59.805: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 21 21:26:59.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1506'
Feb 21 21:26:59.881: INFO: stderr: ""
Feb 21 21:26:59.881: INFO: stdout: "update-demo-nautilus-77x55 update-demo-nautilus-q75fd "
Feb 21 21:26:59.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:26:59.977: INFO: stderr: ""
Feb 21 21:26:59.977: INFO: stdout: "true"
Feb 21 21:26:59.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:27:00.073: INFO: stderr: ""
Feb 21 21:27:00.073: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 21:27:00.073: INFO: validating pod update-demo-nautilus-77x55
Feb 21 21:27:00.083: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 21:27:00.083: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 21:27:00.083: INFO: update-demo-nautilus-77x55 is verified up and running
Feb 21 21:27:00.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-q75fd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:27:00.159: INFO: stderr: ""
Feb 21 21:27:00.159: INFO: stdout: ""
Feb 21 21:27:00.159: INFO: update-demo-nautilus-q75fd is created but not running
Feb 21 21:27:05.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1506'
Feb 21 21:27:05.234: INFO: stderr: ""
Feb 21 21:27:05.234: INFO: stdout: "update-demo-nautilus-77x55 update-demo-nautilus-q75fd "
Feb 21 21:27:05.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:27:05.319: INFO: stderr: ""
Feb 21 21:27:05.319: INFO: stdout: "true"
Feb 21 21:27:05.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-77x55 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:27:05.408: INFO: stderr: ""
Feb 21 21:27:05.408: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 21:27:05.408: INFO: validating pod update-demo-nautilus-77x55
Feb 21 21:27:05.415: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 21:27:05.415: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 21:27:05.415: INFO: update-demo-nautilus-77x55 is verified up and running
Feb 21 21:27:05.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-q75fd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:27:05.507: INFO: stderr: ""
Feb 21 21:27:05.507: INFO: stdout: "true"
Feb 21 21:27:05.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods update-demo-nautilus-q75fd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1506'
Feb 21 21:27:05.585: INFO: stderr: ""
Feb 21 21:27:05.585: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 21 21:27:05.585: INFO: validating pod update-demo-nautilus-q75fd
Feb 21 21:27:05.593: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 21 21:27:05.593: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 21 21:27:05.594: INFO: update-demo-nautilus-q75fd is verified up and running
STEP: using delete to clean up resources
Feb 21 21:27:05.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 delete --grace-period=0 --force -f - --namespace=kubectl-1506'
Feb 21 21:27:05.675: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 21 21:27:05.675: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 21 21:27:05.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1506'
Feb 21 21:27:05.766: INFO: stderr: "No resources found.\n"
Feb 21 21:27:05.766: INFO: stdout: ""
Feb 21 21:27:05.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -l name=update-demo --namespace=kubectl-1506 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 21:27:05.845: INFO: stderr: ""
Feb 21 21:27:05.845: INFO: stdout: "update-demo-nautilus-77x55\nupdate-demo-nautilus-q75fd\n"
Feb 21 21:27:06.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1506'
Feb 21 21:27:06.444: INFO: stderr: "No resources found.\n"
Feb 21 21:27:06.444: INFO: stdout: ""
Feb 21 21:27:06.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -l name=update-demo --namespace=kubectl-1506 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 21:27:06.521: INFO: stderr: ""
Feb 21 21:27:06.521: INFO: stdout: "update-demo-nautilus-77x55\nupdate-demo-nautilus-q75fd\n"
Feb 21 21:27:06.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1506'
Feb 21 21:27:06.931: INFO: stderr: "No resources found.\n"
Feb 21 21:27:06.931: INFO: stdout: ""
Feb 21 21:27:06.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 get pods -l name=update-demo --namespace=kubectl-1506 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 21 21:27:07.028: INFO: stderr: ""
Feb 21 21:27:07.028: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:27:07.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1506" for this suite.
Feb 21 21:27:29.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:27:29.373: INFO: namespace kubectl-1506 deletion completed in 22.337587844s

• [SLOW TEST:43.178 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:27:29.375: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 21 21:27:29.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937" in namespace "downward-api-9050" to be "success or failure"
Feb 21 21:27:29.669: INFO: Pod "downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937": Phase="Pending", Reason="", readiness=false. Elapsed: 9.096583ms
Feb 21 21:27:31.675: INFO: Pod "downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01491803s
STEP: Saw pod success
Feb 21 21:27:31.675: INFO: Pod "downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937" satisfied condition "success or failure"
Feb 21 21:27:31.686: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937 container client-container: <nil>
STEP: delete the pod
Feb 21 21:27:31.716: INFO: Waiting for pod downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937 to disappear
Feb 21 21:27:31.721: INFO: Pod downwardapi-volume-9c5af09a-b742-498d-b6cf-e6ef4aacd937 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:27:31.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9050" for this suite.
Feb 21 21:27:37.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:27:37.974: INFO: namespace downward-api-9050 deletion completed in 6.247699807s

• [SLOW TEST:8.599 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:27:37.974: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4609
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Feb 21 21:27:38.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-389596720 --namespace=kubectl-4609 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 21 21:27:39.541: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 21 21:27:39.541: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:27:41.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4609" for this suite.
Feb 21 21:27:47.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:27:47.871: INFO: namespace kubectl-4609 deletion completed in 6.308630048s

• [SLOW TEST:9.897 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:27:47.876: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-398da0bb-d697-41ad-b747-6ca9bbad5347
STEP: Creating a pod to test consume secrets
Feb 21 21:27:48.057: INFO: Waiting up to 5m0s for pod "pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204" in namespace "secrets-3198" to be "success or failure"
Feb 21 21:27:48.063: INFO: Pod "pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204": Phase="Pending", Reason="", readiness=false. Elapsed: 6.246778ms
Feb 21 21:27:50.068: INFO: Pod "pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011440191s
STEP: Saw pod success
Feb 21 21:27:50.068: INFO: Pod "pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204" satisfied condition "success or failure"
Feb 21 21:27:50.077: INFO: Trying to get logs from node ip-172-20-17-149.us-east-2.compute.internal pod pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204 container secret-volume-test: <nil>
STEP: delete the pod
Feb 21 21:27:50.112: INFO: Waiting for pod pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204 to disappear
Feb 21 21:27:50.117: INFO: Pod pod-secrets-d82d53d3-234d-4e01-b818-243dcc677204 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:27:50.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3198" for this suite.
Feb 21 21:27:56.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:27:56.371: INFO: namespace secrets-3198 deletion completed in 6.248138007s

• [SLOW TEST:8.494 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:27:56.371: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:27:59.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4160" for this suite.
Feb 21 21:28:21.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:28:21.903: INFO: namespace replication-controller-4160 deletion completed in 22.272941624s

• [SLOW TEST:25.532 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 21 21:28:21.903: INFO: >>> kubeConfig: /tmp/kubeconfig-389596720
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 21 21:28:22.089: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 21 21:28:22.102: INFO: Waiting for terminating namespaces to be deleted...
Feb 21 21:28:22.107: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-16-60.us-east-2.compute.internal before test
Feb 21 21:28:22.123: INFO: calico-typha-5d656748c8-mtssq from kube-system started at 2020-02-21 19:41:16 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 21:28:22.123: INFO: linkerd-identity-55c9b95464-lg24p from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container identity ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: linkerd-proxy-injector-67fd7b6f8c-gbnlh from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.123: INFO: 	Container proxy-injector ready: true, restart count 0
Feb 21 21:28:22.123: INFO: cert-manager-75c599cb9b-cmdr6 from cert-manager started at 2020-02-21 19:48:53 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container cert-manager ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: kube-dns-autoscaler-577b4774b5-6978f from kube-system started at 2020-02-21 19:41:16 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container autoscaler ready: true, restart count 0
Feb 21 21:28:22.123: INFO: linkerd-tap-f6665c587-mqw6z from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.123: INFO: 	Container tap ready: true, restart count 0
Feb 21 21:28:22.123: INFO: goldilocks-vpa-install-smd28 from goldilocks started at 2020-02-21 19:49:40 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container vpa-install ready: false, restart count 0
Feb 21 21:28:22.123: INFO: rbac-manager-6497c75d88-n6574 from rbac-manager started at 2020-02-21 19:48:59 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container rbac-manager ready: true, restart count 1
Feb 21 21:28:22.123: INFO: prometheus-operator-prometheus-node-exporter-4w66c from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 21:28:22.123: INFO: nginx-ingress-default-backend-8bb99946f-l74lg from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.123: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 21 21:28:22.123: INFO: goldilocks-dashboard-547bc67bdc-5k6pq from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 21:28:22.123: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-nl44f from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 21 21:28:22.123: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 21:28:22.123: INFO: kube-proxy-ip-172-20-16-60.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:14 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: calico-node-wl9bf from kube-system started at 2020-02-21 19:40:59 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 21:28:22.123: INFO: linkerd-destination-8894499df-fhn96 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container destination ready: true, restart count 3
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 4
Feb 21 21:28:22.123: INFO: linkerd-sp-validator-6c76b79bdd-9chl4 from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 21:28:22.123: INFO: external-dns-5b6fdf4bbd-xg7zx from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container external-dns ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: linkerd-controller-766f7cc6b-ktn5s from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container destination ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container public-api ready: true, restart count 0
Feb 21 21:28:22.123: INFO: nginx-ingress-controller-6676bbc48c-xhjdt from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 21:28:22.123: INFO: kube-dns-5fdb85bb5b-fjsw9 from kube-system started at 2020-02-21 19:41:16 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container kubedns ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container sidecar ready: true, restart count 0
Feb 21 21:28:22.123: INFO: basic-demo-548c455c8-9w97q from demo started at 2020-02-21 19:49:39 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container basic-demo ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.123: INFO: vpa-recommender-77cbff9874-f8ccd from kube-system started at 2020-02-21 19:50:23 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.123: INFO: 	Container recommender ready: true, restart count 0
Feb 21 21:28:22.123: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-17-148.us-east-2.compute.internal before test
Feb 21 21:28:22.146: INFO: prometheus-operator-grafana-769db59dcd-8pr2z from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container grafana ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.146: INFO: prometheus-operator-prometheus-node-exporter-kkc6f from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 21:28:22.146: INFO: goldilocks-dashboard-547bc67bdc-scfmz from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 21:28:22.146: INFO: linkerd-sp-validator-6c76b79bdd-42xpr from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.146: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 21:28:22.146: INFO: external-dns-5b6fdf4bbd-5kncn from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container external-dns ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-4nkm6 from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 21 21:28:22.146: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 21:28:22.146: INFO: kube-dns-5fdb85bb5b-dk4sz from kube-system started at 2020-02-21 19:41:56 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container kubedns ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container sidecar ready: true, restart count 0
Feb 21 21:28:22.146: INFO: linkerd-tap-f6665c587-p2qjz from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container tap ready: true, restart count 0
Feb 21 21:28:22.146: INFO: linkerd-proxy-injector-67fd7b6f8c-h44bb from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 6
Feb 21 21:28:22.146: INFO: 	Container proxy-injector ready: true, restart count 7
Feb 21 21:28:22.146: INFO: nginx-ingress-controller-6676bbc48c-s48vd from nginx-ingress started at 2020-02-21 19:48:56 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 21:28:22.146: INFO: metrics-server-7f7cb69697-h977w from metrics-server started at 2020-02-21 19:49:07 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container metrics-server ready: true, restart count 0
Feb 21 21:28:22.146: INFO: prometheus-operator-operator-74d4bb4cc4-xvw88 from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.146: INFO: 	Container prometheus-operator ready: true, restart count 2
Feb 21 21:28:22.146: INFO: linkerd-controller-766f7cc6b-wxnj8 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container destination ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container public-api ready: true, restart count 0
Feb 21 21:28:22.146: INFO: linkerd-grafana-6848df775f-dcxvr from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container grafana ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 3
Feb 21 21:28:22.146: INFO: calico-typha-5d656748c8-25s5c from kube-system started at 2020-02-21 19:41:46 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 21:28:22.146: INFO: linkerd-identity-55c9b95464-cp6f2 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container identity ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.146: INFO: linkerd-destination-8894499df-7w8rg from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container destination ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: cert-manager-cainjector-7c8cff6f8c-nrtbx from cert-manager started at 2020-02-21 19:48:53 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container cainjector ready: true, restart count 1
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: basic-demo-548c455c8-f6hmk from demo started at 2020-02-21 19:49:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container basic-demo ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.146: INFO: alertmanager-prometheus-operator-alertmanager-0 from prometheus-operator started at 2020-02-21 19:50:09 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container alertmanager ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container config-reloader ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: kube-proxy-ip-172-20-17-148.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:36 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 21:28:22.146: INFO: calico-node-lcqwl from kube-system started at 2020-02-21 19:41:25 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.146: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 21:28:22.146: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-17-149.us-east-2.compute.internal before test
Feb 21 21:28:22.155: INFO: calico-node-r6hl2 from kube-system started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.155: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 21:28:22.155: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-pnrct from sonobuoy started at 2020-02-21 19:52:37 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.155: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 21 21:28:22.155: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 21 21:28:22.155: INFO: kube-proxy-ip-172-20-17-149.us-east-2.compute.internal from kube-system started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.155: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 21:28:22.155: INFO: prometheus-prometheus-operator-prometheus-0 from prometheus-operator started at 2020-02-21 19:53:00 +0000 UTC (4 container statuses recorded)
Feb 21 21:28:22.155: INFO: 	Container linkerd-proxy ready: true, restart count 14
Feb 21 21:28:22.155: INFO: 	Container prometheus ready: true, restart count 20
Feb 21 21:28:22.155: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 21 21:28:22.155: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 21 21:28:22.155: INFO: prometheus-operator-prometheus-node-exporter-lv8jq from prometheus-operator started at 2020-02-21 19:52:37 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.155: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 21:28:22.155: INFO: 
Logging pods the kubelet thinks is on node ip-172-20-18-137.us-east-2.compute.internal before test
Feb 21 21:28:22.171: INFO: calico-typha-5d656748c8-pc9zv from kube-system started at 2020-02-21 19:42:09 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.171: INFO: 	Container calico-typha ready: true, restart count 0
Feb 21 21:28:22.171: INFO: linkerd-prometheus-799db67749-n279t from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.171: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.171: INFO: 	Container prometheus ready: true, restart count 0
Feb 21 21:28:22.172: INFO: prometheus-operator-prometheus-node-exporter-bcqpd from prometheus-operator started at 2020-02-21 19:49:37 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container node-exporter ready: true, restart count 0
Feb 21 21:28:22.172: INFO: goldilocks-controller-6d64cc65c7-5fng8 from goldilocks started at 2020-02-21 19:50:25 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container goldilocks ready: true, restart count 0
Feb 21 21:28:22.172: INFO: linkerd-destination-8894499df-jbvgn from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container destination ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: cluster-autoscaler-aws-cluster-autoscaler-5ff6c85c6d-sjlzh from cluster-autoscaler started at 2020-02-21 19:49:03 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container aws-cluster-autoscaler ready: true, restart count 3
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.172: INFO: basic-demo-548c455c8-6s29j from demo started at 2020-02-21 19:49:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container basic-demo ready: true, restart count 3
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 4
Feb 21 21:28:22.172: INFO: linkerd-proxy-injector-67fd7b6f8c-t9vtj from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container proxy-injector ready: true, restart count 0
Feb 21 21:28:22.172: INFO: nginx-ingress-controller-6676bbc48c-m69kk from nginx-ingress started at 2020-02-21 19:48:56 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 21 21:28:22.172: INFO: external-dns-5b6fdf4bbd-ssg7b from external-dns started at 2020-02-21 19:48:57 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container external-dns ready: true, restart count 1
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.172: INFO: kube-proxy-ip-172-20-18-137.us-east-2.compute.internal from kube-system started at 2020-02-21 19:36:17 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: linkerd-web-6fc4b84756-m5b28 from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 3
Feb 21 21:28:22.172: INFO: 	Container web ready: true, restart count 1
Feb 21 21:28:22.172: INFO: prometheus-operator-kube-state-metrics-5d46566c59-6jn4w from prometheus-operator started at 2020-02-21 19:49:36 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container kube-state-metrics ready: true, restart count 2
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.172: INFO: sonobuoy from sonobuoy started at 2020-02-21 19:52:20 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: linkerd-sp-validator-6c76b79bdd-nf5gb from linkerd started at 2020-02-21 19:46:55 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 1
Feb 21 21:28:22.172: INFO: 	Container sp-validator ready: true, restart count 0
Feb 21 21:28:22.172: INFO: linkerd-tap-f6665c587-bwb9s from linkerd started at 2020-02-21 19:46:56 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.172: INFO: 	Container tap ready: true, restart count 0
Feb 21 21:28:22.172: INFO: nginx-ingress-default-backend-8bb99946f-2mkfz from nginx-ingress started at 2020-02-21 19:48:41 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Feb 21 21:28:22.172: INFO: calico-node-wfjj9 from kube-system started at 2020-02-21 19:41:42 +0000 UTC (1 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container calico-node ready: true, restart count 0
Feb 21 21:28:22.172: INFO: linkerd-identity-55c9b95464-zh22t from linkerd started at 2020-02-21 19:46:54 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container identity ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 2
Feb 21 21:28:22.172: INFO: linkerd-controller-766f7cc6b-zk2jg from linkerd started at 2020-02-21 19:46:54 +0000 UTC (3 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container destination ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container linkerd-proxy ready: true, restart count 0
Feb 21 21:28:22.172: INFO: 	Container public-api ready: true, restart count 0
Feb 21 21:28:22.172: INFO: sonobuoy-systemd-logs-daemon-set-23035895473a4986-rvbkh from sonobuoy started at 2020-02-21 19:52:25 +0000 UTC (2 container statuses recorded)
Feb 21 21:28:22.172: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 21 21:28:22.172: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-172-20-16-60.us-east-2.compute.internal
STEP: verifying the node has the label node ip-172-20-17-148.us-east-2.compute.internal
STEP: verifying the node has the label node ip-172-20-17-149.us-east-2.compute.internal
STEP: verifying the node has the label node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod cert-manager-75c599cb9b-cmdr6 requesting resource cpu=60m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod cert-manager-cainjector-7c8cff6f8c-nrtbx requesting resource cpu=10m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod cluster-autoscaler-aws-cluster-autoscaler-5ff6c85c6d-sjlzh requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod basic-demo-548c455c8-6s29j requesting resource cpu=20m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod basic-demo-548c455c8-9w97q requesting resource cpu=20m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod basic-demo-548c455c8-f6hmk requesting resource cpu=20m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod external-dns-5b6fdf4bbd-5kncn requesting resource cpu=160m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod external-dns-5b6fdf4bbd-ssg7b requesting resource cpu=160m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod external-dns-5b6fdf4bbd-xg7zx requesting resource cpu=160m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod goldilocks-controller-6d64cc65c7-5fng8 requesting resource cpu=25m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod goldilocks-dashboard-547bc67bdc-5k6pq requesting resource cpu=25m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod goldilocks-dashboard-547bc67bdc-scfmz requesting resource cpu=25m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-node-lcqwl requesting resource cpu=90m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-node-r6hl2 requesting resource cpu=90m on Node ip-172-20-17-149.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-node-wfjj9 requesting resource cpu=90m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-node-wl9bf requesting resource cpu=90m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-typha-5d656748c8-25s5c requesting resource cpu=0m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-typha-5d656748c8-mtssq requesting resource cpu=0m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod calico-typha-5d656748c8-pc9zv requesting resource cpu=0m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-dns-5fdb85bb5b-dk4sz requesting resource cpu=260m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-dns-5fdb85bb5b-fjsw9 requesting resource cpu=260m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-dns-autoscaler-577b4774b5-6978f requesting resource cpu=20m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-proxy-ip-172-20-16-60.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-proxy-ip-172-20-17-148.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-proxy-ip-172-20-17-149.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-20-17-149.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod kube-proxy-ip-172-20-18-137.us-east-2.compute.internal requesting resource cpu=100m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod vpa-recommender-77cbff9874-f8ccd requesting resource cpu=50m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-controller-766f7cc6b-ktn5s requesting resource cpu=210m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-controller-766f7cc6b-wxnj8 requesting resource cpu=210m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-controller-766f7cc6b-zk2jg requesting resource cpu=210m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-destination-8894499df-7w8rg requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-destination-8894499df-fhn96 requesting resource cpu=110m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-destination-8894499df-jbvgn requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-grafana-6848df775f-dcxvr requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-identity-55c9b95464-cp6f2 requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-identity-55c9b95464-lg24p requesting resource cpu=110m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-identity-55c9b95464-zh22t requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-prometheus-799db67749-n279t requesting resource cpu=310m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-proxy-injector-67fd7b6f8c-gbnlh requesting resource cpu=110m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-proxy-injector-67fd7b6f8c-h44bb requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-proxy-injector-67fd7b6f8c-t9vtj requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-sp-validator-6c76b79bdd-42xpr requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-sp-validator-6c76b79bdd-9chl4 requesting resource cpu=110m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-sp-validator-6c76b79bdd-nf5gb requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.283: INFO: Pod linkerd-tap-f6665c587-bwb9s requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod linkerd-tap-f6665c587-mqw6z requesting resource cpu=110m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod linkerd-tap-f6665c587-p2qjz requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod linkerd-web-6fc4b84756-m5b28 requesting resource cpu=110m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod metrics-server-7f7cb69697-h977w requesting resource cpu=60m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod nginx-ingress-controller-6676bbc48c-m69kk requesting resource cpu=200m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod nginx-ingress-controller-6676bbc48c-s48vd requesting resource cpu=200m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod nginx-ingress-controller-6676bbc48c-xhjdt requesting resource cpu=200m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod nginx-ingress-default-backend-8bb99946f-2mkfz requesting resource cpu=35m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod nginx-ingress-default-backend-8bb99946f-l74lg requesting resource cpu=35m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod alertmanager-prometheus-operator-alertmanager-0 requesting resource cpu=110m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-grafana-769db59dcd-8pr2z requesting resource cpu=10m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-kube-state-metrics-5d46566c59-6jn4w requesting resource cpu=10m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-operator-74d4bb4cc4-xvw88 requesting resource cpu=10m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-prometheus-node-exporter-4w66c requesting resource cpu=0m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-prometheus-node-exporter-bcqpd requesting resource cpu=0m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-prometheus-node-exporter-kkc6f requesting resource cpu=0m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-operator-prometheus-node-exporter-lv8jq requesting resource cpu=0m on Node ip-172-20-17-149.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod prometheus-prometheus-operator-prometheus-0 requesting resource cpu=210m on Node ip-172-20-17-149.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod rbac-manager-6497c75d88-n6574 requesting resource cpu=110m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-20-18-137.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-23035895473a4986-4nkm6 requesting resource cpu=0m on Node ip-172-20-17-148.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-23035895473a4986-nl44f requesting resource cpu=0m on Node ip-172-20-16-60.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-23035895473a4986-pnrct requesting resource cpu=0m on Node ip-172-20-17-149.us-east-2.compute.internal
Feb 21 21:28:22.284: INFO: Pod sonobuoy-systemd-logs-daemon-set-23035895473a4986-rvbkh requesting resource cpu=0m on Node ip-172-20-18-137.us-east-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e.15f5881d0f1c9c8b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1635/filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e to ip-172-20-17-149.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e.15f5881d4053a1f4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e.15f5881d432c0fcf], Reason = [Created], Message = [Created container filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e.15f5881d4fd2145e], Reason = [Started], Message = [Started container filler-pod-0d3b31c9-35a6-4777-b99e-cca993b3375e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f.15f5881d04d2bc3c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1635/filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f to ip-172-20-16-60.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f.15f5881d3ca59fe0], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f.15f5881d75be7172], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f.15f5881d78f0ec11], Reason = [Created], Message = [Created container filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f.15f5881d8fd94a30], Reason = [Started], Message = [Started container filler-pod-8a3744d7-6cd9-4557-a695-6f7fb9b5c24f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2.15f5881d0fd74a83], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1635/filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2 to ip-172-20-18-137.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2.15f5881d567c3b0e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2.15f5881d8fc3592a], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2.15f5881d92727513], Reason = [Created], Message = [Created container filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2.15f5881dae6b120e], Reason = [Started], Message = [Started container filler-pod-c03d2c1a-7832-4bb4-be09-834bfdbbcef2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9756852-fc47-405d-9725-c47ce27568f2.15f5881d0e58a3a6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1635/filler-pod-d9756852-fc47-405d-9725-c47ce27568f2 to ip-172-20-17-148.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9756852-fc47-405d-9725-c47ce27568f2.15f5881d4df2eccc], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9756852-fc47-405d-9725-c47ce27568f2.15f5881d86dab5c3], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9756852-fc47-405d-9725-c47ce27568f2.15f5881d895bd5e4], Reason = [Created], Message = [Created container filler-pod-d9756852-fc47-405d-9725-c47ce27568f2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9756852-fc47-405d-9725-c47ce27568f2.15f5881da1cb32ed], Reason = [Started], Message = [Started container filler-pod-d9756852-fc47-405d-9725-c47ce27568f2]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f5881e024739c3], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taints that the pod didn't tolerate, 4 Insufficient cpu.]
STEP: removing the label node off the node ip-172-20-18-137.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-20-16-60.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-20-17-148.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-20-17-149.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 21 21:28:27.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1635" for this suite.
Feb 21 21:28:33.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 21 21:28:33.993: INFO: namespace sched-pred-1635 deletion completed in 6.2777899s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.090 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.7-beta.0.34+76e131a00b86a9/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSFeb 21 21:28:33.994: INFO: Running AfterSuite actions on all nodes
Feb 21 21:28:33.994: INFO: Running AfterSuite actions on node 1
Feb 21 21:28:33.994: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 5751.340 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h35m53.684635305s
Test Suite Passed
