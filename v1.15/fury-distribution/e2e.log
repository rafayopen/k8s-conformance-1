I0222 13:44:36.754076      18 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-674502360
I0222 13:44:36.754168      18 e2e.go:243] Starting e2e run "dd852fe9-49af-4294-a650-5dca0253e0c4" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1582379075 - Will randomize all specs
Will run 215 of 4412 specs

Feb 22 13:44:36.840: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 13:44:36.841: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 22 13:44:36.854: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 22 13:44:36.875: INFO: The status of Pod minio-setup-jp76l is Succeeded, skipping waiting
Feb 22 13:44:36.875: INFO: 17 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 22 13:44:36.875: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 22 13:44:36.875: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 22 13:44:36.881: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 22 13:44:36.881: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 22 13:44:36.881: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'velero-restic' (0 seconds elapsed)
Feb 22 13:44:36.881: INFO: e2e test version: v1.15.10
Feb 22 13:44:36.882: INFO: kube-apiserver version: v1.15.10
SSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:44:36.882: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
Feb 22 13:44:36.919: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-38c3cacb-8796-4d6c-a609-d4169d5b870f
STEP: Creating secret with name secret-projected-all-test-volume-0f2d7483-41d7-486e-b85e-0b294282a7cd
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 22 13:44:36.944: INFO: Waiting up to 5m0s for pod "projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0" in namespace "projected-7011" to be "success or failure"
Feb 22 13:44:36.972: INFO: Pod "projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 27.622396ms
Feb 22 13:44:38.978: INFO: Pod "projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033517363s
Feb 22 13:44:40.979: INFO: Pod "projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035403238s
STEP: Saw pod success
Feb 22 13:44:40.979: INFO: Pod "projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0" satisfied condition "success or failure"
Feb 22 13:44:40.981: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 22 13:44:41.007: INFO: Waiting for pod projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0 to disappear
Feb 22 13:44:41.010: INFO: Pod projected-volume-0986200a-d901-405a-886e-09d86e2fe4c0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:44:41.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7011" for this suite.
Feb 22 13:44:47.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:44:47.179: INFO: namespace projected-7011 deletion completed in 6.166944713s

â€¢ [SLOW TEST:10.297 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:44:47.179: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 13:44:47.202: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 22 13:44:52.205: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 22 13:44:52.205: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 22 13:44:52.215: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-278,SelfLink:/apis/apps/v1/namespaces/deployment-278/deployments/test-cleanup-deployment,UID:3c81f4a7-ae36-41c3-ae00-9c0ecce6f014,ResourceVersion:3163,Generation:1,CreationTimestamp:2020-02-22 13:44:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 22 13:44:52.221: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Feb 22 13:44:52.221: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 22 13:44:52.222: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-278,SelfLink:/apis/apps/v1/namespaces/deployment-278/replicasets/test-cleanup-controller,UID:588f0372-3641-42f0-bff5-fce823e23e14,ResourceVersion:3164,Generation:1,CreationTimestamp:2020-02-22 13:44:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 3c81f4a7-ae36-41c3-ae00-9c0ecce6f014 0xc001b9b7ef 0xc001b9b800}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 22 13:44:52.232: INFO: Pod "test-cleanup-controller-st49l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-st49l,GenerateName:test-cleanup-controller-,Namespace:deployment-278,SelfLink:/api/v1/namespaces/deployment-278/pods/test-cleanup-controller-st49l,UID:4b166e68-a639-4168-b2ba-a654bec6dde1,ResourceVersion:3160,Generation:0,CreationTimestamp:2020-02-22 13:44:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.14/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 588f0372-3641-42f0-bff5-fce823e23e14 0xc001c7113f 0xc001c71160}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lqh56 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lqh56,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lqh56 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c711c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c711e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 13:44:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 13:44:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 13:44:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 13:44:47 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.14,StartTime:2020-02-22 13:44:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 13:44:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://803c0bd03edb9a436eba1bc9d4935d058a9b280e8548f4f90bf9f4c9c22187f7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:44:52.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-278" for this suite.
Feb 22 13:44:58.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:44:58.416: INFO: namespace deployment-278 deletion completed in 6.179804329s

â€¢ [SLOW TEST:11.237 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:44:58.416: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 13:44:58.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-2907'
Feb 22 13:44:58.786: INFO: stderr: ""
Feb 22 13:44:58.786: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 22 13:44:58.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-2907'
Feb 22 13:44:58.981: INFO: stderr: ""
Feb 22 13:44:58.981: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 22 13:44:59.983: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 13:44:59.983: INFO: Found 0 / 1
Feb 22 13:45:00.983: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 13:45:00.983: INFO: Found 0 / 1
Feb 22 13:45:01.983: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 13:45:01.983: INFO: Found 0 / 1
Feb 22 13:45:02.983: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 13:45:02.983: INFO: Found 1 / 1
Feb 22 13:45:02.983: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 22 13:45:02.985: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 13:45:02.985: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 22 13:45:02.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 describe pod redis-master-6w6kd --namespace=kubectl-2907'
Feb 22 13:45:03.055: INFO: stderr: ""
Feb 22 13:45:03.055: INFO: stdout: "Name:           redis-master-6w6kd\nNamespace:      kubectl-2907\nPriority:       0\nNode:           ip-10-100-10-41.eu-west-1.compute.internal/10.100.10.41\nStart Time:     Sat, 22 Feb 2020 13:44:58 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 172.16.8.15/32\nStatus:         Running\nIP:             172.16.8.15\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://6a911167e1125d8b85d7d29e08efb04475343ea237958c573d9dddc275145a0e\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 22 Feb 2020 13:45:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mfcwg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mfcwg:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mfcwg\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                 Message\n  ----    ------     ----  ----                                                 -------\n  Normal  Scheduled  5s    default-scheduler                                    Successfully assigned kubectl-2907/redis-master-6w6kd to ip-10-100-10-41.eu-west-1.compute.internal\n  Normal  Pulling    4s    kubelet, ip-10-100-10-41.eu-west-1.compute.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, ip-10-100-10-41.eu-west-1.compute.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, ip-10-100-10-41.eu-west-1.compute.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-10-100-10-41.eu-west-1.compute.internal  Started container redis-master\n"
Feb 22 13:45:03.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 describe rc redis-master --namespace=kubectl-2907'
Feb 22 13:45:03.125: INFO: stderr: ""
Feb 22 13:45:03.125: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2907\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-6w6kd\n"
Feb 22 13:45:03.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 describe service redis-master --namespace=kubectl-2907'
Feb 22 13:45:03.191: INFO: stderr: ""
Feb 22 13:45:03.191: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2907\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.105.253.53\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.8.15:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 22 13:45:03.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 describe node ip-10-100-0-4.eu-west-1.compute.internal'
Feb 22 13:45:03.272: INFO: stderr: ""
Feb 22 13:45:03.272: INFO: stdout: "Name:               ip-10-100-0-4.eu-west-1.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-100-0-4.eu-west-1.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.100.0.4/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.168.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 22 Feb 2020 13:36:33 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 22 Feb 2020 13:39:46 +0000   Sat, 22 Feb 2020 13:39:46 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 22 Feb 2020 13:44:35 +0000   Sat, 22 Feb 2020 13:36:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 22 Feb 2020 13:44:35 +0000   Sat, 22 Feb 2020 13:36:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 22 Feb 2020 13:44:35 +0000   Sat, 22 Feb 2020 13:36:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 22 Feb 2020 13:44:35 +0000   Sat, 22 Feb 2020 13:39:54 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.100.0.4\n  Hostname:    ip-10-100-0-4.eu-west-1.compute.internal\nCapacity:\n cpu:                2\n ephemeral-storage:  101583780Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             7865432Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  93619611493\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             7763032Ki\n pods:               110\nSystem Info:\n Machine ID:                 ec2a59b7aa018c9af83197b28fa78acc\n System UUID:                EC2A59B7-AA01-8C9A-F831-97B28FA78ACC\n Boot ID:                    921c05b0-e14f-4997-9481-2fb94ff28582\n Kernel Version:             4.15.0-1058-aws\n OS Image:                   Ubuntu 18.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://19.3.6\n Kubelet Version:            v1.15.5\n Kube-Proxy Version:         v1.15.5\nPodCIDR:                     172.16.0.0/24\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                                ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-gqq2p                                                   250m (12%)    0 (0%)      0 (0%)           0 (0%)         5m31s\n  kube-system                etcd-ip-10-100-0-4.eu-west-1.compute.internal                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m17s\n  kube-system                kube-apiserver-ip-10-100-0-4.eu-west-1.compute.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         7m5s\n  kube-system                kube-controller-manager-ip-10-100-0-4.eu-west-1.compute.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         7m18s\n  kube-system                kube-proxy-xhcx7                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m10s\n  kube-system                kube-scheduler-ip-10-100-0-4.eu-west-1.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         7m19s\n  logging                    fluentd-7vmdk                                                       300m (15%)    1 (50%)     400Mi (5%)       400Mi (5%)     5m30s\n  monitoring                 goldpinger-758c4                                                    1m (0%)       0 (0%)      40Mi (0%)        80Mi (1%)      5m30s\n  monitoring                 node-exporter-mhb9n                                                 102m (5%)     102m (5%)   180Mi (2%)       180Mi (2%)     5m30s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-ffddc             0 (0%)        0 (0%)      0 (0%)           0 (0%)         58s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1203m (60%)  1102m (55%)\n  memory             620Mi (8%)   660Mi (8%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                    From                                                  Message\n  ----    ------                   ----                   ----                                                  -------\n  Normal  NodeHasSufficientMemory  8m37s (x8 over 8m38s)  kubelet, ip-10-100-0-4.eu-west-1.compute.internal     Node ip-10-100-0-4.eu-west-1.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    8m37s (x7 over 8m38s)  kubelet, ip-10-100-0-4.eu-west-1.compute.internal     Node ip-10-100-0-4.eu-west-1.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     8m37s (x8 over 8m38s)  kubelet, ip-10-100-0-4.eu-west-1.compute.internal     Node ip-10-100-0-4.eu-west-1.compute.internal status is now: NodeHasSufficientPID\n  Normal  Starting                 8m9s                   kube-proxy, ip-10-100-0-4.eu-west-1.compute.internal  Starting kube-proxy.\n"
Feb 22 13:45:03.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 describe namespace kubectl-2907'
Feb 22 13:45:03.339: INFO: stderr: ""
Feb 22 13:45:03.339: INFO: stdout: "Name:         kubectl-2907\nLabels:       e2e-framework=kubectl\n              e2e-run=dd852fe9-49af-4294-a650-5dca0253e0c4\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:45:03.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2907" for this suite.
Feb 22 13:45:25.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:45:25.512: INFO: namespace kubectl-2907 deletion completed in 22.17042823s

â€¢ [SLOW TEST:27.096 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:45:25.512: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e0f76b73-e25f-40c9-aaec-92f96757da39
STEP: Creating configMap with name cm-test-opt-upd-d63d822e-9209-4cdc-8abf-19c3735c9fb1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e0f76b73-e25f-40c9-aaec-92f96757da39
STEP: Updating configmap cm-test-opt-upd-d63d822e-9209-4cdc-8abf-19c3735c9fb1
STEP: Creating configMap with name cm-test-opt-create-56cb96f1-71f7-4329-a58a-a4e1ecb40d7b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:46:53.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7627" for this suite.
Feb 22 13:47:15.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:47:16.006: INFO: namespace configmap-7627 deletion completed in 22.167789948s

â€¢ [SLOW TEST:110.494 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:47:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-422
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 22 13:47:16.038: INFO: Found 0 stateful pods, waiting for 3
Feb 22 13:47:26.040: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 13:47:26.040: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 13:47:26.040: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Feb 22 13:47:36.040: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 13:47:36.040: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 13:47:36.040: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 22 13:47:36.068: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 22 13:47:46.096: INFO: Updating stateful set ss2
Feb 22 13:47:46.101: INFO: Waiting for Pod statefulset-422/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 22 13:47:56.105: INFO: Waiting for Pod statefulset-422/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Feb 22 13:48:06.246: INFO: Found 2 stateful pods, waiting for 3
Feb 22 13:48:16.249: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 13:48:16.249: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 13:48:16.249: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 22 13:48:16.265: INFO: Updating stateful set ss2
Feb 22 13:48:16.270: INFO: Waiting for Pod statefulset-422/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 22 13:48:26.273: INFO: Waiting for Pod statefulset-422/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 22 13:48:36.287: INFO: Updating stateful set ss2
Feb 22 13:48:36.297: INFO: Waiting for StatefulSet statefulset-422/ss2 to complete update
Feb 22 13:48:36.297: INFO: Waiting for Pod statefulset-422/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 22 13:48:46.303: INFO: Waiting for StatefulSet statefulset-422/ss2 to complete update
Feb 22 13:48:46.303: INFO: Waiting for Pod statefulset-422/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 22 13:48:56.301: INFO: Deleting all statefulset in ns statefulset-422
Feb 22 13:48:56.303: INFO: Scaling statefulset ss2 to 0
Feb 22 13:49:16.312: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 13:49:16.314: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:49:16.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-422" for this suite.
Feb 22 13:49:22.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:49:22.511: INFO: namespace statefulset-422 deletion completed in 6.168867018s

â€¢ [SLOW TEST:126.505 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:49:22.511: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-6338/secret-test-cfbbbcfa-4aed-4226-be14-8ce84a88a3f1
STEP: Creating a pod to test consume secrets
Feb 22 13:49:22.538: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf" in namespace "secrets-6338" to be "success or failure"
Feb 22 13:49:22.541: INFO: Pod "pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.45317ms
Feb 22 13:49:24.543: INFO: Pod "pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004538737s
Feb 22 13:49:26.545: INFO: Pod "pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006921344s
Feb 22 13:49:28.547: INFO: Pod "pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00891733s
STEP: Saw pod success
Feb 22 13:49:28.547: INFO: Pod "pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf" satisfied condition "success or failure"
Feb 22 13:49:28.549: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf container env-test: <nil>
STEP: delete the pod
Feb 22 13:49:28.570: INFO: Waiting for pod pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf to disappear
Feb 22 13:49:28.573: INFO: Pod pod-configmaps-1d9b1cc6-03cb-4ee7-9d88-80c7cc627cbf no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:49:28.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6338" for this suite.
Feb 22 13:49:34.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:49:34.744: INFO: namespace secrets-6338 deletion completed in 6.16879408s

â€¢ [SLOW TEST:12.233 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:49:34.745: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9e776d52-1429-48ca-a0f1-0d24f3c1da70
STEP: Creating secret with name s-test-opt-upd-80cc894a-6193-4d34-a8a7-f069cc129b4c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9e776d52-1429-48ca-a0f1-0d24f3c1da70
STEP: Updating secret s-test-opt-upd-80cc894a-6193-4d34-a8a7-f069cc129b4c
STEP: Creating secret with name s-test-opt-create-61d42f18-2f69-48cc-81b5-44cfc2a973d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:50:41.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4405" for this suite.
Feb 22 13:51:03.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:51:03.203: INFO: namespace projected-4405 deletion completed in 22.175994811s

â€¢ [SLOW TEST:88.458 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:51:03.203: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-77ac74c4-f5af-4c9b-9a37-18391ab28b87
STEP: Creating a pod to test consume configMaps
Feb 22 13:51:03.233: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a" in namespace "configmap-7178" to be "success or failure"
Feb 22 13:51:03.238: INFO: Pod "pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587297ms
Feb 22 13:51:05.240: INFO: Pod "pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00655305s
Feb 22 13:51:07.242: INFO: Pod "pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008725064s
STEP: Saw pod success
Feb 22 13:51:07.242: INFO: Pod "pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a" satisfied condition "success or failure"
Feb 22 13:51:07.244: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 13:51:07.259: INFO: Waiting for pod pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a to disappear
Feb 22 13:51:07.261: INFO: Pod pod-configmaps-3c932847-03df-4a72-ad09-4dcc3d43219a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:51:07.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7178" for this suite.
Feb 22 13:51:13.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:51:13.430: INFO: namespace configmap-7178 deletion completed in 6.167327193s

â€¢ [SLOW TEST:10.227 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:51:13.431: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 22 13:51:13.507: INFO: Waiting up to 5m0s for pod "downward-api-5018f247-9b69-46fa-bf3f-e8461e134427" in namespace "downward-api-5993" to be "success or failure"
Feb 22 13:51:13.514: INFO: Pod "downward-api-5018f247-9b69-46fa-bf3f-e8461e134427": Phase="Pending", Reason="", readiness=false. Elapsed: 6.818189ms
Feb 22 13:51:15.515: INFO: Pod "downward-api-5018f247-9b69-46fa-bf3f-e8461e134427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008511709s
STEP: Saw pod success
Feb 22 13:51:15.516: INFO: Pod "downward-api-5018f247-9b69-46fa-bf3f-e8461e134427" satisfied condition "success or failure"
Feb 22 13:51:15.517: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod downward-api-5018f247-9b69-46fa-bf3f-e8461e134427 container dapi-container: <nil>
STEP: delete the pod
Feb 22 13:51:15.530: INFO: Waiting for pod downward-api-5018f247-9b69-46fa-bf3f-e8461e134427 to disappear
Feb 22 13:51:15.532: INFO: Pod downward-api-5018f247-9b69-46fa-bf3f-e8461e134427 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:51:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5993" for this suite.
Feb 22 13:51:21.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:51:21.700: INFO: namespace downward-api-5993 deletion completed in 6.165834164s

â€¢ [SLOW TEST:8.269 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:51:21.700: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 13:51:21.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce" in namespace "downward-api-5642" to be "success or failure"
Feb 22 13:51:21.731: INFO: Pod "downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.446049ms
Feb 22 13:51:23.734: INFO: Pod "downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008892457s
Feb 22 13:51:25.737: INFO: Pod "downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011470773s
STEP: Saw pod success
Feb 22 13:51:25.737: INFO: Pod "downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce" satisfied condition "success or failure"
Feb 22 13:51:25.739: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce container client-container: <nil>
STEP: delete the pod
Feb 22 13:51:25.757: INFO: Waiting for pod downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce to disappear
Feb 22 13:51:25.758: INFO: Pod downwardapi-volume-74009f85-4944-4605-bcf2-05b7ccd82cce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:51:25.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5642" for this suite.
Feb 22 13:51:31.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:51:31.972: INFO: namespace downward-api-5642 deletion completed in 6.212439134s

â€¢ [SLOW TEST:10.272 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:51:31.973: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 22 13:51:32.003: INFO: Waiting up to 5m0s for pod "downward-api-d8659f25-3063-4303-a413-a96ede6536c1" in namespace "downward-api-2669" to be "success or failure"
Feb 22 13:51:32.013: INFO: Pod "downward-api-d8659f25-3063-4303-a413-a96ede6536c1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.425595ms
Feb 22 13:51:34.015: INFO: Pod "downward-api-d8659f25-3063-4303-a413-a96ede6536c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012572165s
Feb 22 13:51:36.017: INFO: Pod "downward-api-d8659f25-3063-4303-a413-a96ede6536c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014693577s
STEP: Saw pod success
Feb 22 13:51:36.018: INFO: Pod "downward-api-d8659f25-3063-4303-a413-a96ede6536c1" satisfied condition "success or failure"
Feb 22 13:51:36.019: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downward-api-d8659f25-3063-4303-a413-a96ede6536c1 container dapi-container: <nil>
STEP: delete the pod
Feb 22 13:51:36.033: INFO: Waiting for pod downward-api-d8659f25-3063-4303-a413-a96ede6536c1 to disappear
Feb 22 13:51:36.037: INFO: Pod downward-api-d8659f25-3063-4303-a413-a96ede6536c1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:51:36.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2669" for this suite.
Feb 22 13:51:42.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:51:42.211: INFO: namespace downward-api-2669 deletion completed in 6.171337803s

â€¢ [SLOW TEST:10.238 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:51:42.211: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:51:46.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7858" for this suite.
Feb 22 13:52:30.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:52:30.422: INFO: namespace kubelet-test-7858 deletion completed in 44.165961722s

â€¢ [SLOW TEST:48.211 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:52:30.423: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 13:52:30.452: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc" in namespace "downward-api-9759" to be "success or failure"
Feb 22 13:52:30.455: INFO: Pod "downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.236252ms
Feb 22 13:52:32.457: INFO: Pod "downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005602635s
Feb 22 13:52:34.460: INFO: Pod "downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007862618s
STEP: Saw pod success
Feb 22 13:52:34.460: INFO: Pod "downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc" satisfied condition "success or failure"
Feb 22 13:52:34.461: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc container client-container: <nil>
STEP: delete the pod
Feb 22 13:52:34.479: INFO: Waiting for pod downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc to disappear
Feb 22 13:52:34.481: INFO: Pod downwardapi-volume-c3348d92-43e7-49af-a3e6-5198178489dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:52:34.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9759" for this suite.
Feb 22 13:52:40.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:52:40.658: INFO: namespace downward-api-9759 deletion completed in 6.175081874s

â€¢ [SLOW TEST:10.236 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:52:40.659: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1561
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 13:52:40.684: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 22 13:53:06.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.88.95:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1561 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 13:53:06.739: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 13:53:06.892: INFO: Found all expected endpoints: [netserver-0]
Feb 22 13:53:06.896: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.8.25:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1561 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 13:53:06.896: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 13:53:07.031: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:53:07.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1561" for this suite.
Feb 22 13:53:29.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:53:29.207: INFO: namespace pod-network-test-1561 deletion completed in 22.170811107s

â€¢ [SLOW TEST:48.548 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:53:29.207: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-48b5aa63-14ff-4b1a-a842-55e71727932c
STEP: Creating a pod to test consume secrets
Feb 22 13:53:29.285: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0" in namespace "projected-3970" to be "success or failure"
Feb 22 13:53:29.289: INFO: Pod "pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.735881ms
Feb 22 13:53:31.291: INFO: Pod "pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006674308s
Feb 22 13:53:33.293: INFO: Pod "pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008828833s
STEP: Saw pod success
Feb 22 13:53:33.293: INFO: Pod "pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0" satisfied condition "success or failure"
Feb 22 13:53:33.295: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 13:53:33.310: INFO: Waiting for pod pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0 to disappear
Feb 22 13:53:33.312: INFO: Pod pod-projected-secrets-b5109cb0-8456-4768-8b4d-9259eaffa1f0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:53:33.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3970" for this suite.
Feb 22 13:53:39.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:53:39.480: INFO: namespace projected-3970 deletion completed in 6.166104321s

â€¢ [SLOW TEST:10.273 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:53:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 13:53:42.520: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:53:42.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8307" for this suite.
Feb 22 13:53:48.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:53:48.762: INFO: namespace container-runtime-8307 deletion completed in 6.226465812s

â€¢ [SLOW TEST:9.281 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:53:48.762: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-900adf20-2102-45f6-a209-3b61e6d737c5
STEP: Creating a pod to test consume secrets
Feb 22 13:53:48.792: INFO: Waiting up to 5m0s for pod "pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5" in namespace "secrets-1282" to be "success or failure"
Feb 22 13:53:48.797: INFO: Pod "pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.170358ms
Feb 22 13:53:50.799: INFO: Pod "pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006291277s
Feb 22 13:53:52.801: INFO: Pod "pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008323789s
STEP: Saw pod success
Feb 22 13:53:52.801: INFO: Pod "pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5" satisfied condition "success or failure"
Feb 22 13:53:52.802: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 13:53:52.818: INFO: Waiting for pod pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5 to disappear
Feb 22 13:53:52.820: INFO: Pod pod-secrets-92167528-ef07-46d0-8504-7ee41cb551f5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:53:52.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1282" for this suite.
Feb 22 13:53:58.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:53:58.989: INFO: namespace secrets-1282 deletion completed in 6.167253589s

â€¢ [SLOW TEST:10.227 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:53:58.990: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-1c949d73-c594-45ac-9420-796a7431b719
STEP: Creating a pod to test consume secrets
Feb 22 13:53:59.017: INFO: Waiting up to 5m0s for pod "pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c" in namespace "secrets-3292" to be "success or failure"
Feb 22 13:53:59.021: INFO: Pod "pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.816186ms
Feb 22 13:54:01.024: INFO: Pod "pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006938493s
Feb 22 13:54:03.025: INFO: Pod "pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008816763s
STEP: Saw pod success
Feb 22 13:54:03.025: INFO: Pod "pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c" satisfied condition "success or failure"
Feb 22 13:54:03.027: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 13:54:03.047: INFO: Waiting for pod pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c to disappear
Feb 22 13:54:03.049: INFO: Pod pod-secrets-b7cbcbc1-9b4f-4870-bb2b-c0f28a3d378c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:54:03.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3292" for this suite.
Feb 22 13:54:09.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:54:09.221: INFO: namespace secrets-3292 deletion completed in 6.170208731s

â€¢ [SLOW TEST:10.232 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:54:09.222: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1657.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1657.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 184.227.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.227.184_udp@PTR;check="$$(dig +tcp +noall +answer +search 184.227.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.227.184_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1657.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1657.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1657.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 184.227.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.227.184_udp@PTR;check="$$(dig +tcp +noall +answer +search 184.227.106.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.106.227.184_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 13:54:23.300: INFO: Unable to read wheezy_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.301: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.303: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.305: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.317: INFO: Unable to read jessie_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.320: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.322: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:23.331: INFO: Lookups using dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66 failed for: [wheezy_udp@dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_udp@dns-test-service.dns-1657.svc.cluster.local jessie_tcp@dns-test-service.dns-1657.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local]

Feb 22 13:54:28.334: INFO: Unable to read wheezy_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.336: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.338: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.340: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.352: INFO: Unable to read jessie_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.355: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:28.366: INFO: Lookups using dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66 failed for: [wheezy_udp@dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_udp@dns-test-service.dns-1657.svc.cluster.local jessie_tcp@dns-test-service.dns-1657.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local]

Feb 22 13:54:33.334: INFO: Unable to read wheezy_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.335: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.337: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.351: INFO: Unable to read jessie_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.354: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.356: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:33.365: INFO: Lookups using dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66 failed for: [wheezy_udp@dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_udp@dns-test-service.dns-1657.svc.cluster.local jessie_tcp@dns-test-service.dns-1657.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local]

Feb 22 13:54:38.334: INFO: Unable to read wheezy_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.336: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.338: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.340: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.351: INFO: Unable to read jessie_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.353: INFO: Unable to read jessie_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.355: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:38.366: INFO: Lookups using dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66 failed for: [wheezy_udp@dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_udp@dns-test-service.dns-1657.svc.cluster.local jessie_tcp@dns-test-service.dns-1657.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local]

Feb 22 13:54:43.339: INFO: Unable to read wheezy_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.341: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.343: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.345: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.356: INFO: Unable to read jessie_udp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.358: INFO: Unable to read jessie_tcp@dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.360: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.361: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local from pod dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66: the server could not find the requested resource (get pods dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66)
Feb 22 13:54:43.371: INFO: Lookups using dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66 failed for: [wheezy_udp@dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@dns-test-service.dns-1657.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_udp@dns-test-service.dns-1657.svc.cluster.local jessie_tcp@dns-test-service.dns-1657.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1657.svc.cluster.local]

Feb 22 13:54:48.365: INFO: DNS probes using dns-1657/dns-test-fda42aee-2c14-4b9c-941a-3e6606ac1c66 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:54:48.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1657" for this suite.
Feb 22 13:54:54.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:54:54.597: INFO: namespace dns-1657 deletion completed in 6.17002223s

â€¢ [SLOW TEST:45.376 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:54:54.598: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0222 13:55:25.140940      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 22 13:55:25.140: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:55:25.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5278" for this suite.
Feb 22 13:55:31.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:55:31.322: INFO: namespace gc-5278 deletion completed in 6.178313857s

â€¢ [SLOW TEST:36.724 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:55:31.324: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 22 13:55:31.345: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 13:55:31.349: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 13:55:31.350: INFO: 
Logging pods the kubelet thinks is on node ip-10-100-10-41.eu-west-1.compute.internal before test
Feb 22 13:55:31.356: INFO: node-exporter-msc8m from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container node-exporter ready: true, restart count 0
Feb 22 13:55:31.356: INFO: velero-restic-2w5bp from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container restic ready: true, restart count 0
Feb 22 13:55:31.356: INFO: coredns-5c98db65d4-p26fz from kube-system started at 2020-02-22 13:39:48 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container coredns ready: true, restart count 0
Feb 22 13:55:31.356: INFO: kube-state-metrics-779c78b7d5-p2w4c from monitoring started at 2020-02-22 13:41:19 +0000 UTC (2 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container addon-resizer ready: true, restart count 0
Feb 22 13:55:31.356: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 22 13:55:31.356: INFO: calico-node-mxbd8 from kube-system started at 2020-02-22 13:39:32 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container calico-node ready: true, restart count 0
Feb 22 13:55:31.356: INFO: elasticsearch-0 from logging started at 2020-02-22 13:40:27 +0000 UTC (2 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container elasticsearch ready: true, restart count 0
Feb 22 13:55:31.356: INFO: 	Container exporter ready: true, restart count 0
Feb 22 13:55:31.356: INFO: kube-proxy-qj2fs from kube-system started at 2020-02-22 13:36:59 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 13:55:31.356: INFO: cert-manager-cainjector-db5466bb4-vz5nk from cert-manager started at 2020-02-22 13:39:41 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container cainjector ready: true, restart count 0
Feb 22 13:55:31.356: INFO: sonobuoy from sonobuoy started at 2020-02-22 13:43:59 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 13:55:31.356: INFO: sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-kxfkz from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 13:55:31.356: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 13:55:31.356: INFO: fluentd-2p5rq from logging started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container fluentd ready: true, restart count 0
Feb 22 13:55:31.356: INFO: nginx-ingress-controller-wzhsm from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 22 13:55:31.356: INFO: minio-0 from kube-system started at 2020-02-22 13:40:30 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container minio ready: true, restart count 0
Feb 22 13:55:31.356: INFO: goldpinger-rtcxd from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.356: INFO: 	Container goldpinger ready: true, restart count 0
Feb 22 13:55:31.356: INFO: 
Logging pods the kubelet thinks is on node ip-10-100-10-53.eu-west-1.compute.internal before test
Feb 22 13:55:31.365: INFO: velero-restic-qbvdz from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container restic ready: true, restart count 0
Feb 22 13:55:31.365: INFO: nginx-ingress-controller-khvgv from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 22 13:55:31.365: INFO: sonobuoy-e2e-job-f6fd58bec6164e4d from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container e2e ready: true, restart count 0
Feb 22 13:55:31.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 13:55:31.365: INFO: fluentd-cbtvh from logging started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container fluentd ready: true, restart count 0
Feb 22 13:55:31.365: INFO: velero-5c4bbbd8d6-drz26 from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container velero ready: true, restart count 0
Feb 22 13:55:31.365: INFO: kibana-5bd7fdf954-k7mrh from logging started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container kibana ready: true, restart count 0
Feb 22 13:55:31.365: INFO: local-path-provisioner-84f4c8b584-xjdmp from local-path-storage started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container local-path-provisioner ready: true, restart count 0
Feb 22 13:55:31.365: INFO: cerebro-78d8f5fc66-nwsj5 from logging started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container cerebro ready: true, restart count 0
Feb 22 13:55:31.365: INFO: prometheus-operator-7f4c684c8b-wxj5q from monitoring started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 22 13:55:31.365: INFO: calico-node-6dg9m from kube-system started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container calico-node ready: true, restart count 0
Feb 22 13:55:31.365: INFO: goldpinger-dxblt from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container goldpinger ready: true, restart count 0
Feb 22 13:55:31.365: INFO: coredns-5c98db65d4-jw6hn from kube-system started at 2020-02-22 13:39:39 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container coredns ready: true, restart count 0
Feb 22 13:55:31.365: INFO: grafana-6cdc7d9f68-4p8g9 from monitoring started at 2020-02-22 13:39:39 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.365: INFO: 	Container grafana ready: true, restart count 0
Feb 22 13:55:31.365: INFO: forecastle-6d4d79f779-fzlqw from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container forecastle ready: true, restart count 0
Feb 22 13:55:31.366: INFO: calico-kube-controllers-8944b5db6-zqdw6 from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 22 13:55:31.366: INFO: cert-manager-594fcfc677-s7wd7 from cert-manager started at 2020-02-22 13:39:41 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container cert-manager ready: true, restart count 0
Feb 22 13:55:31.366: INFO: prometheus-k8s-0 from monitoring started at 2020-02-22 13:41:20 +0000 UTC (3 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container prometheus ready: true, restart count 1
Feb 22 13:55:31.366: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 22 13:55:31.366: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 22 13:55:31.366: INFO: kube-proxy-9ckc7 from kube-system started at 2020-02-22 13:36:59 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 13:55:31.366: INFO: node-exporter-cnhxz from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container node-exporter ready: true, restart count 0
Feb 22 13:55:31.366: INFO: cert-manager-webhook-5f988b8bbc-sq7k7 from cert-manager started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container webhook ready: true, restart count 0
Feb 22 13:55:31.366: INFO: minio-setup-jp76l from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container mc ready: false, restart count 0
Feb 22 13:55:31.366: INFO: sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-4zplg from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 13:55:31.366: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 13:55:31.366: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f5bdfb5cdfe996], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:55:32.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8959" for this suite.
Feb 22 13:55:38.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:55:38.557: INFO: namespace sched-pred-8959 deletion completed in 6.1687067s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:7.233 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:55:38.557: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 13:55:38.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2023'
Feb 22 13:55:38.775: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 22 13:55:38.775: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Feb 22 13:55:40.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2023'
Feb 22 13:55:40.868: INFO: stderr: ""
Feb 22 13:55:40.868: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:55:40.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2023" for this suite.
Feb 22 13:56:02.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:56:03.048: INFO: namespace kubectl-2023 deletion completed in 22.176410088s

â€¢ [SLOW TEST:24.490 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:56:03.048: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:56:03.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2029" for this suite.
Feb 22 13:56:25.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:56:25.259: INFO: namespace pods-2029 deletion completed in 22.170991501s

â€¢ [SLOW TEST:22.212 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:56:25.259: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 22 13:56:25.288: INFO: Waiting up to 5m0s for pod "pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c" in namespace "emptydir-1562" to be "success or failure"
Feb 22 13:56:25.298: INFO: Pod "pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.678169ms
Feb 22 13:56:27.300: INFO: Pod "pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011735132s
Feb 22 13:56:29.302: INFO: Pod "pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013615889s
STEP: Saw pod success
Feb 22 13:56:29.302: INFO: Pod "pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c" satisfied condition "success or failure"
Feb 22 13:56:29.304: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c container test-container: <nil>
STEP: delete the pod
Feb 22 13:56:29.316: INFO: Waiting for pod pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c to disappear
Feb 22 13:56:29.320: INFO: Pod pod-ea5addf4-ba64-4f15-bda3-6c0c6965ca2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:56:29.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1562" for this suite.
Feb 22 13:56:35.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:56:35.492: INFO: namespace emptydir-1562 deletion completed in 6.169304672s

â€¢ [SLOW TEST:10.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:56:35.492: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 22 13:56:35.515: INFO: Waiting up to 5m0s for pod "downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf" in namespace "downward-api-932" to be "success or failure"
Feb 22 13:56:35.522: INFO: Pod "downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.676522ms
Feb 22 13:56:37.524: INFO: Pod "downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009781644s
Feb 22 13:56:39.526: INFO: Pod "downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011550604s
STEP: Saw pod success
Feb 22 13:56:39.526: INFO: Pod "downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf" satisfied condition "success or failure"
Feb 22 13:56:39.528: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf container dapi-container: <nil>
STEP: delete the pod
Feb 22 13:56:39.543: INFO: Waiting for pod downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf to disappear
Feb 22 13:56:39.545: INFO: Pod downward-api-9c7ad44b-3c47-4c8b-8b0b-5e160ef150cf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:56:39.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-932" for this suite.
Feb 22 13:56:45.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:56:45.714: INFO: namespace downward-api-932 deletion completed in 6.166787706s

â€¢ [SLOW TEST:10.222 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:56:45.714: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 13:56:47.802: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:56:47.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2435" for this suite.
Feb 22 13:56:53.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:56:53.984: INFO: namespace container-runtime-2435 deletion completed in 6.169002678s

â€¢ [SLOW TEST:8.270 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:56:53.985: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 22 13:56:54.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8971,SelfLink:/api/v1/namespaces/watch-8971/configmaps/e2e-watch-test-label-changed,UID:97a2ba7a-99b1-4f16-951b-cf03e5adf58d,ResourceVersion:6345,Generation:0,CreationTimestamp:2020-02-22 13:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 22 13:56:54.014: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8971,SelfLink:/api/v1/namespaces/watch-8971/configmaps/e2e-watch-test-label-changed,UID:97a2ba7a-99b1-4f16-951b-cf03e5adf58d,ResourceVersion:6346,Generation:0,CreationTimestamp:2020-02-22 13:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 22 13:56:54.015: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8971,SelfLink:/api/v1/namespaces/watch-8971/configmaps/e2e-watch-test-label-changed,UID:97a2ba7a-99b1-4f16-951b-cf03e5adf58d,ResourceVersion:6347,Generation:0,CreationTimestamp:2020-02-22 13:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 22 13:57:04.035: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8971,SelfLink:/api/v1/namespaces/watch-8971/configmaps/e2e-watch-test-label-changed,UID:97a2ba7a-99b1-4f16-951b-cf03e5adf58d,ResourceVersion:6377,Generation:0,CreationTimestamp:2020-02-22 13:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 22 13:57:04.035: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8971,SelfLink:/api/v1/namespaces/watch-8971/configmaps/e2e-watch-test-label-changed,UID:97a2ba7a-99b1-4f16-951b-cf03e5adf58d,ResourceVersion:6378,Generation:0,CreationTimestamp:2020-02-22 13:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 22 13:57:04.035: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8971,SelfLink:/api/v1/namespaces/watch-8971/configmaps/e2e-watch-test-label-changed,UID:97a2ba7a-99b1-4f16-951b-cf03e5adf58d,ResourceVersion:6379,Generation:0,CreationTimestamp:2020-02-22 13:56:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:57:04.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8971" for this suite.
Feb 22 13:57:10.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:57:10.205: INFO: namespace watch-8971 deletion completed in 6.168272681s

â€¢ [SLOW TEST:16.220 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:57:10.206: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Feb 22 13:57:14.241: INFO: Pod pod-hostip-1ebcfb67-61e7-4e37-898b-dce839c8c797 has hostIP: 10.100.10.41
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:57:14.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8665" for this suite.
Feb 22 13:57:36.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:57:36.417: INFO: namespace pods-8665 deletion completed in 22.173213122s

â€¢ [SLOW TEST:26.211 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:57:36.417: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-67612312-0aa6-4b69-a429-a138f9b9e1b6 in namespace container-probe-3613
Feb 22 13:57:40.445: INFO: Started pod liveness-67612312-0aa6-4b69-a429-a138f9b9e1b6 in namespace container-probe-3613
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 13:57:40.447: INFO: Initial restart count of pod liveness-67612312-0aa6-4b69-a429-a138f9b9e1b6 is 0
Feb 22 13:58:02.471: INFO: Restart count of pod container-probe-3613/liveness-67612312-0aa6-4b69-a429-a138f9b9e1b6 is now 1 (22.024154393s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:58:02.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3613" for this suite.
Feb 22 13:58:08.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:58:08.659: INFO: namespace container-probe-3613 deletion completed in 6.174362606s

â€¢ [SLOW TEST:32.242 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:58:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Feb 22 13:58:08.736: INFO: Waiting up to 5m0s for pod "var-expansion-797f802f-0168-4d2d-884a-be9692946171" in namespace "var-expansion-5511" to be "success or failure"
Feb 22 13:58:08.740: INFO: Pod "var-expansion-797f802f-0168-4d2d-884a-be9692946171": Phase="Pending", Reason="", readiness=false. Elapsed: 3.661658ms
Feb 22 13:58:10.742: INFO: Pod "var-expansion-797f802f-0168-4d2d-884a-be9692946171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005721236s
Feb 22 13:58:12.744: INFO: Pod "var-expansion-797f802f-0168-4d2d-884a-be9692946171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007619586s
STEP: Saw pod success
Feb 22 13:58:12.744: INFO: Pod "var-expansion-797f802f-0168-4d2d-884a-be9692946171" satisfied condition "success or failure"
Feb 22 13:58:12.746: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod var-expansion-797f802f-0168-4d2d-884a-be9692946171 container dapi-container: <nil>
STEP: delete the pod
Feb 22 13:58:12.761: INFO: Waiting for pod var-expansion-797f802f-0168-4d2d-884a-be9692946171 to disappear
Feb 22 13:58:12.762: INFO: Pod var-expansion-797f802f-0168-4d2d-884a-be9692946171 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:58:12.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5511" for this suite.
Feb 22 13:58:18.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:58:18.937: INFO: namespace var-expansion-5511 deletion completed in 6.171720568s

â€¢ [SLOW TEST:10.278 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:58:18.937: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-7b91e2fb-5547-4ca7-9287-cc5d35fd8515
STEP: Creating a pod to test consume configMaps
Feb 22 13:58:18.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa" in namespace "projected-1700" to be "success or failure"
Feb 22 13:58:18.973: INFO: Pod "pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.694158ms
Feb 22 13:58:20.975: INFO: Pod "pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010798331s
Feb 22 13:58:22.977: INFO: Pod "pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012874776s
STEP: Saw pod success
Feb 22 13:58:22.977: INFO: Pod "pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa" satisfied condition "success or failure"
Feb 22 13:58:22.979: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 13:58:22.993: INFO: Waiting for pod pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa to disappear
Feb 22 13:58:22.995: INFO: Pod pod-projected-configmaps-697667bf-0aca-4cdf-8d0d-49765e1259aa no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:58:22.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1700" for this suite.
Feb 22 13:58:29.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:58:29.164: INFO: namespace projected-1700 deletion completed in 6.166931171s

â€¢ [SLOW TEST:10.227 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:58:29.164: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 22 13:58:29.239: INFO: Waiting up to 5m0s for pod "pod-67b656d1-431c-454c-b17d-af4f453e94ae" in namespace "emptydir-4791" to be "success or failure"
Feb 22 13:58:29.244: INFO: Pod "pod-67b656d1-431c-454c-b17d-af4f453e94ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.453546ms
Feb 22 13:58:31.247: INFO: Pod "pod-67b656d1-431c-454c-b17d-af4f453e94ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008251456s
Feb 22 13:58:33.249: INFO: Pod "pod-67b656d1-431c-454c-b17d-af4f453e94ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010476139s
STEP: Saw pod success
Feb 22 13:58:33.249: INFO: Pod "pod-67b656d1-431c-454c-b17d-af4f453e94ae" satisfied condition "success or failure"
Feb 22 13:58:33.252: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-67b656d1-431c-454c-b17d-af4f453e94ae container test-container: <nil>
STEP: delete the pod
Feb 22 13:58:33.267: INFO: Waiting for pod pod-67b656d1-431c-454c-b17d-af4f453e94ae to disappear
Feb 22 13:58:33.270: INFO: Pod pod-67b656d1-431c-454c-b17d-af4f453e94ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:58:33.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4791" for this suite.
Feb 22 13:58:39.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:58:39.439: INFO: namespace emptydir-4791 deletion completed in 6.16662546s

â€¢ [SLOW TEST:10.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:58:39.439: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 22 13:58:47.541: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 22 13:58:47.545: INFO: Pod pod-with-poststart-http-hook still exists
Feb 22 13:58:49.545: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 22 13:58:49.547: INFO: Pod pod-with-poststart-http-hook still exists
Feb 22 13:58:51.545: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 22 13:58:51.548: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 13:58:51.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7986" for this suite.
Feb 22 13:59:13.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 13:59:13.717: INFO: namespace container-lifecycle-hook-7986 deletion completed in 22.166549417s

â€¢ [SLOW TEST:34.277 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 13:59:13.717: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-c070d7ff-24c0-4458-b45b-6039ea214f44
STEP: Creating configMap with name cm-test-opt-upd-61650dd2-81f0-495c-8775-ca091b159cff
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c070d7ff-24c0-4458-b45b-6039ea214f44
STEP: Updating configmap cm-test-opt-upd-61650dd2-81f0-495c-8775-ca091b159cff
STEP: Creating configMap with name cm-test-opt-create-68e36d01-6e93-4a33-8bba-510a2d73d377
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:00:34.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5792" for this suite.
Feb 22 14:00:56.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:00:56.245: INFO: namespace projected-5792 deletion completed in 22.169026555s

â€¢ [SLOW TEST:102.528 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:00:56.245: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 22 14:00:56.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-4661'
Feb 22 14:00:56.442: INFO: stderr: ""
Feb 22 14:00:56.442: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 14:00:56.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4661'
Feb 22 14:00:56.513: INFO: stderr: ""
Feb 22 14:00:56.513: INFO: stdout: "update-demo-nautilus-sbvb5 update-demo-nautilus-w9kjr "
Feb 22 14:00:56.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-sbvb5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Feb 22 14:00:56.570: INFO: stderr: ""
Feb 22 14:00:56.570: INFO: stdout: ""
Feb 22 14:00:56.570: INFO: update-demo-nautilus-sbvb5 is created but not running
Feb 22 14:01:01.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4661'
Feb 22 14:01:01.636: INFO: stderr: ""
Feb 22 14:01:01.636: INFO: stdout: "update-demo-nautilus-sbvb5 update-demo-nautilus-w9kjr "
Feb 22 14:01:01.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-sbvb5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Feb 22 14:01:01.695: INFO: stderr: ""
Feb 22 14:01:01.695: INFO: stdout: "true"
Feb 22 14:01:01.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-sbvb5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Feb 22 14:01:01.753: INFO: stderr: ""
Feb 22 14:01:01.753: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:01:01.753: INFO: validating pod update-demo-nautilus-sbvb5
Feb 22 14:01:01.756: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:01:01.756: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:01:01.756: INFO: update-demo-nautilus-sbvb5 is verified up and running
Feb 22 14:01:01.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-w9kjr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Feb 22 14:01:01.814: INFO: stderr: ""
Feb 22 14:01:01.814: INFO: stdout: "true"
Feb 22 14:01:01.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-w9kjr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Feb 22 14:01:01.871: INFO: stderr: ""
Feb 22 14:01:01.871: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:01:01.871: INFO: validating pod update-demo-nautilus-w9kjr
Feb 22 14:01:01.874: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:01:01.874: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:01:01.874: INFO: update-demo-nautilus-w9kjr is verified up and running
STEP: using delete to clean up resources
Feb 22 14:01:01.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-4661'
Feb 22 14:01:01.941: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:01:01.941: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 22 14:01:01.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4661'
Feb 22 14:01:02.007: INFO: stderr: "No resources found.\n"
Feb 22 14:01:02.007: INFO: stdout: ""
Feb 22 14:01:02.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -l name=update-demo --namespace=kubectl-4661 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 14:01:02.067: INFO: stderr: ""
Feb 22 14:01:02.067: INFO: stdout: "update-demo-nautilus-sbvb5\nupdate-demo-nautilus-w9kjr\n"
Feb 22 14:01:02.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4661'
Feb 22 14:01:02.644: INFO: stderr: "No resources found.\n"
Feb 22 14:01:02.644: INFO: stdout: ""
Feb 22 14:01:02.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -l name=update-demo --namespace=kubectl-4661 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 14:01:02.716: INFO: stderr: ""
Feb 22 14:01:02.716: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:01:02.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4661" for this suite.
Feb 22 14:01:24.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:01:24.888: INFO: namespace kubectl-4661 deletion completed in 22.169192525s

â€¢ [SLOW TEST:28.642 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:01:24.888: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 22 14:01:24.926: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5376,SelfLink:/api/v1/namespaces/watch-5376/configmaps/e2e-watch-test-resource-version,UID:03f45af0-653d-44ca-b0c0-c1bdfcd48f04,ResourceVersion:7420,Generation:0,CreationTimestamp:2020-02-22 14:01:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 22 14:01:24.927: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5376,SelfLink:/api/v1/namespaces/watch-5376/configmaps/e2e-watch-test-resource-version,UID:03f45af0-653d-44ca-b0c0-c1bdfcd48f04,ResourceVersion:7421,Generation:0,CreationTimestamp:2020-02-22 14:01:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:01:24.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5376" for this suite.
Feb 22 14:01:30.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:01:31.096: INFO: namespace watch-5376 deletion completed in 6.166549725s

â€¢ [SLOW TEST:6.209 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:01:31.097: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 14:01:31.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8052'
Feb 22 14:01:31.179: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 22 14:01:31.180: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Feb 22 14:01:31.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete jobs e2e-test-nginx-job --namespace=kubectl-8052'
Feb 22 14:01:31.265: INFO: stderr: ""
Feb 22 14:01:31.265: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:01:31.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8052" for this suite.
Feb 22 14:01:37.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:01:37.503: INFO: namespace kubectl-8052 deletion completed in 6.222784142s

â€¢ [SLOW TEST:6.407 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:01:37.504: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 22 14:01:37.523: INFO: namespace kubectl-6380
Feb 22 14:01:37.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6380'
Feb 22 14:01:37.674: INFO: stderr: ""
Feb 22 14:01:37.674: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 22 14:01:38.677: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:01:38.677: INFO: Found 0 / 1
Feb 22 14:01:39.677: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:01:39.677: INFO: Found 0 / 1
Feb 22 14:01:40.677: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:01:40.677: INFO: Found 1 / 1
Feb 22 14:01:40.677: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 22 14:01:40.678: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:01:40.678: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 22 14:01:40.678: INFO: wait on redis-master startup in kubectl-6380 
Feb 22 14:01:40.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-grswj redis-master --namespace=kubectl-6380'
Feb 22 14:01:40.744: INFO: stderr: ""
Feb 22 14:01:40.744: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Feb 14:01:39.144 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Feb 14:01:39.144 # Server started, Redis version 3.2.12\n1:M 22 Feb 14:01:39.144 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Feb 14:01:39.144 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 22 14:01:40.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6380'
Feb 22 14:01:40.820: INFO: stderr: ""
Feb 22 14:01:40.820: INFO: stdout: "service/rm2 exposed\n"
Feb 22 14:01:40.826: INFO: Service rm2 in namespace kubectl-6380 found.
STEP: exposing service
Feb 22 14:01:42.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6380'
Feb 22 14:01:42.913: INFO: stderr: ""
Feb 22 14:01:42.913: INFO: stdout: "service/rm3 exposed\n"
Feb 22 14:01:42.916: INFO: Service rm3 in namespace kubectl-6380 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:01:44.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6380" for this suite.
Feb 22 14:01:58.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:01:59.087: INFO: namespace kubectl-6380 deletion completed in 14.16624628s

â€¢ [SLOW TEST:21.584 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:01:59.087: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:02:02.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9151" for this suite.
Feb 22 14:02:24.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:02:24.307: INFO: namespace replication-controller-9151 deletion completed in 22.166589519s

â€¢ [SLOW TEST:25.219 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:02:24.307: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Feb 22 14:02:24.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-2608'
Feb 22 14:02:24.485: INFO: stderr: ""
Feb 22 14:02:24.485: INFO: stdout: "pod/pause created\n"
Feb 22 14:02:24.485: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 22 14:02:24.485: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2608" to be "running and ready"
Feb 22 14:02:24.491: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.092227ms
Feb 22 14:02:26.493: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007207903s
Feb 22 14:02:28.495: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.009289367s
Feb 22 14:02:28.495: INFO: Pod "pause" satisfied condition "running and ready"
Feb 22 14:02:28.495: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 22 14:02:28.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 label pods pause testing-label=testing-label-value --namespace=kubectl-2608'
Feb 22 14:02:28.557: INFO: stderr: ""
Feb 22 14:02:28.557: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 22 14:02:28.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pod pause -L testing-label --namespace=kubectl-2608'
Feb 22 14:02:28.621: INFO: stderr: ""
Feb 22 14:02:28.621: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 22 14:02:28.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 label pods pause testing-label- --namespace=kubectl-2608'
Feb 22 14:02:28.686: INFO: stderr: ""
Feb 22 14:02:28.686: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 22 14:02:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pod pause -L testing-label --namespace=kubectl-2608'
Feb 22 14:02:28.744: INFO: stderr: ""
Feb 22 14:02:28.744: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Feb 22 14:02:28.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-2608'
Feb 22 14:02:28.816: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:02:28.816: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 22 14:02:28.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get rc,svc -l name=pause --no-headers --namespace=kubectl-2608'
Feb 22 14:02:28.901: INFO: stderr: "No resources found.\n"
Feb 22 14:02:28.901: INFO: stdout: ""
Feb 22 14:02:28.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -l name=pause --namespace=kubectl-2608 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 14:02:28.979: INFO: stderr: ""
Feb 22 14:02:28.979: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:02:28.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2608" for this suite.
Feb 22 14:02:34.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:02:35.148: INFO: namespace kubectl-2608 deletion completed in 6.166473951s

â€¢ [SLOW TEST:10.842 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:02:35.149: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 22 14:02:35.178: INFO: Waiting up to 5m0s for pod "downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5" in namespace "downward-api-878" to be "success or failure"
Feb 22 14:02:35.186: INFO: Pod "downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.673167ms
Feb 22 14:02:37.188: INFO: Pod "downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009658844s
Feb 22 14:02:39.190: INFO: Pod "downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011640948s
STEP: Saw pod success
Feb 22 14:02:39.190: INFO: Pod "downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5" satisfied condition "success or failure"
Feb 22 14:02:39.192: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5 container dapi-container: <nil>
STEP: delete the pod
Feb 22 14:02:39.209: INFO: Waiting for pod downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5 to disappear
Feb 22 14:02:39.212: INFO: Pod downward-api-79913fc8-f76d-4185-aa2e-4603c942c3d5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:02:39.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-878" for this suite.
Feb 22 14:02:45.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:02:45.383: INFO: namespace downward-api-878 deletion completed in 6.167507152s

â€¢ [SLOW TEST:10.235 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:02:45.384: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:02:45.443: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a92833c5-5f24-42b6-993d-5afa9ea78778", Controller:(*bool)(0xc002f313a6), BlockOwnerDeletion:(*bool)(0xc002f313a7)}}
Feb 22 14:02:45.453: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a76db3c1-fed1-46c8-955d-a65751256f5d", Controller:(*bool)(0xc0033964ce), BlockOwnerDeletion:(*bool)(0xc0033964cf)}}
Feb 22 14:02:45.457: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3720e478-acce-463c-8c63-3e581ebb7dc2", Controller:(*bool)(0xc002f315a6), BlockOwnerDeletion:(*bool)(0xc002f315a7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:02:50.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6937" for this suite.
Feb 22 14:02:56.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:02:56.634: INFO: namespace gc-6937 deletion completed in 6.167088939s

â€¢ [SLOW TEST:11.250 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:02:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 14:02:56.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7903'
Feb 22 14:02:56.728: INFO: stderr: ""
Feb 22 14:02:56.728: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Feb 22 14:02:56.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete pods e2e-test-nginx-pod --namespace=kubectl-7903'
Feb 22 14:03:09.042: INFO: stderr: ""
Feb 22 14:03:09.042: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:03:09.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7903" for this suite.
Feb 22 14:03:15.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:03:15.214: INFO: namespace kubectl-7903 deletion completed in 6.166188592s

â€¢ [SLOW TEST:18.580 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:03:15.215: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Feb 22 14:03:15.242: INFO: Waiting up to 5m0s for pod "client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126" in namespace "containers-7120" to be "success or failure"
Feb 22 14:03:15.245: INFO: Pod "client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396429ms
Feb 22 14:03:17.247: INFO: Pod "client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004613835s
Feb 22 14:03:19.249: INFO: Pod "client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006732109s
STEP: Saw pod success
Feb 22 14:03:19.249: INFO: Pod "client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126" satisfied condition "success or failure"
Feb 22 14:03:19.251: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126 container test-container: <nil>
STEP: delete the pod
Feb 22 14:03:19.266: INFO: Waiting for pod client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126 to disappear
Feb 22 14:03:19.267: INFO: Pod client-containers-a11482cc-ac1f-4f51-b1ef-66a44104a126 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:03:19.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7120" for this suite.
Feb 22 14:03:25.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:03:25.439: INFO: namespace containers-7120 deletion completed in 6.168883415s

â€¢ [SLOW TEST:10.224 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:03:25.439: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:03:25.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b" in namespace "projected-5422" to be "success or failure"
Feb 22 14:03:25.524: INFO: Pod "downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.564641ms
Feb 22 14:03:27.526: INFO: Pod "downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005714287s
Feb 22 14:03:29.528: INFO: Pod "downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007843145s
STEP: Saw pod success
Feb 22 14:03:29.528: INFO: Pod "downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b" satisfied condition "success or failure"
Feb 22 14:03:29.530: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b container client-container: <nil>
STEP: delete the pod
Feb 22 14:03:29.543: INFO: Waiting for pod downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b to disappear
Feb 22 14:03:29.547: INFO: Pod downwardapi-volume-02dbb098-8619-4cda-82d3-599ca9db4c7b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:03:29.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5422" for this suite.
Feb 22 14:03:35.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:03:35.717: INFO: namespace projected-5422 deletion completed in 6.165845052s

â€¢ [SLOW TEST:10.278 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:03:35.718: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-c1cb64d0-9b9a-4614-af04-8879a1084b9f
STEP: Creating a pod to test consume configMaps
Feb 22 14:03:35.800: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6" in namespace "projected-173" to be "success or failure"
Feb 22 14:03:35.804: INFO: Pod "pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917274ms
Feb 22 14:03:37.813: INFO: Pod "pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013238449s
STEP: Saw pod success
Feb 22 14:03:37.813: INFO: Pod "pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6" satisfied condition "success or failure"
Feb 22 14:03:37.815: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:03:37.831: INFO: Waiting for pod pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6 to disappear
Feb 22 14:03:37.832: INFO: Pod pod-projected-configmaps-da18e001-81d9-4690-befd-1ade2a71a8a6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:03:37.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-173" for this suite.
Feb 22 14:03:43.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:03:44.000: INFO: namespace projected-173 deletion completed in 6.164655282s

â€¢ [SLOW TEST:8.282 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:03:44.000: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:03:44.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8395" for this suite.
Feb 22 14:03:50.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:03:50.247: INFO: namespace services-8395 deletion completed in 6.171141204s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:6.247 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:03:50.247: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Feb 22 14:03:50.330: INFO: Waiting up to 5m0s for pod "client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216" in namespace "containers-8104" to be "success or failure"
Feb 22 14:03:50.334: INFO: Pod "client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286399ms
Feb 22 14:03:52.336: INFO: Pod "client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006166244s
Feb 22 14:03:54.338: INFO: Pod "client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008012753s
STEP: Saw pod success
Feb 22 14:03:54.338: INFO: Pod "client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216" satisfied condition "success or failure"
Feb 22 14:03:54.340: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216 container test-container: <nil>
STEP: delete the pod
Feb 22 14:03:54.362: INFO: Waiting for pod client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216 to disappear
Feb 22 14:03:54.363: INFO: Pod client-containers-d815dc34-4723-4681-9fab-a5a0d64d3216 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:03:54.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8104" for this suite.
Feb 22 14:04:00.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:04:00.534: INFO: namespace containers-8104 deletion completed in 6.167257856s

â€¢ [SLOW TEST:10.287 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:04:00.534: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:05:00.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8006" for this suite.
Feb 22 14:05:22.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:05:22.730: INFO: namespace container-probe-8006 deletion completed in 22.165889527s

â€¢ [SLOW TEST:82.196 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:05:22.731: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:05:22.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e" in namespace "projected-5207" to be "success or failure"
Feb 22 14:05:22.762: INFO: Pod "downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.229998ms
Feb 22 14:05:24.764: INFO: Pod "downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006300926s
Feb 22 14:05:26.766: INFO: Pod "downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008481319s
STEP: Saw pod success
Feb 22 14:05:26.766: INFO: Pod "downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e" satisfied condition "success or failure"
Feb 22 14:05:26.768: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e container client-container: <nil>
STEP: delete the pod
Feb 22 14:05:26.799: INFO: Waiting for pod downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e to disappear
Feb 22 14:05:26.803: INFO: Pod downwardapi-volume-51c1c925-0748-4764-ac01-ed5fc116a69e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:05:26.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5207" for this suite.
Feb 22 14:05:32.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:05:32.975: INFO: namespace projected-5207 deletion completed in 6.168985281s

â€¢ [SLOW TEST:10.244 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:05:32.975: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5145, will wait for the garbage collector to delete the pods
Feb 22 14:05:37.059: INFO: Deleting Job.batch foo took: 3.442749ms
Feb 22 14:05:39.559: INFO: Terminating Job.batch foo pods took: 2.500161015s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:06:11.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5145" for this suite.
Feb 22 14:06:17.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:06:17.729: INFO: namespace job-5145 deletion completed in 6.166298755s

â€¢ [SLOW TEST:44.754 seconds]
[sig-apps] Job
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:06:17.730: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 22 14:06:17.762: INFO: Waiting up to 5m0s for pod "pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269" in namespace "emptydir-7039" to be "success or failure"
Feb 22 14:06:17.773: INFO: Pod "pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269": Phase="Pending", Reason="", readiness=false. Elapsed: 10.906388ms
Feb 22 14:06:19.775: INFO: Pod "pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012831444s
STEP: Saw pod success
Feb 22 14:06:19.775: INFO: Pod "pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269" satisfied condition "success or failure"
Feb 22 14:06:19.776: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269 container test-container: <nil>
STEP: delete the pod
Feb 22 14:06:19.795: INFO: Waiting for pod pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269 to disappear
Feb 22 14:06:19.796: INFO: Pod pod-aa27fe34-4b10-4cb7-92d1-f8ca86fd1269 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:06:19.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7039" for this suite.
Feb 22 14:06:25.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:06:25.969: INFO: namespace emptydir-7039 deletion completed in 6.170626001s

â€¢ [SLOW TEST:8.240 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:06:25.970: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 22 14:06:25.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-2997'
Feb 22 14:06:26.381: INFO: stderr: ""
Feb 22 14:06:26.381: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 22 14:06:27.383: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:06:27.383: INFO: Found 0 / 1
Feb 22 14:06:28.383: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:06:28.383: INFO: Found 0 / 1
Feb 22 14:06:29.383: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:06:29.383: INFO: Found 1 / 1
Feb 22 14:06:29.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 22 14:06:29.385: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:06:29.385: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 22 14:06:29.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 patch pod redis-master-7cd9c --namespace=kubectl-2997 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 22 14:06:29.447: INFO: stderr: ""
Feb 22 14:06:29.447: INFO: stdout: "pod/redis-master-7cd9c patched\n"
STEP: checking annotations
Feb 22 14:06:29.452: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 14:06:29.452: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:06:29.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2997" for this suite.
Feb 22 14:06:51.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:06:51.621: INFO: namespace kubectl-2997 deletion completed in 22.167332253s

â€¢ [SLOW TEST:25.652 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:06:51.622: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 22 14:06:51.645: INFO: Waiting up to 5m0s for pod "downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9" in namespace "downward-api-8408" to be "success or failure"
Feb 22 14:06:51.650: INFO: Pod "downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752381ms
Feb 22 14:06:53.652: INFO: Pod "downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006811828s
Feb 22 14:06:55.654: INFO: Pod "downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008989221s
STEP: Saw pod success
Feb 22 14:06:55.654: INFO: Pod "downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9" satisfied condition "success or failure"
Feb 22 14:06:55.656: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9 container dapi-container: <nil>
STEP: delete the pod
Feb 22 14:06:55.673: INFO: Waiting for pod downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9 to disappear
Feb 22 14:06:55.674: INFO: Pod downward-api-a4a93a9b-d99f-48b3-bae9-9b6fe5d973c9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:06:55.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8408" for this suite.
Feb 22 14:07:01.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:07:01.843: INFO: namespace downward-api-8408 deletion completed in 6.166891417s

â€¢ [SLOW TEST:10.222 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:07:01.844: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:07:01.870: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033" in namespace "projected-6970" to be "success or failure"
Feb 22 14:07:01.877: INFO: Pod "downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033": Phase="Pending", Reason="", readiness=false. Elapsed: 6.720811ms
Feb 22 14:07:03.879: INFO: Pod "downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008917877s
Feb 22 14:07:05.881: INFO: Pod "downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011117562s
STEP: Saw pod success
Feb 22 14:07:05.881: INFO: Pod "downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033" satisfied condition "success or failure"
Feb 22 14:07:05.883: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033 container client-container: <nil>
STEP: delete the pod
Feb 22 14:07:05.906: INFO: Waiting for pod downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033 to disappear
Feb 22 14:07:05.908: INFO: Pod downwardapi-volume-3b233214-0c7a-40f0-9ff2-33801f9ae033 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:07:05.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6970" for this suite.
Feb 22 14:07:11.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:07:12.077: INFO: namespace projected-6970 deletion completed in 6.166922258s

â€¢ [SLOW TEST:10.233 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:07:12.077: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:07:12.107: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 14:07:12.112: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:12.115: INFO: Number of nodes with available pods: 0
Feb 22 14:07:12.115: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:07:13.118: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:13.120: INFO: Number of nodes with available pods: 0
Feb 22 14:07:13.120: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:07:14.118: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:14.119: INFO: Number of nodes with available pods: 1
Feb 22 14:07:14.119: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:07:15.118: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:15.120: INFO: Number of nodes with available pods: 2
Feb 22 14:07:15.120: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 22 14:07:15.141: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:15.141: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:15.144: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:16.148: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:16.148: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:16.156: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:17.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:17.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:17.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:18.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:18.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:18.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:18.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:19.146: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:19.146: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:19.146: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:19.148: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:20.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:20.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:20.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:20.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:21.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:21.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:21.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:21.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:22.146: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:22.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:22.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:22.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:23.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:23.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:23.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:23.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:24.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:24.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:24.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:24.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:25.152: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:25.152: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:25.152: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:25.154: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:26.151: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:26.151: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:26.151: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:26.159: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:27.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:27.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:27.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:27.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:28.147: INFO: Wrong image for pod: daemon-set-79qzq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:28.147: INFO: Pod daemon-set-79qzq is not available
Feb 22 14:07:28.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:28.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:29.146: INFO: Pod daemon-set-bbq7n is not available
Feb 22 14:07:29.146: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:29.148: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:30.146: INFO: Pod daemon-set-bbq7n is not available
Feb 22 14:07:30.146: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:30.148: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:31.147: INFO: Pod daemon-set-bbq7n is not available
Feb 22 14:07:31.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:31.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:32.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:32.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:33.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:33.147: INFO: Pod daemon-set-m7fq7 is not available
Feb 22 14:07:33.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:34.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:34.147: INFO: Pod daemon-set-m7fq7 is not available
Feb 22 14:07:34.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:35.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:35.147: INFO: Pod daemon-set-m7fq7 is not available
Feb 22 14:07:35.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:36.149: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:36.149: INFO: Pod daemon-set-m7fq7 is not available
Feb 22 14:07:36.155: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:37.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:37.147: INFO: Pod daemon-set-m7fq7 is not available
Feb 22 14:07:37.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:38.147: INFO: Wrong image for pod: daemon-set-m7fq7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 22 14:07:38.147: INFO: Pod daemon-set-m7fq7 is not available
Feb 22 14:07:38.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:39.147: INFO: Pod daemon-set-rg4xd is not available
Feb 22 14:07:39.149: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 22 14:07:39.152: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:39.154: INFO: Number of nodes with available pods: 1
Feb 22 14:07:39.154: INFO: Node ip-10-100-10-53.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:07:40.156: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:40.158: INFO: Number of nodes with available pods: 1
Feb 22 14:07:40.158: INFO: Node ip-10-100-10-53.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:07:41.156: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:41.158: INFO: Number of nodes with available pods: 1
Feb 22 14:07:41.158: INFO: Node ip-10-100-10-53.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:07:42.156: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:07:42.158: INFO: Number of nodes with available pods: 2
Feb 22 14:07:42.158: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5322, will wait for the garbage collector to delete the pods
Feb 22 14:07:42.222: INFO: Deleting DaemonSet.extensions daemon-set took: 3.860637ms
Feb 22 14:07:42.722: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.178533ms
Feb 22 14:07:49.124: INFO: Number of nodes with available pods: 0
Feb 22 14:07:49.124: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 14:07:49.126: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5322/daemonsets","resourceVersion":"9191"},"items":null}

Feb 22 14:07:49.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5322/pods","resourceVersion":"9191"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:07:49.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5322" for this suite.
Feb 22 14:07:55.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:07:55.301: INFO: namespace daemonsets-5322 deletion completed in 6.166386596s

â€¢ [SLOW TEST:43.224 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:07:55.301: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:07:55.322: INFO: Creating deployment "test-recreate-deployment"
Feb 22 14:07:55.329: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 22 14:07:55.338: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 22 14:07:57.342: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 22 14:07:57.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717977275, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717977275, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717977275, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717977275, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:07:59.346: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 22 14:07:59.350: INFO: Updating deployment test-recreate-deployment
Feb 22 14:07:59.350: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 22 14:07:59.424: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9439,SelfLink:/apis/apps/v1/namespaces/deployment-9439/deployments/test-recreate-deployment,UID:b3c94fa5-81fa-4df8-be11-b5ce3255d6b7,ResourceVersion:9294,Generation:2,CreationTimestamp:2020-02-22 14:07:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-02-22 14:07:59 +0000 UTC 2020-02-22 14:07:59 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-02-22 14:07:59 +0000 UTC 2020-02-22 14:07:55 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 22 14:07:59.427: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9439,SelfLink:/apis/apps/v1/namespaces/deployment-9439/replicasets/test-recreate-deployment-5c8c9cc69d,UID:b5f61013-cb93-4b2f-964a-676a941240c5,ResourceVersion:9293,Generation:1,CreationTimestamp:2020-02-22 14:07:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b3c94fa5-81fa-4df8-be11-b5ce3255d6b7 0xc0030f44a7 0xc0030f44a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 22 14:07:59.427: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 22 14:07:59.427: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9439,SelfLink:/apis/apps/v1/namespaces/deployment-9439/replicasets/test-recreate-deployment-6df85df6b9,UID:2c5c0aa1-11f1-4734-a750-42cac82806ea,ResourceVersion:9282,Generation:2,CreationTimestamp:2020-02-22 14:07:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment b3c94fa5-81fa-4df8-be11-b5ce3255d6b7 0xc0030f4567 0xc0030f4568}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 22 14:07:59.429: INFO: Pod "test-recreate-deployment-5c8c9cc69d-bbvlp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-bbvlp,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9439,SelfLink:/api/v1/namespaces/deployment-9439/pods/test-recreate-deployment-5c8c9cc69d-bbvlp,UID:f7dcc3c3-2153-40aa-9183-2c7ad9faaf69,ResourceVersion:9295,Generation:0,CreationTimestamp:2020-02-22 14:07:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d b5f61013-cb93-4b2f-964a-676a941240c5 0xc0030f4e47 0xc0030f4e48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-j4wlx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-j4wlx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-j4wlx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0030f4eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0030f4ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:07:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:07:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:07:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:07:59 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:,StartTime:2020-02-22 14:07:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:07:59.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9439" for this suite.
Feb 22 14:08:05.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:08:05.598: INFO: namespace deployment-9439 deletion completed in 6.167520348s

â€¢ [SLOW TEST:10.297 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:08:05.599: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-1059d2a4-71a6-487a-98b8-95bd0e53a4b0
STEP: Creating a pod to test consume secrets
Feb 22 14:08:05.628: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704" in namespace "projected-8744" to be "success or failure"
Feb 22 14:08:05.632: INFO: Pod "pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704": Phase="Pending", Reason="", readiness=false. Elapsed: 3.681335ms
Feb 22 14:08:07.634: INFO: Pod "pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005579854s
Feb 22 14:08:09.636: INFO: Pod "pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00773886s
STEP: Saw pod success
Feb 22 14:08:09.636: INFO: Pod "pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704" satisfied condition "success or failure"
Feb 22 14:08:09.638: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:08:09.652: INFO: Waiting for pod pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704 to disappear
Feb 22 14:08:09.655: INFO: Pod pod-projected-secrets-bbf4f96c-554c-43f4-ae28-891bfa341704 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:08:09.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8744" for this suite.
Feb 22 14:08:15.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:08:15.824: INFO: namespace projected-8744 deletion completed in 6.167163472s

â€¢ [SLOW TEST:10.226 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:08:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-00a49726-bdd5-45da-9f5e-979e22c17f00
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-00a49726-bdd5-45da-9f5e-979e22c17f00
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:09:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6516" for this suite.
Feb 22 14:09:46.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:09:46.230: INFO: namespace configmap-6516 deletion completed in 22.168681494s

â€¢ [SLOW TEST:90.406 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:09:46.230: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-224
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-224
STEP: Creating statefulset with conflicting port in namespace statefulset-224
STEP: Waiting until pod test-pod will start running in namespace statefulset-224
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-224
Feb 22 14:09:48.315: INFO: Observed stateful pod in namespace: statefulset-224, name: ss-0, uid: 1f292b22-d3bc-48e6-91f1-e5ad90897e61, status phase: Pending. Waiting for statefulset controller to delete.
Feb 22 14:09:49.035: INFO: Observed stateful pod in namespace: statefulset-224, name: ss-0, uid: 1f292b22-d3bc-48e6-91f1-e5ad90897e61, status phase: Failed. Waiting for statefulset controller to delete.
Feb 22 14:09:49.040: INFO: Observed stateful pod in namespace: statefulset-224, name: ss-0, uid: 1f292b22-d3bc-48e6-91f1-e5ad90897e61, status phase: Failed. Waiting for statefulset controller to delete.
Feb 22 14:09:49.047: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-224
STEP: Removing pod with conflicting port in namespace statefulset-224
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-224 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 22 14:09:53.074: INFO: Deleting all statefulset in ns statefulset-224
Feb 22 14:09:53.076: INFO: Scaling statefulset ss to 0
Feb 22 14:10:03.084: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:10:03.086: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:10:03.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-224" for this suite.
Feb 22 14:10:09.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:10:09.272: INFO: namespace statefulset-224 deletion completed in 6.169583094s

â€¢ [SLOW TEST:23.042 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:10:09.272: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 14:10:09.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7013'
Feb 22 14:10:09.368: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 22 14:10:09.368: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 22 14:10:09.382: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-nvflb]
Feb 22 14:10:09.382: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-nvflb" in namespace "kubectl-7013" to be "running and ready"
Feb 22 14:10:09.390: INFO: Pod "e2e-test-nginx-rc-nvflb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054223ms
Feb 22 14:10:11.392: INFO: Pod "e2e-test-nginx-rc-nvflb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009975115s
Feb 22 14:10:11.392: INFO: Pod "e2e-test-nginx-rc-nvflb" satisfied condition "running and ready"
Feb 22 14:10:11.392: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-nvflb]
Feb 22 14:10:11.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs rc/e2e-test-nginx-rc --namespace=kubectl-7013'
Feb 22 14:10:11.466: INFO: stderr: ""
Feb 22 14:10:11.466: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Feb 22 14:10:11.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete rc e2e-test-nginx-rc --namespace=kubectl-7013'
Feb 22 14:10:11.529: INFO: stderr: ""
Feb 22 14:10:11.529: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:10:11.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7013" for this suite.
Feb 22 14:10:17.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:10:17.702: INFO: namespace kubectl-7013 deletion completed in 6.169624423s

â€¢ [SLOW TEST:8.430 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:10:17.703: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-85b16383-f7cd-4b3a-ac11-11b6e3c6a11d
STEP: Creating a pod to test consume configMaps
Feb 22 14:10:17.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612" in namespace "configmap-9633" to be "success or failure"
Feb 22 14:10:17.784: INFO: Pod "pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612": Phase="Pending", Reason="", readiness=false. Elapsed: 1.976782ms
Feb 22 14:10:19.786: INFO: Pod "pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004057402s
Feb 22 14:10:21.788: INFO: Pod "pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006019655s
STEP: Saw pod success
Feb 22 14:10:21.788: INFO: Pod "pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612" satisfied condition "success or failure"
Feb 22 14:10:21.790: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:10:21.803: INFO: Waiting for pod pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612 to disappear
Feb 22 14:10:21.807: INFO: Pod pod-configmaps-7cc5e240-cd65-44e9-b17a-6f78c0f38612 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:10:21.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9633" for this suite.
Feb 22 14:10:27.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:10:27.977: INFO: namespace configmap-9633 deletion completed in 6.167606771s

â€¢ [SLOW TEST:10.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:10:27.977: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-4afffde6-fd61-4809-9c81-effc218b5989
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:10:30.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9182" for this suite.
Feb 22 14:10:52.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:10:52.209: INFO: namespace configmap-9182 deletion completed in 22.16678378s

â€¢ [SLOW TEST:24.232 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:10:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-de108591-fc42-45dc-a8ce-36da70cf6e9b
STEP: Creating a pod to test consume configMaps
Feb 22 14:10:52.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7" in namespace "configmap-9119" to be "success or failure"
Feb 22 14:10:52.240: INFO: Pod "pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.890283ms
Feb 22 14:10:54.242: INFO: Pod "pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007817768s
Feb 22 14:10:56.243: INFO: Pod "pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009628839s
STEP: Saw pod success
Feb 22 14:10:56.244: INFO: Pod "pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7" satisfied condition "success or failure"
Feb 22 14:10:56.245: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:10:56.257: INFO: Waiting for pod pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7 to disappear
Feb 22 14:10:56.261: INFO: Pod pod-configmaps-11cc3f32-1bbb-48d2-8cc1-725289dd84b7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:10:56.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9119" for this suite.
Feb 22 14:11:02.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:11:02.432: INFO: namespace configmap-9119 deletion completed in 6.168331327s

â€¢ [SLOW TEST:10.223 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:11:02.432: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:11:02.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9819" for this suite.
Feb 22 14:11:24.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:11:24.654: INFO: namespace kubelet-test-9819 deletion completed in 22.171923537s

â€¢ [SLOW TEST:22.222 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:11:24.655: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 22 14:11:28.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec pod-sharedvolume-df94fa32-5dda-4abd-9fb7-7884f65184f0 -c busybox-main-container --namespace=emptydir-3912 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 22 14:11:28.882: INFO: stderr: ""
Feb 22 14:11:28.882: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:11:28.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3912" for this suite.
Feb 22 14:11:34.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:11:35.055: INFO: namespace emptydir-3912 deletion completed in 6.169491179s

â€¢ [SLOW TEST:10.400 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:11:35.055: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-40718615-b1b8-4cb5-9d34-9ec23f560451
STEP: Creating a pod to test consume configMaps
Feb 22 14:11:35.082: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb" in namespace "projected-4449" to be "success or failure"
Feb 22 14:11:35.092: INFO: Pod "pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639563ms
Feb 22 14:11:37.093: INFO: Pod "pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011346963s
STEP: Saw pod success
Feb 22 14:11:37.093: INFO: Pod "pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb" satisfied condition "success or failure"
Feb 22 14:11:37.095: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:11:37.109: INFO: Waiting for pod pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb to disappear
Feb 22 14:11:37.112: INFO: Pod pod-projected-configmaps-b7045c15-9584-4032-9c84-3fca0950d7cb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:11:37.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4449" for this suite.
Feb 22 14:11:43.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:11:43.281: INFO: namespace projected-4449 deletion completed in 6.167115889s

â€¢ [SLOW TEST:8.226 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:11:43.281: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Feb 22 14:11:43.301: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-674502360 proxy --unix-socket=/tmp/kubectl-proxy-unix502706263/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:11:43.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5967" for this suite.
Feb 22 14:11:49.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:11:49.524: INFO: namespace kubectl-5967 deletion completed in 6.169075278s

â€¢ [SLOW TEST:6.243 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:11:49.524: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:11:49.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 version'
Feb 22 14:11:49.604: INFO: stderr: ""
Feb 22 14:11:49.604: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.10\", GitCommit:\"1bea6c00a7055edef03f1d4bb58b773fa8917f11\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T20:13:57Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.10\", GitCommit:\"1bea6c00a7055edef03f1d4bb58b773fa8917f11\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T20:05:26Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:11:49.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8844" for this suite.
Feb 22 14:11:55.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:11:55.774: INFO: namespace kubectl-8844 deletion completed in 6.166493078s

â€¢ [SLOW TEST:6.250 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:11:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 22 14:11:55.847: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:12:09.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1048" for this suite.
Feb 22 14:12:15.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:12:15.214: INFO: namespace pods-1048 deletion completed in 6.16621127s

â€¢ [SLOW TEST:19.440 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:12:15.214: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d27ab1f7-209c-48d2-abda-d5f7d42c9b03 in namespace container-probe-4730
Feb 22 14:12:17.250: INFO: Started pod busybox-d27ab1f7-209c-48d2-abda-d5f7d42c9b03 in namespace container-probe-4730
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 14:12:17.251: INFO: Initial restart count of pod busybox-d27ab1f7-209c-48d2-abda-d5f7d42c9b03 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:16:17.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4730" for this suite.
Feb 22 14:16:23.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:16:23.686: INFO: namespace container-probe-4730 deletion completed in 6.168630855s

â€¢ [SLOW TEST:248.472 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:16:23.687: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:16:23.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101" in namespace "projected-1843" to be "success or failure"
Feb 22 14:16:23.766: INFO: Pod "downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101": Phase="Pending", Reason="", readiness=false. Elapsed: 3.14787ms
Feb 22 14:16:25.768: INFO: Pod "downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005050159s
Feb 22 14:16:27.770: INFO: Pod "downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007049306s
STEP: Saw pod success
Feb 22 14:16:27.770: INFO: Pod "downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101" satisfied condition "success or failure"
Feb 22 14:16:27.772: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101 container client-container: <nil>
STEP: delete the pod
Feb 22 14:16:27.787: INFO: Waiting for pod downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101 to disappear
Feb 22 14:16:27.789: INFO: Pod downwardapi-volume-2eb4bd98-cfb5-448a-bb35-a7ba23407101 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:16:27.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1843" for this suite.
Feb 22 14:16:33.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:16:33.959: INFO: namespace projected-1843 deletion completed in 6.167898542s

â€¢ [SLOW TEST:10.272 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:16:33.959: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8fd924d9-b009-45d8-8058-7085cdbfd0ea
STEP: Creating a pod to test consume configMaps
Feb 22 14:16:33.986: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7" in namespace "projected-7760" to be "success or failure"
Feb 22 14:16:33.991: INFO: Pod "pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.91646ms
Feb 22 14:16:35.993: INFO: Pod "pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006892459s
Feb 22 14:16:37.995: INFO: Pod "pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008753713s
STEP: Saw pod success
Feb 22 14:16:37.995: INFO: Pod "pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7" satisfied condition "success or failure"
Feb 22 14:16:37.997: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:16:38.016: INFO: Waiting for pod pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7 to disappear
Feb 22 14:16:38.019: INFO: Pod pod-projected-configmaps-a86989f5-db50-484e-8450-0a2650de69f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:16:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7760" for this suite.
Feb 22 14:16:44.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:16:44.236: INFO: namespace projected-7760 deletion completed in 6.214769893s

â€¢ [SLOW TEST:10.277 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:16:44.237: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 22 14:16:52.287: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:16:52.293: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:16:54.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:16:54.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:16:56.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:16:56.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:16:58.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:16:58.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:00.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:00.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:02.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:02.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:04.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:04.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:06.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:06.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:08.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:08.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:10.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:10.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:12.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:12.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:14.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:14.297: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:16.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:16.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:18.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:18.295: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 22 14:17:20.293: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 22 14:17:20.295: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:17:20.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4698" for this suite.
Feb 22 14:17:42.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:17:42.473: INFO: namespace container-lifecycle-hook-4698 deletion completed in 22.169673248s

â€¢ [SLOW TEST:58.237 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:17:42.474: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-9990
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9990
STEP: Deleting pre-stop pod
Feb 22 14:17:57.526: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:17:57.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9990" for this suite.
Feb 22 14:18:35.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:18:35.702: INFO: namespace prestop-9990 deletion completed in 38.168780827s

â€¢ [SLOW TEST:53.229 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:18:35.703: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Feb 22 14:18:35.776: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 22 14:18:35.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6302'
Feb 22 14:18:36.238: INFO: stderr: ""
Feb 22 14:18:36.238: INFO: stdout: "service/redis-slave created\n"
Feb 22 14:18:36.239: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 22 14:18:36.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6302'
Feb 22 14:18:36.421: INFO: stderr: ""
Feb 22 14:18:36.421: INFO: stdout: "service/redis-master created\n"
Feb 22 14:18:36.421: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 22 14:18:36.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6302'
Feb 22 14:18:36.589: INFO: stderr: ""
Feb 22 14:18:36.589: INFO: stdout: "service/frontend created\n"
Feb 22 14:18:36.589: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 22 14:18:36.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6302'
Feb 22 14:18:36.758: INFO: stderr: ""
Feb 22 14:18:36.758: INFO: stdout: "deployment.apps/frontend created\n"
Feb 22 14:18:36.758: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 22 14:18:36.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6302'
Feb 22 14:18:36.932: INFO: stderr: ""
Feb 22 14:18:36.932: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 22 14:18:36.932: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 22 14:18:36.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-6302'
Feb 22 14:18:37.101: INFO: stderr: ""
Feb 22 14:18:37.101: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 22 14:18:37.101: INFO: Waiting for all frontend pods to be Running.
Feb 22 14:18:52.151: INFO: Waiting for frontend to serve content.
Feb 22 14:18:57.165: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 22 14:19:02.173: INFO: Trying to add a new entry to the guestbook.
Feb 22 14:19:02.181: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 22 14:19:02.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-6302'
Feb 22 14:19:02.266: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:19:02.266: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 14:19:02.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-6302'
Feb 22 14:19:02.353: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:19:02.353: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 14:19:02.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-6302'
Feb 22 14:19:02.432: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:19:02.432: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 14:19:02.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-6302'
Feb 22 14:19:02.515: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:19:02.515: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 14:19:02.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-6302'
Feb 22 14:19:02.577: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:19:02.577: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 22 14:19:02.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-6302'
Feb 22 14:19:02.636: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:19:02.636: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:19:02.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6302" for this suite.
Feb 22 14:19:40.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:19:40.804: INFO: namespace kubectl-6302 deletion completed in 38.166006863s

â€¢ [SLOW TEST:65.102 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:19:40.805: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Feb 22 14:19:40.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 cluster-info'
Feb 22 14:19:40.893: INFO: stderr: ""
Feb 22 14:19:40.893: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:19:40.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6785" for this suite.
Feb 22 14:19:46.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:19:47.064: INFO: namespace kubectl-6785 deletion completed in 6.168796183s

â€¢ [SLOW TEST:6.260 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:19:47.065: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:19:47.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c" in namespace "downward-api-976" to be "success or failure"
Feb 22 14:19:47.095: INFO: Pod "downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.71423ms
Feb 22 14:19:49.097: INFO: Pod "downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006888669s
Feb 22 14:19:51.099: INFO: Pod "downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009050031s
STEP: Saw pod success
Feb 22 14:19:51.099: INFO: Pod "downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c" satisfied condition "success or failure"
Feb 22 14:19:51.101: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c container client-container: <nil>
STEP: delete the pod
Feb 22 14:19:51.119: INFO: Waiting for pod downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c to disappear
Feb 22 14:19:51.121: INFO: Pod downwardapi-volume-97cceef2-f98c-4e5b-8f57-d1fce304979c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:19:51.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-976" for this suite.
Feb 22 14:19:57.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:19:57.290: INFO: namespace downward-api-976 deletion completed in 6.166680029s

â€¢ [SLOW TEST:10.225 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:19:57.290: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Feb 22 14:19:57.321: INFO: Waiting up to 5m0s for pod "var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204" in namespace "var-expansion-111" to be "success or failure"
Feb 22 14:19:57.325: INFO: Pod "var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204": Phase="Pending", Reason="", readiness=false. Elapsed: 3.10906ms
Feb 22 14:19:59.326: INFO: Pod "var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00500008s
STEP: Saw pod success
Feb 22 14:19:59.326: INFO: Pod "var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204" satisfied condition "success or failure"
Feb 22 14:19:59.328: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204 container dapi-container: <nil>
STEP: delete the pod
Feb 22 14:19:59.343: INFO: Waiting for pod var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204 to disappear
Feb 22 14:19:59.344: INFO: Pod var-expansion-0815a9b6-6858-4f1d-a498-6ede73820204 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:19:59.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-111" for this suite.
Feb 22 14:20:05.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:20:05.515: INFO: namespace var-expansion-111 deletion completed in 6.168363973s

â€¢ [SLOW TEST:8.225 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:20:05.515: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6171
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 14:20:05.590: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 22 14:20:25.654: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.88.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6171 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:20:25.654: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:20:26.797: INFO: Found all expected endpoints: [netserver-0]
Feb 22 14:20:26.808: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.8.14 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6171 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:20:26.808: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:20:27.969: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:20:27.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6171" for this suite.
Feb 22 14:20:49.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:20:50.140: INFO: namespace pod-network-test-6171 deletion completed in 22.167849039s

â€¢ [SLOW TEST:44.625 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:20:50.140: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-7gcf
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 14:20:50.174: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7gcf" in namespace "subpath-5652" to be "success or failure"
Feb 22 14:20:50.181: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.881137ms
Feb 22 14:20:52.190: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015209683s
Feb 22 14:20:54.192: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 4.017296993s
Feb 22 14:20:56.195: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 6.020330864s
Feb 22 14:20:58.197: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 8.022220524s
Feb 22 14:21:00.198: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 10.024119112s
Feb 22 14:21:02.200: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 12.026086989s
Feb 22 14:21:04.202: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 14.028080463s
Feb 22 14:21:06.205: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 16.031041383s
Feb 22 14:21:08.207: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 18.033118263s
Feb 22 14:21:10.209: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 20.035089221s
Feb 22 14:21:12.211: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Running", Reason="", readiness=true. Elapsed: 22.037083031s
Feb 22 14:21:14.213: INFO: Pod "pod-subpath-test-configmap-7gcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.039175437s
STEP: Saw pod success
Feb 22 14:21:14.214: INFO: Pod "pod-subpath-test-configmap-7gcf" satisfied condition "success or failure"
Feb 22 14:21:14.215: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-subpath-test-configmap-7gcf container test-container-subpath-configmap-7gcf: <nil>
STEP: delete the pod
Feb 22 14:21:14.241: INFO: Waiting for pod pod-subpath-test-configmap-7gcf to disappear
Feb 22 14:21:14.244: INFO: Pod pod-subpath-test-configmap-7gcf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7gcf
Feb 22 14:21:14.244: INFO: Deleting pod "pod-subpath-test-configmap-7gcf" in namespace "subpath-5652"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:21:14.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5652" for this suite.
Feb 22 14:21:20.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:21:20.420: INFO: namespace subpath-5652 deletion completed in 6.171506507s

â€¢ [SLOW TEST:30.280 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:21:20.421: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:21:44.505: INFO: Container started at 2020-02-22 14:21:21 +0000 UTC, pod became ready at 2020-02-22 14:21:43 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:21:44.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2068" for this suite.
Feb 22 14:21:58.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:21:58.674: INFO: namespace container-probe-2068 deletion completed in 14.166532957s

â€¢ [SLOW TEST:38.253 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:21:58.675: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 22 14:22:04.726: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:04.726: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:04.846: INFO: Exec stderr: ""
Feb 22 14:22:04.846: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:04.999: INFO: Exec stderr: ""
Feb 22 14:22:04.999: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:04.999: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.138: INFO: Exec stderr: ""
Feb 22 14:22:05.138: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.138: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.300: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 22 14:22:05.300: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.426: INFO: Exec stderr: ""
Feb 22 14:22:05.426: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.426: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.563: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 22 14:22:05.563: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.563: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.690: INFO: Exec stderr: ""
Feb 22 14:22:05.690: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.690: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.840: INFO: Exec stderr: ""
Feb 22 14:22:05.840: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.840: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:05.968: INFO: Exec stderr: ""
Feb 22 14:22:05.968: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6857 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:22:05.968: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:22:06.121: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:22:06.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6857" for this suite.
Feb 22 14:22:44.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:22:44.305: INFO: namespace e2e-kubelet-etc-hosts-6857 deletion completed in 38.177211792s

â€¢ [SLOW TEST:45.630 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:22:44.306: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 22 14:23:24.353: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0222 14:23:24.353964      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:23:24.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8886" for this suite.
Feb 22 14:23:30.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:23:30.525: INFO: namespace gc-8886 deletion completed in 6.168509433s

â€¢ [SLOW TEST:46.219 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:23:30.525: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-601f5565-a228-4637-81c9-29a1abbeb0af
STEP: Creating a pod to test consume secrets
Feb 22 14:23:30.558: INFO: Waiting up to 5m0s for pod "pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7" in namespace "secrets-1131" to be "success or failure"
Feb 22 14:23:30.562: INFO: Pod "pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.51484ms
Feb 22 14:23:32.564: INFO: Pod "pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006561722s
Feb 22 14:23:34.567: INFO: Pod "pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008883751s
STEP: Saw pod success
Feb 22 14:23:34.567: INFO: Pod "pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7" satisfied condition "success or failure"
Feb 22 14:23:34.569: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:23:34.583: INFO: Waiting for pod pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7 to disappear
Feb 22 14:23:34.587: INFO: Pod pod-secrets-e4fd94d1-d25b-4440-9d30-01f39a62d9a7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:23:34.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1131" for this suite.
Feb 22 14:23:40.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:23:40.755: INFO: namespace secrets-1131 deletion completed in 6.166643984s

â€¢ [SLOW TEST:10.230 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:23:40.756: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 22 14:23:40.782: INFO: Waiting up to 5m0s for pod "pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c" in namespace "emptydir-3404" to be "success or failure"
Feb 22 14:23:40.787: INFO: Pod "pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.788794ms
Feb 22 14:23:42.789: INFO: Pod "pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006748392s
STEP: Saw pod success
Feb 22 14:23:42.789: INFO: Pod "pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c" satisfied condition "success or failure"
Feb 22 14:23:42.790: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c container test-container: <nil>
STEP: delete the pod
Feb 22 14:23:42.809: INFO: Waiting for pod pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c to disappear
Feb 22 14:23:42.811: INFO: Pod pod-8fe67d07-8e2b-4153-9415-03ac25dcb31c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:23:42.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3404" for this suite.
Feb 22 14:23:48.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:23:48.982: INFO: namespace emptydir-3404 deletion completed in 6.166654153s

â€¢ [SLOW TEST:8.226 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:23:48.983: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-6456
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6456 to expose endpoints map[]
Feb 22 14:23:49.015: INFO: successfully validated that service endpoint-test2 in namespace services-6456 exposes endpoints map[] (1.706451ms elapsed)
STEP: Creating pod pod1 in namespace services-6456
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6456 to expose endpoints map[pod1:[80]]
Feb 22 14:23:51.037: INFO: successfully validated that service endpoint-test2 in namespace services-6456 exposes endpoints map[pod1:[80]] (2.017431473s elapsed)
STEP: Creating pod pod2 in namespace services-6456
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6456 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 22 14:23:55.085: INFO: successfully validated that service endpoint-test2 in namespace services-6456 exposes endpoints map[pod1:[80] pod2:[80]] (4.043949574s elapsed)
STEP: Deleting pod pod1 in namespace services-6456
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6456 to expose endpoints map[pod2:[80]]
Feb 22 14:23:55.102: INFO: successfully validated that service endpoint-test2 in namespace services-6456 exposes endpoints map[pod2:[80]] (10.268687ms elapsed)
STEP: Deleting pod pod2 in namespace services-6456
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6456 to expose endpoints map[]
Feb 22 14:23:55.112: INFO: successfully validated that service endpoint-test2 in namespace services-6456 exposes endpoints map[] (2.547468ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:23:55.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6456" for this suite.
Feb 22 14:24:17.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:24:17.310: INFO: namespace services-6456 deletion completed in 22.17489162s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:28.327 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:24:17.310: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:24:17.333: INFO: Creating ReplicaSet my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0
Feb 22 14:24:17.343: INFO: Pod name my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0: Found 1 pods out of 1
Feb 22 14:24:17.343: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0" is running
Feb 22 14:24:21.353: INFO: Pod "my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0-rp9h8" is running (conditions: [])
Feb 22 14:24:21.353: INFO: Trying to dial the pod
Feb 22 14:24:26.359: INFO: Controller my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0: Got expected result from replica 1 [my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0-rp9h8]: "my-hostname-basic-4fd01a71-18b7-495c-bc0d-32635c63b0e0-rp9h8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:24:26.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6294" for this suite.
Feb 22 14:24:32.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:24:32.528: INFO: namespace replicaset-6294 deletion completed in 6.166986187s

â€¢ [SLOW TEST:15.218 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:24:32.529: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-5239afbb-5f81-473f-9b02-803780d9e60b
STEP: Creating a pod to test consume secrets
Feb 22 14:24:32.561: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1" in namespace "projected-3361" to be "success or failure"
Feb 22 14:24:32.565: INFO: Pod "pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.364424ms
Feb 22 14:24:34.567: INFO: Pod "pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006298948s
Feb 22 14:24:36.569: INFO: Pod "pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008328655s
STEP: Saw pod success
Feb 22 14:24:36.569: INFO: Pod "pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1" satisfied condition "success or failure"
Feb 22 14:24:36.571: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:24:36.586: INFO: Waiting for pod pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1 to disappear
Feb 22 14:24:36.587: INFO: Pod pod-projected-secrets-6711d1cb-a5ac-4b19-9c75-728bf6cf56a1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:24:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3361" for this suite.
Feb 22 14:24:42.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:24:42.756: INFO: namespace projected-3361 deletion completed in 6.166525484s

â€¢ [SLOW TEST:10.227 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:24:42.756: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d in namespace container-probe-2765
Feb 22 14:24:46.842: INFO: Started pod liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d in namespace container-probe-2765
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 14:24:46.844: INFO: Initial restart count of pod liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d is 0
Feb 22 14:25:04.869: INFO: Restart count of pod container-probe-2765/liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d is now 1 (18.025467427s elapsed)
Feb 22 14:25:22.890: INFO: Restart count of pod container-probe-2765/liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d is now 2 (36.046902116s elapsed)
Feb 22 14:25:42.913: INFO: Restart count of pod container-probe-2765/liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d is now 3 (56.069829445s elapsed)
Feb 22 14:26:02.935: INFO: Restart count of pod container-probe-2765/liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d is now 4 (1m16.091140255s elapsed)
Feb 22 14:26:24.958: INFO: Restart count of pod container-probe-2765/liveness-7eb72de5-4b9a-4ab5-8c40-de2d49f3ac6d is now 5 (1m38.114015153s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:26:24.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2765" for this suite.
Feb 22 14:26:30.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:26:31.147: INFO: namespace container-probe-2765 deletion completed in 6.175973866s

â€¢ [SLOW TEST:108.391 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:26:31.147: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:26:31.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12" in namespace "projected-7015" to be "success or failure"
Feb 22 14:26:31.176: INFO: Pod "downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327367ms
Feb 22 14:26:33.178: INFO: Pod "downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004240432s
Feb 22 14:26:35.180: INFO: Pod "downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006100193s
STEP: Saw pod success
Feb 22 14:26:35.180: INFO: Pod "downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12" satisfied condition "success or failure"
Feb 22 14:26:35.182: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12 container client-container: <nil>
STEP: delete the pod
Feb 22 14:26:35.199: INFO: Waiting for pod downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12 to disappear
Feb 22 14:26:35.201: INFO: Pod downwardapi-volume-ecad3e3e-7b3d-4501-81a8-f3766ab13a12 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:26:35.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7015" for this suite.
Feb 22 14:26:41.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:26:41.369: INFO: namespace projected-7015 deletion completed in 6.166079132s

â€¢ [SLOW TEST:10.222 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:26:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6445
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-6445
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6445
Feb 22 14:26:41.450: INFO: Found 0 stateful pods, waiting for 1
Feb 22 14:26:51.453: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 22 14:26:51.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:26:51.660: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:26:51.661: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:26:51.661: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:26:51.663: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 22 14:27:01.665: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:27:01.665: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:27:01.674: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:01.674: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:01.674: INFO: 
Feb 22 14:27:01.674: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 22 14:27:02.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997414196s
Feb 22 14:27:03.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995442106s
Feb 22 14:27:04.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99325538s
Feb 22 14:27:05.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990919654s
Feb 22 14:27:06.685: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.988898705s
Feb 22 14:27:07.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.986373922s
Feb 22 14:27:08.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.984321666s
Feb 22 14:27:09.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.98179302s
Feb 22 14:27:10.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 979.513617ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6445
Feb 22 14:27:11.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:27:11.896: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:27:11.896: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:27:11.896: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:27:11.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:27:12.107: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 22 14:27:12.107: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:27:12.107: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:27:12.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:27:12.304: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 22 14:27:12.304: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:27:12.304: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:27:12.307: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 22 14:27:22.309: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:27:22.309: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:27:22.309: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 22 14:27:22.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:27:22.514: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:27:22.514: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:27:22.514: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:27:22.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:27:22.730: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:27:22.730: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:27:22.730: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:27:22.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-6445 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:27:22.934: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:27:22.934: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:27:22.934: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:27:22.934: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:27:22.936: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 22 14:27:32.940: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:27:32.940: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:27:32.940: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:27:32.948: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:32.948: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:32.948: INFO: ss-1  ip-10-100-10-53.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:32.948: INFO: ss-2  ip-10-100-10-41.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:32.948: INFO: 
Feb 22 14:27:32.948: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 22 14:27:33.950: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:33.950: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:33.950: INFO: ss-1  ip-10-100-10-53.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:33.950: INFO: ss-2  ip-10-100-10-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:33.950: INFO: 
Feb 22 14:27:33.950: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 22 14:27:34.952: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:34.953: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:34.953: INFO: ss-1  ip-10-100-10-53.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:34.953: INFO: ss-2  ip-10-100-10-41.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:34.953: INFO: 
Feb 22 14:27:34.953: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 22 14:27:35.955: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:35.955: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:35.955: INFO: ss-1  ip-10-100-10-53.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:35.955: INFO: 
Feb 22 14:27:35.955: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 22 14:27:36.957: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:36.957: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:36.957: INFO: ss-1  ip-10-100-10-53.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:36.957: INFO: 
Feb 22 14:27:36.957: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 22 14:27:37.960: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:37.960: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:37.960: INFO: ss-1  ip-10-100-10-53.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:01 +0000 UTC  }]
Feb 22 14:27:37.960: INFO: 
Feb 22 14:27:37.960: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 22 14:27:38.963: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Feb 22 14:27:38.963: INFO: ss-0  ip-10-100-10-41.eu-west-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:27:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:26:41 +0000 UTC  }]
Feb 22 14:27:38.963: INFO: 
Feb 22 14:27:38.963: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 22 14:27:39.965: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.98042046s
Feb 22 14:27:40.967: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.978361659s
Feb 22 14:27:41.969: INFO: Verifying statefulset ss doesn't scale past 0 for another 976.201115ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6445
Feb 22 14:27:42.972: INFO: Scaling statefulset ss to 0
Feb 22 14:27:42.977: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 22 14:27:42.978: INFO: Deleting all statefulset in ns statefulset-6445
Feb 22 14:27:42.980: INFO: Scaling statefulset ss to 0
Feb 22 14:27:42.984: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:27:42.986: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:27:42.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6445" for this suite.
Feb 22 14:27:49.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:27:49.166: INFO: namespace statefulset-6445 deletion completed in 6.16776649s

â€¢ [SLOW TEST:67.796 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:27:49.166: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 22 14:27:49.185: INFO: PodSpec: initContainers in spec.initContainers
Feb 22 14:28:29.809: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2ec3fca3-75fb-42ec-aef4-04209cd49bf5", GenerateName:"", Namespace:"init-container-1935", SelfLink:"/api/v1/namespaces/init-container-1935/pods/pod-init-2ec3fca3-75fb-42ec-aef4-04209cd49bf5", UID:"f59a6754-711b-4973-9375-0a6c3bd840e2", ResourceVersion:"14524", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717978469, loc:(*time.Location)(0x7ed4a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"185726065"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.16.88.93/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-nhz4z", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0009a2940), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nhz4z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nhz4z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-nhz4z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002afa078), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-100-10-53.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a618c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002afa0f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002afa110)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002afa118), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002afa11c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717978469, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717978469, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717978469, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717978469, loc:(*time.Location)(0x7ed4a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.100.10.53", PodIP:"172.16.88.93", StartTime:(*v1.Time)(0xc0013ac300), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c37b90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c37c00)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://a6e0445cf17e0497628b2d7db6bf4b0e3ee65cef8cd686c23d3940bff6cf27e0"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0013ac340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0013ac320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:28:29.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1935" for this suite.
Feb 22 14:28:51.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:28:51.983: INFO: namespace init-container-1935 deletion completed in 22.167472233s

â€¢ [SLOW TEST:62.817 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:28:51.983: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-34c832e8-94e2-498d-8b67-7f0708638f44
STEP: Creating a pod to test consume configMaps
Feb 22 14:28:52.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a" in namespace "configmap-2405" to be "success or failure"
Feb 22 14:28:52.018: INFO: Pod "pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.196676ms
Feb 22 14:28:54.020: INFO: Pod "pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007094595s
Feb 22 14:28:56.035: INFO: Pod "pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021667197s
STEP: Saw pod success
Feb 22 14:28:56.035: INFO: Pod "pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a" satisfied condition "success or failure"
Feb 22 14:28:56.037: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:28:56.056: INFO: Waiting for pod pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a to disappear
Feb 22 14:28:56.057: INFO: Pod pod-configmaps-5edcda7a-92c7-42da-8eb2-1a6d3a41b74a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:28:56.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2405" for this suite.
Feb 22 14:29:02.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:29:02.230: INFO: namespace configmap-2405 deletion completed in 6.169506325s

â€¢ [SLOW TEST:10.247 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:29:02.230: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:29:02.303: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:29:06.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8352" for this suite.
Feb 22 14:29:52.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:29:52.501: INFO: namespace pods-8352 deletion completed in 46.168048392s

â€¢ [SLOW TEST:50.271 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:29:52.501: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7969
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7969
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7969
Feb 22 14:29:52.542: INFO: Found 0 stateful pods, waiting for 1
Feb 22 14:30:02.545: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 22 14:30:02.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:30:02.875: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:30:02.875: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:30:02.875: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:30:02.877: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 22 14:30:12.880: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:30:12.880: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:30:12.889: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999816s
Feb 22 14:30:13.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996093054s
Feb 22 14:30:14.894: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993643524s
Feb 22 14:30:15.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991459612s
Feb 22 14:30:16.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989198861s
Feb 22 14:30:17.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.986523446s
Feb 22 14:30:18.903: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984258839s
Feb 22 14:30:19.906: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.982174166s
Feb 22 14:30:20.908: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.979992634s
Feb 22 14:30:21.910: INFO: Verifying statefulset ss doesn't scale past 1 for another 977.613904ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7969
Feb 22 14:30:22.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:30:23.119: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:30:23.119: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:30:23.120: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:30:23.122: INFO: Found 1 stateful pods, waiting for 3
Feb 22 14:30:33.125: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:30:33.125: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:30:33.125: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 22 14:30:33.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:30:33.318: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:30:33.318: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:30:33.318: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:30:33.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:30:33.530: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:30:33.530: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:30:33.530: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:30:33.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:30:33.733: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:30:33.733: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:30:33.733: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:30:33.733: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:30:33.739: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 22 14:30:43.743: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:30:43.743: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:30:43.743: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 22 14:30:43.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999774s
Feb 22 14:30:44.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997621074s
Feb 22 14:30:45.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995123635s
Feb 22 14:30:46.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992785219s
Feb 22 14:30:47.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990421424s
Feb 22 14:30:48.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98799653s
Feb 22 14:30:49.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985558377s
Feb 22 14:30:50.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982941254s
Feb 22 14:30:51.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.980591614s
Feb 22 14:30:52.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 978.089108ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7969
Feb 22 14:30:53.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:30:53.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:30:53.963: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:30:53.963: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:30:53.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:30:54.154: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:30:54.154: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:30:54.154: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:30:54.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-7969 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:30:54.345: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:30:54.345: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:30:54.345: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:30:54.345: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 22 14:31:04.354: INFO: Deleting all statefulset in ns statefulset-7969
Feb 22 14:31:04.356: INFO: Scaling statefulset ss to 0
Feb 22 14:31:04.361: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:31:04.362: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:31:04.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7969" for this suite.
Feb 22 14:31:10.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:31:10.544: INFO: namespace statefulset-7969 deletion completed in 6.168727649s

â€¢ [SLOW TEST:78.043 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:31:10.544: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300
Feb 22 14:31:10.569: INFO: Pod name my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300: Found 0 pods out of 1
Feb 22 14:31:15.571: INFO: Pod name my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300: Found 1 pods out of 1
Feb 22 14:31:15.571: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300" are running
Feb 22 14:31:15.573: INFO: Pod "my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300-tr7t7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-22 14:31:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-22 14:31:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-22 14:31:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-22 14:31:10 +0000 UTC Reason: Message:}])
Feb 22 14:31:15.573: INFO: Trying to dial the pod
Feb 22 14:31:20.579: INFO: Controller my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300: Got expected result from replica 1 [my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300-tr7t7]: "my-hostname-basic-c7d72b98-807d-4adf-979c-adcea46f8300-tr7t7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:31:20.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8831" for this suite.
Feb 22 14:31:26.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:31:26.759: INFO: namespace replication-controller-8831 deletion completed in 6.176452783s

â€¢ [SLOW TEST:16.215 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:31:26.759: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:31:26.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617" in namespace "downward-api-8143" to be "success or failure"
Feb 22 14:31:26.815: INFO: Pod "downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617": Phase="Pending", Reason="", readiness=false. Elapsed: 17.854293ms
Feb 22 14:31:28.819: INFO: Pod "downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022256358s
Feb 22 14:31:30.821: INFO: Pod "downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024352502s
STEP: Saw pod success
Feb 22 14:31:30.821: INFO: Pod "downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617" satisfied condition "success or failure"
Feb 22 14:31:30.823: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617 container client-container: <nil>
STEP: delete the pod
Feb 22 14:31:30.837: INFO: Waiting for pod downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617 to disappear
Feb 22 14:31:30.839: INFO: Pod downwardapi-volume-1ab4056b-714b-4eef-ae82-11e079fd8617 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:31:30.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8143" for this suite.
Feb 22 14:31:36.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:31:37.014: INFO: namespace downward-api-8143 deletion completed in 6.172905457s

â€¢ [SLOW TEST:10.255 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:31:37.015: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tj2lp in namespace proxy-4443
I0222 14:31:37.050298      18 runners.go:180] Created replication controller with name: proxy-service-tj2lp, namespace: proxy-4443, replica count: 1
I0222 14:31:38.100611      18 runners.go:180] proxy-service-tj2lp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 14:31:39.100763      18 runners.go:180] proxy-service-tj2lp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 14:31:40.100899      18 runners.go:180] proxy-service-tj2lp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0222 14:31:41.101065      18 runners.go:180] proxy-service-tj2lp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0222 14:31:42.101243      18 runners.go:180] proxy-service-tj2lp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 14:31:42.103: INFO: setup took 5.069882779s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 22 14:31:42.109: INFO: (0) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 6.503283ms)
Feb 22 14:31:42.110: INFO: (0) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.666043ms)
Feb 22 14:31:42.112: INFO: (0) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 8.765597ms)
Feb 22 14:31:42.119: INFO: (0) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 15.292071ms)
Feb 22 14:31:42.119: INFO: (0) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 15.896055ms)
Feb 22 14:31:42.119: INFO: (0) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 15.684473ms)
Feb 22 14:31:42.121: INFO: (0) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 18.196315ms)
Feb 22 14:31:42.121: INFO: (0) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 18.303184ms)
Feb 22 14:31:42.121: INFO: (0) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 18.365204ms)
Feb 22 14:31:42.121: INFO: (0) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 18.178914ms)
Feb 22 14:31:42.121: INFO: (0) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 18.255217ms)
Feb 22 14:31:42.122: INFO: (0) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 18.075797ms)
Feb 22 14:31:42.122: INFO: (0) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 18.487926ms)
Feb 22 14:31:42.122: INFO: (0) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 18.479909ms)
Feb 22 14:31:42.124: INFO: (0) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 21.039303ms)
Feb 22 14:31:42.126: INFO: (0) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 22.969554ms)
Feb 22 14:31:42.132: INFO: (1) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 5.26033ms)
Feb 22 14:31:42.132: INFO: (1) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 5.456151ms)
Feb 22 14:31:42.132: INFO: (1) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 5.663444ms)
Feb 22 14:31:42.135: INFO: (1) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.209588ms)
Feb 22 14:31:42.136: INFO: (1) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 8.99101ms)
Feb 22 14:31:42.136: INFO: (1) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 9.81598ms)
Feb 22 14:31:42.136: INFO: (1) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 9.72806ms)
Feb 22 14:31:42.136: INFO: (1) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.775555ms)
Feb 22 14:31:42.137: INFO: (1) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.750952ms)
Feb 22 14:31:42.138: INFO: (1) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.921191ms)
Feb 22 14:31:42.138: INFO: (1) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 11.237291ms)
Feb 22 14:31:42.138: INFO: (1) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 11.496708ms)
Feb 22 14:31:42.138: INFO: (1) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 11.929047ms)
Feb 22 14:31:42.139: INFO: (1) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 11.80641ms)
Feb 22 14:31:42.139: INFO: (1) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 12.007938ms)
Feb 22 14:31:42.139: INFO: (1) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 11.716596ms)
Feb 22 14:31:42.146: INFO: (2) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 7.072191ms)
Feb 22 14:31:42.147: INFO: (2) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 7.96257ms)
Feb 22 14:31:42.148: INFO: (2) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.477523ms)
Feb 22 14:31:42.148: INFO: (2) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 8.731601ms)
Feb 22 14:31:42.148: INFO: (2) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.835881ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.83751ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.876618ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 10.366423ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.313796ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 10.133382ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 10.244206ms)
Feb 22 14:31:42.149: INFO: (2) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.329514ms)
Feb 22 14:31:42.150: INFO: (2) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 10.316143ms)
Feb 22 14:31:42.150: INFO: (2) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.633305ms)
Feb 22 14:31:42.150: INFO: (2) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 10.54851ms)
Feb 22 14:31:42.150: INFO: (2) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.81892ms)
Feb 22 14:31:42.156: INFO: (3) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 5.753999ms)
Feb 22 14:31:42.156: INFO: (3) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 5.794306ms)
Feb 22 14:31:42.157: INFO: (3) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 7.085867ms)
Feb 22 14:31:42.158: INFO: (3) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 7.574232ms)
Feb 22 14:31:42.158: INFO: (3) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 7.78654ms)
Feb 22 14:31:42.159: INFO: (3) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.50068ms)
Feb 22 14:31:42.159: INFO: (3) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 8.575433ms)
Feb 22 14:31:42.159: INFO: (3) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 8.714333ms)
Feb 22 14:31:42.159: INFO: (3) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 8.618379ms)
Feb 22 14:31:42.159: INFO: (3) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.206264ms)
Feb 22 14:31:42.159: INFO: (3) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 9.297087ms)
Feb 22 14:31:42.161: INFO: (3) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.731098ms)
Feb 22 14:31:42.161: INFO: (3) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 10.590232ms)
Feb 22 14:31:42.161: INFO: (3) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 11.214815ms)
Feb 22 14:31:42.161: INFO: (3) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.991807ms)
Feb 22 14:31:42.161: INFO: (3) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 11.085803ms)
Feb 22 14:31:42.168: INFO: (4) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 6.798827ms)
Feb 22 14:31:42.169: INFO: (4) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 6.870196ms)
Feb 22 14:31:42.169: INFO: (4) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 7.363274ms)
Feb 22 14:31:42.170: INFO: (4) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 8.704123ms)
Feb 22 14:31:42.171: INFO: (4) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 9.713927ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 10.340197ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 10.283219ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 10.702186ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 10.683724ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.745919ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.796512ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.850903ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 11.056032ms)
Feb 22 14:31:42.172: INFO: (4) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 11.02792ms)
Feb 22 14:31:42.173: INFO: (4) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.73451ms)
Feb 22 14:31:42.174: INFO: (4) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 11.9486ms)
Feb 22 14:31:42.180: INFO: (5) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.105749ms)
Feb 22 14:31:42.180: INFO: (5) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 5.999907ms)
Feb 22 14:31:42.180: INFO: (5) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 6.604345ms)
Feb 22 14:31:42.181: INFO: (5) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 7.193896ms)
Feb 22 14:31:42.181: INFO: (5) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 7.544429ms)
Feb 22 14:31:42.182: INFO: (5) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.391602ms)
Feb 22 14:31:42.182: INFO: (5) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 8.275299ms)
Feb 22 14:31:42.182: INFO: (5) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 8.421186ms)
Feb 22 14:31:42.182: INFO: (5) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 8.671979ms)
Feb 22 14:31:42.182: INFO: (5) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 8.43635ms)
Feb 22 14:31:42.184: INFO: (5) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 9.870596ms)
Feb 22 14:31:42.185: INFO: (5) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.895729ms)
Feb 22 14:31:42.185: INFO: (5) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 10.920084ms)
Feb 22 14:31:42.185: INFO: (5) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 11.079684ms)
Feb 22 14:31:42.185: INFO: (5) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 11.471086ms)
Feb 22 14:31:42.185: INFO: (5) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 11.616512ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.715014ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 9.55217ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 9.053766ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 9.191832ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.154973ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.247613ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 9.282493ms)
Feb 22 14:31:42.195: INFO: (6) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 9.524ms)
Feb 22 14:31:42.196: INFO: (6) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.651445ms)
Feb 22 14:31:42.196: INFO: (6) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 10.231763ms)
Feb 22 14:31:42.196: INFO: (6) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.209794ms)
Feb 22 14:31:42.197: INFO: (6) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.878144ms)
Feb 22 14:31:42.197: INFO: (6) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 10.6036ms)
Feb 22 14:31:42.196: INFO: (6) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.291912ms)
Feb 22 14:31:42.197: INFO: (6) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 11.161613ms)
Feb 22 14:31:42.197: INFO: (6) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 11.14907ms)
Feb 22 14:31:42.204: INFO: (7) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 6.42872ms)
Feb 22 14:31:42.205: INFO: (7) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 7.157674ms)
Feb 22 14:31:42.205: INFO: (7) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 6.799047ms)
Feb 22 14:31:42.205: INFO: (7) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 7.157429ms)
Feb 22 14:31:42.205: INFO: (7) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 7.465393ms)
Feb 22 14:31:42.206: INFO: (7) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 8.935329ms)
Feb 22 14:31:42.207: INFO: (7) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 8.784593ms)
Feb 22 14:31:42.207: INFO: (7) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 8.637346ms)
Feb 22 14:31:42.206: INFO: (7) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.793465ms)
Feb 22 14:31:42.206: INFO: (7) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 8.721953ms)
Feb 22 14:31:42.207: INFO: (7) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 8.954985ms)
Feb 22 14:31:42.209: INFO: (7) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.762187ms)
Feb 22 14:31:42.209: INFO: (7) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 11.470581ms)
Feb 22 14:31:42.209: INFO: (7) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 11.690649ms)
Feb 22 14:31:42.209: INFO: (7) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 11.639166ms)
Feb 22 14:31:42.209: INFO: (7) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 11.513919ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 7.223415ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 7.123062ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 7.235511ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 7.745255ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 7.613054ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 7.692927ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 7.715209ms)
Feb 22 14:31:42.217: INFO: (8) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 7.96925ms)
Feb 22 14:31:42.218: INFO: (8) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.408518ms)
Feb 22 14:31:42.218: INFO: (8) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 8.643541ms)
Feb 22 14:31:42.219: INFO: (8) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 9.179683ms)
Feb 22 14:31:42.221: INFO: (8) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 11.624069ms)
Feb 22 14:31:42.222: INFO: (8) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 11.84078ms)
Feb 22 14:31:42.222: INFO: (8) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 12.088601ms)
Feb 22 14:31:42.222: INFO: (8) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 12.070543ms)
Feb 22 14:31:42.222: INFO: (8) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 12.132597ms)
Feb 22 14:31:42.226: INFO: (9) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 3.69144ms)
Feb 22 14:31:42.230: INFO: (9) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.218501ms)
Feb 22 14:31:42.231: INFO: (9) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.427207ms)
Feb 22 14:31:42.231: INFO: (9) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 8.057443ms)
Feb 22 14:31:42.232: INFO: (9) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 10.2763ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 10.607322ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 10.25388ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.060669ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.421999ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.49922ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 11.135062ms)
Feb 22 14:31:42.233: INFO: (9) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 10.976357ms)
Feb 22 14:31:42.234: INFO: (9) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 11.594347ms)
Feb 22 14:31:42.234: INFO: (9) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 10.850437ms)
Feb 22 14:31:42.234: INFO: (9) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.893528ms)
Feb 22 14:31:42.234: INFO: (9) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 11.296323ms)
Feb 22 14:31:42.239: INFO: (10) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 4.76797ms)
Feb 22 14:31:42.242: INFO: (10) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.140524ms)
Feb 22 14:31:42.242: INFO: (10) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 8.442454ms)
Feb 22 14:31:42.243: INFO: (10) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 8.499348ms)
Feb 22 14:31:42.243: INFO: (10) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.779496ms)
Feb 22 14:31:42.243: INFO: (10) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 8.914864ms)
Feb 22 14:31:42.243: INFO: (10) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 9.046481ms)
Feb 22 14:31:42.244: INFO: (10) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.584701ms)
Feb 22 14:31:42.244: INFO: (10) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 9.387367ms)
Feb 22 14:31:42.244: INFO: (10) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.020082ms)
Feb 22 14:31:42.244: INFO: (10) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 10.346134ms)
Feb 22 14:31:42.244: INFO: (10) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.203416ms)
Feb 22 14:31:42.246: INFO: (10) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 11.466647ms)
Feb 22 14:31:42.246: INFO: (10) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 12.362328ms)
Feb 22 14:31:42.247: INFO: (10) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 12.350595ms)
Feb 22 14:31:42.247: INFO: (10) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 12.490069ms)
Feb 22 14:31:42.251: INFO: (11) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 4.635978ms)
Feb 22 14:31:42.254: INFO: (11) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 6.900777ms)
Feb 22 14:31:42.255: INFO: (11) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 7.994914ms)
Feb 22 14:31:42.255: INFO: (11) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 8.18569ms)
Feb 22 14:31:42.256: INFO: (11) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.032905ms)
Feb 22 14:31:42.256: INFO: (11) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.369719ms)
Feb 22 14:31:42.257: INFO: (11) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 9.762523ms)
Feb 22 14:31:42.257: INFO: (11) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.332261ms)
Feb 22 14:31:42.257: INFO: (11) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 10.124337ms)
Feb 22 14:31:42.257: INFO: (11) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.664557ms)
Feb 22 14:31:42.257: INFO: (11) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 10.294441ms)
Feb 22 14:31:42.258: INFO: (11) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.702746ms)
Feb 22 14:31:42.258: INFO: (11) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 10.84907ms)
Feb 22 14:31:42.258: INFO: (11) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 10.591721ms)
Feb 22 14:31:42.258: INFO: (11) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 10.917595ms)
Feb 22 14:31:42.260: INFO: (11) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 13.059352ms)
Feb 22 14:31:42.265: INFO: (12) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 5.336379ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.28876ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 8.564896ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 8.913837ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 8.602193ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 8.705039ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 8.945626ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 9.066899ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 9.023096ms)
Feb 22 14:31:42.269: INFO: (12) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.116705ms)
Feb 22 14:31:42.270: INFO: (12) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.227396ms)
Feb 22 14:31:42.270: INFO: (12) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 10.112717ms)
Feb 22 14:31:42.271: INFO: (12) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 10.319427ms)
Feb 22 14:31:42.271: INFO: (12) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 10.355059ms)
Feb 22 14:31:42.271: INFO: (12) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.837337ms)
Feb 22 14:31:42.271: INFO: (12) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 11.002306ms)
Feb 22 14:31:42.278: INFO: (13) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.961917ms)
Feb 22 14:31:42.280: INFO: (13) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 8.270057ms)
Feb 22 14:31:42.281: INFO: (13) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 9.631555ms)
Feb 22 14:31:42.281: INFO: (13) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.598582ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 10.180463ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 10.118905ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 10.100443ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 10.249942ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.162437ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 10.321475ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.640585ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.418597ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 10.577141ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.892741ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.797251ms)
Feb 22 14:31:42.282: INFO: (13) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 11.086957ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.098161ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 6.3698ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 6.702227ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 6.305811ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 6.748095ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 6.406222ms)
Feb 22 14:31:42.289: INFO: (14) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 6.713765ms)
Feb 22 14:31:42.290: INFO: (14) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 6.704239ms)
Feb 22 14:31:42.290: INFO: (14) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 7.037082ms)
Feb 22 14:31:42.290: INFO: (14) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.739625ms)
Feb 22 14:31:42.292: INFO: (14) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 9.312571ms)
Feb 22 14:31:42.293: INFO: (14) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 9.758519ms)
Feb 22 14:31:42.293: INFO: (14) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 10.444713ms)
Feb 22 14:31:42.294: INFO: (14) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.709146ms)
Feb 22 14:31:42.294: INFO: (14) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.613033ms)
Feb 22 14:31:42.295: INFO: (14) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 11.756649ms)
Feb 22 14:31:42.300: INFO: (15) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 4.982056ms)
Feb 22 14:31:42.301: INFO: (15) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 5.563321ms)
Feb 22 14:31:42.301: INFO: (15) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 6.063484ms)
Feb 22 14:31:42.301: INFO: (15) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 6.526386ms)
Feb 22 14:31:42.301: INFO: (15) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 6.13759ms)
Feb 22 14:31:42.302: INFO: (15) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.876743ms)
Feb 22 14:31:42.302: INFO: (15) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 7.348924ms)
Feb 22 14:31:42.303: INFO: (15) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 7.696269ms)
Feb 22 14:31:42.306: INFO: (15) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 11.486195ms)
Feb 22 14:31:42.308: INFO: (15) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 13.122606ms)
Feb 22 14:31:42.310: INFO: (15) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 14.434384ms)
Feb 22 14:31:42.311: INFO: (15) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 16.168924ms)
Feb 22 14:31:42.311: INFO: (15) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 16.270332ms)
Feb 22 14:31:42.311: INFO: (15) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 16.41108ms)
Feb 22 14:31:42.311: INFO: (15) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 16.467334ms)
Feb 22 14:31:42.314: INFO: (15) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 19.027545ms)
Feb 22 14:31:42.319: INFO: (16) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 4.586831ms)
Feb 22 14:31:42.322: INFO: (16) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.274404ms)
Feb 22 14:31:42.323: INFO: (16) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 8.828994ms)
Feb 22 14:31:42.323: INFO: (16) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 8.771178ms)
Feb 22 14:31:42.323: INFO: (16) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 9.123915ms)
Feb 22 14:31:42.323: INFO: (16) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 8.746723ms)
Feb 22 14:31:42.324: INFO: (16) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.44461ms)
Feb 22 14:31:42.324: INFO: (16) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 9.839624ms)
Feb 22 14:31:42.324: INFO: (16) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 9.836703ms)
Feb 22 14:31:42.324: INFO: (16) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 9.462432ms)
Feb 22 14:31:42.325: INFO: (16) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.56309ms)
Feb 22 14:31:42.325: INFO: (16) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 10.330021ms)
Feb 22 14:31:42.325: INFO: (16) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 10.425238ms)
Feb 22 14:31:42.325: INFO: (16) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.922054ms)
Feb 22 14:31:42.325: INFO: (16) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 10.45698ms)
Feb 22 14:31:42.325: INFO: (16) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 10.869308ms)
Feb 22 14:31:42.330: INFO: (17) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 4.641503ms)
Feb 22 14:31:42.330: INFO: (17) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 4.908927ms)
Feb 22 14:31:42.334: INFO: (17) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 8.062891ms)
Feb 22 14:31:42.335: INFO: (17) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 9.37455ms)
Feb 22 14:31:42.335: INFO: (17) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 9.983081ms)
Feb 22 14:31:42.335: INFO: (17) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 9.94083ms)
Feb 22 14:31:42.336: INFO: (17) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 9.645777ms)
Feb 22 14:31:42.336: INFO: (17) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.531036ms)
Feb 22 14:31:42.336: INFO: (17) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.601735ms)
Feb 22 14:31:42.336: INFO: (17) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.828664ms)
Feb 22 14:31:42.336: INFO: (17) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 10.011743ms)
Feb 22 14:31:42.337: INFO: (17) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 10.827474ms)
Feb 22 14:31:42.337: INFO: (17) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 11.123771ms)
Feb 22 14:31:42.337: INFO: (17) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 11.635002ms)
Feb 22 14:31:42.337: INFO: (17) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 11.659919ms)
Feb 22 14:31:42.338: INFO: (17) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 12.677211ms)
Feb 22 14:31:42.345: INFO: (18) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.870599ms)
Feb 22 14:31:42.346: INFO: (18) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 7.117652ms)
Feb 22 14:31:42.346: INFO: (18) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 7.194966ms)
Feb 22 14:31:42.347: INFO: (18) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 8.283983ms)
Feb 22 14:31:42.347: INFO: (18) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 8.740932ms)
Feb 22 14:31:42.347: INFO: (18) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 8.728338ms)
Feb 22 14:31:42.348: INFO: (18) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 9.029815ms)
Feb 22 14:31:42.348: INFO: (18) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 8.961499ms)
Feb 22 14:31:42.348: INFO: (18) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 9.435632ms)
Feb 22 14:31:42.348: INFO: (18) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 9.595486ms)
Feb 22 14:31:42.349: INFO: (18) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.423227ms)
Feb 22 14:31:42.350: INFO: (18) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 11.71945ms)
Feb 22 14:31:42.350: INFO: (18) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 11.67258ms)
Feb 22 14:31:42.350: INFO: (18) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 11.67121ms)
Feb 22 14:31:42.350: INFO: (18) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 11.782159ms)
Feb 22 14:31:42.351: INFO: (18) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 12.519469ms)
Feb 22 14:31:42.357: INFO: (19) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">... (200; 6.147683ms)
Feb 22 14:31:42.357: INFO: (19) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 6.26975ms)
Feb 22 14:31:42.357: INFO: (19) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g/proxy/rewriteme">test</a> (200; 6.196435ms)
Feb 22 14:31:42.358: INFO: (19) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 6.678032ms)
Feb 22 14:31:42.358: INFO: (19) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:460/proxy/: tls baz (200; 6.887081ms)
Feb 22 14:31:42.358: INFO: (19) /api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/proxy-service-tj2lp-bhh5g:1080/proxy/rewriteme">test<... (200; 6.994949ms)
Feb 22 14:31:42.358: INFO: (19) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/: <a href="/api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:443/proxy/tlsrewritem... (200; 7.28883ms)
Feb 22 14:31:42.358: INFO: (19) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:160/proxy/: foo (200; 7.16778ms)
Feb 22 14:31:42.359: INFO: (19) /api/v1/namespaces/proxy-4443/pods/http:proxy-service-tj2lp-bhh5g:162/proxy/: bar (200; 7.204841ms)
Feb 22 14:31:42.359: INFO: (19) /api/v1/namespaces/proxy-4443/pods/https:proxy-service-tj2lp-bhh5g:462/proxy/: tls qux (200; 7.457665ms)
Feb 22 14:31:42.362: INFO: (19) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname1/proxy/: foo (200; 10.684214ms)
Feb 22 14:31:42.365: INFO: (19) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname2/proxy/: bar (200; 13.964853ms)
Feb 22 14:31:42.365: INFO: (19) /api/v1/namespaces/proxy-4443/services/proxy-service-tj2lp:portname2/proxy/: bar (200; 14.049181ms)
Feb 22 14:31:42.365: INFO: (19) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname2/proxy/: tls qux (200; 14.314771ms)
Feb 22 14:31:42.365: INFO: (19) /api/v1/namespaces/proxy-4443/services/http:proxy-service-tj2lp:portname1/proxy/: foo (200; 14.241912ms)
Feb 22 14:31:42.366: INFO: (19) /api/v1/namespaces/proxy-4443/services/https:proxy-service-tj2lp:tlsportname1/proxy/: tls baz (200; 14.218578ms)
STEP: deleting ReplicationController proxy-service-tj2lp in namespace proxy-4443, will wait for the garbage collector to delete the pods
Feb 22 14:31:42.423: INFO: Deleting ReplicationController proxy-service-tj2lp took: 5.604375ms
Feb 22 14:31:42.925: INFO: Terminating ReplicationController proxy-service-tj2lp pods took: 502.190796ms
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:31:45.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4443" for this suite.
Feb 22 14:31:51.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:31:51.423: INFO: namespace proxy-4443 deletion completed in 6.095266125s

â€¢ [SLOW TEST:14.409 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:31:51.424: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 22 14:31:51.454: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:31:52.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9718" for this suite.
Feb 22 14:31:58.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:31:58.643: INFO: namespace replication-controller-9718 deletion completed in 6.165683586s

â€¢ [SLOW TEST:7.219 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:31:58.643: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 22 14:31:58.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-2322'
Feb 22 14:31:58.841: INFO: stderr: ""
Feb 22 14:31:58.841: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 14:31:58.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2322'
Feb 22 14:31:58.910: INFO: stderr: ""
Feb 22 14:31:58.911: INFO: stdout: "update-demo-nautilus-flkzr update-demo-nautilus-grsz8 "
Feb 22 14:31:58.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:31:58.971: INFO: stderr: ""
Feb 22 14:31:58.971: INFO: stdout: ""
Feb 22 14:31:58.971: INFO: update-demo-nautilus-flkzr is created but not running
Feb 22 14:32:03.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2322'
Feb 22 14:32:04.034: INFO: stderr: ""
Feb 22 14:32:04.034: INFO: stdout: "update-demo-nautilus-flkzr update-demo-nautilus-grsz8 "
Feb 22 14:32:04.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:04.091: INFO: stderr: ""
Feb 22 14:32:04.092: INFO: stdout: "true"
Feb 22 14:32:04.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:04.147: INFO: stderr: ""
Feb 22 14:32:04.147: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:32:04.147: INFO: validating pod update-demo-nautilus-flkzr
Feb 22 14:32:04.150: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:32:04.150: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:32:04.150: INFO: update-demo-nautilus-flkzr is verified up and running
Feb 22 14:32:04.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-grsz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:04.209: INFO: stderr: ""
Feb 22 14:32:04.209: INFO: stdout: "true"
Feb 22 14:32:04.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-grsz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:04.268: INFO: stderr: ""
Feb 22 14:32:04.269: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:32:04.269: INFO: validating pod update-demo-nautilus-grsz8
Feb 22 14:32:04.271: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:32:04.271: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:32:04.271: INFO: update-demo-nautilus-grsz8 is verified up and running
STEP: scaling down the replication controller
Feb 22 14:32:04.273: INFO: scanned /root for discovery docs: <nil>
Feb 22 14:32:04.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2322'
Feb 22 14:32:05.353: INFO: stderr: ""
Feb 22 14:32:05.353: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 14:32:05.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2322'
Feb 22 14:32:05.414: INFO: stderr: ""
Feb 22 14:32:05.414: INFO: stdout: "update-demo-nautilus-flkzr update-demo-nautilus-grsz8 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 22 14:32:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2322'
Feb 22 14:32:10.474: INFO: stderr: ""
Feb 22 14:32:10.474: INFO: stdout: "update-demo-nautilus-flkzr "
Feb 22 14:32:10.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:10.540: INFO: stderr: ""
Feb 22 14:32:10.540: INFO: stdout: "true"
Feb 22 14:32:10.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:10.625: INFO: stderr: ""
Feb 22 14:32:10.625: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:32:10.625: INFO: validating pod update-demo-nautilus-flkzr
Feb 22 14:32:10.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:32:10.628: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:32:10.628: INFO: update-demo-nautilus-flkzr is verified up and running
STEP: scaling up the replication controller
Feb 22 14:32:10.630: INFO: scanned /root for discovery docs: <nil>
Feb 22 14:32:10.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2322'
Feb 22 14:32:11.729: INFO: stderr: ""
Feb 22 14:32:11.729: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 14:32:11.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2322'
Feb 22 14:32:11.816: INFO: stderr: ""
Feb 22 14:32:11.816: INFO: stdout: "update-demo-nautilus-flkzr update-demo-nautilus-vq5gl "
Feb 22 14:32:11.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:11.876: INFO: stderr: ""
Feb 22 14:32:11.876: INFO: stdout: "true"
Feb 22 14:32:11.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:11.949: INFO: stderr: ""
Feb 22 14:32:11.949: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:32:11.949: INFO: validating pod update-demo-nautilus-flkzr
Feb 22 14:32:11.952: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:32:11.952: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:32:11.952: INFO: update-demo-nautilus-flkzr is verified up and running
Feb 22 14:32:11.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-vq5gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:12.008: INFO: stderr: ""
Feb 22 14:32:12.008: INFO: stdout: ""
Feb 22 14:32:12.008: INFO: update-demo-nautilus-vq5gl is created but not running
Feb 22 14:32:17.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2322'
Feb 22 14:32:17.068: INFO: stderr: ""
Feb 22 14:32:17.068: INFO: stdout: "update-demo-nautilus-flkzr update-demo-nautilus-vq5gl "
Feb 22 14:32:17.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:17.127: INFO: stderr: ""
Feb 22 14:32:17.127: INFO: stdout: "true"
Feb 22 14:32:17.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-flkzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:17.189: INFO: stderr: ""
Feb 22 14:32:17.189: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:32:17.189: INFO: validating pod update-demo-nautilus-flkzr
Feb 22 14:32:17.192: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:32:17.192: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:32:17.192: INFO: update-demo-nautilus-flkzr is verified up and running
Feb 22 14:32:17.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-vq5gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:17.251: INFO: stderr: ""
Feb 22 14:32:17.251: INFO: stdout: "true"
Feb 22 14:32:17.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-vq5gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2322'
Feb 22 14:32:17.309: INFO: stderr: ""
Feb 22 14:32:17.309: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:32:17.309: INFO: validating pod update-demo-nautilus-vq5gl
Feb 22 14:32:17.312: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:32:17.312: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:32:17.312: INFO: update-demo-nautilus-vq5gl is verified up and running
STEP: using delete to clean up resources
Feb 22 14:32:17.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-2322'
Feb 22 14:32:17.373: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 14:32:17.373: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 22 14:32:17.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2322'
Feb 22 14:32:17.434: INFO: stderr: "No resources found.\n"
Feb 22 14:32:17.434: INFO: stdout: ""
Feb 22 14:32:17.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -l name=update-demo --namespace=kubectl-2322 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 14:32:17.494: INFO: stderr: ""
Feb 22 14:32:17.494: INFO: stdout: "update-demo-nautilus-flkzr\nupdate-demo-nautilus-vq5gl\n"
Feb 22 14:32:17.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2322'
Feb 22 14:32:18.072: INFO: stderr: "No resources found.\n"
Feb 22 14:32:18.072: INFO: stdout: ""
Feb 22 14:32:18.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -l name=update-demo --namespace=kubectl-2322 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 14:32:18.143: INFO: stderr: ""
Feb 22 14:32:18.143: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:32:18.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2322" for this suite.
Feb 22 14:32:40.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:32:40.313: INFO: namespace kubectl-2322 deletion completed in 22.167114245s

â€¢ [SLOW TEST:41.671 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:32:40.314: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 14:32:40.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1035'
Feb 22 14:32:40.407: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 22 14:32:40.407: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 22 14:32:40.416: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 22 14:32:40.417: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 22 14:32:40.426: INFO: scanned /root for discovery docs: <nil>
Feb 22 14:32:40.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1035'
Feb 22 14:32:56.167: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 22 14:32:56.167: INFO: stdout: "Created e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78\nScaling up e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 22 14:32:56.167: INFO: stdout: "Created e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78\nScaling up e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 22 14:32:56.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1035'
Feb 22 14:32:56.232: INFO: stderr: ""
Feb 22 14:32:56.232: INFO: stdout: "e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78-jkcg4 "
Feb 22 14:32:56.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78-jkcg4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1035'
Feb 22 14:32:56.289: INFO: stderr: ""
Feb 22 14:32:56.289: INFO: stdout: "true"
Feb 22 14:32:56.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78-jkcg4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1035'
Feb 22 14:32:56.349: INFO: stderr: ""
Feb 22 14:32:56.349: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 22 14:32:56.349: INFO: e2e-test-nginx-rc-f69c66d0b77ca290aaa49bcf72f00d78-jkcg4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Feb 22 14:32:56.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete rc e2e-test-nginx-rc --namespace=kubectl-1035'
Feb 22 14:32:56.410: INFO: stderr: ""
Feb 22 14:32:56.410: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:32:56.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1035" for this suite.
Feb 22 14:33:18.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:33:18.585: INFO: namespace kubectl-1035 deletion completed in 22.172680585s

â€¢ [SLOW TEST:38.271 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:33:18.585: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 22 14:33:18.610: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:15963,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 22 14:33:18.610: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:15963,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 22 14:33:28.614: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:15991,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 22 14:33:28.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:15991,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 22 14:33:38.619: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:16017,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 22 14:33:38.619: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:16017,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 22 14:33:48.623: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:16044,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 22 14:33:48.623: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-a,UID:f07cc9d8-7e5e-435b-aaab-143acf39abbc,ResourceVersion:16044,Generation:0,CreationTimestamp:2020-02-22 14:33:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 22 14:33:58.627: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-b,UID:a45a4a69-2c67-4a5f-a049-38967bf57a3a,ResourceVersion:16073,Generation:0,CreationTimestamp:2020-02-22 14:33:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 22 14:33:58.627: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-b,UID:a45a4a69-2c67-4a5f-a049-38967bf57a3a,ResourceVersion:16073,Generation:0,CreationTimestamp:2020-02-22 14:33:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 22 14:34:08.631: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-b,UID:a45a4a69-2c67-4a5f-a049-38967bf57a3a,ResourceVersion:16102,Generation:0,CreationTimestamp:2020-02-22 14:33:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 22 14:34:08.631: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-2981,SelfLink:/api/v1/namespaces/watch-2981/configmaps/e2e-watch-test-configmap-b,UID:a45a4a69-2c67-4a5f-a049-38967bf57a3a,ResourceVersion:16102,Generation:0,CreationTimestamp:2020-02-22 14:33:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:34:18.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2981" for this suite.
Feb 22 14:34:24.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:34:24.799: INFO: namespace watch-2981 deletion completed in 6.165759622s

â€¢ [SLOW TEST:66.214 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:34:24.800: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:34:28.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7955" for this suite.
Feb 22 14:34:34.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:34:35.034: INFO: namespace emptydir-wrapper-7955 deletion completed in 6.165992807s

â€¢ [SLOW TEST:10.234 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:34:35.034: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 22 14:34:35.067: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4406,SelfLink:/api/v1/namespaces/watch-4406/configmaps/e2e-watch-test-watch-closed,UID:68f9b637-3b10-4405-a33f-f970b62ae3c1,ResourceVersion:16217,Generation:0,CreationTimestamp:2020-02-22 14:34:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 22 14:34:35.067: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4406,SelfLink:/api/v1/namespaces/watch-4406/configmaps/e2e-watch-test-watch-closed,UID:68f9b637-3b10-4405-a33f-f970b62ae3c1,ResourceVersion:16218,Generation:0,CreationTimestamp:2020-02-22 14:34:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 22 14:34:35.074: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4406,SelfLink:/api/v1/namespaces/watch-4406/configmaps/e2e-watch-test-watch-closed,UID:68f9b637-3b10-4405-a33f-f970b62ae3c1,ResourceVersion:16219,Generation:0,CreationTimestamp:2020-02-22 14:34:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 22 14:34:35.074: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4406,SelfLink:/api/v1/namespaces/watch-4406/configmaps/e2e-watch-test-watch-closed,UID:68f9b637-3b10-4405-a33f-f970b62ae3c1,ResourceVersion:16220,Generation:0,CreationTimestamp:2020-02-22 14:34:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:34:35.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4406" for this suite.
Feb 22 14:34:41.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:34:41.245: INFO: namespace watch-4406 deletion completed in 6.16735533s

â€¢ [SLOW TEST:6.211 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:34:41.245: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 22 14:34:45.786: INFO: Successfully updated pod "labelsupdate7874bbbd-3ec4-4cd8-886a-bade2de57b4e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:34:47.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6510" for this suite.
Feb 22 14:35:09.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:35:09.973: INFO: namespace projected-6510 deletion completed in 22.171511403s

â€¢ [SLOW TEST:28.728 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:35:09.973: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 22 14:35:10.043: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:35:14.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4101" for this suite.
Feb 22 14:35:36.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:35:36.362: INFO: namespace init-container-4101 deletion completed in 22.17178136s

â€¢ [SLOW TEST:26.389 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:35:36.362: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:35:40.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4749" for this suite.
Feb 22 14:36:18.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:36:18.574: INFO: namespace kubelet-test-4749 deletion completed in 38.168046505s

â€¢ [SLOW TEST:42.213 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:36:18.574: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-8c335541-e7ec-4aca-821e-c773bc8f2449 in namespace container-probe-9629
Feb 22 14:36:20.611: INFO: Started pod busybox-8c335541-e7ec-4aca-821e-c773bc8f2449 in namespace container-probe-9629
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 14:36:20.613: INFO: Initial restart count of pod busybox-8c335541-e7ec-4aca-821e-c773bc8f2449 is 0
Feb 22 14:37:10.670: INFO: Restart count of pod container-probe-9629/busybox-8c335541-e7ec-4aca-821e-c773bc8f2449 is now 1 (50.057446724s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:37:10.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9629" for this suite.
Feb 22 14:37:16.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:37:16.856: INFO: namespace container-probe-9629 deletion completed in 6.173129043s

â€¢ [SLOW TEST:58.281 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:37:16.856: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:37:16.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184" in namespace "projected-6050" to be "success or failure"
Feb 22 14:37:16.898: INFO: Pod "downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184": Phase="Pending", Reason="", readiness=false. Elapsed: 4.995976ms
Feb 22 14:37:18.901: INFO: Pod "downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007345676s
Feb 22 14:37:20.903: INFO: Pod "downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009602919s
STEP: Saw pod success
Feb 22 14:37:20.903: INFO: Pod "downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184" satisfied condition "success or failure"
Feb 22 14:37:20.905: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184 container client-container: <nil>
STEP: delete the pod
Feb 22 14:37:20.919: INFO: Waiting for pod downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184 to disappear
Feb 22 14:37:20.921: INFO: Pod downwardapi-volume-2aadbbd3-c9e3-463e-9a5a-174daf2b1184 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:37:20.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6050" for this suite.
Feb 22 14:37:26.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:37:27.095: INFO: namespace projected-6050 deletion completed in 6.172533335s

â€¢ [SLOW TEST:10.240 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:37:27.096: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-1c0b2e31-d94f-4f02-bd8d-8a37dba81611
STEP: Creating a pod to test consume secrets
Feb 22 14:37:27.132: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1" in namespace "projected-289" to be "success or failure"
Feb 22 14:37:27.135: INFO: Pod "pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.679337ms
Feb 22 14:37:29.137: INFO: Pod "pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00464093s
STEP: Saw pod success
Feb 22 14:37:29.137: INFO: Pod "pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1" satisfied condition "success or failure"
Feb 22 14:37:29.139: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:37:29.153: INFO: Waiting for pod pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1 to disappear
Feb 22 14:37:29.155: INFO: Pod pod-projected-secrets-79c48cdd-bfa6-4bf9-9057-838f4afbd2b1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:37:29.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-289" for this suite.
Feb 22 14:37:35.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:37:35.324: INFO: namespace projected-289 deletion completed in 6.167537155s

â€¢ [SLOW TEST:8.229 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:37:35.325: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Feb 22 14:37:35.910: INFO: created pod pod-service-account-defaultsa
Feb 22 14:37:35.910: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 22 14:37:35.913: INFO: created pod pod-service-account-mountsa
Feb 22 14:37:35.913: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 22 14:37:35.927: INFO: created pod pod-service-account-nomountsa
Feb 22 14:37:35.927: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 22 14:37:35.937: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 22 14:37:35.937: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 22 14:37:35.941: INFO: created pod pod-service-account-mountsa-mountspec
Feb 22 14:37:35.941: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 22 14:37:35.956: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 22 14:37:35.956: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 22 14:37:35.967: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 22 14:37:35.967: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 22 14:37:35.978: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 22 14:37:35.978: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 22 14:37:35.985: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 22 14:37:35.985: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:37:35.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4126" for this suite.
Feb 22 14:37:42.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:37:42.163: INFO: namespace svcaccounts-4126 deletion completed in 6.171498021s

â€¢ [SLOW TEST:6.838 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:37:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Feb 22 14:37:42.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 api-versions'
Feb 22 14:37:42.249: INFO: stderr: ""
Feb 22 14:37:42.249: INFO: stdout: "admission.certmanager.k8s.io/v1beta1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:37:42.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3046" for this suite.
Feb 22 14:37:48.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:37:48.420: INFO: namespace kubectl-3046 deletion completed in 6.169061953s

â€¢ [SLOW TEST:6.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:37:48.421: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 14:37:52.482: INFO: DNS probes using dns-test-3ad40e04-91c2-4f01-8502-cce82895a8f1 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 14:37:54.655: INFO: File wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:37:54.657: INFO: File jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:37:54.657: INFO: Lookups using dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 failed for: [wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local]

Feb 22 14:37:59.659: INFO: File wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:37:59.661: INFO: File jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:37:59.661: INFO: Lookups using dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 failed for: [wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local]

Feb 22 14:38:04.659: INFO: File wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:04.661: INFO: File jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:04.661: INFO: Lookups using dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 failed for: [wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local]

Feb 22 14:38:09.659: INFO: File wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:09.662: INFO: File jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:09.662: INFO: Lookups using dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 failed for: [wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local]

Feb 22 14:38:14.659: INFO: File wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:14.661: INFO: File jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:14.661: INFO: Lookups using dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 failed for: [wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local]

Feb 22 14:38:19.659: INFO: File wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:19.661: INFO: File jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local from pod  dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 contains 'foo.example.com.
' instead of 'bar.example.com.'
Feb 22 14:38:19.661: INFO: Lookups using dns-5528/dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 failed for: [wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local]

Feb 22 14:38:24.661: INFO: DNS probes using dns-test-1708b9b0-c7d0-476a-af46-22f3f1655433 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5528.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5528.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 14:38:36.714: INFO: DNS probes using dns-test-2810eedf-fd94-4a0f-bf60-07a5f9f969ed succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:38:36.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5528" for this suite.
Feb 22 14:38:42.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:38:42.918: INFO: namespace dns-5528 deletion completed in 6.167546192s

â€¢ [SLOW TEST:54.497 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:38:42.918: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:38:42.936: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:38:44.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8472" for this suite.
Feb 22 14:38:50.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:38:50.343: INFO: namespace custom-resource-definition-8472 deletion completed in 6.167924273s

â€¢ [SLOW TEST:7.425 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:38:50.343: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:38:56.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3806" for this suite.
Feb 22 14:39:02.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:39:02.632: INFO: namespace namespaces-3806 deletion completed in 6.166322363s
STEP: Destroying namespace "nsdeletetest-2905" for this suite.
Feb 22 14:39:02.634: INFO: Namespace nsdeletetest-2905 was already deleted
STEP: Destroying namespace "nsdeletetest-6414" for this suite.
Feb 22 14:39:08.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:39:08.800: INFO: namespace nsdeletetest-6414 deletion completed in 6.16619508s

â€¢ [SLOW TEST:18.457 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:39:08.800: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:39:08.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6" in namespace "downward-api-3220" to be "success or failure"
Feb 22 14:39:08.891: INFO: Pod "downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.824319ms
Feb 22 14:39:10.892: INFO: Pod "downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012475528s
Feb 22 14:39:12.895: INFO: Pod "downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014567147s
STEP: Saw pod success
Feb 22 14:39:12.895: INFO: Pod "downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6" satisfied condition "success or failure"
Feb 22 14:39:12.896: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6 container client-container: <nil>
STEP: delete the pod
Feb 22 14:39:12.907: INFO: Waiting for pod downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6 to disappear
Feb 22 14:39:12.910: INFO: Pod downwardapi-volume-d5ba52f1-b945-45b1-90be-b118c128cbd6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:39:12.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3220" for this suite.
Feb 22 14:39:18.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:39:19.078: INFO: namespace downward-api-3220 deletion completed in 6.165563987s

â€¢ [SLOW TEST:10.277 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:39:19.078: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 22 14:39:19.102: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 14:39:19.110: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 14:39:19.112: INFO: 
Logging pods the kubelet thinks is on node ip-10-100-10-41.eu-west-1.compute.internal before test
Feb 22 14:39:19.117: INFO: coredns-5c98db65d4-p26fz from kube-system started at 2020-02-22 13:39:48 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container coredns ready: true, restart count 0
Feb 22 14:39:19.117: INFO: kube-state-metrics-779c78b7d5-p2w4c from monitoring started at 2020-02-22 13:41:19 +0000 UTC (2 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container addon-resizer ready: true, restart count 0
Feb 22 14:39:19.117: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 22 14:39:19.117: INFO: calico-node-mxbd8 from kube-system started at 2020-02-22 13:39:32 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container calico-node ready: true, restart count 0
Feb 22 14:39:19.117: INFO: node-exporter-msc8m from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container node-exporter ready: true, restart count 0
Feb 22 14:39:19.117: INFO: velero-restic-2w5bp from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container restic ready: true, restart count 0
Feb 22 14:39:19.117: INFO: kube-proxy-qj2fs from kube-system started at 2020-02-22 13:36:59 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 14:39:19.117: INFO: elasticsearch-0 from logging started at 2020-02-22 13:40:27 +0000 UTC (2 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container elasticsearch ready: true, restart count 0
Feb 22 14:39:19.117: INFO: 	Container exporter ready: true, restart count 0
Feb 22 14:39:19.117: INFO: sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-kxfkz from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 14:39:19.117: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 14:39:19.117: INFO: fluentd-2p5rq from logging started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container fluentd ready: true, restart count 0
Feb 22 14:39:19.117: INFO: cert-manager-cainjector-db5466bb4-vz5nk from cert-manager started at 2020-02-22 13:39:41 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container cainjector ready: true, restart count 0
Feb 22 14:39:19.117: INFO: sonobuoy from sonobuoy started at 2020-02-22 13:43:59 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 14:39:19.117: INFO: goldpinger-rtcxd from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container goldpinger ready: true, restart count 0
Feb 22 14:39:19.117: INFO: nginx-ingress-controller-wzhsm from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 22 14:39:19.117: INFO: minio-0 from kube-system started at 2020-02-22 13:40:30 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.117: INFO: 	Container minio ready: true, restart count 0
Feb 22 14:39:19.117: INFO: 
Logging pods the kubelet thinks is on node ip-10-100-10-53.eu-west-1.compute.internal before test
Feb 22 14:39:19.129: INFO: local-path-provisioner-84f4c8b584-xjdmp from local-path-storage started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container local-path-provisioner ready: true, restart count 0
Feb 22 14:39:19.129: INFO: calico-node-6dg9m from kube-system started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container calico-node ready: true, restart count 0
Feb 22 14:39:19.129: INFO: coredns-5c98db65d4-jw6hn from kube-system started at 2020-02-22 13:39:39 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container coredns ready: true, restart count 0
Feb 22 14:39:19.129: INFO: forecastle-6d4d79f779-fzlqw from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container forecastle ready: true, restart count 0
Feb 22 14:39:19.129: INFO: cerebro-78d8f5fc66-nwsj5 from logging started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container cerebro ready: true, restart count 0
Feb 22 14:39:19.129: INFO: prometheus-operator-7f4c684c8b-wxj5q from monitoring started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 22 14:39:19.129: INFO: kube-proxy-9ckc7 from kube-system started at 2020-02-22 13:36:59 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 14:39:19.129: INFO: node-exporter-cnhxz from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container node-exporter ready: true, restart count 0
Feb 22 14:39:19.129: INFO: fluentd-cbtvh from logging started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container fluentd ready: true, restart count 0
Feb 22 14:39:19.129: INFO: velero-5c4bbbd8d6-drz26 from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container velero ready: true, restart count 0
Feb 22 14:39:19.129: INFO: kibana-5bd7fdf954-k7mrh from logging started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container kibana ready: true, restart count 0
Feb 22 14:39:19.129: INFO: goldpinger-dxblt from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container goldpinger ready: true, restart count 0
Feb 22 14:39:19.129: INFO: grafana-6cdc7d9f68-4p8g9 from monitoring started at 2020-02-22 13:39:39 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container grafana ready: true, restart count 0
Feb 22 14:39:19.129: INFO: calico-kube-controllers-8944b5db6-zqdw6 from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 22 14:39:19.129: INFO: cert-manager-594fcfc677-s7wd7 from cert-manager started at 2020-02-22 13:39:41 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container cert-manager ready: true, restart count 0
Feb 22 14:39:19.129: INFO: prometheus-k8s-0 from monitoring started at 2020-02-22 13:41:20 +0000 UTC (3 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container prometheus ready: true, restart count 1
Feb 22 14:39:19.129: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 22 14:39:19.129: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 22 14:39:19.129: INFO: cert-manager-webhook-5f988b8bbc-sq7k7 from cert-manager started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container webhook ready: true, restart count 0
Feb 22 14:39:19.129: INFO: minio-setup-jp76l from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container mc ready: false, restart count 0
Feb 22 14:39:19.129: INFO: sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-4zplg from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 14:39:19.129: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 14:39:19.129: INFO: velero-restic-qbvdz from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container restic ready: true, restart count 0
Feb 22 14:39:19.129: INFO: nginx-ingress-controller-khvgv from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 22 14:39:19.129: INFO: sonobuoy-e2e-job-f6fd58bec6164e4d from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 14:39:19.129: INFO: 	Container e2e ready: true, restart count 0
Feb 22 14:39:19.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-35373852-080d-49da-a8c8-ea90988a06e3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-35373852-080d-49da-a8c8-ea90988a06e3 off the node ip-10-100-10-53.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-35373852-080d-49da-a8c8-ea90988a06e3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:39:25.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4634" for this suite.
Feb 22 14:39:43.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:39:43.356: INFO: namespace sched-pred-4634 deletion completed in 18.170517261s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:24.278 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:39:43.356: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 14:39:43.397: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:43.399: INFO: Number of nodes with available pods: 0
Feb 22 14:39:43.399: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:44.401: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:44.403: INFO: Number of nodes with available pods: 0
Feb 22 14:39:44.403: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:45.402: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:45.404: INFO: Number of nodes with available pods: 0
Feb 22 14:39:45.404: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:46.401: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:46.403: INFO: Number of nodes with available pods: 2
Feb 22 14:39:46.403: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 22 14:39:46.414: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:46.415: INFO: Number of nodes with available pods: 1
Feb 22 14:39:46.415: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:47.418: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:47.420: INFO: Number of nodes with available pods: 1
Feb 22 14:39:47.420: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:48.418: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:48.424: INFO: Number of nodes with available pods: 1
Feb 22 14:39:48.424: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:49.418: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:49.420: INFO: Number of nodes with available pods: 1
Feb 22 14:39:49.420: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:50.418: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:50.420: INFO: Number of nodes with available pods: 1
Feb 22 14:39:50.420: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:51.418: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:51.420: INFO: Number of nodes with available pods: 1
Feb 22 14:39:51.420: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:39:52.418: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:39:52.420: INFO: Number of nodes with available pods: 2
Feb 22 14:39:52.420: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4195, will wait for the garbage collector to delete the pods
Feb 22 14:39:52.477: INFO: Deleting DaemonSet.extensions daemon-set took: 3.8098ms
Feb 22 14:39:52.977: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.197374ms
Feb 22 14:39:55.779: INFO: Number of nodes with available pods: 0
Feb 22 14:39:55.779: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 14:39:55.781: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4195/daemonsets","resourceVersion":"17781"},"items":null}

Feb 22 14:39:55.782: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4195/pods","resourceVersion":"17781"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:39:55.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4195" for this suite.
Feb 22 14:40:01.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:40:01.957: INFO: namespace daemonsets-4195 deletion completed in 6.168254178s

â€¢ [SLOW TEST:18.601 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:40:01.958: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c4f402a6-be57-48a0-8e50-bdc876e88d23
STEP: Creating a pod to test consume secrets
Feb 22 14:40:01.999: INFO: Waiting up to 5m0s for pod "pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584" in namespace "secrets-5123" to be "success or failure"
Feb 22 14:40:02.004: INFO: Pod "pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584": Phase="Pending", Reason="", readiness=false. Elapsed: 5.03117ms
Feb 22 14:40:04.006: INFO: Pod "pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006965906s
Feb 22 14:40:06.008: INFO: Pod "pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008872336s
STEP: Saw pod success
Feb 22 14:40:06.008: INFO: Pod "pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584" satisfied condition "success or failure"
Feb 22 14:40:06.010: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:40:06.022: INFO: Waiting for pod pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584 to disappear
Feb 22 14:40:06.024: INFO: Pod pod-secrets-deb77a7d-e6fa-4ef2-afa8-dd4d468f5584 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:40:06.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5123" for this suite.
Feb 22 14:40:12.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:40:12.192: INFO: namespace secrets-5123 deletion completed in 6.165674476s

â€¢ [SLOW TEST:10.234 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:40:12.192: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-8681
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8681 to expose endpoints map[]
Feb 22 14:40:12.222: INFO: successfully validated that service multi-endpoint-test in namespace services-8681 exposes endpoints map[] (2.864905ms elapsed)
STEP: Creating pod pod1 in namespace services-8681
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8681 to expose endpoints map[pod1:[100]]
Feb 22 14:40:15.253: INFO: successfully validated that service multi-endpoint-test in namespace services-8681 exposes endpoints map[pod1:[100]] (3.020741846s elapsed)
STEP: Creating pod pod2 in namespace services-8681
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8681 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 22 14:40:18.279: INFO: successfully validated that service multi-endpoint-test in namespace services-8681 exposes endpoints map[pod1:[100] pod2:[101]] (3.023373996s elapsed)
STEP: Deleting pod pod1 in namespace services-8681
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8681 to expose endpoints map[pod2:[101]]
Feb 22 14:40:19.289: INFO: successfully validated that service multi-endpoint-test in namespace services-8681 exposes endpoints map[pod2:[101]] (1.007704155s elapsed)
STEP: Deleting pod pod2 in namespace services-8681
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8681 to expose endpoints map[]
Feb 22 14:40:20.297: INFO: successfully validated that service multi-endpoint-test in namespace services-8681 exposes endpoints map[] (1.003987512s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:40:20.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8681" for this suite.
Feb 22 14:40:42.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:40:42.482: INFO: namespace services-8681 deletion completed in 22.168281173s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:30.290 seconds]
[sig-network] Services
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:40:42.482: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-068971a0-603d-490a-bcd6-9856781fce36
STEP: Creating a pod to test consume secrets
Feb 22 14:40:42.514: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460" in namespace "projected-5304" to be "success or failure"
Feb 22 14:40:42.518: INFO: Pod "pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460": Phase="Pending", Reason="", readiness=false. Elapsed: 3.489664ms
Feb 22 14:40:44.520: INFO: Pod "pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005524131s
Feb 22 14:40:46.522: INFO: Pod "pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007610659s
STEP: Saw pod success
Feb 22 14:40:46.522: INFO: Pod "pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460" satisfied condition "success or failure"
Feb 22 14:40:46.524: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:40:46.540: INFO: Waiting for pod pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460 to disappear
Feb 22 14:40:46.541: INFO: Pod pod-projected-secrets-eaa9cf1c-06a2-4863-9b27-33b713d98460 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:40:46.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5304" for this suite.
Feb 22 14:40:52.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:40:52.710: INFO: namespace projected-5304 deletion completed in 6.165877464s

â€¢ [SLOW TEST:10.228 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:40:52.710: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:40:52.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1" in namespace "projected-4039" to be "success or failure"
Feb 22 14:40:52.741: INFO: Pod "downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.456094ms
Feb 22 14:40:54.743: INFO: Pod "downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008446171s
Feb 22 14:40:56.745: INFO: Pod "downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010608506s
STEP: Saw pod success
Feb 22 14:40:56.745: INFO: Pod "downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1" satisfied condition "success or failure"
Feb 22 14:40:56.747: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1 container client-container: <nil>
STEP: delete the pod
Feb 22 14:40:56.760: INFO: Waiting for pod downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1 to disappear
Feb 22 14:40:56.761: INFO: Pod downwardapi-volume-7d3c9afe-be94-4e91-8a58-2d7157e44be1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:40:56.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4039" for this suite.
Feb 22 14:41:02.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:41:02.930: INFO: namespace projected-4039 deletion completed in 6.166316846s

â€¢ [SLOW TEST:10.219 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:41:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:41:02.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b" in namespace "downward-api-3390" to be "success or failure"
Feb 22 14:41:02.958: INFO: Pod "downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.098304ms
Feb 22 14:41:04.960: INFO: Pod "downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00925688s
Feb 22 14:41:06.962: INFO: Pod "downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011352278s
STEP: Saw pod success
Feb 22 14:41:06.962: INFO: Pod "downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b" satisfied condition "success or failure"
Feb 22 14:41:06.964: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b container client-container: <nil>
STEP: delete the pod
Feb 22 14:41:06.975: INFO: Waiting for pod downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b to disappear
Feb 22 14:41:06.977: INFO: Pod downwardapi-volume-0e46cc41-25f9-4bdf-ad79-b4ccfbc6967b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:41:06.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3390" for this suite.
Feb 22 14:41:12.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:41:13.146: INFO: namespace downward-api-3390 deletion completed in 6.16628401s

â€¢ [SLOW TEST:10.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:41:13.146: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 22 14:41:13.170: INFO: Waiting up to 5m0s for pod "pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62" in namespace "emptydir-9717" to be "success or failure"
Feb 22 14:41:13.173: INFO: Pod "pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62": Phase="Pending", Reason="", readiness=false. Elapsed: 3.075611ms
Feb 22 14:41:15.175: INFO: Pod "pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005068676s
Feb 22 14:41:17.177: INFO: Pod "pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007199561s
STEP: Saw pod success
Feb 22 14:41:17.177: INFO: Pod "pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62" satisfied condition "success or failure"
Feb 22 14:41:17.180: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62 container test-container: <nil>
STEP: delete the pod
Feb 22 14:41:17.192: INFO: Waiting for pod pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62 to disappear
Feb 22 14:41:17.194: INFO: Pod pod-37e299db-f3b5-4e06-8a4f-83d4fab96a62 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:41:17.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9717" for this suite.
Feb 22 14:41:23.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:41:23.364: INFO: namespace emptydir-9717 deletion completed in 6.166834645s

â€¢ [SLOW TEST:10.218 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:41:23.364: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 22 14:41:33.434: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0222 14:41:33.434036      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:41:33.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6478" for this suite.
Feb 22 14:41:39.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:41:39.602: INFO: namespace gc-6478 deletion completed in 6.165274127s

â€¢ [SLOW TEST:16.239 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:41:39.603: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 22 14:41:47.652: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:47.654: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:41:49.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:49.660: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:41:51.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:51.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:41:53.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:53.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:41:55.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:55.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:41:57.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:57.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:41:59.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:41:59.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:42:01.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:42:01.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:42:03.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:42:03.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:42:05.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:42:05.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:42:07.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:42:07.656: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 22 14:42:09.654: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 22 14:42:09.656: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:42:09.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7660" for this suite.
Feb 22 14:42:31.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:42:31.824: INFO: namespace container-lifecycle-hook-7660 deletion completed in 22.165282964s

â€¢ [SLOW TEST:52.221 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:42:31.825: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 22 14:42:31.852: INFO: Waiting up to 5m0s for pod "pod-05300cab-ac7c-461d-9310-ed0fcf1d5523" in namespace "emptydir-3294" to be "success or failure"
Feb 22 14:42:31.857: INFO: Pod "pod-05300cab-ac7c-461d-9310-ed0fcf1d5523": Phase="Pending", Reason="", readiness=false. Elapsed: 4.543333ms
Feb 22 14:42:33.858: INFO: Pod "pod-05300cab-ac7c-461d-9310-ed0fcf1d5523": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006401386s
Feb 22 14:42:35.860: INFO: Pod "pod-05300cab-ac7c-461d-9310-ed0fcf1d5523": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008357529s
STEP: Saw pod success
Feb 22 14:42:35.860: INFO: Pod "pod-05300cab-ac7c-461d-9310-ed0fcf1d5523" satisfied condition "success or failure"
Feb 22 14:42:35.862: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-05300cab-ac7c-461d-9310-ed0fcf1d5523 container test-container: <nil>
STEP: delete the pod
Feb 22 14:42:35.875: INFO: Waiting for pod pod-05300cab-ac7c-461d-9310-ed0fcf1d5523 to disappear
Feb 22 14:42:35.876: INFO: Pod pod-05300cab-ac7c-461d-9310-ed0fcf1d5523 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:42:35.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3294" for this suite.
Feb 22 14:42:41.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:42:42.054: INFO: namespace emptydir-3294 deletion completed in 6.167901217s

â€¢ [SLOW TEST:10.229 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:42:42.054: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e4b14a5c-2b4a-4049-aa9e-6bba39b54fd2
STEP: Creating a pod to test consume configMaps
Feb 22 14:42:42.080: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da" in namespace "projected-4905" to be "success or failure"
Feb 22 14:42:42.085: INFO: Pod "pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010354ms
Feb 22 14:42:44.087: INFO: Pod "pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006899947s
Feb 22 14:42:46.089: INFO: Pod "pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009158252s
STEP: Saw pod success
Feb 22 14:42:46.089: INFO: Pod "pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da" satisfied condition "success or failure"
Feb 22 14:42:46.091: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:42:46.108: INFO: Waiting for pod pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da to disappear
Feb 22 14:42:46.112: INFO: Pod pod-projected-configmaps-dab45078-0215-40d3-8793-5ef0102514da no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:42:46.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4905" for this suite.
Feb 22 14:42:52.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:42:52.285: INFO: namespace projected-4905 deletion completed in 6.169050722s

â€¢ [SLOW TEST:10.231 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:42:52.285: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:42:52.318: INFO: Create a RollingUpdate DaemonSet
Feb 22 14:42:52.320: INFO: Check that daemon pods launch on every node of the cluster
Feb 22 14:42:52.331: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:42:52.406: INFO: Number of nodes with available pods: 0
Feb 22 14:42:52.406: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:42:53.408: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:42:53.410: INFO: Number of nodes with available pods: 0
Feb 22 14:42:53.410: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:42:54.408: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:42:54.410: INFO: Number of nodes with available pods: 0
Feb 22 14:42:54.410: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 14:42:55.408: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:42:55.410: INFO: Number of nodes with available pods: 2
Feb 22 14:42:55.410: INFO: Number of running nodes: 2, number of available pods: 2
Feb 22 14:42:55.410: INFO: Update the DaemonSet to trigger a rollout
Feb 22 14:42:55.414: INFO: Updating DaemonSet daemon-set
Feb 22 14:43:09.422: INFO: Roll back the DaemonSet before rollout is complete
Feb 22 14:43:09.425: INFO: Updating DaemonSet daemon-set
Feb 22 14:43:09.425: INFO: Make sure DaemonSet rollback is complete
Feb 22 14:43:09.428: INFO: Wrong image for pod: daemon-set-rnmpb. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 22 14:43:09.428: INFO: Pod daemon-set-rnmpb is not available
Feb 22 14:43:09.432: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:43:10.434: INFO: Wrong image for pod: daemon-set-rnmpb. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 22 14:43:10.434: INFO: Pod daemon-set-rnmpb is not available
Feb 22 14:43:10.436: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:43:11.434: INFO: Wrong image for pod: daemon-set-rnmpb. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 22 14:43:11.434: INFO: Pod daemon-set-rnmpb is not available
Feb 22 14:43:11.436: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 14:43:12.434: INFO: Pod daemon-set-k2q8b is not available
Feb 22 14:43:12.436: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9665, will wait for the garbage collector to delete the pods
Feb 22 14:43:12.495: INFO: Deleting DaemonSet.extensions daemon-set took: 3.497141ms
Feb 22 14:43:12.995: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.176844ms
Feb 22 14:44:45.297: INFO: Number of nodes with available pods: 0
Feb 22 14:44:45.297: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 14:44:45.299: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9665/daemonsets","resourceVersion":"19315"},"items":null}

Feb 22 14:44:45.300: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9665/pods","resourceVersion":"19315"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:44:45.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9665" for this suite.
Feb 22 14:44:51.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:44:51.472: INFO: namespace daemonsets-9665 deletion completed in 6.165330195s

â€¢ [SLOW TEST:119.187 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:44:51.472: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 14:44:51.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8" in namespace "downward-api-4120" to be "success or failure"
Feb 22 14:44:51.500: INFO: Pod "downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.824478ms
Feb 22 14:44:53.502: INFO: Pod "downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005873201s
Feb 22 14:44:55.504: INFO: Pod "downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008120186s
STEP: Saw pod success
Feb 22 14:44:55.505: INFO: Pod "downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8" satisfied condition "success or failure"
Feb 22 14:44:55.506: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8 container client-container: <nil>
STEP: delete the pod
Feb 22 14:44:55.520: INFO: Waiting for pod downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8 to disappear
Feb 22 14:44:55.522: INFO: Pod downwardapi-volume-367d5588-1396-4061-ae5a-fac98cdfb4a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:44:55.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4120" for this suite.
Feb 22 14:45:01.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:45:01.699: INFO: namespace downward-api-4120 deletion completed in 6.17489885s

â€¢ [SLOW TEST:10.227 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:45:01.700: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ca4d375e-501d-4a84-8ec1-7244b3eb97c9
STEP: Creating a pod to test consume secrets
Feb 22 14:45:01.726: INFO: Waiting up to 5m0s for pod "pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02" in namespace "secrets-6529" to be "success or failure"
Feb 22 14:45:01.730: INFO: Pod "pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.553565ms
Feb 22 14:45:03.732: INFO: Pod "pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005539537s
Feb 22 14:45:05.735: INFO: Pod "pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008440464s
STEP: Saw pod success
Feb 22 14:45:05.735: INFO: Pod "pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02" satisfied condition "success or failure"
Feb 22 14:45:05.737: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02 container secret-env-test: <nil>
STEP: delete the pod
Feb 22 14:45:05.760: INFO: Waiting for pod pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02 to disappear
Feb 22 14:45:05.765: INFO: Pod pod-secrets-24ab6260-0ca9-4127-b103-0c11b1606f02 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:45:05.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6529" for this suite.
Feb 22 14:45:11.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:45:11.934: INFO: namespace secrets-6529 deletion completed in 6.166669299s

â€¢ [SLOW TEST:10.234 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:45:11.934: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:45:11.961: INFO: (0) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.149337ms)
Feb 22 14:45:11.964: INFO: (1) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.437359ms)
Feb 22 14:45:11.966: INFO: (2) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.227887ms)
Feb 22 14:45:11.968: INFO: (3) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.238734ms)
Feb 22 14:45:11.970: INFO: (4) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.127752ms)
Feb 22 14:45:11.973: INFO: (5) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.227302ms)
Feb 22 14:45:11.977: INFO: (6) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.002004ms)
Feb 22 14:45:11.979: INFO: (7) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.415695ms)
Feb 22 14:45:11.986: INFO: (8) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.862227ms)
Feb 22 14:45:11.989: INFO: (9) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.821336ms)
Feb 22 14:45:11.991: INFO: (10) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 1.856371ms)
Feb 22 14:45:11.993: INFO: (11) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.553222ms)
Feb 22 14:45:12.001: INFO: (12) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.965059ms)
Feb 22 14:45:12.004: INFO: (13) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.219951ms)
Feb 22 14:45:12.005: INFO: (14) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 1.943622ms)
Feb 22 14:45:12.007: INFO: (15) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 1.962232ms)
Feb 22 14:45:12.009: INFO: (16) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 1.974769ms)
Feb 22 14:45:12.012: INFO: (17) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.114948ms)
Feb 22 14:45:12.014: INFO: (18) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.273454ms)
Feb 22 14:45:12.016: INFO: (19) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.139717ms)
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:45:12.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8958" for this suite.
Feb 22 14:45:18.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:45:18.097: INFO: namespace proxy-8958 deletion completed in 6.079102776s

â€¢ [SLOW TEST:6.163 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:45:18.097: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0222 14:45:19.139994      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 22 14:45:19.140: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:45:19.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5755" for this suite.
Feb 22 14:45:25.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:45:25.314: INFO: namespace gc-5755 deletion completed in 6.172520161s

â€¢ [SLOW TEST:7.217 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:45:25.314: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 14:45:27.347: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:45:27.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7771" for this suite.
Feb 22 14:45:33.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:45:33.526: INFO: namespace container-runtime-7771 deletion completed in 6.165238172s

â€¢ [SLOW TEST:8.211 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:45:33.526: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-f8bg
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 14:45:33.555: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f8bg" in namespace "subpath-5938" to be "success or failure"
Feb 22 14:45:33.558: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.66844ms
Feb 22 14:45:35.560: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005429635s
Feb 22 14:45:37.562: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 4.007372942s
Feb 22 14:45:39.564: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 6.009248039s
Feb 22 14:45:41.566: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 8.011240083s
Feb 22 14:45:43.568: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 10.013331964s
Feb 22 14:45:45.570: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 12.015120534s
Feb 22 14:45:47.572: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 14.017226795s
Feb 22 14:45:49.574: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 16.019346257s
Feb 22 14:45:51.576: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 18.021423606s
Feb 22 14:45:53.578: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 20.023482034s
Feb 22 14:45:55.589: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Running", Reason="", readiness=true. Elapsed: 22.034676116s
Feb 22 14:45:57.592: INFO: Pod "pod-subpath-test-configmap-f8bg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036753133s
STEP: Saw pod success
Feb 22 14:45:57.592: INFO: Pod "pod-subpath-test-configmap-f8bg" satisfied condition "success or failure"
Feb 22 14:45:57.594: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-subpath-test-configmap-f8bg container test-container-subpath-configmap-f8bg: <nil>
STEP: delete the pod
Feb 22 14:45:57.609: INFO: Waiting for pod pod-subpath-test-configmap-f8bg to disappear
Feb 22 14:45:57.610: INFO: Pod pod-subpath-test-configmap-f8bg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f8bg
Feb 22 14:45:57.610: INFO: Deleting pod "pod-subpath-test-configmap-f8bg" in namespace "subpath-5938"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:45:57.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5938" for this suite.
Feb 22 14:46:03.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:46:03.780: INFO: namespace subpath-5938 deletion completed in 6.164955762s

â€¢ [SLOW TEST:30.254 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:46:03.780: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 22 14:46:03.847: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Feb 22 14:46:04.434: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 22 14:46:06.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:46:08.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:46:10.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:46:12.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:46:14.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717979564, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:46:18.002: INFO: Waited 1.514400471s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:46:18.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-141" for this suite.
Feb 22 14:46:24.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:46:24.983: INFO: namespace aggregator-141 deletion completed in 6.201171418s

â€¢ [SLOW TEST:21.203 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:46:24.983: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6e762082-a0d3-4712-a17b-a831a41f257d
STEP: Creating a pod to test consume secrets
Feb 22 14:46:25.016: INFO: Waiting up to 5m0s for pod "pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36" in namespace "secrets-9840" to be "success or failure"
Feb 22 14:46:25.019: INFO: Pod "pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.887528ms
Feb 22 14:46:27.021: INFO: Pod "pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004876725s
Feb 22 14:46:29.023: INFO: Pod "pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006808804s
STEP: Saw pod success
Feb 22 14:46:29.023: INFO: Pod "pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36" satisfied condition "success or failure"
Feb 22 14:46:29.024: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:46:29.036: INFO: Waiting for pod pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36 to disappear
Feb 22 14:46:29.037: INFO: Pod pod-secrets-5c33f011-f9b9-4b81-9310-7cd2b136aa36 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:46:29.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9840" for this suite.
Feb 22 14:46:35.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:46:35.221: INFO: namespace secrets-9840 deletion completed in 6.1790506s

â€¢ [SLOW TEST:10.237 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:46:35.221: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-dee3e5c0-1888-4048-9388-4ca7e7f0f833
STEP: Creating a pod to test consume configMaps
Feb 22 14:46:35.249: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136" in namespace "projected-5980" to be "success or failure"
Feb 22 14:46:35.254: INFO: Pod "pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458611ms
Feb 22 14:46:37.256: INFO: Pod "pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00738286s
STEP: Saw pod success
Feb 22 14:46:37.256: INFO: Pod "pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136" satisfied condition "success or failure"
Feb 22 14:46:37.258: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 14:46:37.271: INFO: Waiting for pod pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136 to disappear
Feb 22 14:46:37.273: INFO: Pod pod-projected-configmaps-4854e3f6-5693-4ebb-9cd4-72e5c98d4136 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:46:37.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5980" for this suite.
Feb 22 14:46:43.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:46:43.442: INFO: namespace projected-5980 deletion completed in 6.166536502s

â€¢ [SLOW TEST:8.221 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:46:43.442: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 22 14:46:43.471: INFO: Waiting up to 5m0s for pod "pod-05586939-fa61-4bd5-bbe2-176c29ab8f55" in namespace "emptydir-1003" to be "success or failure"
Feb 22 14:46:43.473: INFO: Pod "pod-05586939-fa61-4bd5-bbe2-176c29ab8f55": Phase="Pending", Reason="", readiness=false. Elapsed: 1.89247ms
Feb 22 14:46:45.475: INFO: Pod "pod-05586939-fa61-4bd5-bbe2-176c29ab8f55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004084303s
Feb 22 14:46:47.477: INFO: Pod "pod-05586939-fa61-4bd5-bbe2-176c29ab8f55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005964766s
STEP: Saw pod success
Feb 22 14:46:47.477: INFO: Pod "pod-05586939-fa61-4bd5-bbe2-176c29ab8f55" satisfied condition "success or failure"
Feb 22 14:46:47.478: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-05586939-fa61-4bd5-bbe2-176c29ab8f55 container test-container: <nil>
STEP: delete the pod
Feb 22 14:46:47.490: INFO: Waiting for pod pod-05586939-fa61-4bd5-bbe2-176c29ab8f55 to disappear
Feb 22 14:46:47.492: INFO: Pod pod-05586939-fa61-4bd5-bbe2-176c29ab8f55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:46:47.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1003" for this suite.
Feb 22 14:46:53.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:46:53.661: INFO: namespace emptydir-1003 deletion completed in 6.167293087s

â€¢ [SLOW TEST:10.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:46:53.661: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-76adf0d1-6671-405e-b5ff-d25bb621b2cb
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:46:53.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1557" for this suite.
Feb 22 14:46:59.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:46:59.905: INFO: namespace configmap-1557 deletion completed in 6.169268551s

â€¢ [SLOW TEST:6.243 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:46:59.905: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Feb 22 14:46:59.928: INFO: Waiting up to 5m0s for pod "client-containers-ef25497d-091e-4d7c-a67c-6049e9639566" in namespace "containers-3879" to be "success or failure"
Feb 22 14:46:59.937: INFO: Pod "client-containers-ef25497d-091e-4d7c-a67c-6049e9639566": Phase="Pending", Reason="", readiness=false. Elapsed: 9.800616ms
Feb 22 14:47:01.941: INFO: Pod "client-containers-ef25497d-091e-4d7c-a67c-6049e9639566": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013750639s
STEP: Saw pod success
Feb 22 14:47:01.941: INFO: Pod "client-containers-ef25497d-091e-4d7c-a67c-6049e9639566" satisfied condition "success or failure"
Feb 22 14:47:01.944: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod client-containers-ef25497d-091e-4d7c-a67c-6049e9639566 container test-container: <nil>
STEP: delete the pod
Feb 22 14:47:01.956: INFO: Waiting for pod client-containers-ef25497d-091e-4d7c-a67c-6049e9639566 to disappear
Feb 22 14:47:01.961: INFO: Pod client-containers-ef25497d-091e-4d7c-a67c-6049e9639566 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:47:01.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3879" for this suite.
Feb 22 14:47:07.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:47:08.142: INFO: namespace containers-3879 deletion completed in 6.176393387s

â€¢ [SLOW TEST:8.237 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:47:08.142: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4746
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 14:47:08.168: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 22 14:47:32.245: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.8.15:8080/dial?request=hostName&protocol=http&host=172.16.8.14&port=8080&tries=1'] Namespace:pod-network-test-4746 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:47:32.245: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:47:32.394: INFO: Waiting for endpoints: map[]
Feb 22 14:47:32.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.8.15:8080/dial?request=hostName&protocol=http&host=172.16.88.86&port=8080&tries=1'] Namespace:pod-network-test-4746 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:47:32.396: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:47:32.542: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:47:32.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4746" for this suite.
Feb 22 14:47:54.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:47:54.716: INFO: namespace pod-network-test-4746 deletion completed in 22.169570804s

â€¢ [SLOW TEST:46.574 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:47:54.717: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:47:54.740: INFO: (0) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.185511ms)
Feb 22 14:47:54.746: INFO: (1) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.329284ms)
Feb 22 14:47:54.749: INFO: (2) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.366439ms)
Feb 22 14:47:54.751: INFO: (3) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.339056ms)
Feb 22 14:47:54.753: INFO: (4) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.04285ms)
Feb 22 14:47:54.755: INFO: (5) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 1.8752ms)
Feb 22 14:47:54.757: INFO: (6) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.008668ms)
Feb 22 14:47:54.759: INFO: (7) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 1.890638ms)
Feb 22 14:47:54.762: INFO: (8) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.509083ms)
Feb 22 14:47:54.764: INFO: (9) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.082493ms)
Feb 22 14:47:54.766: INFO: (10) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.431621ms)
Feb 22 14:47:54.768: INFO: (11) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.218647ms)
Feb 22 14:47:54.770: INFO: (12) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.039618ms)
Feb 22 14:47:54.773: INFO: (13) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.192025ms)
Feb 22 14:47:54.776: INFO: (14) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 3.048107ms)
Feb 22 14:47:54.781: INFO: (15) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.547012ms)
Feb 22 14:47:54.785: INFO: (16) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 3.597721ms)
Feb 22 14:47:54.787: INFO: (17) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.225087ms)
Feb 22 14:47:54.790: INFO: (18) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 2.522368ms)
Feb 22 14:47:54.794: INFO: (19) /api/v1/nodes/ip-10-100-10-41.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.456619ms)
[AfterEach] version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:47:54.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3056" for this suite.
Feb 22 14:48:00.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:48:00.884: INFO: namespace proxy-3056 deletion completed in 6.086393739s

â€¢ [SLOW TEST:6.167 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:48:00.884: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Feb 22 14:48:00.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-2831'
Feb 22 14:48:01.254: INFO: stderr: ""
Feb 22 14:48:01.254: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 14:48:01.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2831'
Feb 22 14:48:01.319: INFO: stderr: ""
Feb 22 14:48:01.319: INFO: stdout: "update-demo-nautilus-n6vxz update-demo-nautilus-ng6tx "
Feb 22 14:48:01.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-n6vxz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:01.377: INFO: stderr: ""
Feb 22 14:48:01.377: INFO: stdout: ""
Feb 22 14:48:01.377: INFO: update-demo-nautilus-n6vxz is created but not running
Feb 22 14:48:06.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2831'
Feb 22 14:48:06.437: INFO: stderr: ""
Feb 22 14:48:06.437: INFO: stdout: "update-demo-nautilus-n6vxz update-demo-nautilus-ng6tx "
Feb 22 14:48:06.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-n6vxz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:06.494: INFO: stderr: ""
Feb 22 14:48:06.494: INFO: stdout: "true"
Feb 22 14:48:06.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-n6vxz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:06.552: INFO: stderr: ""
Feb 22 14:48:06.552: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:48:06.552: INFO: validating pod update-demo-nautilus-n6vxz
Feb 22 14:48:06.555: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:48:06.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:48:06.555: INFO: update-demo-nautilus-n6vxz is verified up and running
Feb 22 14:48:06.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-ng6tx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:06.613: INFO: stderr: ""
Feb 22 14:48:06.613: INFO: stdout: "true"
Feb 22 14:48:06.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-nautilus-ng6tx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:06.672: INFO: stderr: ""
Feb 22 14:48:06.672: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 22 14:48:06.672: INFO: validating pod update-demo-nautilus-ng6tx
Feb 22 14:48:06.675: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 22 14:48:06.675: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 22 14:48:06.675: INFO: update-demo-nautilus-ng6tx is verified up and running
STEP: rolling-update to new replication controller
Feb 22 14:48:06.677: INFO: scanned /root for discovery docs: <nil>
Feb 22 14:48:06.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2831'
Feb 22 14:48:31.020: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 22 14:48:31.020: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 22 14:48:31.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2831'
Feb 22 14:48:31.082: INFO: stderr: ""
Feb 22 14:48:31.082: INFO: stdout: "update-demo-kitten-kshrz update-demo-kitten-w7mdr "
Feb 22 14:48:31.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-kitten-kshrz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:31.141: INFO: stderr: ""
Feb 22 14:48:31.141: INFO: stdout: "true"
Feb 22 14:48:31.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-kitten-kshrz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:31.198: INFO: stderr: ""
Feb 22 14:48:31.198: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 22 14:48:31.198: INFO: validating pod update-demo-kitten-kshrz
Feb 22 14:48:31.201: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 22 14:48:31.201: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 22 14:48:31.201: INFO: update-demo-kitten-kshrz is verified up and running
Feb 22 14:48:31.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-kitten-w7mdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:31.259: INFO: stderr: ""
Feb 22 14:48:31.259: INFO: stdout: "true"
Feb 22 14:48:31.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods update-demo-kitten-w7mdr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2831'
Feb 22 14:48:31.318: INFO: stderr: ""
Feb 22 14:48:31.318: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 22 14:48:31.318: INFO: validating pod update-demo-kitten-w7mdr
Feb 22 14:48:31.325: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 22 14:48:31.325: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 22 14:48:31.325: INFO: update-demo-kitten-w7mdr is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:48:31.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2831" for this suite.
Feb 22 14:48:53.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:48:53.495: INFO: namespace kubectl-2831 deletion completed in 22.1663957s

â€¢ [SLOW TEST:52.611 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:48:53.495: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1aaf2175-7a1f-45d1-926a-c8a8f0794b38
STEP: Creating a pod to test consume secrets
Feb 22 14:48:53.538: INFO: Waiting up to 5m0s for pod "pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109" in namespace "secrets-8674" to be "success or failure"
Feb 22 14:48:53.542: INFO: Pod "pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109": Phase="Pending", Reason="", readiness=false. Elapsed: 3.444039ms
Feb 22 14:48:55.544: INFO: Pod "pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005395611s
Feb 22 14:48:57.546: INFO: Pod "pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007246397s
STEP: Saw pod success
Feb 22 14:48:57.546: INFO: Pod "pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109" satisfied condition "success or failure"
Feb 22 14:48:57.547: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:48:57.560: INFO: Waiting for pod pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109 to disappear
Feb 22 14:48:57.561: INFO: Pod pod-secrets-f07892bf-5946-4910-9661-8b5c3d0df109 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:48:57.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8674" for this suite.
Feb 22 14:49:03.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:49:03.738: INFO: namespace secrets-8674 deletion completed in 6.174433024s
STEP: Destroying namespace "secret-namespace-5830" for this suite.
Feb 22 14:49:09.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:49:09.910: INFO: namespace secret-namespace-5830 deletion completed in 6.171745838s

â€¢ [SLOW TEST:16.415 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:49:09.911: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-6418/configmap-test-933de459-9ebd-4ed2-a644-6bf863fdddf5
STEP: Creating a pod to test consume configMaps
Feb 22 14:49:09.950: INFO: Waiting up to 5m0s for pod "pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04" in namespace "configmap-6418" to be "success or failure"
Feb 22 14:49:09.954: INFO: Pod "pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03839ms
Feb 22 14:49:11.955: INFO: Pod "pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005792015s
STEP: Saw pod success
Feb 22 14:49:11.955: INFO: Pod "pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04" satisfied condition "success or failure"
Feb 22 14:49:11.957: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04 container env-test: <nil>
STEP: delete the pod
Feb 22 14:49:11.968: INFO: Waiting for pod pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04 to disappear
Feb 22 14:49:11.970: INFO: Pod pod-configmaps-326bd86e-1e0a-4e0e-9e69-6b3bf08e0b04 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:49:11.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6418" for this suite.
Feb 22 14:49:17.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:49:18.143: INFO: namespace configmap-6418 deletion completed in 6.170574376s

â€¢ [SLOW TEST:8.232 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:49:18.143: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 22 14:49:20.187: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-674502360 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 22 14:49:30.261: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:49:30.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4293" for this suite.
Feb 22 14:49:36.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:49:36.432: INFO: namespace pods-4293 deletion completed in 6.167189454s

â€¢ [SLOW TEST:18.289 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:49:36.432: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 22 14:49:46.566: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0222 14:49:46.566895      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:49:46.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2728" for this suite.
Feb 22 14:49:52.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:49:52.735: INFO: namespace gc-2728 deletion completed in 6.165548675s

â€¢ [SLOW TEST:16.303 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:49:52.736: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 22 14:49:57.288: INFO: Successfully updated pod "annotationupdate86c86999-7190-46ed-aa59-3cb6cf7adbbd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:49:59.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9076" for this suite.
Feb 22 14:50:21.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:50:21.468: INFO: namespace projected-9076 deletion completed in 22.168450381s

â€¢ [SLOW TEST:28.732 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:50:21.469: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-902
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 22 14:50:21.491: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 22 14:50:45.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.8.19:8080/dial?request=hostName&protocol=udp&host=172.16.8.21&port=8081&tries=1'] Namespace:pod-network-test-902 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:50:45.533: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:50:45.653: INFO: Waiting for endpoints: map[]
Feb 22 14:50:45.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.8.19:8080/dial?request=hostName&protocol=udp&host=172.16.88.92&port=8081&tries=1'] Namespace:pod-network-test-902 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 22 14:50:45.655: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
Feb 22 14:50:45.794: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:50:45.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-902" for this suite.
Feb 22 14:51:07.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:51:07.967: INFO: namespace pod-network-test-902 deletion completed in 22.17022446s

â€¢ [SLOW TEST:46.498 seconds]
[sig-network] Networking
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:51:07.967: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 22 14:51:11.012: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:51:11.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9708" for this suite.
Feb 22 14:51:17.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:51:17.191: INFO: namespace container-runtime-9708 deletion completed in 6.166358809s

â€¢ [SLOW TEST:9.224 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:51:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7003/configmap-test-3e41f992-7970-4768-b6c9-bd72e120348b
STEP: Creating a pod to test consume configMaps
Feb 22 14:51:17.218: INFO: Waiting up to 5m0s for pod "pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910" in namespace "configmap-7003" to be "success or failure"
Feb 22 14:51:17.221: INFO: Pod "pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354219ms
Feb 22 14:51:19.223: INFO: Pod "pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005444499s
Feb 22 14:51:21.225: INFO: Pod "pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007291274s
STEP: Saw pod success
Feb 22 14:51:21.225: INFO: Pod "pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910" satisfied condition "success or failure"
Feb 22 14:51:21.226: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910 container env-test: <nil>
STEP: delete the pod
Feb 22 14:51:21.238: INFO: Waiting for pod pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910 to disappear
Feb 22 14:51:21.239: INFO: Pod pod-configmaps-8445c426-85a1-4681-b325-fdb5630cc910 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:51:21.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7003" for this suite.
Feb 22 14:51:27.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:51:27.407: INFO: namespace configmap-7003 deletion completed in 6.165759903s

â€¢ [SLOW TEST:10.216 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:51:27.408: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1543
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 22 14:51:27.450: INFO: Found 0 stateful pods, waiting for 3
Feb 22 14:51:37.452: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:51:37.452: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:51:37.452: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 22 14:51:37.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-1543 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:51:37.657: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:51:37.657: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:51:37.657: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 22 14:51:47.681: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 22 14:51:57.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-1543 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:51:57.895: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:51:57.895: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:51:57.895: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:52:07.906: INFO: Waiting for StatefulSet statefulset-1543/ss2 to complete update
Feb 22 14:52:07.906: INFO: Waiting for Pod statefulset-1543/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Feb 22 14:52:17.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-1543 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 22 14:52:18.129: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 22 14:52:18.129: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 22 14:52:18.129: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 22 14:52:28.151: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 22 14:52:38.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 exec --namespace=statefulset-1543 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 22 14:52:38.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 22 14:52:38.365: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 22 14:52:38.365: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 22 14:52:48.375: INFO: Waiting for StatefulSet statefulset-1543/ss2 to complete update
Feb 22 14:52:48.375: INFO: Waiting for Pod statefulset-1543/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 22 14:52:48.375: INFO: Waiting for Pod statefulset-1543/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 22 14:52:48.375: INFO: Waiting for Pod statefulset-1543/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 22 14:52:58.379: INFO: Waiting for StatefulSet statefulset-1543/ss2 to complete update
Feb 22 14:52:58.379: INFO: Waiting for Pod statefulset-1543/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 22 14:52:58.379: INFO: Waiting for Pod statefulset-1543/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 22 14:53:08.379: INFO: Deleting all statefulset in ns statefulset-1543
Feb 22 14:53:08.381: INFO: Scaling statefulset ss2 to 0
Feb 22 14:53:28.390: INFO: Waiting for statefulset status.replicas updated to 0
Feb 22 14:53:28.392: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:53:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1543" for this suite.
Feb 22 14:53:34.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:53:34.573: INFO: namespace statefulset-1543 deletion completed in 6.166804592s

â€¢ [SLOW TEST:127.165 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:53:34.573: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 22 14:53:39.123: INFO: Successfully updated pod "annotationupdate68b07f1b-aa01-45b7-8dff-8f574aa8daeb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:53:41.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-527" for this suite.
Feb 22 14:54:03.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:54:03.306: INFO: namespace downward-api-527 deletion completed in 22.167356232s

â€¢ [SLOW TEST:28.733 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:54:03.306: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 22 14:54:07.340: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-ae5f9cfd-ef98-4b08-acb1-97ab6d875436,GenerateName:,Namespace:events-4351,SelfLink:/api/v1/namespaces/events-4351/pods/send-events-ae5f9cfd-ef98-4b08-acb1-97ab6d875436,UID:e90447fb-c5f4-4964-bcdf-fb413569701a,ResourceVersion:22270,Generation:0,CreationTimestamp:2020-02-22 14:54:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 325726693,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.29/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r6gbq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6gbq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-r6gbq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023f69d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023f6a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:54:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:54:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:54:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:54:03 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.29,StartTime:2020-02-22 14:54:03 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-02-22 14:54:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://653e75e61ed6edd01160913ef1b77e1c706e8328ba236435b6b1ce6e3eed5534}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 22 14:54:09.342: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 22 14:54:11.344: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:54:11.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4351" for this suite.
Feb 22 14:54:49.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:54:49.518: INFO: namespace events-4351 deletion completed in 38.167530542s

â€¢ [SLOW TEST:46.212 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:54:49.518: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 22 14:54:49.782: INFO: Pod name wrapped-volume-race-c49d8b92-ddf0-4e69-882b-7d1d0e6f1671: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c49d8b92-ddf0-4e69-882b-7d1d0e6f1671 in namespace emptydir-wrapper-7964, will wait for the garbage collector to delete the pods
Feb 22 14:55:05.888: INFO: Deleting ReplicationController wrapped-volume-race-c49d8b92-ddf0-4e69-882b-7d1d0e6f1671 took: 5.082177ms
Feb 22 14:55:06.388: INFO: Terminating ReplicationController wrapped-volume-race-c49d8b92-ddf0-4e69-882b-7d1d0e6f1671 pods took: 500.155874ms
STEP: Creating RC which spawns configmap-volume pods
Feb 22 14:55:49.098: INFO: Pod name wrapped-volume-race-56cb6dad-a7b0-4539-b1bd-058f4524f0c8: Found 0 pods out of 5
Feb 22 14:55:54.102: INFO: Pod name wrapped-volume-race-56cb6dad-a7b0-4539-b1bd-058f4524f0c8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-56cb6dad-a7b0-4539-b1bd-058f4524f0c8 in namespace emptydir-wrapper-7964, will wait for the garbage collector to delete the pods
Feb 22 14:56:06.172: INFO: Deleting ReplicationController wrapped-volume-race-56cb6dad-a7b0-4539-b1bd-058f4524f0c8 took: 7.068522ms
Feb 22 14:56:06.672: INFO: Terminating ReplicationController wrapped-volume-race-56cb6dad-a7b0-4539-b1bd-058f4524f0c8 pods took: 500.192524ms
STEP: Creating RC which spawns configmap-volume pods
Feb 22 14:56:49.083: INFO: Pod name wrapped-volume-race-e42d22d0-9d24-4269-8730-dafe2440120b: Found 0 pods out of 5
Feb 22 14:56:54.087: INFO: Pod name wrapped-volume-race-e42d22d0-9d24-4269-8730-dafe2440120b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e42d22d0-9d24-4269-8730-dafe2440120b in namespace emptydir-wrapper-7964, will wait for the garbage collector to delete the pods
Feb 22 14:57:06.163: INFO: Deleting ReplicationController wrapped-volume-race-e42d22d0-9d24-4269-8730-dafe2440120b took: 8.550905ms
Feb 22 14:57:06.663: INFO: Terminating ReplicationController wrapped-volume-race-e42d22d0-9d24-4269-8730-dafe2440120b pods took: 500.173103ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:57:49.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7964" for this suite.
Feb 22 14:57:55.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:57:55.376: INFO: namespace emptydir-wrapper-7964 deletion completed in 6.173368029s

â€¢ [SLOW TEST:185.858 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:57:55.377: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:57:59.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9143" for this suite.
Feb 22 14:58:41.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:58:41.580: INFO: namespace kubelet-test-9143 deletion completed in 42.166134618s

â€¢ [SLOW TEST:46.203 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:58:41.580: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 14:58:41.607: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 22 14:58:46.609: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 22 14:58:46.609: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 22 14:58:48.611: INFO: Creating deployment "test-rollover-deployment"
Feb 22 14:58:48.619: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 22 14:58:50.625: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 22 14:58:50.628: INFO: Ensure that both replica sets have 1 created replica
Feb 22 14:58:50.631: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 22 14:58:50.635: INFO: Updating deployment test-rollover-deployment
Feb 22 14:58:50.635: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 22 14:58:52.641: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 22 14:58:52.645: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 22 14:58:52.647: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 14:58:52.647: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980330, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:58:54.651: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 14:58:54.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980333, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:58:56.651: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 14:58:56.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980333, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:58:58.651: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 14:58:58.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980333, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:59:00.651: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 14:59:00.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980333, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:59:02.651: INFO: all replica sets need to contain the pod-template-hash label
Feb 22 14:59:02.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980333, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717980328, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 14:59:04.652: INFO: 
Feb 22 14:59:04.652: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 22 14:59:04.657: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-6175,SelfLink:/apis/apps/v1/namespaces/deployment-6175/deployments/test-rollover-deployment,UID:ebdf11ae-afb0-4845-8950-8789a26f9ccf,ResourceVersion:24135,Generation:2,CreationTimestamp:2020-02-22 14:58:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-22 14:58:48 +0000 UTC 2020-02-22 14:58:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-22 14:59:03 +0000 UTC 2020-02-22 14:58:48 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 22 14:59:04.660: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-6175,SelfLink:/apis/apps/v1/namespaces/deployment-6175/replicasets/test-rollover-deployment-854595fc44,UID:89e6b93a-01d6-417a-bf42-eda832b37933,ResourceVersion:24124,Generation:2,CreationTimestamp:2020-02-22 14:58:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ebdf11ae-afb0-4845-8950-8789a26f9ccf 0xc000ab9627 0xc000ab9628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 22 14:59:04.660: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 22 14:59:04.660: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-6175,SelfLink:/apis/apps/v1/namespaces/deployment-6175/replicasets/test-rollover-controller,UID:589343d8-ceda-4e49-b73b-8e657255ff44,ResourceVersion:24133,Generation:2,CreationTimestamp:2020-02-22 14:58:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ebdf11ae-afb0-4845-8950-8789a26f9ccf 0xc000ab9557 0xc000ab9558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 22 14:59:04.660: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-6175,SelfLink:/apis/apps/v1/namespaces/deployment-6175/replicasets/test-rollover-deployment-9b8b997cf,UID:bc65350b-fe0a-488f-ae46-04aa972e039a,ResourceVersion:24073,Generation:2,CreationTimestamp:2020-02-22 14:58:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ebdf11ae-afb0-4845-8950-8789a26f9ccf 0xc000ab9700 0xc000ab9701}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 22 14:59:04.662: INFO: Pod "test-rollover-deployment-854595fc44-zc777" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-zc777,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-6175,SelfLink:/api/v1/namespaces/deployment-6175/pods/test-rollover-deployment-854595fc44-zc777,UID:9d4db919-fa50-4f39-90c9-756fe17b7afe,ResourceVersion:24095,Generation:0,CreationTimestamp:2020-02-22 14:58:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.45/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 89e6b93a-01d6-417a-bf42-eda832b37933 0xc0027446d7 0xc0027446d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wq5cs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wq5cs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wq5cs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002744740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002744760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:58:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:58:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:58:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 14:58:50 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.45,StartTime:2020-02-22 14:58:50 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-22 14:58:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://065563f9b584a51c08d3bac48a9a7aff90e2013761000f57d1d57ca01d757c63}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:59:04.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6175" for this suite.
Feb 22 14:59:10.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:59:10.830: INFO: namespace deployment-6175 deletion completed in 6.16570641s

â€¢ [SLOW TEST:29.250 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:59:10.831: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Feb 22 14:59:10.850: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-674502360 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:59:10.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3164" for this suite.
Feb 22 14:59:16.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:59:17.085: INFO: namespace kubectl-3164 deletion completed in 6.171818233s

â€¢ [SLOW TEST:6.254 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:59:17.085: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 22 14:59:19.620: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7104 pod-service-account-6ef830c1-7af1-4fd9-b514-841b086e9e13 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 22 14:59:19.926: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7104 pod-service-account-6ef830c1-7af1-4fd9-b514-841b086e9e13 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 22 14:59:20.117: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7104 pod-service-account-6ef830c1-7af1-4fd9-b514-841b086e9e13 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:59:20.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7104" for this suite.
Feb 22 14:59:26.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:59:26.487: INFO: namespace svcaccounts-7104 deletion completed in 6.17330238s

â€¢ [SLOW TEST:9.401 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:59:26.487: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 22 14:59:26.511: INFO: Waiting up to 5m0s for pod "pod-b846d871-4c1b-40de-9871-73906ce3b11e" in namespace "emptydir-2766" to be "success or failure"
Feb 22 14:59:26.512: INFO: Pod "pod-b846d871-4c1b-40de-9871-73906ce3b11e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.56774ms
Feb 22 14:59:28.514: INFO: Pod "pod-b846d871-4c1b-40de-9871-73906ce3b11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003728652s
Feb 22 14:59:30.516: INFO: Pod "pod-b846d871-4c1b-40de-9871-73906ce3b11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0058145s
STEP: Saw pod success
Feb 22 14:59:30.516: INFO: Pod "pod-b846d871-4c1b-40de-9871-73906ce3b11e" satisfied condition "success or failure"
Feb 22 14:59:30.519: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-b846d871-4c1b-40de-9871-73906ce3b11e container test-container: <nil>
STEP: delete the pod
Feb 22 14:59:30.533: INFO: Waiting for pod pod-b846d871-4c1b-40de-9871-73906ce3b11e to disappear
Feb 22 14:59:30.534: INFO: Pod pod-b846d871-4c1b-40de-9871-73906ce3b11e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:59:30.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2766" for this suite.
Feb 22 14:59:36.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:59:36.702: INFO: namespace emptydir-2766 deletion completed in 6.165645825s

â€¢ [SLOW TEST:10.215 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:59:36.703: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-aa4a8fcc-dcba-46db-9a59-5bab14bff230
STEP: Creating a pod to test consume secrets
Feb 22 14:59:36.733: INFO: Waiting up to 5m0s for pod "pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32" in namespace "secrets-4415" to be "success or failure"
Feb 22 14:59:36.737: INFO: Pod "pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.843824ms
Feb 22 14:59:38.739: INFO: Pod "pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005787176s
Feb 22 14:59:40.741: INFO: Pod "pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007946352s
STEP: Saw pod success
Feb 22 14:59:40.741: INFO: Pod "pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32" satisfied condition "success or failure"
Feb 22 14:59:40.743: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32 container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 14:59:40.755: INFO: Waiting for pod pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32 to disappear
Feb 22 14:59:40.757: INFO: Pod pod-secrets-c386fab2-2680-41f6-828d-5d4acc471b32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:59:40.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4415" for this suite.
Feb 22 14:59:46.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 14:59:46.937: INFO: namespace secrets-4415 deletion completed in 6.174381385s

â€¢ [SLOW TEST:10.234 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 14:59:46.937: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 22 14:59:51.486: INFO: Successfully updated pod "labelsupdatec1382c65-6d2c-474f-942a-15d558178d70"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 14:59:53.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-710" for this suite.
Feb 22 15:00:15.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:00:15.669: INFO: namespace downward-api-710 deletion completed in 22.166968339s

â€¢ [SLOW TEST:28.732 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:00:15.669: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 15:00:15.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c" in namespace "projected-648" to be "success or failure"
Feb 22 15:00:15.699: INFO: Pod "downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.901092ms
Feb 22 15:00:17.700: INFO: Pod "downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003687028s
STEP: Saw pod success
Feb 22 15:00:17.701: INFO: Pod "downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c" satisfied condition "success or failure"
Feb 22 15:00:17.702: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c container client-container: <nil>
STEP: delete the pod
Feb 22 15:00:17.713: INFO: Waiting for pod downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c to disappear
Feb 22 15:00:17.715: INFO: Pod downwardapi-volume-04bb3a75-4c44-4c5d-b7c7-899212683f7c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:00:17.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-648" for this suite.
Feb 22 15:00:23.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:00:23.896: INFO: namespace projected-648 deletion completed in 6.177944232s

â€¢ [SLOW TEST:8.226 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:00:23.896: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:00:29.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9763" for this suite.
Feb 22 15:00:35.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:00:35.665: INFO: namespace watch-9763 deletion completed in 6.265110521s

â€¢ [SLOW TEST:11.769 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:00:35.665: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 22 15:00:35.684: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:00:40.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8647" for this suite.
Feb 22 15:00:46.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:00:46.209: INFO: namespace init-container-8647 deletion completed in 6.174538262s

â€¢ [SLOW TEST:10.544 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:00:46.210: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 15:00:46.232: INFO: Creating deployment "nginx-deployment"
Feb 22 15:00:46.237: INFO: Waiting for observed generation 1
Feb 22 15:00:48.241: INFO: Waiting for all required pods to come up
Feb 22 15:00:48.243: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 22 15:00:50.257: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 22 15:00:50.260: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 22 15:00:50.264: INFO: Updating deployment nginx-deployment
Feb 22 15:00:50.264: INFO: Waiting for observed generation 2
Feb 22 15:00:52.268: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 22 15:00:52.270: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 22 15:00:52.271: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 22 15:00:52.275: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 22 15:00:52.275: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 22 15:00:52.277: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 22 15:00:52.279: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 22 15:00:52.279: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 22 15:00:52.282: INFO: Updating deployment nginx-deployment
Feb 22 15:00:52.282: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 22 15:00:52.291: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 22 15:00:52.310: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 22 15:00:52.343: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9308,SelfLink:/apis/apps/v1/namespaces/deployment-9308/deployments/nginx-deployment,UID:756d8ff3-ab94-486a-9f1c-cd2c4811c686,ResourceVersion:25031,Generation:3,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2020-02-22 15:00:50 +0000 UTC 2020-02-22 15:00:46 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2020-02-22 15:00:52 +0000 UTC 2020-02-22 15:00:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 22 15:00:52.361: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-9308,SelfLink:/apis/apps/v1/namespaces/deployment-9308/replicasets/nginx-deployment-55fb7cb77f,UID:49897d00-a85d-4520-b879-6a52fd9177e7,ResourceVersion:25011,Generation:3,CreationTimestamp:2020-02-22 15:00:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 756d8ff3-ab94-486a-9f1c-cd2c4811c686 0xc002b254c7 0xc002b254c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 22 15:00:52.361: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 22 15:00:52.361: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-9308,SelfLink:/apis/apps/v1/namespaces/deployment-9308/replicasets/nginx-deployment-7b8c6f4498,UID:4a6c1d66-56ed-4788-9e26-987ca1345eed,ResourceVersion:25009,Generation:3,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 756d8ff3-ab94-486a-9f1c-cd2c4811c686 0xc002b25597 0xc002b25598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 22 15:00:52.403: INFO: Pod "nginx-deployment-55fb7cb77f-2599z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2599z,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-2599z,UID:c8b23ced-e1e3-4e41-a506-b06236e8610e,ResourceVersion:25033,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc003389f27 0xc003389f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003389f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003389fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.403: INFO: Pod "nginx-deployment-55fb7cb77f-49v2v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-49v2v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-49v2v,UID:97dadb87-ce46-4df1-9b55-1b61c0da0cca,ResourceVersion:25059,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4030 0xc002fc4031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc40a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc40c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.403: INFO: Pod "nginx-deployment-55fb7cb77f-5xf5w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5xf5w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-5xf5w,UID:3776dce4-5923-481a-a3b8-aa9c940c3a44,ResourceVersion:25006,Generation:0,CreationTimestamp:2020-02-22 15:00:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4150 0xc002fc4151}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc41c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc41e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:,StartTime:2020-02-22 15:00:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.403: INFO: Pod "nginx-deployment-55fb7cb77f-84bs8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-84bs8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-84bs8,UID:3b3e16a0-f61b-4d39-867b-3b5f5a2c50e2,ResourceVersion:25068,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc42b0 0xc002fc42b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.403: INFO: Pod "nginx-deployment-55fb7cb77f-9dfmd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9dfmd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-9dfmd,UID:f4c8019d-2a8c-4352-a2b6-6606a8355833,ResourceVersion:25035,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc43c0 0xc002fc43c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.403: INFO: Pod "nginx-deployment-55fb7cb77f-b4mpx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-b4mpx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-b4mpx,UID:273fdcdd-2f9e-4a30-880d-5608bf1be0f7,ResourceVersion:25052,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc44d0 0xc002fc44d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:,StartTime:2020-02-22 15:00:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-ffzzd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ffzzd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-ffzzd,UID:601b8f91-6277-481c-8620-728e1edc8faa,ResourceVersion:24994,Generation:0,CreationTimestamp:2020-02-22 15:00:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.54/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4640 0xc002fc4641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc46b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc46d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:,StartTime:2020-02-22 15:00:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-hbqwh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hbqwh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-hbqwh,UID:a79198f3-d1b9-469b-bf45-8d1940e8313a,ResourceVersion:24999,Generation:0,CreationTimestamp:2020-02-22 15:00:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.108/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc47b0 0xc002fc47b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:,StartTime:2020-02-22 15:00:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-j9srg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j9srg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-j9srg,UID:89701bca-8578-4568-ae61-bf2a54380383,ResourceVersion:25061,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4910 0xc002fc4911}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc49a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-prxgs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-prxgs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-prxgs,UID:eaa331bc-01cd-40ae-a9d7-dd2c0f9e26f0,ResourceVersion:25002,Generation:0,CreationTimestamp:2020-02-22 15:00:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.58/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4a30 0xc002fc4a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:,StartTime:2020-02-22 15:00:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-q4tqz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-q4tqz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-q4tqz,UID:90a334c6-0de7-4e68-befc-2d173ffd9513,ResourceVersion:25057,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4b90 0xc002fc4b91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-w7llw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-w7llw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-w7llw,UID:75c8fcbf-d17d-4d21-b2cd-afd5053b371e,ResourceVersion:25049,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4ca0 0xc002fc4ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-55fb7cb77f-zhp7t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zhp7t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-55fb7cb77f-zhp7t,UID:6338668a-a855-4950-91a2-30bad58dcf06,ResourceVersion:24989,Generation:0,CreationTimestamp:2020-02-22 15:00:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.110/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 49897d00-a85d-4520-b879-6a52fd9177e7 0xc002fc4dc0 0xc002fc4dc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:50 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:,StartTime:2020-02-22 15:00:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-7b8c6f4498-4brdk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4brdk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-4brdk,UID:e6786cfb-f87d-4849-9f67-0e3a5b244b0e,ResourceVersion:25056,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc4f20 0xc002fc4f21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc4f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc4fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-7b8c6f4498-9j59k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9j59k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-9j59k,UID:e7f90828-ed4e-468e-8f69-27b2ac9f759e,ResourceVersion:24912,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.53/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5030 0xc002fc5031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc50b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.53,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fb977aebb7a9ecc4aa4ffe390ddd22ef227257ae6e0847b7f20133b5b341e37d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.404: INFO: Pod "nginx-deployment-7b8c6f4498-blc4j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-blc4j,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-blc4j,UID:d91c2332-d01d-47ac-8e09-36a3ae48f006,ResourceVersion:25048,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5180 0xc002fc5181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc51e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-bpdt2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bpdt2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-bpdt2,UID:40779494-a58b-4b53-8149-ea05ab8ca8db,ResourceVersion:24881,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.103/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5290 0xc002fc5291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc52f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:172.16.88.103,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:47 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bcc0ff02f0af02bc7e45371a4d5fa6f18968d57245dd51a9af0b5a4275c1e6a7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-fzfn4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fzfn4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-fzfn4,UID:0c95281b-b3f3-4f6f-9f21-ef694da5b803,ResourceVersion:24900,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.52/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc53f7 0xc002fc53f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.52,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5caf45434f8347a33abf0e9f9a855b2d5a9b420d8f47a5489943b1f5226e25bc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-gk2gj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gk2gj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-gk2gj,UID:d705b5b1-9592-4816-9e05-f737ea3c0f25,ResourceVersion:25041,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5550 0xc002fc5551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc55b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc55d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-hfp94" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hfp94,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-hfp94,UID:c67ba240-1676-4223-a5da-1605a8254b65,ResourceVersion:25064,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5650 0xc002fc5651}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc56b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc56d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-hpmzv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hpmzv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-hpmzv,UID:2c42c3f0-baed-4d9b-b038-6fc36637b1df,ResourceVersion:25053,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5750 0xc002fc5751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc57b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc57d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:,StartTime:2020-02-22 15:00:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-j4jq4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j4jq4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-j4jq4,UID:2466e7a7-aae1-4b03-a226-5554e55fb9bf,ResourceVersion:24919,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.107/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc58a7 0xc002fc58a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:172.16.88.107,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b86b764d332194deb2d2484aa1cfa3f9696f8e9852df18856eb8364a16f00a66}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-p62n8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p62n8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-p62n8,UID:500dda41-759a-4a7b-bcb7-71b268227980,ResourceVersion:25066,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5a07 0xc002fc5a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-rrzhn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rrzhn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-rrzhn,UID:ef1393f3-c5ef-4caf-9605-94b3c5221e0f,ResourceVersion:24893,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.104/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5b20 0xc002fc5b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:172.16.88.104,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c0e23667d5da5c72460e3a9d276ad4cdb12e8069e79c54e8e0e018afa91e3455}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-sdgsp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sdgsp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-sdgsp,UID:31718507-9cf7-4d57-9106-4be0cc9f91d0,ResourceVersion:24916,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.106/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5c87 0xc002fc5c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:172.16.88.106,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e077b339f5bf434bac7ad258eb6b188ab0d0da36c903274d7e65f2e37f200fe2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.405: INFO: Pod "nginx-deployment-7b8c6f4498-sm8gz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sm8gz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-sm8gz,UID:f685ea2c-fa84-4582-9e54-bb4519452904,ResourceVersion:24909,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5df7 0xc002fc5df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5e60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5e80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.56,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2bda334099df034f17a512da96e46941e43af33b62d401d4f1d35e246cb58fcd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-tdsps" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tdsps,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-tdsps,UID:e193a4ae-ee2c-4824-bef6-a0ba774a6e13,ResourceVersion:25067,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc002fc5f50 0xc002fc5f51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fc5fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fc5fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-tlg9t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tlg9t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-tlg9t,UID:2780cea6-7304-43ae-98a5-7f76976f353e,ResourceVersion:25030,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc003574050 0xc003574051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0035740b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0035740d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:,StartTime:2020-02-22 15:00:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-tn7gm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tn7gm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-tn7gm,UID:109073e2-94fc-40b4-b5d6-3657ff86ada3,ResourceVersion:25060,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc003574197 0xc003574198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003574200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003574220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-v86km" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-v86km,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-v86km,UID:72014a70-f792-409e-a869-164d9a788332,ResourceVersion:25036,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc0035742a0 0xc0035742a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003574300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003574320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-x2pj5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x2pj5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-x2pj5,UID:b0f45a5a-3427-4c62-bb3b-92e9ae67de90,ResourceVersion:25022,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc0035743a0 0xc0035743a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003574400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003574420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-xrt6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xrt6x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-xrt6x,UID:25509d89-3436-45b0-aae2-9d36d626804b,ResourceVersion:25065,Generation:0,CreationTimestamp:2020-02-22 15:00:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc0035744a0 0xc0035744a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003574500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003574520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 22 15:00:52.406: INFO: Pod "nginx-deployment-7b8c6f4498-xwxxs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xwxxs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9308,SelfLink:/api/v1/namespaces/deployment-9308/pods/nginx-deployment-7b8c6f4498-xwxxs,UID:55bbfeae-49cc-420e-915a-b076cb970faa,ResourceVersion:24922,Generation:0,CreationTimestamp:2020-02-22 15:00:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.88.105/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 4a6c1d66-56ed-4788-9e26-987ca1345eed 0xc0035745b0 0xc0035745b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvvs7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvvs7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qvvs7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-53.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003574610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003574630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:00:46 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.53,PodIP:172.16.88.105,StartTime:2020-02-22 15:00:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-22 15:00:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c2ed5fd9341b6582e08108755d973248c40cda54a8e6030c92a13c725cfb0b4a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:00:52.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9308" for this suite.
Feb 22 15:00:58.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:00:58.609: INFO: namespace deployment-9308 deletion completed in 6.187781484s

â€¢ [SLOW TEST:12.400 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:00:58.610: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 15:00:58.646: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:01:04.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-461" for this suite.
Feb 22 15:01:50.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:01:51.019: INFO: namespace pods-461 deletion completed in 46.166403477s

â€¢ [SLOW TEST:52.409 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:01:51.019: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 15:01:51.047: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 22 15:01:51.054: INFO: Number of nodes with available pods: 0
Feb 22 15:01:51.054: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 22 15:01:51.070: INFO: Number of nodes with available pods: 0
Feb 22 15:01:51.070: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:52.072: INFO: Number of nodes with available pods: 0
Feb 22 15:01:52.072: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:53.072: INFO: Number of nodes with available pods: 0
Feb 22 15:01:53.072: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:54.072: INFO: Number of nodes with available pods: 1
Feb 22 15:01:54.072: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 22 15:01:54.085: INFO: Number of nodes with available pods: 1
Feb 22 15:01:54.085: INFO: Number of running nodes: 0, number of available pods: 1
Feb 22 15:01:55.087: INFO: Number of nodes with available pods: 0
Feb 22 15:01:55.087: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 22 15:01:55.097: INFO: Number of nodes with available pods: 0
Feb 22 15:01:55.097: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:56.101: INFO: Number of nodes with available pods: 0
Feb 22 15:01:56.101: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:57.099: INFO: Number of nodes with available pods: 0
Feb 22 15:01:57.099: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:58.099: INFO: Number of nodes with available pods: 0
Feb 22 15:01:58.099: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:01:59.099: INFO: Number of nodes with available pods: 0
Feb 22 15:01:59.099: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:02:00.099: INFO: Number of nodes with available pods: 1
Feb 22 15:02:00.099: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6028, will wait for the garbage collector to delete the pods
Feb 22 15:02:00.157: INFO: Deleting DaemonSet.extensions daemon-set took: 3.47239ms
Feb 22 15:02:00.657: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.152741ms
Feb 22 15:02:03.459: INFO: Number of nodes with available pods: 0
Feb 22 15:02:03.459: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 15:02:03.461: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6028/daemonsets","resourceVersion":"25757"},"items":null}

Feb 22 15:02:03.462: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6028/pods","resourceVersion":"25757"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:02:03.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6028" for this suite.
Feb 22 15:02:09.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:02:09.654: INFO: namespace daemonsets-6028 deletion completed in 6.171381126s

â€¢ [SLOW TEST:18.634 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:02:09.654: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-cb57
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 15:02:09.682: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cb57" in namespace "subpath-9193" to be "success or failure"
Feb 22 15:02:09.686: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.771236ms
Feb 22 15:02:11.688: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005828642s
Feb 22 15:02:13.690: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 4.00791695s
Feb 22 15:02:15.692: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 6.009906497s
Feb 22 15:02:17.694: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 8.011979515s
Feb 22 15:02:19.696: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 10.013856128s
Feb 22 15:02:21.698: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 12.015852482s
Feb 22 15:02:23.700: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 14.017927723s
Feb 22 15:02:25.702: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 16.02000073s
Feb 22 15:02:27.704: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 18.021668457s
Feb 22 15:02:29.705: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Running", Reason="", readiness=true. Elapsed: 20.023622556s
Feb 22 15:02:31.708: INFO: Pod "pod-subpath-test-secret-cb57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.025670775s
STEP: Saw pod success
Feb 22 15:02:31.708: INFO: Pod "pod-subpath-test-secret-cb57" satisfied condition "success or failure"
Feb 22 15:02:31.709: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-subpath-test-secret-cb57 container test-container-subpath-secret-cb57: <nil>
STEP: delete the pod
Feb 22 15:02:31.720: INFO: Waiting for pod pod-subpath-test-secret-cb57 to disappear
Feb 22 15:02:31.722: INFO: Pod pod-subpath-test-secret-cb57 no longer exists
STEP: Deleting pod pod-subpath-test-secret-cb57
Feb 22 15:02:31.722: INFO: Deleting pod "pod-subpath-test-secret-cb57" in namespace "subpath-9193"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:02:31.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9193" for this suite.
Feb 22 15:02:37.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:02:37.902: INFO: namespace subpath-9193 deletion completed in 6.173397426s

â€¢ [SLOW TEST:28.249 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:02:37.903: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5519ebf9-0a0d-413e-8c8c-4b3f22812b04
STEP: Creating a pod to test consume configMaps
Feb 22 15:02:37.926: INFO: Waiting up to 5m0s for pod "pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3" in namespace "configmap-8413" to be "success or failure"
Feb 22 15:02:37.931: INFO: Pod "pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626003ms
Feb 22 15:02:39.933: INFO: Pod "pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006882775s
Feb 22 15:02:41.935: INFO: Pod "pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008885761s
STEP: Saw pod success
Feb 22 15:02:41.935: INFO: Pod "pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3" satisfied condition "success or failure"
Feb 22 15:02:41.937: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 15:02:41.947: INFO: Waiting for pod pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3 to disappear
Feb 22 15:02:41.950: INFO: Pod pod-configmaps-d48858bc-a65d-47d0-a01f-fff8e4f27fd3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:02:41.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8413" for this suite.
Feb 22 15:02:47.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:02:48.118: INFO: namespace configmap-8413 deletion completed in 6.165192128s

â€¢ [SLOW TEST:10.215 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:02:48.118: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 15:02:48.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2375'
Feb 22 15:02:48.206: INFO: stderr: ""
Feb 22 15:02:48.206: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 22 15:02:53.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pod e2e-test-nginx-pod --namespace=kubectl-2375 -o json'
Feb 22 15:02:53.317: INFO: stderr: ""
Feb 22 15:02:53.317: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.16.88.122/32\"\n        },\n        \"creationTimestamp\": \"2020-02-22T15:02:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2375\",\n        \"resourceVersion\": \"25978\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2375/pods/e2e-test-nginx-pod\",\n        \"uid\": \"42ef4ba8-1601-4244-8993-54aed79b82d2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-g8qlr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-100-10-53.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-g8qlr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-g8qlr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-22T15:02:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-22T15:02:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-22T15:02:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-22T15:02:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://de821440843380fa6fd492121c2f2a48c6c161466e590b534d15ff360eed712f\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-22T15:02:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.100.10.53\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.88.122\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-22T15:02:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 22 15:02:53.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 replace -f - --namespace=kubectl-2375'
Feb 22 15:02:53.472: INFO: stderr: ""
Feb 22 15:02:53.472: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Feb 22 15:02:53.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete pods e2e-test-nginx-pod --namespace=kubectl-2375'
Feb 22 15:02:55.259: INFO: stderr: ""
Feb 22 15:02:55.259: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:02:55.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2375" for this suite.
Feb 22 15:03:01.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:03:01.428: INFO: namespace kubectl-2375 deletion completed in 6.165903877s

â€¢ [SLOW TEST:13.310 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:03:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 22 15:03:01.452: INFO: Waiting up to 5m0s for pod "pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee" in namespace "emptydir-7162" to be "success or failure"
Feb 22 15:03:01.456: INFO: Pod "pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.544121ms
Feb 22 15:03:03.458: INFO: Pod "pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005868945s
Feb 22 15:03:05.460: INFO: Pod "pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007859539s
STEP: Saw pod success
Feb 22 15:03:05.460: INFO: Pod "pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee" satisfied condition "success or failure"
Feb 22 15:03:05.461: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee container test-container: <nil>
STEP: delete the pod
Feb 22 15:03:05.484: INFO: Waiting for pod pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee to disappear
Feb 22 15:03:05.487: INFO: Pod pod-89e24eb2-f1c2-4482-884c-3cb67ae343ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:03:05.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7162" for this suite.
Feb 22 15:03:11.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:03:11.657: INFO: namespace emptydir-7162 deletion completed in 6.168019816s

â€¢ [SLOW TEST:10.229 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:03:11.658: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:03:34.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4808" for this suite.
Feb 22 15:03:40.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:03:41.007: INFO: namespace container-runtime-4808 deletion completed in 6.166266148s

â€¢ [SLOW TEST:29.349 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:03:41.007: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 22 15:03:41.030: INFO: Waiting up to 5m0s for pod "pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551" in namespace "emptydir-3467" to be "success or failure"
Feb 22 15:03:41.032: INFO: Pod "pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510549ms
Feb 22 15:03:43.034: INFO: Pod "pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004521373s
Feb 22 15:03:45.036: INFO: Pod "pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006548936s
STEP: Saw pod success
Feb 22 15:03:45.036: INFO: Pod "pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551" satisfied condition "success or failure"
Feb 22 15:03:45.038: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551 container test-container: <nil>
STEP: delete the pod
Feb 22 15:03:45.049: INFO: Waiting for pod pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551 to disappear
Feb 22 15:03:45.051: INFO: Pod pod-41f59cfd-d7d9-4d56-8c21-0a1331dc4551 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:03:45.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3467" for this suite.
Feb 22 15:03:51.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:03:51.220: INFO: namespace emptydir-3467 deletion completed in 6.166354367s

â€¢ [SLOW TEST:10.213 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:03:51.221: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-9f4d74d3-61d5-4f53-8a2d-d1ce5e39d3e4
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:03:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4416" for this suite.
Feb 22 15:03:57.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:03:57.407: INFO: namespace secrets-4416 deletion completed in 6.164502437s

â€¢ [SLOW TEST:6.186 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:03:57.407: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Feb 22 15:03:57.433: INFO: Waiting up to 5m0s for pod "var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715" in namespace "var-expansion-5027" to be "success or failure"
Feb 22 15:03:57.442: INFO: Pod "var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715": Phase="Pending", Reason="", readiness=false. Elapsed: 9.436674ms
Feb 22 15:03:59.447: INFO: Pod "var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013632901s
STEP: Saw pod success
Feb 22 15:03:59.447: INFO: Pod "var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715" satisfied condition "success or failure"
Feb 22 15:03:59.448: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715 container dapi-container: <nil>
STEP: delete the pod
Feb 22 15:03:59.467: INFO: Waiting for pod var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715 to disappear
Feb 22 15:03:59.469: INFO: Pod var-expansion-c8c4da5e-6cde-4402-8b6b-24305d91b715 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:03:59.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5027" for this suite.
Feb 22 15:04:05.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:04:05.642: INFO: namespace var-expansion-5027 deletion completed in 6.169339383s

â€¢ [SLOW TEST:8.236 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:04:05.642: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 22 15:04:05.666: INFO: Waiting up to 5m0s for pod "pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c" in namespace "emptydir-9252" to be "success or failure"
Feb 22 15:04:05.673: INFO: Pod "pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.576229ms
Feb 22 15:04:07.675: INFO: Pod "pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008731386s
Feb 22 15:04:09.682: INFO: Pod "pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015452112s
STEP: Saw pod success
Feb 22 15:04:09.682: INFO: Pod "pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c" satisfied condition "success or failure"
Feb 22 15:04:09.684: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c container test-container: <nil>
STEP: delete the pod
Feb 22 15:04:09.700: INFO: Waiting for pod pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c to disappear
Feb 22 15:04:09.701: INFO: Pod pod-7c78ff9c-4ace-4333-b853-b2aeaf77171c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:04:09.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9252" for this suite.
Feb 22 15:04:15.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:04:15.868: INFO: namespace emptydir-9252 deletion completed in 6.164305069s

â€¢ [SLOW TEST:10.226 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:04:15.868: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 15:04:15.891: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 22 15:04:16.914: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:04:16.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5107" for this suite.
Feb 22 15:04:22.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:04:23.105: INFO: namespace replication-controller-5107 deletion completed in 6.181918695s

â€¢ [SLOW TEST:7.237 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:04:23.105: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 22 15:04:23.182: INFO: Waiting up to 5m0s for pod "pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d" in namespace "emptydir-3828" to be "success or failure"
Feb 22 15:04:23.190: INFO: Pod "pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.77593ms
Feb 22 15:04:25.193: INFO: Pod "pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010365943s
Feb 22 15:04:27.195: INFO: Pod "pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012462823s
STEP: Saw pod success
Feb 22 15:04:27.195: INFO: Pod "pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d" satisfied condition "success or failure"
Feb 22 15:04:27.197: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d container test-container: <nil>
STEP: delete the pod
Feb 22 15:04:27.207: INFO: Waiting for pod pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d to disappear
Feb 22 15:04:27.209: INFO: Pod pod-c7d4aaeb-35f8-4467-a1f5-d66c14e7da6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:04:27.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3828" for this suite.
Feb 22 15:04:33.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:04:33.380: INFO: namespace emptydir-3828 deletion completed in 6.168622578s

â€¢ [SLOW TEST:10.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:04:33.381: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Feb 22 15:04:33.405: INFO: Waiting up to 5m0s for pod "client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6" in namespace "containers-7579" to be "success or failure"
Feb 22 15:04:33.409: INFO: Pod "client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252842ms
Feb 22 15:04:35.410: INFO: Pod "client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005210948s
Feb 22 15:04:37.413: INFO: Pod "client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007306369s
STEP: Saw pod success
Feb 22 15:04:37.413: INFO: Pod "client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6" satisfied condition "success or failure"
Feb 22 15:04:37.414: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6 container test-container: <nil>
STEP: delete the pod
Feb 22 15:04:37.427: INFO: Waiting for pod client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6 to disappear
Feb 22 15:04:37.429: INFO: Pod client-containers-8e6c4216-d360-415e-9f4f-2cc39380c2d6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:04:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7579" for this suite.
Feb 22 15:04:43.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:04:43.597: INFO: namespace containers-7579 deletion completed in 6.166060544s

â€¢ [SLOW TEST:10.216 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:04:43.598: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 22 15:04:49.635: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:04:49.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0222 15:04:49.635764      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2194" for this suite.
Feb 22 15:04:55.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:04:55.803: INFO: namespace gc-2194 deletion completed in 6.166145357s

â€¢ [SLOW TEST:12.206 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:04:55.804: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5963
I0222 15:04:55.825872      18 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5963, replica count: 1
I0222 15:04:56.876172      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 15:04:57.876324      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0222 15:04:58.876480      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 22 15:04:58.983: INFO: Created: latency-svc-8nrj2
Feb 22 15:04:58.987: INFO: Got endpoints: latency-svc-8nrj2 [10.899285ms]
Feb 22 15:04:59.002: INFO: Created: latency-svc-2sc22
Feb 22 15:04:59.012: INFO: Got endpoints: latency-svc-2sc22 [24.252775ms]
Feb 22 15:04:59.021: INFO: Created: latency-svc-b25jv
Feb 22 15:04:59.023: INFO: Got endpoints: latency-svc-b25jv [36.125469ms]
Feb 22 15:04:59.031: INFO: Created: latency-svc-lwrwg
Feb 22 15:04:59.032: INFO: Got endpoints: latency-svc-lwrwg [44.837376ms]
Feb 22 15:04:59.065: INFO: Created: latency-svc-jllzx
Feb 22 15:04:59.065: INFO: Got endpoints: latency-svc-jllzx [77.503903ms]
Feb 22 15:04:59.066: INFO: Created: latency-svc-c2hjj
Feb 22 15:04:59.069: INFO: Got endpoints: latency-svc-c2hjj [81.155318ms]
Feb 22 15:04:59.075: INFO: Created: latency-svc-d2x7d
Feb 22 15:04:59.080: INFO: Got endpoints: latency-svc-d2x7d [92.479503ms]
Feb 22 15:04:59.089: INFO: Created: latency-svc-mmm6h
Feb 22 15:04:59.092: INFO: Got endpoints: latency-svc-mmm6h [104.111525ms]
Feb 22 15:04:59.108: INFO: Created: latency-svc-h6xgn
Feb 22 15:04:59.109: INFO: Got endpoints: latency-svc-h6xgn [121.333443ms]
Feb 22 15:04:59.120: INFO: Created: latency-svc-67c47
Feb 22 15:04:59.125: INFO: Got endpoints: latency-svc-67c47 [136.90887ms]
Feb 22 15:04:59.134: INFO: Created: latency-svc-z74zs
Feb 22 15:04:59.138: INFO: Got endpoints: latency-svc-z74zs [150.136422ms]
Feb 22 15:04:59.146: INFO: Created: latency-svc-zcdt8
Feb 22 15:04:59.149: INFO: Got endpoints: latency-svc-zcdt8 [160.357897ms]
Feb 22 15:04:59.161: INFO: Created: latency-svc-f2nwh
Feb 22 15:04:59.163: INFO: Got endpoints: latency-svc-f2nwh [174.58181ms]
Feb 22 15:04:59.178: INFO: Created: latency-svc-bjnl4
Feb 22 15:04:59.180: INFO: Got endpoints: latency-svc-bjnl4 [192.183914ms]
Feb 22 15:04:59.189: INFO: Created: latency-svc-lctr9
Feb 22 15:04:59.203: INFO: Got endpoints: latency-svc-lctr9 [214.651566ms]
Feb 22 15:04:59.218: INFO: Created: latency-svc-j7w5n
Feb 22 15:04:59.221: INFO: Got endpoints: latency-svc-j7w5n [232.740088ms]
Feb 22 15:04:59.238: INFO: Created: latency-svc-754gn
Feb 22 15:04:59.245: INFO: Got endpoints: latency-svc-754gn [233.838695ms]
Feb 22 15:04:59.256: INFO: Created: latency-svc-r8rc2
Feb 22 15:04:59.268: INFO: Got endpoints: latency-svc-r8rc2 [244.494941ms]
Feb 22 15:04:59.271: INFO: Created: latency-svc-wglbm
Feb 22 15:04:59.278: INFO: Got endpoints: latency-svc-wglbm [245.616457ms]
Feb 22 15:04:59.294: INFO: Created: latency-svc-4j5q2
Feb 22 15:04:59.296: INFO: Got endpoints: latency-svc-4j5q2 [230.929755ms]
Feb 22 15:04:59.303: INFO: Created: latency-svc-qfvfr
Feb 22 15:04:59.306: INFO: Got endpoints: latency-svc-qfvfr [237.368858ms]
Feb 22 15:04:59.312: INFO: Created: latency-svc-gkxg9
Feb 22 15:04:59.316: INFO: Got endpoints: latency-svc-gkxg9 [235.775047ms]
Feb 22 15:04:59.320: INFO: Created: latency-svc-mlb8s
Feb 22 15:04:59.320: INFO: Got endpoints: latency-svc-mlb8s [228.195953ms]
Feb 22 15:04:59.332: INFO: Created: latency-svc-zscg9
Feb 22 15:04:59.334: INFO: Got endpoints: latency-svc-zscg9 [224.851025ms]
Feb 22 15:04:59.344: INFO: Created: latency-svc-gwh5d
Feb 22 15:04:59.353: INFO: Got endpoints: latency-svc-gwh5d [227.637795ms]
Feb 22 15:04:59.361: INFO: Created: latency-svc-2nmjq
Feb 22 15:04:59.362: INFO: Got endpoints: latency-svc-2nmjq [223.691666ms]
Feb 22 15:04:59.370: INFO: Created: latency-svc-zjrkg
Feb 22 15:04:59.373: INFO: Got endpoints: latency-svc-zjrkg [224.604041ms]
Feb 22 15:04:59.377: INFO: Created: latency-svc-4lpk8
Feb 22 15:04:59.380: INFO: Got endpoints: latency-svc-4lpk8 [217.10886ms]
Feb 22 15:04:59.390: INFO: Created: latency-svc-7lzp2
Feb 22 15:04:59.399: INFO: Got endpoints: latency-svc-7lzp2 [219.564974ms]
Feb 22 15:04:59.403: INFO: Created: latency-svc-92l7v
Feb 22 15:04:59.408: INFO: Got endpoints: latency-svc-92l7v [204.674672ms]
Feb 22 15:04:59.415: INFO: Created: latency-svc-ztr5b
Feb 22 15:04:59.416: INFO: Got endpoints: latency-svc-ztr5b [195.136361ms]
Feb 22 15:04:59.438: INFO: Created: latency-svc-rgj8r
Feb 22 15:04:59.440: INFO: Got endpoints: latency-svc-rgj8r [194.857013ms]
Feb 22 15:04:59.451: INFO: Created: latency-svc-whws6
Feb 22 15:04:59.456: INFO: Got endpoints: latency-svc-whws6 [188.154178ms]
Feb 22 15:04:59.471: INFO: Created: latency-svc-njxgj
Feb 22 15:04:59.471: INFO: Got endpoints: latency-svc-njxgj [192.453284ms]
Feb 22 15:04:59.487: INFO: Created: latency-svc-dwsfw
Feb 22 15:04:59.491: INFO: Got endpoints: latency-svc-dwsfw [194.897299ms]
Feb 22 15:04:59.509: INFO: Created: latency-svc-6n2p7
Feb 22 15:04:59.513: INFO: Got endpoints: latency-svc-6n2p7 [206.524894ms]
Feb 22 15:04:59.532: INFO: Created: latency-svc-w2w56
Feb 22 15:04:59.535: INFO: Got endpoints: latency-svc-w2w56 [218.92267ms]
Feb 22 15:04:59.545: INFO: Created: latency-svc-hqh29
Feb 22 15:04:59.549: INFO: Got endpoints: latency-svc-hqh29 [228.861688ms]
Feb 22 15:04:59.557: INFO: Created: latency-svc-s58nz
Feb 22 15:04:59.560: INFO: Got endpoints: latency-svc-s58nz [225.568005ms]
Feb 22 15:04:59.585: INFO: Created: latency-svc-qhq9k
Feb 22 15:04:59.585: INFO: Got endpoints: latency-svc-qhq9k [232.51029ms]
Feb 22 15:04:59.598: INFO: Created: latency-svc-kvrgs
Feb 22 15:04:59.604: INFO: Got endpoints: latency-svc-kvrgs [241.927767ms]
Feb 22 15:04:59.618: INFO: Created: latency-svc-mtlpk
Feb 22 15:04:59.625: INFO: Got endpoints: latency-svc-mtlpk [251.396059ms]
Feb 22 15:04:59.637: INFO: Created: latency-svc-f82j8
Feb 22 15:04:59.640: INFO: Got endpoints: latency-svc-f82j8 [260.127081ms]
Feb 22 15:04:59.650: INFO: Created: latency-svc-bxhnm
Feb 22 15:04:59.656: INFO: Created: latency-svc-z9b9p
Feb 22 15:04:59.669: INFO: Created: latency-svc-7dl4g
Feb 22 15:04:59.676: INFO: Created: latency-svc-pns4h
Feb 22 15:04:59.690: INFO: Created: latency-svc-6gnxk
Feb 22 15:04:59.693: INFO: Got endpoints: latency-svc-bxhnm [293.228398ms]
Feb 22 15:04:59.705: INFO: Created: latency-svc-ft5qs
Feb 22 15:04:59.715: INFO: Created: latency-svc-bm6fm
Feb 22 15:04:59.732: INFO: Created: latency-svc-tjmhp
Feb 22 15:04:59.744: INFO: Created: latency-svc-hwtqx
Feb 22 15:04:59.751: INFO: Got endpoints: latency-svc-z9b9p [343.077307ms]
Feb 22 15:04:59.760: INFO: Created: latency-svc-g9llx
Feb 22 15:04:59.771: INFO: Created: latency-svc-tnwgb
Feb 22 15:04:59.779: INFO: Created: latency-svc-jpnng
Feb 22 15:04:59.788: INFO: Created: latency-svc-th42z
Feb 22 15:04:59.791: INFO: Got endpoints: latency-svc-7dl4g [374.99176ms]
Feb 22 15:04:59.799: INFO: Created: latency-svc-kh7qf
Feb 22 15:04:59.988: INFO: Got endpoints: latency-svc-pns4h [548.030969ms]
Feb 22 15:04:59.989: INFO: Got endpoints: latency-svc-6gnxk [532.41114ms]
Feb 22 15:04:59.990: INFO: Created: latency-svc-fq8lr
Feb 22 15:04:59.994: INFO: Got endpoints: latency-svc-bm6fm [502.503547ms]
Feb 22 15:04:59.994: INFO: Got endpoints: latency-svc-ft5qs [523.397807ms]
Feb 22 15:05:00.019: INFO: Created: latency-svc-ldgdg
Feb 22 15:05:00.023: INFO: Created: latency-svc-ksmwh
Feb 22 15:05:00.044: INFO: Got endpoints: latency-svc-tjmhp [531.022564ms]
Feb 22 15:05:00.044: INFO: Created: latency-svc-zxrqm
Feb 22 15:05:00.052: INFO: Created: latency-svc-wd72k
Feb 22 15:05:00.086: INFO: Created: latency-svc-5fgt5
Feb 22 15:05:00.093: INFO: Got endpoints: latency-svc-hwtqx [557.977242ms]
Feb 22 15:05:00.111: INFO: Created: latency-svc-prbd8
Feb 22 15:05:00.120: INFO: Created: latency-svc-fzvbz
Feb 22 15:05:00.129: INFO: Created: latency-svc-4khcc
Feb 22 15:05:00.137: INFO: Created: latency-svc-2hjht
Feb 22 15:05:00.139: INFO: Got endpoints: latency-svc-g9llx [589.607188ms]
Feb 22 15:05:00.151: INFO: Created: latency-svc-9v5sg
Feb 22 15:05:00.188: INFO: Got endpoints: latency-svc-tnwgb [627.698535ms]
Feb 22 15:05:00.202: INFO: Created: latency-svc-vzjgn
Feb 22 15:05:00.238: INFO: Got endpoints: latency-svc-jpnng [652.724711ms]
Feb 22 15:05:00.253: INFO: Created: latency-svc-bp7f7
Feb 22 15:05:00.292: INFO: Got endpoints: latency-svc-th42z [688.37235ms]
Feb 22 15:05:00.304: INFO: Created: latency-svc-snc5p
Feb 22 15:05:00.337: INFO: Got endpoints: latency-svc-kh7qf [712.062318ms]
Feb 22 15:05:00.348: INFO: Created: latency-svc-hlgjs
Feb 22 15:05:00.387: INFO: Got endpoints: latency-svc-fq8lr [747.169987ms]
Feb 22 15:05:00.398: INFO: Created: latency-svc-77vsh
Feb 22 15:05:00.436: INFO: Got endpoints: latency-svc-ldgdg [743.744381ms]
Feb 22 15:05:00.450: INFO: Created: latency-svc-w4znf
Feb 22 15:05:00.487: INFO: Got endpoints: latency-svc-ksmwh [735.932014ms]
Feb 22 15:05:00.498: INFO: Created: latency-svc-nvdgr
Feb 22 15:05:00.538: INFO: Got endpoints: latency-svc-zxrqm [746.289637ms]
Feb 22 15:05:00.550: INFO: Created: latency-svc-p46xt
Feb 22 15:05:00.589: INFO: Got endpoints: latency-svc-wd72k [600.708186ms]
Feb 22 15:05:00.602: INFO: Created: latency-svc-5v2xz
Feb 22 15:05:00.641: INFO: Got endpoints: latency-svc-5fgt5 [653.013739ms]
Feb 22 15:05:00.657: INFO: Created: latency-svc-lfnnt
Feb 22 15:05:00.688: INFO: Got endpoints: latency-svc-prbd8 [694.113624ms]
Feb 22 15:05:00.700: INFO: Created: latency-svc-nltt8
Feb 22 15:05:00.739: INFO: Got endpoints: latency-svc-fzvbz [744.766196ms]
Feb 22 15:05:00.756: INFO: Created: latency-svc-zm74q
Feb 22 15:05:00.787: INFO: Got endpoints: latency-svc-4khcc [742.949403ms]
Feb 22 15:05:00.797: INFO: Created: latency-svc-m8rvk
Feb 22 15:05:00.837: INFO: Got endpoints: latency-svc-2hjht [743.557709ms]
Feb 22 15:05:00.849: INFO: Created: latency-svc-dt9mn
Feb 22 15:05:00.888: INFO: Got endpoints: latency-svc-9v5sg [748.860322ms]
Feb 22 15:05:00.901: INFO: Created: latency-svc-nqptg
Feb 22 15:05:00.937: INFO: Got endpoints: latency-svc-vzjgn [749.203612ms]
Feb 22 15:05:00.951: INFO: Created: latency-svc-jhnwf
Feb 22 15:05:00.987: INFO: Got endpoints: latency-svc-bp7f7 [748.936709ms]
Feb 22 15:05:00.998: INFO: Created: latency-svc-znsgn
Feb 22 15:05:01.039: INFO: Got endpoints: latency-svc-snc5p [746.758592ms]
Feb 22 15:05:01.053: INFO: Created: latency-svc-rxpms
Feb 22 15:05:01.088: INFO: Got endpoints: latency-svc-hlgjs [750.658134ms]
Feb 22 15:05:01.103: INFO: Created: latency-svc-rp9zf
Feb 22 15:05:01.138: INFO: Got endpoints: latency-svc-77vsh [750.244849ms]
Feb 22 15:05:01.178: INFO: Created: latency-svc-sl8nf
Feb 22 15:05:01.187: INFO: Got endpoints: latency-svc-w4znf [750.88344ms]
Feb 22 15:05:01.198: INFO: Created: latency-svc-9tbkn
Feb 22 15:05:01.241: INFO: Got endpoints: latency-svc-nvdgr [754.355652ms]
Feb 22 15:05:01.253: INFO: Created: latency-svc-74wv7
Feb 22 15:05:01.291: INFO: Got endpoints: latency-svc-p46xt [753.438743ms]
Feb 22 15:05:01.314: INFO: Created: latency-svc-8cx92
Feb 22 15:05:01.337: INFO: Got endpoints: latency-svc-5v2xz [747.752423ms]
Feb 22 15:05:01.352: INFO: Created: latency-svc-84xnb
Feb 22 15:05:01.388: INFO: Got endpoints: latency-svc-lfnnt [746.232613ms]
Feb 22 15:05:01.401: INFO: Created: latency-svc-pdkjl
Feb 22 15:05:01.437: INFO: Got endpoints: latency-svc-nltt8 [748.751665ms]
Feb 22 15:05:01.457: INFO: Created: latency-svc-sl9vs
Feb 22 15:05:01.488: INFO: Got endpoints: latency-svc-zm74q [749.272393ms]
Feb 22 15:05:01.504: INFO: Created: latency-svc-8g4rv
Feb 22 15:05:01.538: INFO: Got endpoints: latency-svc-m8rvk [751.415336ms]
Feb 22 15:05:01.551: INFO: Created: latency-svc-kpzsn
Feb 22 15:05:01.588: INFO: Got endpoints: latency-svc-dt9mn [750.574773ms]
Feb 22 15:05:01.601: INFO: Created: latency-svc-9xrtr
Feb 22 15:05:01.638: INFO: Got endpoints: latency-svc-nqptg [749.668383ms]
Feb 22 15:05:01.653: INFO: Created: latency-svc-ptbfx
Feb 22 15:05:01.687: INFO: Got endpoints: latency-svc-jhnwf [750.410313ms]
Feb 22 15:05:01.698: INFO: Created: latency-svc-hrvhc
Feb 22 15:05:01.740: INFO: Got endpoints: latency-svc-znsgn [752.977378ms]
Feb 22 15:05:01.753: INFO: Created: latency-svc-7k7z6
Feb 22 15:05:01.789: INFO: Got endpoints: latency-svc-rxpms [749.574986ms]
Feb 22 15:05:01.801: INFO: Created: latency-svc-ds56m
Feb 22 15:05:01.838: INFO: Got endpoints: latency-svc-rp9zf [750.506896ms]
Feb 22 15:05:01.850: INFO: Created: latency-svc-vqfgx
Feb 22 15:05:01.889: INFO: Got endpoints: latency-svc-sl8nf [750.915113ms]
Feb 22 15:05:01.900: INFO: Created: latency-svc-v2v68
Feb 22 15:05:01.938: INFO: Got endpoints: latency-svc-9tbkn [750.409111ms]
Feb 22 15:05:01.952: INFO: Created: latency-svc-qfkm4
Feb 22 15:05:01.989: INFO: Got endpoints: latency-svc-74wv7 [747.549849ms]
Feb 22 15:05:02.003: INFO: Created: latency-svc-gq2kg
Feb 22 15:05:02.037: INFO: Got endpoints: latency-svc-8cx92 [746.277179ms]
Feb 22 15:05:02.054: INFO: Created: latency-svc-2p5kf
Feb 22 15:05:02.089: INFO: Got endpoints: latency-svc-84xnb [751.618614ms]
Feb 22 15:05:02.105: INFO: Created: latency-svc-lgz2g
Feb 22 15:05:02.138: INFO: Got endpoints: latency-svc-pdkjl [750.194242ms]
Feb 22 15:05:02.154: INFO: Created: latency-svc-nckcv
Feb 22 15:05:02.188: INFO: Got endpoints: latency-svc-sl9vs [750.944183ms]
Feb 22 15:05:02.204: INFO: Created: latency-svc-7sl6v
Feb 22 15:05:02.238: INFO: Got endpoints: latency-svc-8g4rv [749.787678ms]
Feb 22 15:05:02.253: INFO: Created: latency-svc-5p6nf
Feb 22 15:05:02.287: INFO: Got endpoints: latency-svc-kpzsn [748.94388ms]
Feb 22 15:05:02.303: INFO: Created: latency-svc-gjm4v
Feb 22 15:05:02.338: INFO: Got endpoints: latency-svc-9xrtr [750.107169ms]
Feb 22 15:05:02.354: INFO: Created: latency-svc-6dgz4
Feb 22 15:05:02.388: INFO: Got endpoints: latency-svc-ptbfx [750.04261ms]
Feb 22 15:05:02.426: INFO: Created: latency-svc-vs5xv
Feb 22 15:05:02.437: INFO: Got endpoints: latency-svc-hrvhc [749.324019ms]
Feb 22 15:05:02.448: INFO: Created: latency-svc-jbv57
Feb 22 15:05:02.488: INFO: Got endpoints: latency-svc-7k7z6 [747.97697ms]
Feb 22 15:05:02.501: INFO: Created: latency-svc-dnhcm
Feb 22 15:05:02.538: INFO: Got endpoints: latency-svc-ds56m [748.964716ms]
Feb 22 15:05:02.550: INFO: Created: latency-svc-lpxtk
Feb 22 15:05:02.587: INFO: Got endpoints: latency-svc-vqfgx [748.961109ms]
Feb 22 15:05:02.602: INFO: Created: latency-svc-bw5mz
Feb 22 15:05:02.637: INFO: Got endpoints: latency-svc-v2v68 [748.209534ms]
Feb 22 15:05:02.645: INFO: Created: latency-svc-64g46
Feb 22 15:05:02.687: INFO: Got endpoints: latency-svc-qfkm4 [749.494705ms]
Feb 22 15:05:02.700: INFO: Created: latency-svc-fpx6c
Feb 22 15:05:02.737: INFO: Got endpoints: latency-svc-gq2kg [748.607927ms]
Feb 22 15:05:02.747: INFO: Created: latency-svc-wr6pw
Feb 22 15:05:02.801: INFO: Got endpoints: latency-svc-2p5kf [763.652223ms]
Feb 22 15:05:02.817: INFO: Created: latency-svc-fvznm
Feb 22 15:05:02.837: INFO: Got endpoints: latency-svc-lgz2g [748.046567ms]
Feb 22 15:05:02.852: INFO: Created: latency-svc-mt6j2
Feb 22 15:05:02.887: INFO: Got endpoints: latency-svc-nckcv [749.023972ms]
Feb 22 15:05:02.898: INFO: Created: latency-svc-bz42n
Feb 22 15:05:02.937: INFO: Got endpoints: latency-svc-7sl6v [749.4378ms]
Feb 22 15:05:02.956: INFO: Created: latency-svc-cthbm
Feb 22 15:05:02.988: INFO: Got endpoints: latency-svc-5p6nf [749.969588ms]
Feb 22 15:05:03.003: INFO: Created: latency-svc-bfhnq
Feb 22 15:05:03.038: INFO: Got endpoints: latency-svc-gjm4v [750.445866ms]
Feb 22 15:05:03.054: INFO: Created: latency-svc-59c84
Feb 22 15:05:03.089: INFO: Got endpoints: latency-svc-6dgz4 [750.915049ms]
Feb 22 15:05:03.102: INFO: Created: latency-svc-j8cng
Feb 22 15:05:03.137: INFO: Got endpoints: latency-svc-vs5xv [748.876552ms]
Feb 22 15:05:03.154: INFO: Created: latency-svc-dzqfq
Feb 22 15:05:03.187: INFO: Got endpoints: latency-svc-jbv57 [750.708619ms]
Feb 22 15:05:03.204: INFO: Created: latency-svc-8mzqc
Feb 22 15:05:03.237: INFO: Got endpoints: latency-svc-dnhcm [748.878425ms]
Feb 22 15:05:03.253: INFO: Created: latency-svc-xlcrm
Feb 22 15:05:03.289: INFO: Got endpoints: latency-svc-lpxtk [750.875986ms]
Feb 22 15:05:03.304: INFO: Created: latency-svc-5qq5c
Feb 22 15:05:03.337: INFO: Got endpoints: latency-svc-bw5mz [749.985807ms]
Feb 22 15:05:03.351: INFO: Created: latency-svc-s2628
Feb 22 15:05:03.387: INFO: Got endpoints: latency-svc-64g46 [749.958843ms]
Feb 22 15:05:03.401: INFO: Created: latency-svc-lhgs5
Feb 22 15:05:03.442: INFO: Got endpoints: latency-svc-fpx6c [754.359152ms]
Feb 22 15:05:03.455: INFO: Created: latency-svc-mb7x6
Feb 22 15:05:03.489: INFO: Got endpoints: latency-svc-wr6pw [751.961636ms]
Feb 22 15:05:03.501: INFO: Created: latency-svc-w29gk
Feb 22 15:05:03.541: INFO: Got endpoints: latency-svc-fvznm [739.865128ms]
Feb 22 15:05:03.560: INFO: Created: latency-svc-2bswd
Feb 22 15:05:03.588: INFO: Got endpoints: latency-svc-mt6j2 [751.116047ms]
Feb 22 15:05:03.604: INFO: Created: latency-svc-nrz5d
Feb 22 15:05:03.637: INFO: Got endpoints: latency-svc-bz42n [749.705349ms]
Feb 22 15:05:03.652: INFO: Created: latency-svc-llgj8
Feb 22 15:05:03.689: INFO: Got endpoints: latency-svc-cthbm [751.375519ms]
Feb 22 15:05:03.721: INFO: Created: latency-svc-x4ft7
Feb 22 15:05:03.745: INFO: Got endpoints: latency-svc-bfhnq [757.227756ms]
Feb 22 15:05:03.761: INFO: Created: latency-svc-ncrp6
Feb 22 15:05:03.787: INFO: Got endpoints: latency-svc-59c84 [749.47165ms]
Feb 22 15:05:03.800: INFO: Created: latency-svc-mfjz8
Feb 22 15:05:03.838: INFO: Got endpoints: latency-svc-j8cng [749.054281ms]
Feb 22 15:05:03.853: INFO: Created: latency-svc-pwnkz
Feb 22 15:05:03.887: INFO: Got endpoints: latency-svc-dzqfq [750.690898ms]
Feb 22 15:05:03.902: INFO: Created: latency-svc-m2ntw
Feb 22 15:05:03.937: INFO: Got endpoints: latency-svc-8mzqc [749.4129ms]
Feb 22 15:05:03.951: INFO: Created: latency-svc-9k2hb
Feb 22 15:05:03.987: INFO: Got endpoints: latency-svc-xlcrm [750.074287ms]
Feb 22 15:05:04.001: INFO: Created: latency-svc-78dtl
Feb 22 15:05:04.037: INFO: Got endpoints: latency-svc-5qq5c [747.767711ms]
Feb 22 15:05:04.051: INFO: Created: latency-svc-jtqg6
Feb 22 15:05:04.087: INFO: Got endpoints: latency-svc-s2628 [749.639635ms]
Feb 22 15:05:04.098: INFO: Created: latency-svc-qsbq2
Feb 22 15:05:04.137: INFO: Got endpoints: latency-svc-lhgs5 [750.545386ms]
Feb 22 15:05:04.151: INFO: Created: latency-svc-2kncr
Feb 22 15:05:04.187: INFO: Got endpoints: latency-svc-mb7x6 [745.555954ms]
Feb 22 15:05:04.197: INFO: Created: latency-svc-xjv6k
Feb 22 15:05:04.238: INFO: Got endpoints: latency-svc-w29gk [748.417085ms]
Feb 22 15:05:04.252: INFO: Created: latency-svc-dmz9d
Feb 22 15:05:04.291: INFO: Got endpoints: latency-svc-2bswd [749.973593ms]
Feb 22 15:05:04.305: INFO: Created: latency-svc-k22bq
Feb 22 15:05:04.338: INFO: Got endpoints: latency-svc-nrz5d [749.476444ms]
Feb 22 15:05:04.352: INFO: Created: latency-svc-d8n6b
Feb 22 15:05:04.388: INFO: Got endpoints: latency-svc-llgj8 [750.91573ms]
Feb 22 15:05:04.418: INFO: Created: latency-svc-fnmfb
Feb 22 15:05:04.437: INFO: Got endpoints: latency-svc-x4ft7 [748.459174ms]
Feb 22 15:05:04.452: INFO: Created: latency-svc-h7mjt
Feb 22 15:05:04.488: INFO: Got endpoints: latency-svc-ncrp6 [742.241029ms]
Feb 22 15:05:04.504: INFO: Created: latency-svc-rfkwm
Feb 22 15:05:04.537: INFO: Got endpoints: latency-svc-mfjz8 [749.817685ms]
Feb 22 15:05:04.554: INFO: Created: latency-svc-sndgw
Feb 22 15:05:04.588: INFO: Got endpoints: latency-svc-pwnkz [749.897264ms]
Feb 22 15:05:04.602: INFO: Created: latency-svc-qlv7h
Feb 22 15:05:04.638: INFO: Got endpoints: latency-svc-m2ntw [750.210079ms]
Feb 22 15:05:04.653: INFO: Created: latency-svc-d7vzv
Feb 22 15:05:04.687: INFO: Got endpoints: latency-svc-9k2hb [749.81748ms]
Feb 22 15:05:04.698: INFO: Created: latency-svc-sbc7n
Feb 22 15:05:04.738: INFO: Got endpoints: latency-svc-78dtl [750.354485ms]
Feb 22 15:05:04.795: INFO: Created: latency-svc-555cd
Feb 22 15:05:04.797: INFO: Got endpoints: latency-svc-jtqg6 [760.687462ms]
Feb 22 15:05:04.903: INFO: Got endpoints: latency-svc-qsbq2 [816.336404ms]
Feb 22 15:05:04.910: INFO: Got endpoints: latency-svc-2kncr [772.996913ms]
Feb 22 15:05:04.923: INFO: Created: latency-svc-sg7tn
Feb 22 15:05:04.937: INFO: Got endpoints: latency-svc-xjv6k [749.338714ms]
Feb 22 15:05:04.945: INFO: Created: latency-svc-cs4t2
Feb 22 15:05:04.951: INFO: Created: latency-svc-p8s2h
Feb 22 15:05:04.960: INFO: Created: latency-svc-hlcj7
Feb 22 15:05:04.987: INFO: Got endpoints: latency-svc-dmz9d [749.571986ms]
Feb 22 15:05:05.001: INFO: Created: latency-svc-p4pgf
Feb 22 15:05:05.039: INFO: Got endpoints: latency-svc-k22bq [748.012392ms]
Feb 22 15:05:05.053: INFO: Created: latency-svc-rh2xb
Feb 22 15:05:05.087: INFO: Got endpoints: latency-svc-d8n6b [749.358672ms]
Feb 22 15:05:05.129: INFO: Created: latency-svc-5sbfz
Feb 22 15:05:05.143: INFO: Got endpoints: latency-svc-fnmfb [755.001314ms]
Feb 22 15:05:05.158: INFO: Created: latency-svc-th89z
Feb 22 15:05:05.187: INFO: Got endpoints: latency-svc-h7mjt [749.976282ms]
Feb 22 15:05:05.200: INFO: Created: latency-svc-dm9bq
Feb 22 15:05:05.237: INFO: Got endpoints: latency-svc-rfkwm [749.171719ms]
Feb 22 15:05:05.251: INFO: Created: latency-svc-52nhq
Feb 22 15:05:05.288: INFO: Got endpoints: latency-svc-sndgw [750.239437ms]
Feb 22 15:05:05.305: INFO: Created: latency-svc-2vxch
Feb 22 15:05:05.339: INFO: Got endpoints: latency-svc-qlv7h [751.395172ms]
Feb 22 15:05:05.354: INFO: Created: latency-svc-4k5b7
Feb 22 15:05:05.387: INFO: Got endpoints: latency-svc-d7vzv [749.082491ms]
Feb 22 15:05:05.397: INFO: Created: latency-svc-ltwvx
Feb 22 15:05:05.437: INFO: Got endpoints: latency-svc-sbc7n [750.338843ms]
Feb 22 15:05:05.450: INFO: Created: latency-svc-trbrr
Feb 22 15:05:05.489: INFO: Got endpoints: latency-svc-555cd [751.406791ms]
Feb 22 15:05:05.506: INFO: Created: latency-svc-482bj
Feb 22 15:05:05.538: INFO: Got endpoints: latency-svc-sg7tn [740.833206ms]
Feb 22 15:05:05.553: INFO: Created: latency-svc-ckxcv
Feb 22 15:05:05.587: INFO: Got endpoints: latency-svc-cs4t2 [683.976828ms]
Feb 22 15:05:05.603: INFO: Created: latency-svc-s6mzm
Feb 22 15:05:05.639: INFO: Got endpoints: latency-svc-p8s2h [728.544417ms]
Feb 22 15:05:05.653: INFO: Created: latency-svc-4mkrv
Feb 22 15:05:05.687: INFO: Got endpoints: latency-svc-hlcj7 [750.373234ms]
Feb 22 15:05:05.699: INFO: Created: latency-svc-rb2m9
Feb 22 15:05:05.756: INFO: Got endpoints: latency-svc-p4pgf [768.49504ms]
Feb 22 15:05:05.771: INFO: Created: latency-svc-cft82
Feb 22 15:05:05.788: INFO: Got endpoints: latency-svc-rh2xb [748.379072ms]
Feb 22 15:05:05.801: INFO: Created: latency-svc-twppb
Feb 22 15:05:05.838: INFO: Got endpoints: latency-svc-5sbfz [751.324457ms]
Feb 22 15:05:05.852: INFO: Created: latency-svc-5rphs
Feb 22 15:05:05.887: INFO: Got endpoints: latency-svc-th89z [744.651176ms]
Feb 22 15:05:05.901: INFO: Created: latency-svc-drnxk
Feb 22 15:05:05.938: INFO: Got endpoints: latency-svc-dm9bq [750.438403ms]
Feb 22 15:05:05.949: INFO: Created: latency-svc-b2g2l
Feb 22 15:05:05.989: INFO: Got endpoints: latency-svc-52nhq [752.408534ms]
Feb 22 15:05:06.005: INFO: Created: latency-svc-7ctlg
Feb 22 15:05:06.040: INFO: Got endpoints: latency-svc-2vxch [752.034848ms]
Feb 22 15:05:06.055: INFO: Created: latency-svc-9hgsr
Feb 22 15:05:06.095: INFO: Got endpoints: latency-svc-4k5b7 [755.518384ms]
Feb 22 15:05:06.115: INFO: Created: latency-svc-rrntt
Feb 22 15:05:06.137: INFO: Got endpoints: latency-svc-ltwvx [750.734295ms]
Feb 22 15:05:06.176: INFO: Created: latency-svc-wgbfr
Feb 22 15:05:06.196: INFO: Got endpoints: latency-svc-trbrr [758.347356ms]
Feb 22 15:05:06.214: INFO: Created: latency-svc-6gxsd
Feb 22 15:05:06.238: INFO: Got endpoints: latency-svc-482bj [748.504572ms]
Feb 22 15:05:06.260: INFO: Created: latency-svc-tzkhr
Feb 22 15:05:06.297: INFO: Got endpoints: latency-svc-ckxcv [759.12352ms]
Feb 22 15:05:06.310: INFO: Created: latency-svc-hd6pq
Feb 22 15:05:06.337: INFO: Got endpoints: latency-svc-s6mzm [749.772069ms]
Feb 22 15:05:06.357: INFO: Created: latency-svc-bqt5q
Feb 22 15:05:06.388: INFO: Got endpoints: latency-svc-4mkrv [749.370361ms]
Feb 22 15:05:06.406: INFO: Created: latency-svc-v269c
Feb 22 15:05:06.439: INFO: Got endpoints: latency-svc-rb2m9 [752.249709ms]
Feb 22 15:05:06.453: INFO: Created: latency-svc-fb5m8
Feb 22 15:05:06.492: INFO: Got endpoints: latency-svc-cft82 [735.898175ms]
Feb 22 15:05:06.509: INFO: Created: latency-svc-dr5k6
Feb 22 15:05:06.539: INFO: Got endpoints: latency-svc-twppb [751.28016ms]
Feb 22 15:05:06.551: INFO: Created: latency-svc-dn8r5
Feb 22 15:05:06.588: INFO: Got endpoints: latency-svc-5rphs [749.799911ms]
Feb 22 15:05:06.627: INFO: Created: latency-svc-k5mkf
Feb 22 15:05:06.637: INFO: Got endpoints: latency-svc-drnxk [749.643224ms]
Feb 22 15:05:06.652: INFO: Created: latency-svc-8fnjf
Feb 22 15:05:06.689: INFO: Got endpoints: latency-svc-b2g2l [751.066413ms]
Feb 22 15:05:06.702: INFO: Created: latency-svc-7j9q5
Feb 22 15:05:06.737: INFO: Got endpoints: latency-svc-7ctlg [747.489322ms]
Feb 22 15:05:06.755: INFO: Created: latency-svc-g827r
Feb 22 15:05:06.787: INFO: Got endpoints: latency-svc-9hgsr [747.525592ms]
Feb 22 15:05:06.803: INFO: Created: latency-svc-kv42b
Feb 22 15:05:06.840: INFO: Got endpoints: latency-svc-rrntt [745.315442ms]
Feb 22 15:05:06.894: INFO: Got endpoints: latency-svc-wgbfr [756.891299ms]
Feb 22 15:05:06.943: INFO: Got endpoints: latency-svc-6gxsd [747.610722ms]
Feb 22 15:05:06.989: INFO: Got endpoints: latency-svc-tzkhr [750.783187ms]
Feb 22 15:05:07.038: INFO: Got endpoints: latency-svc-hd6pq [740.188817ms]
Feb 22 15:05:07.088: INFO: Got endpoints: latency-svc-bqt5q [751.15273ms]
Feb 22 15:05:07.137: INFO: Got endpoints: latency-svc-v269c [748.123481ms]
Feb 22 15:05:07.188: INFO: Got endpoints: latency-svc-fb5m8 [748.307558ms]
Feb 22 15:05:07.238: INFO: Got endpoints: latency-svc-dr5k6 [745.857495ms]
Feb 22 15:05:07.288: INFO: Got endpoints: latency-svc-dn8r5 [748.322894ms]
Feb 22 15:05:07.338: INFO: Got endpoints: latency-svc-k5mkf [749.460336ms]
Feb 22 15:05:07.387: INFO: Got endpoints: latency-svc-8fnjf [749.5002ms]
Feb 22 15:05:07.439: INFO: Got endpoints: latency-svc-7j9q5 [749.965916ms]
Feb 22 15:05:07.488: INFO: Got endpoints: latency-svc-g827r [751.100067ms]
Feb 22 15:05:07.539: INFO: Got endpoints: latency-svc-kv42b [751.807652ms]
Feb 22 15:05:07.539: INFO: Latencies: [24.252775ms 36.125469ms 44.837376ms 77.503903ms 81.155318ms 92.479503ms 104.111525ms 121.333443ms 136.90887ms 150.136422ms 160.357897ms 174.58181ms 188.154178ms 192.183914ms 192.453284ms 194.857013ms 194.897299ms 195.136361ms 204.674672ms 206.524894ms 214.651566ms 217.10886ms 218.92267ms 219.564974ms 223.691666ms 224.604041ms 224.851025ms 225.568005ms 227.637795ms 228.195953ms 228.861688ms 230.929755ms 232.51029ms 232.740088ms 233.838695ms 235.775047ms 237.368858ms 241.927767ms 244.494941ms 245.616457ms 251.396059ms 260.127081ms 293.228398ms 343.077307ms 374.99176ms 502.503547ms 523.397807ms 531.022564ms 532.41114ms 548.030969ms 557.977242ms 589.607188ms 600.708186ms 627.698535ms 652.724711ms 653.013739ms 683.976828ms 688.37235ms 694.113624ms 712.062318ms 728.544417ms 735.898175ms 735.932014ms 739.865128ms 740.188817ms 740.833206ms 742.241029ms 742.949403ms 743.557709ms 743.744381ms 744.651176ms 744.766196ms 745.315442ms 745.555954ms 745.857495ms 746.232613ms 746.277179ms 746.289637ms 746.758592ms 747.169987ms 747.489322ms 747.525592ms 747.549849ms 747.610722ms 747.752423ms 747.767711ms 747.97697ms 748.012392ms 748.046567ms 748.123481ms 748.209534ms 748.307558ms 748.322894ms 748.379072ms 748.417085ms 748.459174ms 748.504572ms 748.607927ms 748.751665ms 748.860322ms 748.876552ms 748.878425ms 748.936709ms 748.94388ms 748.961109ms 748.964716ms 749.023972ms 749.054281ms 749.082491ms 749.171719ms 749.203612ms 749.272393ms 749.324019ms 749.338714ms 749.358672ms 749.370361ms 749.4129ms 749.4378ms 749.460336ms 749.47165ms 749.476444ms 749.494705ms 749.5002ms 749.571986ms 749.574986ms 749.639635ms 749.643224ms 749.668383ms 749.705349ms 749.772069ms 749.787678ms 749.799911ms 749.81748ms 749.817685ms 749.897264ms 749.958843ms 749.965916ms 749.969588ms 749.973593ms 749.976282ms 749.985807ms 750.04261ms 750.074287ms 750.107169ms 750.194242ms 750.210079ms 750.239437ms 750.244849ms 750.338843ms 750.354485ms 750.373234ms 750.409111ms 750.410313ms 750.438403ms 750.445866ms 750.506896ms 750.545386ms 750.574773ms 750.658134ms 750.690898ms 750.708619ms 750.734295ms 750.783187ms 750.875986ms 750.88344ms 750.915049ms 750.915113ms 750.91573ms 750.944183ms 751.066413ms 751.100067ms 751.116047ms 751.15273ms 751.28016ms 751.324457ms 751.375519ms 751.395172ms 751.406791ms 751.415336ms 751.618614ms 751.807652ms 751.961636ms 752.034848ms 752.249709ms 752.408534ms 752.977378ms 753.438743ms 754.355652ms 754.359152ms 755.001314ms 755.518384ms 756.891299ms 757.227756ms 758.347356ms 759.12352ms 760.687462ms 763.652223ms 768.49504ms 772.996913ms 816.336404ms]
Feb 22 15:05:07.540: INFO: 50 %ile: 748.876552ms
Feb 22 15:05:07.540: INFO: 90 %ile: 751.807652ms
Feb 22 15:05:07.540: INFO: 99 %ile: 772.996913ms
Feb 22 15:05:07.540: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:05:07.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5963" for this suite.
Feb 22 15:05:19.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:05:19.712: INFO: namespace svc-latency-5963 deletion completed in 12.169387738s

â€¢ [SLOW TEST:23.908 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:05:19.712: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-nzch
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 15:05:19.758: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nzch" in namespace "subpath-4628" to be "success or failure"
Feb 22 15:05:19.760: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Pending", Reason="", readiness=false. Elapsed: 1.889345ms
Feb 22 15:05:21.762: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00407622s
Feb 22 15:05:23.764: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 4.005987035s
Feb 22 15:05:25.766: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 6.007950033s
Feb 22 15:05:27.768: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 8.009889875s
Feb 22 15:05:29.770: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 10.012059472s
Feb 22 15:05:31.772: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 12.014088079s
Feb 22 15:05:33.774: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 14.016170275s
Feb 22 15:05:35.776: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 16.018279456s
Feb 22 15:05:37.778: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 18.020268346s
Feb 22 15:05:39.780: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 20.022239073s
Feb 22 15:05:41.782: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Running", Reason="", readiness=true. Elapsed: 22.024187201s
Feb 22 15:05:43.784: INFO: Pod "pod-subpath-test-projected-nzch": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.026256683s
STEP: Saw pod success
Feb 22 15:05:43.784: INFO: Pod "pod-subpath-test-projected-nzch" satisfied condition "success or failure"
Feb 22 15:05:43.786: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-subpath-test-projected-nzch container test-container-subpath-projected-nzch: <nil>
STEP: delete the pod
Feb 22 15:05:43.802: INFO: Waiting for pod pod-subpath-test-projected-nzch to disappear
Feb 22 15:05:43.804: INFO: Pod pod-subpath-test-projected-nzch no longer exists
STEP: Deleting pod pod-subpath-test-projected-nzch
Feb 22 15:05:43.804: INFO: Deleting pod "pod-subpath-test-projected-nzch" in namespace "subpath-4628"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:05:43.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4628" for this suite.
Feb 22 15:05:49.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:05:49.973: INFO: namespace subpath-4628 deletion completed in 6.165582446s

â€¢ [SLOW TEST:30.261 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:05:49.973: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:05:54.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6385" for this suite.
Feb 22 15:06:00.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:06:00.224: INFO: namespace kubelet-test-6385 deletion completed in 6.168064122s

â€¢ [SLOW TEST:10.251 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:06:00.225: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cbb6e738-cfd2-40d9-a990-623e1ce8ff1f
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-cbb6e738-cfd2-40d9-a990-623e1ce8ff1f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:07:24.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9956" for this suite.
Feb 22 15:07:46.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:07:46.684: INFO: namespace projected-9956 deletion completed in 22.165100152s

â€¢ [SLOW TEST:106.459 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:07:46.685: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 22 15:07:46.707: INFO: Waiting up to 5m0s for pod "pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7" in namespace "emptydir-6399" to be "success or failure"
Feb 22 15:07:46.711: INFO: Pod "pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890482ms
Feb 22 15:07:48.713: INFO: Pod "pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00605324s
Feb 22 15:07:50.715: INFO: Pod "pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008046309s
STEP: Saw pod success
Feb 22 15:07:50.715: INFO: Pod "pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7" satisfied condition "success or failure"
Feb 22 15:07:50.717: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7 container test-container: <nil>
STEP: delete the pod
Feb 22 15:07:50.729: INFO: Waiting for pod pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7 to disappear
Feb 22 15:07:50.731: INFO: Pod pod-6c6aae84-e8fe-4944-83d3-25db9ee990a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:07:50.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6399" for this suite.
Feb 22 15:07:56.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:07:56.903: INFO: namespace emptydir-6399 deletion completed in 6.167822545s

â€¢ [SLOW TEST:10.219 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:07:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 22 15:08:04.967: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 22 15:08:04.968: INFO: Pod pod-with-prestop-http-hook still exists
Feb 22 15:08:06.968: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 22 15:08:06.970: INFO: Pod pod-with-prestop-http-hook still exists
Feb 22 15:08:08.968: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 22 15:08:08.971: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:08:08.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4203" for this suite.
Feb 22 15:08:30.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:08:31.149: INFO: namespace container-lifecycle-hook-4203 deletion completed in 22.169266783s

â€¢ [SLOW TEST:34.246 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:08:31.150: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 22 15:08:31.171: INFO: Waiting up to 5m0s for pod "pod-4a0bf499-d606-4c68-9d54-66574116dd6a" in namespace "emptydir-5078" to be "success or failure"
Feb 22 15:08:31.175: INFO: Pod "pod-4a0bf499-d606-4c68-9d54-66574116dd6a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13367ms
Feb 22 15:08:33.177: INFO: Pod "pod-4a0bf499-d606-4c68-9d54-66574116dd6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005207298s
Feb 22 15:08:35.179: INFO: Pod "pod-4a0bf499-d606-4c68-9d54-66574116dd6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00732173s
STEP: Saw pod success
Feb 22 15:08:35.179: INFO: Pod "pod-4a0bf499-d606-4c68-9d54-66574116dd6a" satisfied condition "success or failure"
Feb 22 15:08:35.181: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-4a0bf499-d606-4c68-9d54-66574116dd6a container test-container: <nil>
STEP: delete the pod
Feb 22 15:08:35.194: INFO: Waiting for pod pod-4a0bf499-d606-4c68-9d54-66574116dd6a to disappear
Feb 22 15:08:35.196: INFO: Pod pod-4a0bf499-d606-4c68-9d54-66574116dd6a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:08:35.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5078" for this suite.
Feb 22 15:08:41.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:08:41.363: INFO: namespace emptydir-5078 deletion completed in 6.164455743s

â€¢ [SLOW TEST:10.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:08:41.364: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5020.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5020.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 15:08:45.412: INFO: DNS probes using dns-5020/dns-test-d3467b51-5bb9-4aa5-9c97-56c9133838a6 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:08:45.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5020" for this suite.
Feb 22 15:08:51.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:08:51.626: INFO: namespace dns-5020 deletion completed in 6.171021718s

â€¢ [SLOW TEST:10.262 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:08:51.626: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 22 15:08:56.167: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6d3dcbb8-80e0-4661-a259-c9b4eefb2b48"
Feb 22 15:08:56.167: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-6d3dcbb8-80e0-4661-a259-c9b4eefb2b48" in namespace "pods-1718" to be "terminated due to deadline exceeded"
Feb 22 15:08:56.170: INFO: Pod "pod-update-activedeadlineseconds-6d3dcbb8-80e0-4661-a259-c9b4eefb2b48": Phase="Running", Reason="", readiness=true. Elapsed: 2.586547ms
Feb 22 15:08:58.172: INFO: Pod "pod-update-activedeadlineseconds-6d3dcbb8-80e0-4661-a259-c9b4eefb2b48": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.004510621s
Feb 22 15:08:58.172: INFO: Pod "pod-update-activedeadlineseconds-6d3dcbb8-80e0-4661-a259-c9b4eefb2b48" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:08:58.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1718" for this suite.
Feb 22 15:09:04.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:09:04.345: INFO: namespace pods-1718 deletion completed in 6.170469749s

â€¢ [SLOW TEST:12.719 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:09:04.345: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 22 15:09:04.384: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:04.388: INFO: Number of nodes with available pods: 0
Feb 22 15:09:04.388: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:09:05.391: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:05.393: INFO: Number of nodes with available pods: 0
Feb 22 15:09:05.393: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:09:06.391: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:06.392: INFO: Number of nodes with available pods: 1
Feb 22 15:09:06.392: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:09:07.390: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:07.392: INFO: Number of nodes with available pods: 2
Feb 22 15:09:07.392: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 22 15:09:07.402: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:07.406: INFO: Number of nodes with available pods: 1
Feb 22 15:09:07.406: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:09:08.409: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:08.410: INFO: Number of nodes with available pods: 1
Feb 22 15:09:08.410: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:09:09.408: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:09.410: INFO: Number of nodes with available pods: 1
Feb 22 15:09:09.410: INFO: Node ip-10-100-10-41.eu-west-1.compute.internal is running more than one daemon pod
Feb 22 15:09:10.409: INFO: DaemonSet pods can't tolerate node ip-10-100-0-4.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 22 15:09:10.410: INFO: Number of nodes with available pods: 2
Feb 22 15:09:10.410: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3322, will wait for the garbage collector to delete the pods
Feb 22 15:09:10.469: INFO: Deleting DaemonSet.extensions daemon-set took: 3.259326ms
Feb 22 15:09:10.969: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.19053ms
Feb 22 15:09:13.671: INFO: Number of nodes with available pods: 0
Feb 22 15:09:13.671: INFO: Number of running nodes: 0, number of available pods: 0
Feb 22 15:09:13.672: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3322/daemonsets","resourceVersion":"29317"},"items":null}

Feb 22 15:09:13.674: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3322/pods","resourceVersion":"29317"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:09:13.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3322" for this suite.
Feb 22 15:09:19.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:09:19.848: INFO: namespace daemonsets-3322 deletion completed in 6.167284429s

â€¢ [SLOW TEST:15.503 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:09:19.848: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 22 15:09:19.868: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:09:22.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6162" for this suite.
Feb 22 15:09:28.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:09:29.134: INFO: namespace init-container-6162 deletion completed in 6.16800909s

â€¢ [SLOW TEST:9.286 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:09:29.134: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 22 15:09:29.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-7768'
Feb 22 15:09:29.362: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 22 15:09:29.362: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Feb 22 15:09:31.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7768'
Feb 22 15:09:31.436: INFO: stderr: ""
Feb 22 15:09:31.436: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:09:31.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7768" for this suite.
Feb 22 15:11:33.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:11:33.610: INFO: namespace kubectl-7768 deletion completed in 2m2.168992462s

â€¢ [SLOW TEST:124.476 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:11:33.610: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 22 15:11:38.194: INFO: Successfully updated pod "pod-update-20b8a467-6cd5-4f5a-85d9-cd35b94a82d0"
STEP: verifying the updated pod is in kubernetes
Feb 22 15:11:38.198: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:11:38.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4791" for this suite.
Feb 22 15:11:58.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:11:58.370: INFO: namespace pods-4791 deletion completed in 20.169436097s

â€¢ [SLOW TEST:24.760 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:11:58.370: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-9551fe7e-e073-40ba-8a75-efd02646251a
STEP: Creating a pod to test consume secrets
Feb 22 15:11:58.445: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd" in namespace "projected-1962" to be "success or failure"
Feb 22 15:11:58.449: INFO: Pod "pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919653ms
Feb 22 15:12:00.451: INFO: Pod "pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005915201s
Feb 22 15:12:02.453: INFO: Pod "pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008008702s
STEP: Saw pod success
Feb 22 15:12:02.453: INFO: Pod "pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd" satisfied condition "success or failure"
Feb 22 15:12:02.455: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd container secret-volume-test: <nil>
STEP: delete the pod
Feb 22 15:12:02.468: INFO: Waiting for pod pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd to disappear
Feb 22 15:12:02.469: INFO: Pod pod-projected-secrets-98de3cb8-e4ce-4084-945e-6a27d97e96dd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:12:02.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1962" for this suite.
Feb 22 15:12:08.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:12:08.639: INFO: namespace projected-1962 deletion completed in 6.16744115s

â€¢ [SLOW TEST:10.269 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:12:08.639: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 22 15:12:11.696: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:12:11.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4193" for this suite.
Feb 22 15:12:33.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:12:33.882: INFO: namespace replicaset-4193 deletion completed in 22.170849367s

â€¢ [SLOW TEST:25.243 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:12:33.882: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-vdp5
STEP: Creating a pod to test atomic-volume-subpath
Feb 22 15:12:33.914: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vdp5" in namespace "subpath-8895" to be "success or failure"
Feb 22 15:12:33.917: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.424113ms
Feb 22 15:12:35.919: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00543867s
Feb 22 15:12:37.921: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 4.007455273s
Feb 22 15:12:39.924: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 6.009687813s
Feb 22 15:12:41.926: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 8.011788248s
Feb 22 15:12:43.928: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 10.01390103s
Feb 22 15:12:45.930: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 12.015778884s
Feb 22 15:12:47.932: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 14.017824974s
Feb 22 15:12:49.934: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 16.019897919s
Feb 22 15:12:51.936: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 18.021836146s
Feb 22 15:12:53.938: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 20.023979217s
Feb 22 15:12:55.940: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Running", Reason="", readiness=true. Elapsed: 22.025904738s
Feb 22 15:12:57.942: INFO: Pod "pod-subpath-test-downwardapi-vdp5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028020289s
STEP: Saw pod success
Feb 22 15:12:57.942: INFO: Pod "pod-subpath-test-downwardapi-vdp5" satisfied condition "success or failure"
Feb 22 15:12:57.944: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-vdp5 container test-container-subpath-downwardapi-vdp5: <nil>
STEP: delete the pod
Feb 22 15:12:57.956: INFO: Waiting for pod pod-subpath-test-downwardapi-vdp5 to disappear
Feb 22 15:12:57.958: INFO: Pod pod-subpath-test-downwardapi-vdp5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vdp5
Feb 22 15:12:57.958: INFO: Deleting pod "pod-subpath-test-downwardapi-vdp5" in namespace "subpath-8895"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:12:57.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8895" for this suite.
Feb 22 15:13:03.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:13:04.130: INFO: namespace subpath-8895 deletion completed in 6.168140807s

â€¢ [SLOW TEST:30.247 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:13:04.130: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 15:13:04.152: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a" in namespace "downward-api-7377" to be "success or failure"
Feb 22 15:13:04.157: INFO: Pod "downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.440079ms
Feb 22 15:13:06.159: INFO: Pod "downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007018067s
Feb 22 15:13:08.161: INFO: Pod "downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009155698s
STEP: Saw pod success
Feb 22 15:13:08.161: INFO: Pod "downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a" satisfied condition "success or failure"
Feb 22 15:13:08.163: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a container client-container: <nil>
STEP: delete the pod
Feb 22 15:13:08.176: INFO: Waiting for pod downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a to disappear
Feb 22 15:13:08.178: INFO: Pod downwardapi-volume-9676aab5-6209-4804-8d76-6f16cbef9f6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:13:08.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7377" for this suite.
Feb 22 15:13:14.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:13:14.347: INFO: namespace downward-api-7377 deletion completed in 6.166893245s

â€¢ [SLOW TEST:10.217 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:13:14.347: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-6c501290-504a-4c72-8815-b8ec9e1a9386
STEP: Creating a pod to test consume configMaps
Feb 22 15:13:14.425: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756" in namespace "projected-7521" to be "success or failure"
Feb 22 15:13:14.428: INFO: Pod "pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529437ms
Feb 22 15:13:16.430: INFO: Pod "pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005433038s
Feb 22 15:13:18.432: INFO: Pod "pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00715408s
STEP: Saw pod success
Feb 22 15:13:18.432: INFO: Pod "pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756" satisfied condition "success or failure"
Feb 22 15:13:18.433: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 15:13:18.446: INFO: Waiting for pod pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756 to disappear
Feb 22 15:13:18.448: INFO: Pod pod-projected-configmaps-03d23c34-c52c-41a1-8d32-3574bbd48756 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:13:18.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7521" for this suite.
Feb 22 15:13:24.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:13:24.622: INFO: namespace projected-7521 deletion completed in 6.171801023s

â€¢ [SLOW TEST:10.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:13:24.622: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-27ceff35-0011-4beb-8d27-c926a7dd59fd in namespace container-probe-5132
Feb 22 15:13:28.652: INFO: Started pod test-webserver-27ceff35-0011-4beb-8d27-c926a7dd59fd in namespace container-probe-5132
STEP: checking the pod's current state and verifying that restartCount is present
Feb 22 15:13:28.653: INFO: Initial restart count of pod test-webserver-27ceff35-0011-4beb-8d27-c926a7dd59fd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:17:28.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5132" for this suite.
Feb 22 15:17:34.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:17:35.094: INFO: namespace container-probe-5132 deletion completed in 6.17025009s

â€¢ [SLOW TEST:250.472 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:17:35.094: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:18:01.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1265" for this suite.
Feb 22 15:18:07.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:18:07.344: INFO: namespace namespaces-1265 deletion completed in 6.165687736s
STEP: Destroying namespace "nsdeletetest-2688" for this suite.
Feb 22 15:18:07.346: INFO: Namespace nsdeletetest-2688 was already deleted
STEP: Destroying namespace "nsdeletetest-4543" for this suite.
Feb 22 15:18:13.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:18:13.515: INFO: namespace nsdeletetest-4543 deletion completed in 6.169905778s

â€¢ [SLOW TEST:38.422 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:18:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4276.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4276.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4276.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4276.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4276.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4276.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 22 15:18:17.568: INFO: DNS probes using dns-4276/dns-test-9e7db20d-dbc2-49c4-b1a1-8214ac43a0e3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:18:17.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4276" for this suite.
Feb 22 15:18:23.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:18:23.756: INFO: namespace dns-4276 deletion completed in 6.169385497s

â€¢ [SLOW TEST:10.240 seconds]
[sig-network] DNS
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:18:23.756: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Feb 22 15:18:23.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 create -f - --namespace=kubectl-5554'
Feb 22 15:18:23.935: INFO: stderr: ""
Feb 22 15:18:23.935: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Feb 22 15:18:24.938: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 15:18:24.938: INFO: Found 0 / 1
Feb 22 15:18:25.938: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 15:18:25.938: INFO: Found 0 / 1
Feb 22 15:18:26.938: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 15:18:26.938: INFO: Found 1 / 1
Feb 22 15:18:26.938: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 22 15:18:26.940: INFO: Selector matched 1 pods for map[app:redis]
Feb 22 15:18:26.940: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 22 15:18:26.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-zj25p redis-master --namespace=kubectl-5554'
Feb 22 15:18:27.007: INFO: stderr: ""
Feb 22 15:18:27.007: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Feb 15:18:25.719 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Feb 15:18:25.719 # Server started, Redis version 3.2.12\n1:M 22 Feb 15:18:25.719 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Feb 15:18:25.719 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 22 15:18:27.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-zj25p redis-master --namespace=kubectl-5554 --tail=1'
Feb 22 15:18:27.074: INFO: stderr: ""
Feb 22 15:18:27.074: INFO: stdout: "1:M 22 Feb 15:18:25.719 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 22 15:18:27.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-zj25p redis-master --namespace=kubectl-5554 --limit-bytes=1'
Feb 22 15:18:27.143: INFO: stderr: ""
Feb 22 15:18:27.143: INFO: stdout: " "
STEP: exposing timestamps
Feb 22 15:18:27.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-zj25p redis-master --namespace=kubectl-5554 --tail=1 --timestamps'
Feb 22 15:18:27.210: INFO: stderr: ""
Feb 22 15:18:27.210: INFO: stdout: "2020-02-22T15:18:25.719432813Z 1:M 22 Feb 15:18:25.719 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 22 15:18:29.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-zj25p redis-master --namespace=kubectl-5554 --since=1s'
Feb 22 15:18:29.777: INFO: stderr: ""
Feb 22 15:18:29.777: INFO: stdout: ""
Feb 22 15:18:29.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 logs redis-master-zj25p redis-master --namespace=kubectl-5554 --since=24h'
Feb 22 15:18:29.856: INFO: stderr: ""
Feb 22 15:18:29.856: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Feb 15:18:25.719 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Feb 15:18:25.719 # Server started, Redis version 3.2.12\n1:M 22 Feb 15:18:25.719 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Feb 15:18:25.719 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Feb 22 15:18:29.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 delete --grace-period=0 --force -f - --namespace=kubectl-5554'
Feb 22 15:18:29.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 22 15:18:29.916: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 22 15:18:29.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get rc,svc -l name=nginx --no-headers --namespace=kubectl-5554'
Feb 22 15:18:29.977: INFO: stderr: "No resources found.\n"
Feb 22 15:18:29.977: INFO: stdout: ""
Feb 22 15:18:29.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 get pods -l name=nginx --namespace=kubectl-5554 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 22 15:18:30.034: INFO: stderr: ""
Feb 22 15:18:30.034: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:18:30.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5554" for this suite.
Feb 22 15:18:52.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:18:52.203: INFO: namespace kubectl-5554 deletion completed in 22.16559026s

â€¢ [SLOW TEST:28.447 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:18:52.203: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Feb 22 15:18:52.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-674502360 --namespace=kubectl-3324 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 22 15:18:54.424: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 22 15:18:54.424: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:18:56.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3324" for this suite.
Feb 22 15:19:02.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:19:02.597: INFO: namespace kubectl-3324 deletion completed in 6.167027004s

â€¢ [SLOW TEST:10.394 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:19:02.597: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-77b288f0-6fdb-4b12-bbb5-d96aad333fea
STEP: Creating a pod to test consume configMaps
Feb 22 15:19:02.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9" in namespace "configmap-5040" to be "success or failure"
Feb 22 15:19:02.627: INFO: Pod "pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407837ms
Feb 22 15:19:04.629: INFO: Pod "pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00645663s
Feb 22 15:19:06.631: INFO: Pod "pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008361772s
STEP: Saw pod success
Feb 22 15:19:06.631: INFO: Pod "pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9" satisfied condition "success or failure"
Feb 22 15:19:06.633: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 15:19:06.645: INFO: Waiting for pod pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9 to disappear
Feb 22 15:19:06.647: INFO: Pod pod-configmaps-5e796568-e3e1-487e-9dc8-c8bb48fa53f9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:19:06.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5040" for this suite.
Feb 22 15:19:12.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:19:12.817: INFO: namespace configmap-5040 deletion completed in 6.167356713s

â€¢ [SLOW TEST:10.219 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:19:12.817: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 15:19:12.835: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 22 15:19:12.843: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 22 15:19:17.845: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 22 15:19:17.845: INFO: Creating deployment "test-rolling-update-deployment"
Feb 22 15:19:17.849: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 22 15:19:17.855: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 22 15:19:19.859: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 22 15:19:19.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717981557, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717981557, loc:(*time.Location)(0x7ed4a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717981557, loc:(*time.Location)(0x7ed4a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717981557, loc:(*time.Location)(0x7ed4a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 22 15:19:21.878: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 22 15:19:21.883: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-7222,SelfLink:/apis/apps/v1/namespaces/deployment-7222/deployments/test-rolling-update-deployment,UID:238f31ad-e28e-4af0-abe2-34f1f8bbb431,ResourceVersion:31519,Generation:1,CreationTimestamp:2020-02-22 15:19:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-22 15:19:17 +0000 UTC 2020-02-22 15:19:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-22 15:19:20 +0000 UTC 2020-02-22 15:19:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 22 15:19:21.885: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-7222,SelfLink:/apis/apps/v1/namespaces/deployment-7222/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:86d0d5ae-e4fc-456d-8133-0269f4790fe7,ResourceVersion:31508,Generation:1,CreationTimestamp:2020-02-22 15:19:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 238f31ad-e28e-4af0-abe2-34f1f8bbb431 0xc0033c6707 0xc0033c6708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 22 15:19:21.885: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 22 15:19:21.885: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-7222,SelfLink:/apis/apps/v1/namespaces/deployment-7222/replicasets/test-rolling-update-controller,UID:57530866-a3a2-44cb-88ec-5fde8e103a7b,ResourceVersion:31518,Generation:2,CreationTimestamp:2020-02-22 15:19:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 238f31ad-e28e-4af0-abe2-34f1f8bbb431 0xc0033c6577 0xc0033c6578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 22 15:19:21.888: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-q5wwj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-q5wwj,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-7222,SelfLink:/api/v1/namespaces/deployment-7222/pods/test-rolling-update-deployment-79f6b9d75c-q5wwj,UID:84de1147-d283-427f-81d0-42c0632a8836,ResourceVersion:31507,Generation:0,CreationTimestamp:2020-02-22 15:19:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.8.44/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 86d0d5ae-e4fc-456d-8133-0269f4790fe7 0xc0029cfcb7 0xc0029cfcb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fdqz4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fdqz4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fdqz4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-100-10-41.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029cfd20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029cfd40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:19:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:19:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:19:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-22 15:19:17 +0000 UTC  }],Message:,Reason:,HostIP:10.100.10.41,PodIP:172.16.8.44,StartTime:2020-02-22 15:19:17 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-22 15:19:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4b76756e04629db1427ff3caa3885507c4e2a5fd02deaa8c650084c8b1d3bd4b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:19:21.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7222" for this suite.
Feb 22 15:19:27.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:19:28.057: INFO: namespace deployment-7222 deletion completed in 6.16743825s

â€¢ [SLOW TEST:15.241 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:19:28.057: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-edeeeafb-4bfc-472d-9d1e-145766cbb195
STEP: Creating a pod to test consume configMaps
Feb 22 15:19:28.083: INFO: Waiting up to 5m0s for pod "pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533" in namespace "configmap-428" to be "success or failure"
Feb 22 15:19:28.086: INFO: Pod "pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319698ms
Feb 22 15:19:30.088: INFO: Pod "pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005008315s
STEP: Saw pod success
Feb 22 15:19:30.088: INFO: Pod "pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533" satisfied condition "success or failure"
Feb 22 15:19:30.090: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 22 15:19:30.102: INFO: Waiting for pod pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533 to disappear
Feb 22 15:19:30.103: INFO: Pod pod-configmaps-f64e35b5-3983-40b1-be6f-30e8341aa533 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:19:30.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-428" for this suite.
Feb 22 15:19:36.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:19:36.277: INFO: namespace configmap-428 deletion completed in 6.171936431s

â€¢ [SLOW TEST:8.220 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:19:36.278: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 15:19:36.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5" in namespace "projected-3970" to be "success or failure"
Feb 22 15:19:36.309: INFO: Pod "downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884955ms
Feb 22 15:19:38.311: INFO: Pod "downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006370564s
Feb 22 15:19:40.313: INFO: Pod "downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008475596s
STEP: Saw pod success
Feb 22 15:19:40.313: INFO: Pod "downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5" satisfied condition "success or failure"
Feb 22 15:19:40.316: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5 container client-container: <nil>
STEP: delete the pod
Feb 22 15:19:40.329: INFO: Waiting for pod downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5 to disappear
Feb 22 15:19:40.331: INFO: Pod downwardapi-volume-db9d39c9-d6d0-4d4b-98b8-d13aba3b60f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:19:40.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3970" for this suite.
Feb 22 15:19:46.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:19:46.500: INFO: namespace projected-3970 deletion completed in 6.166533487s

â€¢ [SLOW TEST:10.223 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:19:46.501: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 22 15:19:48.550: INFO: Waiting up to 5m0s for pod "client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e" in namespace "pods-7572" to be "success or failure"
Feb 22 15:19:48.557: INFO: Pod "client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.053415ms
Feb 22 15:19:50.559: INFO: Pod "client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008977408s
Feb 22 15:19:52.561: INFO: Pod "client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010997701s
STEP: Saw pod success
Feb 22 15:19:52.561: INFO: Pod "client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e" satisfied condition "success or failure"
Feb 22 15:19:52.563: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e container env3cont: <nil>
STEP: delete the pod
Feb 22 15:19:52.576: INFO: Waiting for pod client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e to disappear
Feb 22 15:19:52.578: INFO: Pod client-envvars-c94a4310-2899-4750-b6c3-9f283be19c4e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:19:52.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7572" for this suite.
Feb 22 15:20:42.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:20:42.745: INFO: namespace pods-7572 deletion completed in 50.164690632s

â€¢ [SLOW TEST:56.245 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:20:42.746: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 22 15:20:42.766: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 22 15:20:42.770: INFO: Waiting for terminating namespaces to be deleted...
Feb 22 15:20:42.771: INFO: 
Logging pods the kubelet thinks is on node ip-10-100-10-41.eu-west-1.compute.internal before test
Feb 22 15:20:42.776: INFO: nginx-ingress-controller-wzhsm from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 22 15:20:42.776: INFO: fluentd-2p5rq from logging started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container fluentd ready: true, restart count 0
Feb 22 15:20:42.776: INFO: cert-manager-cainjector-db5466bb4-vz5nk from cert-manager started at 2020-02-22 13:39:41 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container cainjector ready: true, restart count 0
Feb 22 15:20:42.776: INFO: sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-kxfkz from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 22 15:20:42.776: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 22 15:20:42.776: INFO: goldpinger-rtcxd from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container goldpinger ready: true, restart count 0
Feb 22 15:20:42.776: INFO: minio-0 from kube-system started at 2020-02-22 13:40:30 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container minio ready: true, restart count 0
Feb 22 15:20:42.776: INFO: calico-node-mxbd8 from kube-system started at 2020-02-22 13:39:32 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container calico-node ready: true, restart count 0
Feb 22 15:20:42.776: INFO: node-exporter-msc8m from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container node-exporter ready: true, restart count 0
Feb 22 15:20:42.776: INFO: velero-restic-2w5bp from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container restic ready: true, restart count 0
Feb 22 15:20:42.776: INFO: coredns-5c98db65d4-p26fz from kube-system started at 2020-02-22 13:39:48 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container coredns ready: true, restart count 0
Feb 22 15:20:42.776: INFO: kube-state-metrics-779c78b7d5-p2w4c from monitoring started at 2020-02-22 13:41:19 +0000 UTC (2 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container addon-resizer ready: true, restart count 0
Feb 22 15:20:42.776: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 22 15:20:42.776: INFO: kube-proxy-qj2fs from kube-system started at 2020-02-22 13:36:59 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 15:20:42.776: INFO: elasticsearch-0 from logging started at 2020-02-22 13:40:27 +0000 UTC (2 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container elasticsearch ready: true, restart count 0
Feb 22 15:20:42.776: INFO: 	Container exporter ready: true, restart count 0
Feb 22 15:20:42.776: INFO: sonobuoy from sonobuoy started at 2020-02-22 13:43:59 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.776: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 22 15:20:42.776: INFO: 
Logging pods the kubelet thinks is on node ip-10-100-10-53.eu-west-1.compute.internal before test
Feb 22 15:20:42.785: INFO: velero-restic-qbvdz from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container restic ready: true, restart count 0
Feb 22 15:20:42.785: INFO: nginx-ingress-controller-khvgv from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 22 15:20:42.785: INFO: sonobuoy-e2e-job-f6fd58bec6164e4d from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container e2e ready: true, restart count 0
Feb 22 15:20:42.785: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 22 15:20:42.785: INFO: local-path-provisioner-84f4c8b584-xjdmp from local-path-storage started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container local-path-provisioner ready: true, restart count 0
Feb 22 15:20:42.785: INFO: prometheus-operator-7f4c684c8b-wxj5q from monitoring started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 22 15:20:42.785: INFO: calico-node-6dg9m from kube-system started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container calico-node ready: true, restart count 0
Feb 22 15:20:42.785: INFO: coredns-5c98db65d4-jw6hn from kube-system started at 2020-02-22 13:39:39 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container coredns ready: true, restart count 0
Feb 22 15:20:42.785: INFO: forecastle-6d4d79f779-fzlqw from ingress-nginx started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container forecastle ready: true, restart count 0
Feb 22 15:20:42.785: INFO: cerebro-78d8f5fc66-nwsj5 from logging started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container cerebro ready: true, restart count 0
Feb 22 15:20:42.785: INFO: kube-proxy-9ckc7 from kube-system started at 2020-02-22 13:36:59 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 22 15:20:42.785: INFO: node-exporter-cnhxz from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container node-exporter ready: true, restart count 0
Feb 22 15:20:42.785: INFO: fluentd-cbtvh from logging started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container fluentd ready: true, restart count 0
Feb 22 15:20:42.785: INFO: velero-5c4bbbd8d6-drz26 from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container velero ready: true, restart count 0
Feb 22 15:20:42.785: INFO: kibana-5bd7fdf954-k7mrh from logging started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container kibana ready: true, restart count 0
Feb 22 15:20:42.785: INFO: prometheus-k8s-0 from monitoring started at 2020-02-22 13:41:20 +0000 UTC (3 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container prometheus ready: true, restart count 1
Feb 22 15:20:42.785: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 22 15:20:42.785: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 22 15:20:42.785: INFO: goldpinger-dxblt from monitoring started at 2020-02-22 13:39:33 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container goldpinger ready: true, restart count 0
Feb 22 15:20:42.785: INFO: grafana-6cdc7d9f68-4p8g9 from monitoring started at 2020-02-22 13:39:39 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container grafana ready: true, restart count 0
Feb 22 15:20:42.785: INFO: calico-kube-controllers-8944b5db6-zqdw6 from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.785: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 22 15:20:42.785: INFO: cert-manager-594fcfc677-s7wd7 from cert-manager started at 2020-02-22 13:39:41 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.786: INFO: 	Container cert-manager ready: true, restart count 0
Feb 22 15:20:42.786: INFO: cert-manager-webhook-5f988b8bbc-sq7k7 from cert-manager started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.786: INFO: 	Container webhook ready: true, restart count 0
Feb 22 15:20:42.786: INFO: minio-setup-jp76l from kube-system started at 2020-02-22 13:39:40 +0000 UTC (1 container statuses recorded)
Feb 22 15:20:42.786: INFO: 	Container mc ready: false, restart count 0
Feb 22 15:20:42.786: INFO: sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-4zplg from sonobuoy started at 2020-02-22 13:44:05 +0000 UTC (2 container statuses recorded)
Feb 22 15:20:42.786: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 22 15:20:42.786: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-10-100-10-41.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.823: INFO: Pod cert-manager-594fcfc677-s7wd7 requesting resource cpu=50m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod cert-manager-cainjector-db5466bb4-vz5nk requesting resource cpu=50m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod cert-manager-webhook-5f988b8bbc-sq7k7 requesting resource cpu=50m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod forecastle-6d4d79f779-fzlqw requesting resource cpu=50m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod nginx-ingress-controller-khvgv requesting resource cpu=0m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod nginx-ingress-controller-wzhsm requesting resource cpu=0m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod calico-kube-controllers-8944b5db6-zqdw6 requesting resource cpu=0m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod calico-node-6dg9m requesting resource cpu=250m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod calico-node-mxbd8 requesting resource cpu=250m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod coredns-5c98db65d4-jw6hn requesting resource cpu=100m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod coredns-5c98db65d4-p26fz requesting resource cpu=100m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod kube-proxy-9ckc7 requesting resource cpu=0m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod kube-proxy-qj2fs requesting resource cpu=0m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod minio-0 requesting resource cpu=0m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod velero-5c4bbbd8d6-drz26 requesting resource cpu=100m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod velero-restic-2w5bp requesting resource cpu=100m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod velero-restic-qbvdz requesting resource cpu=100m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod local-path-provisioner-84f4c8b584-xjdmp requesting resource cpu=0m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod cerebro-78d8f5fc66-nwsj5 requesting resource cpu=500m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod elasticsearch-0 requesting resource cpu=1600m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod fluentd-2p5rq requesting resource cpu=300m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod fluentd-cbtvh requesting resource cpu=300m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod kibana-5bd7fdf954-k7mrh requesting resource cpu=100m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod goldpinger-dxblt requesting resource cpu=1m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod goldpinger-rtcxd requesting resource cpu=1m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod grafana-6cdc7d9f68-4p8g9 requesting resource cpu=100m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod kube-state-metrics-779c78b7d5-p2w4c requesting resource cpu=116m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod node-exporter-cnhxz requesting resource cpu=102m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod node-exporter-msc8m requesting resource cpu=102m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod prometheus-k8s-0 requesting resource cpu=700m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod prometheus-operator-7f4c684c8b-wxj5q requesting resource cpu=100m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-100-10-41.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod sonobuoy-e2e-job-f6fd58bec6164e4d requesting resource cpu=0m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-4zplg requesting resource cpu=0m on Node ip-10-100-10-53.eu-west-1.compute.internal
Feb 22 15:20:42.824: INFO: Pod sonobuoy-systemd-logs-daemon-set-c3e36b01a28d4d8a-kxfkz requesting resource cpu=0m on Node ip-10-100-10-41.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e.15f5c2a1776624dd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4852/filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e to ip-10-100-10-53.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e.15f5c2a1b7016a1e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e.15f5c2a1bc4843a6], Reason = [Created], Message = [Created container filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e.15f5c2a1c82c1b6c], Reason = [Started], Message = [Started container filler-pod-ca289dc0-8a53-4d07-b16e-ec143705ad5e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d.15f5c2a1776967b0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4852/filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d to ip-10-100-10-41.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d.15f5c2a1c8084b3e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d.15f5c2a1ce8de99b], Reason = [Created], Message = [Created container filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d.15f5c2a1da7f60bd], Reason = [Started], Message = [Started container filler-pod-f00c6d4d-e9f3-464f-acb5-893259450b7d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f5c2a266ee8da6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-10-100-10-41.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-100-10-53.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:20:47.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4852" for this suite.
Feb 22 15:20:53.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:20:54.061: INFO: namespace sched-pred-4852 deletion completed in 6.166676765s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:11.316 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:20:54.062: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 22 15:20:54.085: INFO: Waiting up to 5m0s for pod "downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd" in namespace "downward-api-3650" to be "success or failure"
Feb 22 15:20:54.089: INFO: Pod "downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.214035ms
Feb 22 15:20:56.101: INFO: Pod "downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015955323s
STEP: Saw pod success
Feb 22 15:20:56.101: INFO: Pod "downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd" satisfied condition "success or failure"
Feb 22 15:20:56.110: INFO: Trying to get logs from node ip-10-100-10-53.eu-west-1.compute.internal pod downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd container client-container: <nil>
STEP: delete the pod
Feb 22 15:20:56.138: INFO: Waiting for pod downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd to disappear
Feb 22 15:20:56.162: INFO: Pod downwardapi-volume-babe337a-d091-43ea-bb3a-953c8e734acd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:20:56.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3650" for this suite.
Feb 22 15:21:02.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:21:02.344: INFO: namespace downward-api-3650 deletion completed in 6.177308556s

â€¢ [SLOW TEST:8.282 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:21:02.344: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Feb 22 15:21:02.372: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5488" to be "success or failure"
Feb 22 15:21:02.377: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.771466ms
Feb 22 15:21:04.379: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006882345s
Feb 22 15:21:06.381: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008797604s
STEP: Saw pod success
Feb 22 15:21:06.381: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 22 15:21:06.382: INFO: Trying to get logs from node ip-10-100-10-41.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 22 15:21:06.398: INFO: Waiting for pod pod-host-path-test to disappear
Feb 22 15:21:06.401: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:21:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5488" for this suite.
Feb 22 15:21:12.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:21:12.569: INFO: namespace hostpath-5488 deletion completed in 6.165721761s

â€¢ [SLOW TEST:10.225 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 22 15:21:12.569: INFO: >>> kubeConfig: /tmp/kubeconfig-674502360
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-ebbfb308-bade-425d-987d-4c04b87cfb13
STEP: Creating secret with name s-test-opt-upd-97e0aeeb-c3f0-4594-a462-5a026425f873
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ebbfb308-bade-425d-987d-4c04b87cfb13
STEP: Updating secret s-test-opt-upd-97e0aeeb-c3f0-4594-a462-5a026425f873
STEP: Creating secret with name s-test-opt-create-a5e68807-7e61-4e89-8f52-9b0e216bf84a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 22 15:21:16.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6207" for this suite.
Feb 22 15:21:38.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 22 15:21:38.838: INFO: namespace secrets-6207 deletion completed in 22.169682607s

â€¢ [SLOW TEST:26.269 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.10-beta.0.23+57df7c4bc188bc/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSFeb 22 15:21:38.838: INFO: Running AfterSuite actions on all nodes
Feb 22 15:21:38.838: INFO: Running AfterSuite actions on node 1
Feb 22 15:21:38.838: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 5822.004 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h37m3.145683534s
Test Suite Passed
