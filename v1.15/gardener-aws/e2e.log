Conformance test: not doing test setup.
I0813 00:32:16.687004    4207 e2e.go:241] Starting e2e run "4ae9d274-76e2-4580-8ab0-3cff2da45d9e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565656335 - Will randomize all specs
Will run 212 of 4413 specs

Aug 13 00:32:56.933: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:32:56.936: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 13 00:32:57.491: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 13 00:32:57.870: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 13 00:32:57.870: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Aug 13 00:32:57.871: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 13 00:32:57.966: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'addons-kube2iam' (0 seconds elapsed)
Aug 13 00:32:57.966: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 13 00:32:57.966: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 13 00:32:57.966: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Aug 13 00:32:57.966: INFO: e2e test version: v1.15.2
Aug 13 00:32:58.055: INFO: kube-apiserver version: v1.15.2
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:32:58.055: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
Aug 13 00:32:58.420: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 13 00:32:58.691: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 00:32:59.149: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:33:02.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3273" for this suite.
Aug 13 00:33:40.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:33:43.843: INFO: namespace pods-3273 deletion completed in 41.67614748s
•SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:33:43.843: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 13 00:33:47.780: INFO: Successfully updated pod "labelsupdate8509c171-f813-416f-8d62-9f9cd5c19352"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:33:49.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2439" for this suite.
Aug 13 00:34:12.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:34:15.734: INFO: namespace projected-2439 deletion completed in 25.673126683s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:34:15.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 00:34:16.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-452'
Aug 13 00:34:16.868: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 00:34:16.868: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 13 00:34:17.048: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-cktks]
Aug 13 00:34:17.048: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-cktks" in namespace "kubectl-452" to be "running and ready"
Aug 13 00:34:17.137: INFO: Pod "e2e-test-nginx-rc-cktks": Phase="Pending", Reason="", readiness=false. Elapsed: 89.632994ms
Aug 13 00:34:19.227: INFO: Pod "e2e-test-nginx-rc-cktks": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179631342s
Aug 13 00:34:21.317: INFO: Pod "e2e-test-nginx-rc-cktks": Phase="Running", Reason="", readiness=true. Elapsed: 4.269581621s
Aug 13 00:34:21.317: INFO: Pod "e2e-test-nginx-rc-cktks" satisfied condition "running and ready"
Aug 13 00:34:21.317: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-cktks]
Aug 13 00:34:21.317: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-452'
Aug 13 00:34:22.085: INFO: stderr: ""
Aug 13 00:34:22.085: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug 13 00:34:22.085: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-452'
Aug 13 00:34:22.617: INFO: stderr: ""
Aug 13 00:34:22.618: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:34:22.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-452" for this suite.
Aug 13 00:34:44.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:34:48.387: INFO: namespace kubectl-452 deletion completed in 25.679767932s
•SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:34:48.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 00:34:49.027: INFO: Creating deployment "nginx-deployment"
Aug 13 00:34:49.117: INFO: Waiting for observed generation 1
Aug 13 00:34:49.206: INFO: Waiting for all required pods to come up
Aug 13 00:34:49.296: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 13 00:34:53.477: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 13 00:34:53.657: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 13 00:34:53.837: INFO: Updating deployment nginx-deployment
Aug 13 00:34:53.837: INFO: Waiting for observed generation 2
Aug 13 00:34:56.017: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 13 00:34:56.107: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 13 00:34:56.197: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 13 00:34:56.471: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 13 00:34:56.471: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 13 00:34:56.560: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 13 00:34:56.739: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 13 00:34:56.739: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 13 00:34:56.919: INFO: Updating deployment nginx-deployment
Aug 13 00:34:56.919: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 13 00:34:57.098: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 13 00:34:57.187: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 13 00:34:57.367: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-2699,SelfLink:/apis/apps/v1/namespaces/deployment-2699/deployments/nginx-deployment,UID:9cab968e-70b9-47c7-b720-2cd3332298e7,ResourceVersion:3000,Generation:3,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-13 00:34:56 +0000 UTC 2019-08-13 00:34:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-13 00:34:57 +0000 UTC 2019-08-13 00:34:49 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 13 00:34:57.457: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-2699,SelfLink:/apis/apps/v1/namespaces/deployment-2699/replicasets/nginx-deployment-55fb7cb77f,UID:088d6340-42ac-49fd-8ad5-76f72cf5de8b,ResourceVersion:2999,Generation:3,CreationTimestamp:2019-08-13 00:34:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 9cab968e-70b9-47c7-b720-2cd3332298e7 0xc002b54497 0xc002b54498}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 00:34:57.457: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 13 00:34:57.457: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-2699,SelfLink:/apis/apps/v1/namespaces/deployment-2699/replicasets/nginx-deployment-7b8c6f4498,UID:5dff0758-d053-480c-987a-6d3854fb8ee2,ResourceVersion:2974,Generation:3,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 9cab968e-70b9-47c7-b720-2cd3332298e7 0xc002b54567 0xc002b54568}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 13 00:34:57.549: INFO: Pod "nginx-deployment-55fb7cb77f-2tbbn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2tbbn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-2tbbn,UID:fcee3b4c-8495-4bb4-89e4-5ad90ece3079,ResourceVersion:2984,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0025cd900 0xc0025cd901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025cd970} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025cd990}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.549: INFO: Pod "nginx-deployment-55fb7cb77f-8p2bw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8p2bw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-8p2bw,UID:38437a03-acc9-455b-a2aa-87da89486c2c,ResourceVersion:2998,Generation:0,CreationTimestamp:2019-08-13 00:34:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0025cda70 0xc0025cda71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025cdae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025cdb00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.15,StartTime:2019-08-13 00:34:54 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.549: INFO: Pod "nginx-deployment-55fb7cb77f-8tk2t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8tk2t,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-8tk2t,UID:45e39886-1bd7-4850-bf75-96bab2878b67,ResourceVersion:3002,Generation:0,CreationTimestamp:2019-08-13 00:34:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.17/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0025cdc00 0xc0025cdc01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025cdc70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025cdc90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.17,StartTime:2019-08-13 00:34:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-cf9pv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cf9pv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-cf9pv,UID:8038d25f-6c23-42ed-ba8f-8c9763aa5bfd,ResourceVersion:2980,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0025cdd80 0xc0025cdd81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025cddf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025cde10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-dmzh8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dmzh8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-dmzh8,UID:67b790bd-cb90-454f-9c58-3b7fe74d1623,ResourceVersion:2922,Generation:0,CreationTimestamp:2019-08-13 00:34:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0025cdef0 0xc0025cdef1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025cdf60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025cdf80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:54 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:100.96.0.15,StartTime:2019-08-13 00:34:54 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-dpxbc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dpxbc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-dpxbc,UID:fce659b7-40ca-4d10-8930-016f0255c776,ResourceVersion:2989,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba070 0xc0027ba071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba0e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-h9cxl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-h9cxl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-h9cxl,UID:199cb1c7-5e1f-4902-970b-2aed46b53f8c,ResourceVersion:2923,Generation:0,CreationTimestamp:2019-08-13 00:34:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.14/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba1e0 0xc0027ba1e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:100.96.0.14,StartTime:2019-08-13 00:34:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-hkj7w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hkj7w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-hkj7w,UID:dd7ab14c-bb41-492c-a4e1-ebc69578ed4e,ResourceVersion:2991,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba360 0xc0027ba361}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-k892w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-k892w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-k892w,UID:5f46f9df-5a79-4186-859d-1ce632b0ff8d,ResourceVersion:2981,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba4c0 0xc0027ba4c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-kd5c7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kd5c7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-kd5c7,UID:28d646cf-999f-4e8a-b5b5-6571bc9e748b,ResourceVersion:2997,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba620 0xc0027ba621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.550: INFO: Pod "nginx-deployment-55fb7cb77f-kqlt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kqlt7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-kqlt7,UID:ed5b6339-a943-4305-bf33-3f20ce1c4d2c,ResourceVersion:3001,Generation:0,CreationTimestamp:2019-08-13 00:34:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.16/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba790 0xc0027ba791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:53 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.16,StartTime:2019-08-13 00:34:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-55fb7cb77f-lhhp9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lhhp9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-lhhp9,UID:1bf75875-67a2-490d-8b9e-7fef891b3b2c,ResourceVersion:2988,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027ba910 0xc0027ba911}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027ba980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027ba9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-55fb7cb77f-w9sk6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-w9sk6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-55fb7cb77f-w9sk6,UID:23467cb6-5bc1-47f3-bfcb-b127bc0bf7db,ResourceVersion:2990,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 088d6340-42ac-49fd-8ad5-76f72cf5de8b 0xc0027baa70 0xc0027baa71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027baae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bab00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-22cf7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-22cf7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-22cf7,UID:14c1a556-1a9f-4ace-80a7-1b864da2dbcc,ResourceVersion:2858,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.13/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027babe0 0xc0027babe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bac40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bac60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:100.96.0.13,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://379761d99e3467d7f9a9c892b5a94537978d77dfe72ebb2d3a77d1ae7e81800f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-47fvj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-47fvj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-47fvj,UID:7f564448-d660-497f-b478-039701c7f25e,ResourceVersion:2992,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bad30 0xc0027bad31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bad90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027badb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-4dlhc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4dlhc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-4dlhc,UID:80683cb6-735e-47ad-b265-0c7c3d5a66a4,ResourceVersion:2843,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.8/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bae87 0xc0027bae88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027baef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027baf10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.8,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9eb0ccba9e5e0fe16b40652d8763271ce0a35bd62d988e315ff2dd6d8dc03601}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-5qwhp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5qwhp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-5qwhp,UID:90e1b901-806c-4203-8ee7-4a7aba3ebfaf,ResourceVersion:2846,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.9/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027baff0 0xc0027baff1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.9,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e48dfd0cea03fb4fa3bb782a39086ba53c527d1d09309c3b76d97e7754b8bd9a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-62tdf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-62tdf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-62tdf,UID:5a4c122e-1b5c-43a4-954d-866a6093b97a,ResourceVersion:2982,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb140 0xc0027bb141}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb1a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb1c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-6shmq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6shmq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-6shmq,UID:c99582f3-a602-418e-8d41-4ade11e78f23,ResourceVersion:2976,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb287 0xc0027bb288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb2f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.551: INFO: Pod "nginx-deployment-7b8c6f4498-77r8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-77r8k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-77r8k,UID:d7f0aa62-f986-4e66-a6af-1d7d517bf3b9,ResourceVersion:2995,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb3d7 0xc0027bb3d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-7gqsx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7gqsx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-7gqsx,UID:e51f3080-63fd-40a4-901d-99674be1c9ba,ResourceVersion:2861,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.12/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb537 0xc0027bb538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:100.96.0.12,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://583e2c404682a315c3b99c58dea690b7ff8b63dc638b7b4189dd02971b1ed03f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-7lxxn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7lxxn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-7lxxn,UID:3742c179-61b2-4c05-bd6c-4681c0c97c86,ResourceVersion:2855,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.6/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb6a0 0xc0027bb6a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.6,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b2e9d71c34bb10dc8c7378a4012cc817396d4f8c0a3d25ac1b9747acd6725f48}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-jb8d8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jb8d8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-jb8d8,UID:f8548d40-529a-491c-80d5-df25f286bb77,ResourceVersion:2983,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb7f0 0xc0027bb7f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-jlcqz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jlcqz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-jlcqz,UID:14fb75e0-4734-44c5-b5ec-ef360e2da1c6,ResourceVersion:2987,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bb937 0xc0027bb938}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bb9a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bb9c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-mjnzr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mjnzr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-mjnzr,UID:a26b57ea-0d8b-4695-b3e4-1f84474b29e2,ResourceVersion:2840,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.7/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bba97 0xc0027bba98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bbb00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bbb20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.7,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://45e973c1608a3583b86610c30f7bf9f8c9af62f78d110d8df7150a1e2f45e5dc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-ntcbc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ntcbc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-ntcbc,UID:903e350c-a432-4713-a5d5-c3d63ed84c88,ResourceVersion:2986,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bbbf0 0xc0027bbbf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bbc50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bbc70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-p9fjz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p9fjz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-p9fjz,UID:5b4e1037-6ecc-42f9-ae33-8511cc45491a,ResourceVersion:2864,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.11/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bbd47 0xc0027bbd48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bbdb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bbdd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:100.96.0.11,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://afa4407323dd3c23f3c33d4c9f3076afea46e675047e3448cbb89400cc060289}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-s289k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s289k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-s289k,UID:db681990-9304-46a7-a0dd-f6ddb39ff9b6,ResourceVersion:2985,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bbea0 0xc0027bbea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027bbf00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027bbf20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-sqv25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sqv25,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-sqv25,UID:e1464984-d38d-4636-8cf5-a13def054375,ResourceVersion:2996,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0027bbfe7 0xc0027bbfe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e2050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e2070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.552: INFO: Pod "nginx-deployment-7b8c6f4498-vh2ct" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vh2ct,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-vh2ct,UID:2e1341f4-54d7-4303-b408-fa1d384d153f,ResourceVersion:2994,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0029e2137 0xc0029e2138}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e21a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e21c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 00:34:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.553: INFO: Pod "nginx-deployment-7b8c6f4498-wdbnd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wdbnd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-wdbnd,UID:162a0d52-1ad8-4703-9441-09b77df7959c,ResourceVersion:2977,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0029e2287 0xc0029e2288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e22f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e2310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.553: INFO: Pod "nginx-deployment-7b8c6f4498-zgtss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zgtss,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-zgtss,UID:639da9f3-8d7f-4464-9f2f-1792a6aceca4,ResourceVersion:2979,Generation:0,CreationTimestamp:2019-08-13 00:34:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0029e23d7 0xc0029e23d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-233.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e2440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e2460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.233,PodIP:,StartTime:2019-08-13 00:34:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 00:34:57.553: INFO: Pod "nginx-deployment-7b8c6f4498-zt5g2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zt5g2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2699,SelfLink:/api/v1/namespaces/deployment-2699/pods/nginx-deployment-7b8c6f4498-zt5g2,UID:323120a0-8a55-4330-9196-6935e836760d,ResourceVersion:2849,Generation:0,CreationTimestamp:2019-08-13 00:34:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.5/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5dff0758-d053-480c-987a-6d3854fb8ee2 0xc0029e2537 0xc0029e2538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrqq9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrqq9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrqq9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029e25a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029e25c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:34:49 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.5,StartTime:2019-08-13 00:34:49 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 00:34:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://050f35a1513de9fd406ca20f13bdfe6ed94dd7575629ff1447e021a3ea012c5e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:34:57.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2699" for this suite.
Aug 13 00:35:03.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:35:07.320: INFO: namespace deployment-2699 deletion completed in 9.677190384s
•SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:35:07.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 00:35:08.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5" in namespace "projected-171" to be "success or failure"
Aug 13 00:35:08.146: INFO: Pod "downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5": Phase="Pending", Reason="", readiness=false. Elapsed: 91.219331ms
Aug 13 00:35:10.236: INFO: Pod "downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181541067s
STEP: Saw pod success
Aug 13 00:35:10.237: INFO: Pod "downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5" satisfied condition "success or failure"
Aug 13 00:35:10.326: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5 container client-container: <nil>
STEP: delete the pod
Aug 13 00:35:10.522: INFO: Waiting for pod downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5 to disappear
Aug 13 00:35:10.612: INFO: Pod downwardapi-volume-0f844398-5b94-45b6-b287-073e5a9d95d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:35:10.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-171" for this suite.
Aug 13 00:35:16.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:35:20.388: INFO: namespace projected-171 deletion completed in 9.685741166s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:35:20.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-e956d02f-5d9a-4ad9-be02-11d431ba68d5 in namespace container-probe-5084
Aug 13 00:35:23.303: INFO: Started pod busybox-e956d02f-5d9a-4ad9-be02-11d431ba68d5 in namespace container-probe-5084
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 00:35:23.392: INFO: Initial restart count of pod busybox-e956d02f-5d9a-4ad9-be02-11d431ba68d5 is 0
Aug 13 00:36:13.651: INFO: Restart count of pod container-probe-5084/busybox-e956d02f-5d9a-4ad9-be02-11d431ba68d5 is now 1 (50.258475133s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:36:13.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5084" for this suite.
Aug 13 00:36:20.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:36:23.523: INFO: namespace container-probe-5084 deletion completed in 9.689563631s
•SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:36:23.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1342
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 13 00:36:26.612: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:36:26.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1342" for this suite.
Aug 13 00:36:33.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:36:36.562: INFO: namespace container-runtime-1342 deletion completed in 9.674399501s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:36:36.562: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4992
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4992
STEP: Creating statefulset with conflicting port in namespace statefulset-4992
STEP: Waiting until pod test-pod will start running in namespace statefulset-4992
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4992
Aug 13 00:36:39.829: INFO: Observed stateful pod in namespace: statefulset-4992, name: ss-0, uid: 0d0de163-2d81-48e0-9293-5705490c1d6d, status phase: Pending. Waiting for statefulset controller to delete.
Aug 13 00:36:39.871: INFO: Observed stateful pod in namespace: statefulset-4992, name: ss-0, uid: 0d0de163-2d81-48e0-9293-5705490c1d6d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 13 00:36:39.873: INFO: Observed stateful pod in namespace: statefulset-4992, name: ss-0, uid: 0d0de163-2d81-48e0-9293-5705490c1d6d, status phase: Failed. Waiting for statefulset controller to delete.
Aug 13 00:36:39.876: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4992
STEP: Removing pod with conflicting port in namespace statefulset-4992
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4992 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 13 00:36:42.150: INFO: Deleting all statefulset in ns statefulset-4992
Aug 13 00:36:42.240: INFO: Scaling statefulset ss to 0
Aug 13 00:36:52.600: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 00:36:52.690: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:36:52.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4992" for this suite.
Aug 13 00:36:59.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:37:02.722: INFO: namespace statefulset-4992 deletion completed in 9.67253279s
•S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:37:02.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
W0813 00:37:03.991001    4207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 00:37:03.991: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:37:03.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6481" for this suite.
Aug 13 00:37:10.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:37:13.756: INFO: namespace gc-6481 deletion completed in 9.675944927s
•SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:37:13.756: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 13 00:37:18.863: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-9d6a39cb-315a-43c6-92a4-f83b11d07455,GenerateName:,Namespace:events-4987,SelfLink:/api/v1/namespaces/events-4987/pods/send-events-9d6a39cb-315a-43c6-92a4-f83b11d07455,UID:1211f78c-672c-4b1e-871b-0b4c957d40bc,ResourceVersion:3763,Generation:0,CreationTimestamp:2019-08-13 00:37:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 412168708,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mvwxn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mvwxn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-mvwxn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000fa5c40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000fa5c60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:37:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:37:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:37:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:37:14 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.33,StartTime:2019-08-13 00:37:14 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-13 00:37:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://b989c62fe0d6a678930620973b69b456b84bac7a1d16f9b5d5775177f56c6c48}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 13 00:37:20.957: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 13 00:37:23.048: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:37:23.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4987" for this suite.
Aug 13 00:38:03.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:38:06.934: INFO: namespace events-4987 deletion completed in 43.705570455s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:38:06.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-3655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 13 00:38:14.364: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:14.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:15.222: INFO: Exec stderr: ""
Aug 13 00:38:15.222: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:15.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:16.053: INFO: Exec stderr: ""
Aug 13 00:38:16.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:16.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:16.885: INFO: Exec stderr: ""
Aug 13 00:38:16.885: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:16.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:17.714: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 13 00:38:17.714: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:17.714: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:18.558: INFO: Exec stderr: ""
Aug 13 00:38:18.558: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:18.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:19.390: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 13 00:38:19.390: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:19.390: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:20.221: INFO: Exec stderr: ""
Aug 13 00:38:20.222: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:20.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:21.047: INFO: Exec stderr: ""
Aug 13 00:38:21.047: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:21.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:21.907: INFO: Exec stderr: ""
Aug 13 00:38:21.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3655 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 00:38:21.907: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 00:38:22.741: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:38:22.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3655" for this suite.
Aug 13 00:39:13.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:39:16.560: INFO: namespace e2e-kubelet-etc-hosts-3655 deletion completed in 53.727429091s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:39:16.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:40:17.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4558" for this suite.
Aug 13 00:40:39.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:40:43.226: INFO: namespace container-probe-4558 deletion completed in 25.681078516s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:40:43.227: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f66f175d-2948-4faa-9a84-4805fb1d4378
STEP: Creating a pod to test consume secrets
Aug 13 00:40:44.048: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502" in namespace "projected-9959" to be "success or failure"
Aug 13 00:40:44.138: INFO: Pod "pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502": Phase="Pending", Reason="", readiness=false. Elapsed: 89.483327ms
Aug 13 00:40:46.228: INFO: Pod "pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179684567s
STEP: Saw pod success
Aug 13 00:40:46.228: INFO: Pod "pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502" satisfied condition "success or failure"
Aug 13 00:40:46.319: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 00:40:46.510: INFO: Waiting for pod pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502 to disappear
Aug 13 00:40:46.600: INFO: Pod pod-projected-secrets-3cbbe4f9-c43b-438d-9d7d-9d55c8116502 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:40:46.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9959" for this suite.
Aug 13 00:40:52.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:40:56.361: INFO: namespace projected-9959 deletion completed in 9.670973796s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:40:56.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3604
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-99f7f31a-cbf9-4795-8455-69bac4c789de
STEP: Creating secret with name s-test-opt-upd-2a7f853c-d01e-4df7-9836-46576e6045bb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-99f7f31a-cbf9-4795-8455-69bac4c789de
STEP: Updating secret s-test-opt-upd-2a7f853c-d01e-4df7-9836-46576e6045bb
STEP: Creating secret with name s-test-opt-create-ad84b2bd-f0c2-454d-ba05-bac63cdbde7f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:42:13.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3604" for this suite.
Aug 13 00:42:36.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:42:39.648: INFO: namespace projected-3604 deletion completed in 25.736796893s
•SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:42:39.648: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 13 00:42:47.196: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:47.286: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:42:49.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:49.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:42:51.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:51.377: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:42:53.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:53.377: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:42:55.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:55.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:42:57.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:57.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:42:59.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:42:59.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:01.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:01.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:03.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:03.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:05.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:05.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:07.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:07.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:09.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:09.376: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:11.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:11.377: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 00:43:13.286: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 00:43:13.376: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:43:13.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2168" for this suite.
Aug 13 00:43:35.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:43:39.146: INFO: namespace container-lifecycle-hook-2168 deletion completed in 25.679845808s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:43:39.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9278
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-70fc36b0-f795-4ba3-833a-68a613f3312f
STEP: Creating a pod to test consume secrets
Aug 13 00:43:39.967: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2" in namespace "projected-9278" to be "success or failure"
Aug 13 00:43:40.057: INFO: Pod "pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.650794ms
Aug 13 00:43:42.147: INFO: Pod "pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179622726s
STEP: Saw pod success
Aug 13 00:43:42.147: INFO: Pod "pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2" satisfied condition "success or failure"
Aug 13 00:43:42.236: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 00:43:42.434: INFO: Waiting for pod pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2 to disappear
Aug 13 00:43:42.523: INFO: Pod pod-projected-secrets-2ee1f5d7-1197-43a6-a119-769c926217a2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:43:42.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9278" for this suite.
Aug 13 00:43:48.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:43:52.287: INFO: namespace projected-9278 deletion completed in 9.673167346s
•SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:43:52.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 13 00:43:56.059: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a9a9fb5b-4bfd-49a7-93c4-bbb561d70f2d"
Aug 13 00:43:56.059: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a9a9fb5b-4bfd-49a7-93c4-bbb561d70f2d" in namespace "pods-6754" to be "terminated due to deadline exceeded"
Aug 13 00:43:56.150: INFO: Pod "pod-update-activedeadlineseconds-a9a9fb5b-4bfd-49a7-93c4-bbb561d70f2d": Phase="Running", Reason="", readiness=true. Elapsed: 90.412259ms
Aug 13 00:43:58.240: INFO: Pod "pod-update-activedeadlineseconds-a9a9fb5b-4bfd-49a7-93c4-bbb561d70f2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.180530445s
Aug 13 00:44:00.330: INFO: Pod "pod-update-activedeadlineseconds-a9a9fb5b-4bfd-49a7-93c4-bbb561d70f2d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.270768673s
Aug 13 00:44:00.330: INFO: Pod "pod-update-activedeadlineseconds-a9a9fb5b-4bfd-49a7-93c4-bbb561d70f2d" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:44:00.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6754" for this suite.
Aug 13 00:44:06.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:44:10.096: INFO: namespace pods-6754 deletion completed in 9.676129275s
•S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:44:10.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8471.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8471.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 13 00:44:19.928: INFO: DNS probes using dns-8471/dns-test-802fb54a-c0f8-4f1b-b3d3-36a08b9a41bd succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:44:20.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8471" for this suite.
Aug 13 00:44:26.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:44:29.784: INFO: namespace dns-8471 deletion completed in 9.671044063s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:44:29.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7595
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-7595
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7595
Aug 13 00:44:30.707: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 13 00:44:40.799: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 13 00:44:40.889: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 00:44:42.689: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 00:44:42.690: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 00:44:42.690: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 00:44:42.779: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 13 00:44:52.871: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 00:44:52.871: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 00:44:53.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999461s
Aug 13 00:44:54.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.909351124s
Aug 13 00:44:55.411: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.819464928s
Aug 13 00:44:56.502: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.728786928s
Aug 13 00:44:57.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.637969434s
Aug 13 00:44:58.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.547579287s
Aug 13 00:44:59.774: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.456955096s
Aug 13 00:45:00.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.366021112s
Aug 13 00:45:01.955: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.275492804s
Aug 13 00:45:03.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 184.327343ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7595
Aug 13 00:45:04.137: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 00:45:05.398: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 00:45:05.398: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 00:45:05.398: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 00:45:05.398: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 00:45:06.653: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 13 00:45:06.653: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 00:45:06.653: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 00:45:06.653: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 00:45:07.945: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 13 00:45:07.945: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 00:45:07.945: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 00:45:08.035: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 00:45:08.035: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 00:45:08.035: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 13 00:45:08.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 00:45:09.383: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 00:45:09.383: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 00:45:09.383: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 00:45:09.383: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 00:45:10.641: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 00:45:10.642: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 00:45:10.642: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 00:45:10.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7595 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 00:45:11.945: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 00:45:11.945: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 00:45:11.945: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 00:45:11.945: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 00:45:12.035: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 13 00:45:22.215: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 00:45:22.215: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 00:45:22.215: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 00:45:22.485: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:22.485: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:22.485: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:22.485: INFO: ss-2  ip-10-250-2-233.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:22.485: INFO: 
Aug 13 00:45:22.485: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 00:45:23.575: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:23.575: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:23.575: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:23.575: INFO: ss-2  ip-10-250-2-233.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:23.575: INFO: 
Aug 13 00:45:23.575: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 00:45:24.665: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:24.665: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:24.665: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:24.665: INFO: 
Aug 13 00:45:24.665: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:25.756: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:25.756: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:25.756: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:25.756: INFO: 
Aug 13 00:45:25.756: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:26.846: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:26.846: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:26.846: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:26.846: INFO: 
Aug 13 00:45:26.846: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:27.937: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:27.937: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:27.937: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:27.937: INFO: 
Aug 13 00:45:27.937: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:29.027: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:29.027: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:29.027: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:29.027: INFO: 
Aug 13 00:45:29.027: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:30.118: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:30.118: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:30.118: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:30.118: INFO: 
Aug 13 00:45:30.118: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:31.208: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Aug 13 00:45:31.208: INFO: ss-0  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:30 +0000 UTC  }]
Aug 13 00:45:31.208: INFO: ss-1  ip-10-250-2-100.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:45:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 00:44:53 +0000 UTC  }]
Aug 13 00:45:31.208: INFO: 
Aug 13 00:45:31.208: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 13 00:45:32.298: INFO: Verifying statefulset ss doesn't scale past 0 for another 186.826866ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7595
Aug 13 00:45:33.388: INFO: Scaling statefulset ss to 0
Aug 13 00:45:33.657: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 13 00:45:33.747: INFO: Deleting all statefulset in ns statefulset-7595
Aug 13 00:45:33.841: INFO: Scaling statefulset ss to 0
Aug 13 00:45:34.111: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 00:45:34.200: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:45:34.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7595" for this suite.
Aug 13 00:45:40.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:45:44.231: INFO: namespace statefulset-7595 deletion completed in 9.670889817s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:45:44.232: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 13 00:45:44.869: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:45:48.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7742" for this suite.
Aug 13 00:46:10.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:46:14.381: INFO: namespace init-container-7742 deletion completed in 25.671628607s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:46:14.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5349
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 00:46:15.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1" in namespace "downward-api-5349" to be "success or failure"
Aug 13 00:46:15.203: INFO: Pod "downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.642312ms
Aug 13 00:46:17.294: INFO: Pod "downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180077333s
STEP: Saw pod success
Aug 13 00:46:17.294: INFO: Pod "downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1" satisfied condition "success or failure"
Aug 13 00:46:17.384: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1 container client-container: <nil>
STEP: delete the pod
Aug 13 00:46:17.575: INFO: Waiting for pod downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1 to disappear
Aug 13 00:46:17.665: INFO: Pod downwardapi-volume-55e47262-da1c-4845-9396-1e7dac5ffec1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:46:17.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5349" for this suite.
Aug 13 00:46:24.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:46:27.440: INFO: namespace downward-api-5349 deletion completed in 9.68510613s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:46:27.440: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6791
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-43d1ad2d-92d6-4c55-8bee-25068b4e39f4
STEP: Creating a pod to test consume configMaps
Aug 13 00:46:28.260: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a" in namespace "projected-6791" to be "success or failure"
Aug 13 00:46:28.350: INFO: Pod "pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.627259ms
Aug 13 00:46:30.440: INFO: Pod "pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179719653s
STEP: Saw pod success
Aug 13 00:46:30.440: INFO: Pod "pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a" satisfied condition "success or failure"
Aug 13 00:46:30.531: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 00:46:30.717: INFO: Waiting for pod pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a to disappear
Aug 13 00:46:30.807: INFO: Pod pod-projected-configmaps-c4f07d1b-9ed0-4c8c-971f-f11dd5e93e8a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:46:30.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6791" for this suite.
Aug 13 00:46:37.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:46:40.568: INFO: namespace projected-6791 deletion completed in 9.671345612s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:46:40.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 13 00:46:41.303: INFO: Waiting up to 5m0s for pod "downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4" in namespace "downward-api-1031" to be "success or failure"
Aug 13 00:46:41.392: INFO: Pod "downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.539918ms
Aug 13 00:46:43.483: INFO: Pod "downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17998594s
STEP: Saw pod success
Aug 13 00:46:43.483: INFO: Pod "downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4" satisfied condition "success or failure"
Aug 13 00:46:43.573: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4 container dapi-container: <nil>
STEP: delete the pod
Aug 13 00:46:43.759: INFO: Waiting for pod downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4 to disappear
Aug 13 00:46:43.849: INFO: Pod downward-api-ddac4751-0f4f-4d7b-9697-2e05aa01cde4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:46:43.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1031" for this suite.
Aug 13 00:46:50.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:46:53.616: INFO: namespace downward-api-1031 deletion completed in 9.676543568s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:46:53.616: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 13 00:46:54.345: INFO: Waiting up to 5m0s for pod "pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75" in namespace "emptydir-1220" to be "success or failure"
Aug 13 00:46:54.435: INFO: Pod "pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75": Phase="Pending", Reason="", readiness=false. Elapsed: 89.429876ms
Aug 13 00:46:56.525: INFO: Pod "pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179496623s
STEP: Saw pod success
Aug 13 00:46:56.525: INFO: Pod "pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75" satisfied condition "success or failure"
Aug 13 00:46:56.615: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75 container test-container: <nil>
STEP: delete the pod
Aug 13 00:46:56.801: INFO: Waiting for pod pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75 to disappear
Aug 13 00:46:56.891: INFO: Pod pod-b82c0371-ba91-4c9a-b81e-bd1b72866d75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:46:56.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1220" for this suite.
Aug 13 00:47:03.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:47:06.656: INFO: namespace emptydir-1220 deletion completed in 9.675082077s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:47:06.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4211
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-395f870f-7c17-4055-b88c-81671ac94bf7
STEP: Creating a pod to test consume secrets
Aug 13 00:47:07.508: INFO: Waiting up to 5m0s for pod "pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65" in namespace "secrets-4211" to be "success or failure"
Aug 13 00:47:07.598: INFO: Pod "pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65": Phase="Pending", Reason="", readiness=false. Elapsed: 89.819747ms
Aug 13 00:47:09.689: INFO: Pod "pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180010471s
STEP: Saw pod success
Aug 13 00:47:09.689: INFO: Pod "pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65" satisfied condition "success or failure"
Aug 13 00:47:09.778: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 00:47:09.966: INFO: Waiting for pod pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65 to disappear
Aug 13 00:47:10.055: INFO: Pod pod-secrets-ff5a3fdc-0c02-47ae-a6e6-f52bb3553f65 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:47:10.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4211" for this suite.
Aug 13 00:47:16.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:47:19.822: INFO: namespace secrets-4211 deletion completed in 9.676241483s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:47:19.822: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug 13 00:47:20.552: INFO: Waiting up to 5m0s for pod "var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6" in namespace "var-expansion-4232" to be "success or failure"
Aug 13 00:47:20.642: INFO: Pod "var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.823231ms
Aug 13 00:47:22.732: INFO: Pod "var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179981776s
STEP: Saw pod success
Aug 13 00:47:22.732: INFO: Pod "var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6" satisfied condition "success or failure"
Aug 13 00:47:22.822: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6 container dapi-container: <nil>
STEP: delete the pod
Aug 13 00:47:23.009: INFO: Waiting for pod var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6 to disappear
Aug 13 00:47:23.098: INFO: Pod var-expansion-9ed5ba29-9ae1-4347-b325-b7a2ef7be0b6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:47:23.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4232" for this suite.
Aug 13 00:47:29.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:47:32.873: INFO: namespace var-expansion-4232 deletion completed in 9.684974453s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:47:32.873: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 13 00:47:33.603: INFO: Waiting up to 5m0s for pod "pod-5e572691-bbf7-4267-88fb-e08035a0cf82" in namespace "emptydir-7074" to be "success or failure"
Aug 13 00:47:33.693: INFO: Pod "pod-5e572691-bbf7-4267-88fb-e08035a0cf82": Phase="Pending", Reason="", readiness=false. Elapsed: 89.387115ms
Aug 13 00:47:35.783: INFO: Pod "pod-5e572691-bbf7-4267-88fb-e08035a0cf82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179572373s
STEP: Saw pod success
Aug 13 00:47:35.783: INFO: Pod "pod-5e572691-bbf7-4267-88fb-e08035a0cf82" satisfied condition "success or failure"
Aug 13 00:47:35.873: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-5e572691-bbf7-4267-88fb-e08035a0cf82 container test-container: <nil>
STEP: delete the pod
Aug 13 00:47:36.060: INFO: Waiting for pod pod-5e572691-bbf7-4267-88fb-e08035a0cf82 to disappear
Aug 13 00:47:36.149: INFO: Pod pod-5e572691-bbf7-4267-88fb-e08035a0cf82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:47:36.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7074" for this suite.
Aug 13 00:47:42.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:47:45.910: INFO: namespace emptydir-7074 deletion completed in 9.670937861s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:47:45.911: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug 13 00:47:46.640: INFO: Waiting up to 5m0s for pod "var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213" in namespace "var-expansion-7036" to be "success or failure"
Aug 13 00:47:46.731: INFO: Pod "var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213": Phase="Pending", Reason="", readiness=false. Elapsed: 90.005278ms
Aug 13 00:47:48.821: INFO: Pod "var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180061395s
STEP: Saw pod success
Aug 13 00:47:48.821: INFO: Pod "var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213" satisfied condition "success or failure"
Aug 13 00:47:48.910: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213 container dapi-container: <nil>
STEP: delete the pod
Aug 13 00:47:49.098: INFO: Waiting for pod var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213 to disappear
Aug 13 00:47:49.188: INFO: Pod var-expansion-0f0037df-053e-4a62-84d9-e3661b85f213 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:47:49.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7036" for this suite.
Aug 13 00:47:55.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:47:58.948: INFO: namespace var-expansion-7036 deletion completed in 9.669068661s
•SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:47:58.948: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5360
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:48:02.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5360" for this suite.
Aug 13 00:48:08.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:48:12.235: INFO: namespace emptydir-wrapper-5360 deletion completed in 9.729097088s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:48:12.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 13 00:48:13.230: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5840,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 00:48:13.230: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5840,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 13 00:48:23.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5890,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 13 00:48:23.410: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5890,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 13 00:48:33.591: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5911,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 00:48:33.591: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5911,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 13 00:48:43.683: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5932,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 00:48:43.683: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-a,UID:b5d89f6d-1466-4fcc-b6e5-41132adbed3a,ResourceVersion:5932,Generation:0,CreationTimestamp:2019-08-13 00:48:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 13 00:48:53.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-b,UID:19b77df3-a558-4bc3-a1db-a58898800729,ResourceVersion:5954,Generation:0,CreationTimestamp:2019-08-13 00:48:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 00:48:53.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-b,UID:19b77df3-a558-4bc3-a1db-a58898800729,ResourceVersion:5954,Generation:0,CreationTimestamp:2019-08-13 00:48:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 13 00:49:03.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-b,UID:19b77df3-a558-4bc3-a1db-a58898800729,ResourceVersion:5975,Generation:0,CreationTimestamp:2019-08-13 00:48:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 00:49:03.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8681,SelfLink:/api/v1/namespaces/watch-8681/configmaps/e2e-watch-test-configmap-b,UID:19b77df3-a558-4bc3-a1db-a58898800729,ResourceVersion:5975,Generation:0,CreationTimestamp:2019-08-13 00:48:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:49:13.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8681" for this suite.
Aug 13 00:49:20.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:49:23.644: INFO: namespace watch-8681 deletion completed in 9.684465838s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:49:23.645: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-95826124-422a-4d70-9c19-cae5055368ab
STEP: Creating a pod to test consume secrets
Aug 13 00:49:24.480: INFO: Waiting up to 5m0s for pod "pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f" in namespace "secrets-274" to be "success or failure"
Aug 13 00:49:24.573: INFO: Pod "pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f": Phase="Pending", Reason="", readiness=false. Elapsed: 93.281246ms
Aug 13 00:49:26.663: INFO: Pod "pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.183145242s
STEP: Saw pod success
Aug 13 00:49:26.663: INFO: Pod "pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f" satisfied condition "success or failure"
Aug 13 00:49:26.752: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 00:49:26.938: INFO: Waiting for pod pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f to disappear
Aug 13 00:49:27.027: INFO: Pod pod-secrets-0c6792cd-2d03-4d66-abca-dc06168b441f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:49:27.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-274" for this suite.
Aug 13 00:49:33.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:49:36.794: INFO: namespace secrets-274 deletion completed in 9.676324884s
•SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:49:36.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 00:49:37.524: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9" in namespace "downward-api-7779" to be "success or failure"
Aug 13 00:49:37.614: INFO: Pod "downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9": Phase="Pending", Reason="", readiness=false. Elapsed: 89.659082ms
Aug 13 00:49:39.704: INFO: Pod "downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179531981s
STEP: Saw pod success
Aug 13 00:49:39.704: INFO: Pod "downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9" satisfied condition "success or failure"
Aug 13 00:49:39.794: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9 container client-container: <nil>
STEP: delete the pod
Aug 13 00:49:39.982: INFO: Waiting for pod downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9 to disappear
Aug 13 00:49:40.071: INFO: Pod downwardapi-volume-fb32eaf1-613a-41be-8190-1b8d21295ae9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:49:40.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7779" for this suite.
Aug 13 00:49:46.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:49:49.830: INFO: namespace downward-api-7779 deletion completed in 9.669068745s
•SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:49:49.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8262
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 13 00:49:53.507: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8262 pod-service-account-f1c2b0ae-91a5-49b5-8848-7713ca7851f3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 13 00:49:54.813: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8262 pod-service-account-f1c2b0ae-91a5-49b5-8848-7713ca7851f3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 13 00:49:56.053: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8262 pod-service-account-f1c2b0ae-91a5-49b5-8848-7713ca7851f3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:49:57.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8262" for this suite.
Aug 13 00:50:03.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:50:07.161: INFO: namespace svcaccounts-8262 deletion completed in 9.67317867s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:50:07.162: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 00:50:07.800: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:50:10.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4476" for this suite.
Aug 13 00:50:52.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:50:56.287: INFO: namespace pods-4476 deletion completed in 45.669019924s
•SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:50:56.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5657/configmap-test-7fbe7716-a23c-47c9-9607-40d91439dd6e
STEP: Creating a pod to test consume configMaps
Aug 13 00:50:57.106: INFO: Waiting up to 5m0s for pod "pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601" in namespace "configmap-5657" to be "success or failure"
Aug 13 00:50:57.195: INFO: Pod "pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601": Phase="Pending", Reason="", readiness=false. Elapsed: 89.432882ms
Aug 13 00:50:59.286: INFO: Pod "pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17958481s
STEP: Saw pod success
Aug 13 00:50:59.286: INFO: Pod "pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601" satisfied condition "success or failure"
Aug 13 00:50:59.376: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601 container env-test: <nil>
STEP: delete the pod
Aug 13 00:50:59.562: INFO: Waiting for pod pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601 to disappear
Aug 13 00:50:59.651: INFO: Pod pod-configmaps-56274e74-14a3-43a7-a7e7-7e70d4729601 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:50:59.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5657" for this suite.
Aug 13 00:51:06.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:51:09.411: INFO: namespace configmap-5657 deletion completed in 9.669669806s
•SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:51:09.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-9e113e44-f29b-449d-9fc0-b2ad6f429150
STEP: Creating a pod to test consume configMaps
Aug 13 00:51:10.480: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e" in namespace "projected-1021" to be "success or failure"
Aug 13 00:51:10.570: INFO: Pod "pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.500166ms
Aug 13 00:51:12.660: INFO: Pod "pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179234524s
STEP: Saw pod success
Aug 13 00:51:12.660: INFO: Pod "pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e" satisfied condition "success or failure"
Aug 13 00:51:12.749: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 00:51:12.935: INFO: Waiting for pod pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e to disappear
Aug 13 00:51:13.025: INFO: Pod pod-projected-configmaps-3f8380f2-8be2-4284-bb1e-b87b0d19907e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:51:13.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1021" for this suite.
Aug 13 00:51:19.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:51:22.786: INFO: namespace projected-1021 deletion completed in 9.670468218s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:51:22.786: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 00:51:23.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d" in namespace "projected-1029" to be "success or failure"
Aug 13 00:51:23.609: INFO: Pod "downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d": Phase="Pending", Reason="", readiness=false. Elapsed: 92.72776ms
Aug 13 00:51:25.699: INFO: Pod "downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182955266s
STEP: Saw pod success
Aug 13 00:51:25.699: INFO: Pod "downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d" satisfied condition "success or failure"
Aug 13 00:51:25.789: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d container client-container: <nil>
STEP: delete the pod
Aug 13 00:51:25.980: INFO: Waiting for pod downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d to disappear
Aug 13 00:51:26.069: INFO: Pod downwardapi-volume-5d22a1f1-f6d1-46f1-83e2-8af3c2f2d98d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:51:26.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1029" for this suite.
Aug 13 00:51:32.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:51:35.837: INFO: namespace projected-1029 deletion completed in 9.677518294s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:51:35.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 00:51:36.495: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8461'
Aug 13 00:51:37.486: INFO: stderr: ""
Aug 13 00:51:37.486: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 13 00:51:37.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8461'
Aug 13 00:51:38.470: INFO: stderr: ""
Aug 13 00:51:38.470: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 13 00:51:39.561: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 00:51:39.561: INFO: Found 1 / 1
Aug 13 00:51:39.561: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 13 00:51:39.651: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 00:51:39.651: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 13 00:51:39.651: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-k7bhc --namespace=kubectl-8461'
Aug 13 00:51:40.268: INFO: stderr: ""
Aug 13 00:51:40.268: INFO: stdout: "Name:           redis-master-k7bhc\nNamespace:      kubectl-8461\nPriority:       0\nNode:           ip-10-250-2-100.ec2.internal/10.250.2.100\nStart Time:     Tue, 13 Aug 2019 00:51:37 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.96.1.62/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.96.1.62\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c9f0ed4583ded7cce13569da11a4cc1c2bf31e1d81b3b1a627cc553afac51983\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 13 Aug 2019 00:51:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-m6pjq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-m6pjq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-m6pjq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                   Message\n  ----    ------     ----  ----                                   -------\n  Normal  Scheduled  3s    default-scheduler                      Successfully assigned kubectl-8461/redis-master-k7bhc to ip-10-250-2-100.ec2.internal\n  Normal  Pulling    2s    kubelet, ip-10-250-2-100.ec2.internal  Pulling image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Pulled     2s    kubelet, ip-10-250-2-100.ec2.internal  Successfully pulled image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\"\n  Normal  Created    2s    kubelet, ip-10-250-2-100.ec2.internal  Created container redis-master\n  Normal  Started    1s    kubelet, ip-10-250-2-100.ec2.internal  Started container redis-master\n"
Aug 13 00:51:40.269: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-8461'
Aug 13 00:51:40.978: INFO: stderr: ""
Aug 13 00:51:40.978: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-8461\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-k7bhc\n"
Aug 13 00:51:40.978: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-8461'
Aug 13 00:51:41.685: INFO: stderr: ""
Aug 13 00:51:41.685: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-8461\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.68.221.238\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.62:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 13 00:51:41.776: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node ip-10-250-2-100.ec2.internal'
Aug 13 00:51:42.496: INFO: stderr: ""
Aug 13 00:51:42.496: INFO: stdout: "Name:               ip-10-250-2-100.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-2-100.ec2.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=cpu-worker\n                    worker.gardener.cloud/pool=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.2.100/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.126.255.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 13 Aug 2019 00:23:44 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 13 Aug 2019 00:23:54 +0000   Tue, 13 Aug 2019 00:23:54 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 13 Aug 2019 00:51:35 +0000   Tue, 13 Aug 2019 00:23:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 13 Aug 2019 00:51:35 +0000   Tue, 13 Aug 2019 00:23:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 13 Aug 2019 00:51:35 +0000   Tue, 13 Aug 2019 00:23:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 13 Aug 2019 00:51:35 +0000   Tue, 13 Aug 2019 00:23:54 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.2.100\n  Hostname:     ip-10-250-2-100.ec2.internal\n  InternalDNS:  ip-10-250-2-100.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           17897500Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7865420Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         1920m\n ephemeral-storage:           17410687987\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6577738746\n pods:                        110\nSystem Info:\n Machine ID:                 ec27c6b4c166860fde11722518c68bfe\n System UUID:                ec27c6b4-c166-860f-de11-722518c68bfe\n Boot ID:                    3da395b1-83d9-4c8e-87e1-f924f323854f\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.2\n Kube-Proxy Version:         v1.15.2\nPodCIDR:                     100.96.1.0/24\nProviderID:                  aws:///us-east-1c/i-06f98a1ff597f209d\nNon-terminated Pods:         (5 in total)\n  Namespace                  Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                     ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kube2iam-rtfmd    10m (0%)      80m (4%)    16Mi (0%)        128Mi (2%)     27m\n  kube-system                calico-node-8pgth        100m (5%)     500m (26%)  100Mi (1%)       700Mi (11%)    27m\n  kube-system                kube-proxy-pncr5         20m (1%)      0 (0%)      64Mi (1%)        0 (0%)         27m\n  kube-system                node-exporter-md2rz      5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     27m\n  kubectl-8461               redis-master-k7bhc       0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         135m (7%)   605m (31%)\n  memory                      190Mi (3%)  928Mi (14%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                 Age                From                                      Message\n  ----    ------                 ----               ----                                      -------\n  Normal  NodeHasNoDiskPressure  27m (x8 over 28m)  kubelet, ip-10-250-2-100.ec2.internal     Node ip-10-250-2-100.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID   27m (x8 over 28m)  kubelet, ip-10-250-2-100.ec2.internal     Node ip-10-250-2-100.ec2.internal status is now: NodeHasSufficientPID\n  Normal  Starting               27m                kube-proxy, ip-10-250-2-100.ec2.internal  Starting kube-proxy.\n"
Aug 13 00:51:42.497: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-8461'
Aug 13 00:51:43.193: INFO: stderr: ""
Aug 13 00:51:43.193: INFO: stdout: "Name:         kubectl-8461\nLabels:       e2e-framework=kubectl\n              e2e-run=4ae9d274-76e2-4580-8ab0-3cff2da45d9e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:51:43.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8461" for this suite.
Aug 13 00:52:05.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:52:08.958: INFO: namespace kubectl-8461 deletion completed in 25.675867436s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:52:08.959: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 13 00:52:09.597: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 13 00:52:09.777: INFO: Waiting for terminating namespaces to be deleted...
Aug 13 00:52:09.867: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-2-100.ec2.internal before test
Aug 13 00:52:09.961: INFO: kube-proxy-pncr5 from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:09.961: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 00:52:09.961: INFO: node-exporter-md2rz from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:09.961: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 00:52:09.961: INFO: addons-kube2iam-rtfmd from kube-system started at 2019-08-13 00:23:54 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:09.961: INFO: 	Container kube2iam ready: true, restart count 0
Aug 13 00:52:09.961: INFO: calico-node-8pgth from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:09.961: INFO: 	Container calico-node ready: true, restart count 0
Aug 13 00:52:09.961: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-2-233.ec2.internal before test
Aug 13 00:52:10.063: INFO: metrics-server-7dd7d74c9b-6dvkl from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container metrics-server ready: true, restart count 0
Aug 13 00:52:10.063: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-9jm4k from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug 13 00:52:10.063: INFO: coredns-85cc454dd8-mxfbk from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container coredns ready: true, restart count 0
Aug 13 00:52:10.063: INFO: node-exporter-bb6ms from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 00:52:10.063: INFO: blackbox-exporter-954dd954b-b4c5l from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 13 00:52:10.063: INFO: calico-kube-controllers-5f4b46ffb5-wxxxf from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 13 00:52:10.063: INFO: addons-kube2iam-zm6dv from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container kube2iam ready: true, restart count 0
Aug 13 00:52:10.063: INFO: addons-nginx-ingress-controller-66c9ddddb9-5cjjg from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 13 00:52:10.063: INFO: kube-proxy-wgll2 from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 00:52:10.063: INFO: coredns-85cc454dd8-nrq5t from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container coredns ready: true, restart count 0
Aug 13 00:52:10.063: INFO: calico-node-qjrkf from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container calico-node ready: true, restart count 0
Aug 13 00:52:10.063: INFO: vpn-shoot-7f79d69868-7vfs5 from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug 13 00:52:10.063: INFO: addons-kubernetes-dashboard-5c8d9945bc-qr4hm from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 00:52:10.063: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-acded847-90de-4032-994d-58532162d3c2 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-acded847-90de-4032-994d-58532162d3c2 off the node ip-10-250-2-100.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-acded847-90de-4032-994d-58532162d3c2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:52:15.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7940" for this suite.
Aug 13 00:52:33.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:52:37.086: INFO: namespace sched-pred-7940 deletion completed in 21.666248894s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:52:37.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 13 00:52:37.906: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:52:38.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3145" for this suite.
Aug 13 00:52:44.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:52:47.940: INFO: namespace replication-controller-3145 deletion completed in 9.671593401s
•SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:52:47.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-cb2w
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 00:52:48.865: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cb2w" in namespace "subpath-790" to be "success or failure"
Aug 13 00:52:48.959: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Pending", Reason="", readiness=false. Elapsed: 93.106171ms
Aug 13 00:52:51.049: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.183424593s
Aug 13 00:52:53.139: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 4.273538208s
Aug 13 00:52:55.229: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 6.36365019s
Aug 13 00:52:57.319: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 8.453816987s
Aug 13 00:52:59.409: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 10.543807978s
Aug 13 00:53:01.500: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 12.634136285s
Aug 13 00:53:03.592: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 14.726314186s
Aug 13 00:53:05.682: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 16.816422682s
Aug 13 00:53:07.772: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 18.906420613s
Aug 13 00:53:09.862: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Running", Reason="", readiness=true. Elapsed: 20.996792705s
Aug 13 00:53:11.953: INFO: Pod "pod-subpath-test-projected-cb2w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.087084344s
STEP: Saw pod success
Aug 13 00:53:11.953: INFO: Pod "pod-subpath-test-projected-cb2w" satisfied condition "success or failure"
Aug 13 00:53:12.042: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-subpath-test-projected-cb2w container test-container-subpath-projected-cb2w: <nil>
STEP: delete the pod
Aug 13 00:53:12.234: INFO: Waiting for pod pod-subpath-test-projected-cb2w to disappear
Aug 13 00:53:12.324: INFO: Pod pod-subpath-test-projected-cb2w no longer exists
STEP: Deleting pod pod-subpath-test-projected-cb2w
Aug 13 00:53:12.324: INFO: Deleting pod "pod-subpath-test-projected-cb2w" in namespace "subpath-790"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:53:12.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-790" for this suite.
Aug 13 00:53:18.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:53:22.180: INFO: namespace subpath-790 deletion completed in 9.677100052s
•SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:53:22.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7768
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d0ffe554-2406-4915-b3f1-ea2c1d9b1952
STEP: Creating a pod to test consume configMaps
Aug 13 00:53:22.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb" in namespace "projected-7768" to be "success or failure"
Aug 13 00:53:23.089: INFO: Pod "pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.809352ms
Aug 13 00:53:25.179: INFO: Pod "pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180296643s
STEP: Saw pod success
Aug 13 00:53:25.179: INFO: Pod "pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb" satisfied condition "success or failure"
Aug 13 00:53:25.268: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 00:53:25.456: INFO: Waiting for pod pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb to disappear
Aug 13 00:53:25.545: INFO: Pod pod-projected-configmaps-b705e929-9f72-4db7-9b26-a166f70724eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:53:25.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7768" for this suite.
Aug 13 00:53:31.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:53:35.305: INFO: namespace projected-7768 deletion completed in 9.669328069s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:53:35.305: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xff2k in namespace proxy-3063
I0813 00:53:36.127324    4207 runners.go:180] Created replication controller with name: proxy-service-xff2k, namespace: proxy-3063, replica count: 1
I0813 00:53:37.228118    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 00:53:38.228416    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:39.228679    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:40.229046    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:41.229351    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:42.229763    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:43.230025    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:44.230378    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:45.230662    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:46.231036    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:47.231516    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 00:53:48.231857    4207 runners.go:180] proxy-service-xff2k Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 13 00:53:48.321: INFO: setup took 12.378128176s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 13 00:53:48.415: INFO: (0) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 93.507821ms)
Aug 13 00:53:48.415: INFO: (0) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 94.198555ms)
Aug 13 00:53:48.415: INFO: (0) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 94.086484ms)
Aug 13 00:53:48.416: INFO: (0) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 94.62674ms)
Aug 13 00:53:48.421: INFO: (0) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 99.478319ms)
Aug 13 00:53:48.421: INFO: (0) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 99.554794ms)
Aug 13 00:53:48.421: INFO: (0) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 99.706274ms)
Aug 13 00:53:48.425: INFO: (0) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 103.461013ms)
Aug 13 00:53:48.429: INFO: (0) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 107.87886ms)
Aug 13 00:53:48.438: INFO: (0) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 116.849008ms)
Aug 13 00:53:48.500: INFO: (0) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 178.434433ms)
Aug 13 00:53:48.501: INFO: (0) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 179.797497ms)
Aug 13 00:53:48.501: INFO: (0) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 179.654896ms)
Aug 13 00:53:48.501: INFO: (0) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 179.617477ms)
Aug 13 00:53:48.503: INFO: (0) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 181.296391ms)
Aug 13 00:53:48.588: INFO: (0) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 266.709894ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.523524ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.973233ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.049935ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.94316ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.957079ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.046142ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.07072ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.078347ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.257196ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.115551ms)
Aug 13 00:53:48.680: INFO: (1) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.271346ms)
Aug 13 00:53:48.681: INFO: (1) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 92.437596ms)
Aug 13 00:53:48.681: INFO: (1) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.443836ms)
Aug 13 00:53:48.681: INFO: (1) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 93.085151ms)
Aug 13 00:53:48.682: INFO: (1) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.282538ms)
Aug 13 00:53:48.682: INFO: (1) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 94.104055ms)
Aug 13 00:53:48.774: INFO: (2) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.761259ms)
Aug 13 00:53:48.774: INFO: (2) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.824413ms)
Aug 13 00:53:48.774: INFO: (2) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.997812ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.874041ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.965172ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.986735ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.012547ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.704505ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.570988ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.524831ms)
Aug 13 00:53:48.775: INFO: (2) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.676266ms)
Aug 13 00:53:48.776: INFO: (2) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.974435ms)
Aug 13 00:53:48.776: INFO: (2) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.060663ms)
Aug 13 00:53:48.777: INFO: (2) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 94.174669ms)
Aug 13 00:53:48.777: INFO: (2) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 94.242384ms)
Aug 13 00:53:48.777: INFO: (2) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 94.669025ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.531755ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.561643ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.610283ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.570639ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.698461ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.742309ms)
Aug 13 00:53:48.870: INFO: (3) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.746372ms)
Aug 13 00:53:48.871: INFO: (3) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 93.221126ms)
Aug 13 00:53:48.871: INFO: (3) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 93.487432ms)
Aug 13 00:53:48.871: INFO: (3) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 93.424406ms)
Aug 13 00:53:48.871: INFO: (3) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 93.748452ms)
Aug 13 00:53:48.871: INFO: (3) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 93.60609ms)
Aug 13 00:53:48.872: INFO: (3) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 94.596205ms)
Aug 13 00:53:48.873: INFO: (3) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 95.5806ms)
Aug 13 00:53:48.873: INFO: (3) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 95.506147ms)
Aug 13 00:53:48.873: INFO: (3) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 95.671229ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.64112ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.917697ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.809598ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.748915ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.702239ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 91.727445ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.807394ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.787747ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.819945ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.750667ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 91.90685ms)
Aug 13 00:53:48.965: INFO: (4) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.749815ms)
Aug 13 00:53:48.966: INFO: (4) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 92.739679ms)
Aug 13 00:53:48.967: INFO: (4) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.673391ms)
Aug 13 00:53:48.967: INFO: (4) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 93.753332ms)
Aug 13 00:53:48.967: INFO: (4) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.887517ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.519178ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.518979ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.587887ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.469042ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.412823ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.569889ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.845527ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.777009ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 91.846251ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.75021ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.939198ms)
Aug 13 00:53:49.059: INFO: (5) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 91.6077ms)
Aug 13 00:53:49.060: INFO: (5) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.640232ms)
Aug 13 00:53:49.061: INFO: (5) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.566973ms)
Aug 13 00:53:49.061: INFO: (5) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.602305ms)
Aug 13 00:53:49.061: INFO: (5) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.530249ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.368293ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.45515ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.445723ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.49473ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.465066ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.523372ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.023315ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.221218ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.147015ms)
Aug 13 00:53:49.153: INFO: (6) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.23263ms)
Aug 13 00:53:49.154: INFO: (6) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 92.715313ms)
Aug 13 00:53:49.154: INFO: (6) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.98226ms)
Aug 13 00:53:49.154: INFO: (6) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.902697ms)
Aug 13 00:53:49.155: INFO: (6) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 93.510117ms)
Aug 13 00:53:49.155: INFO: (6) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.542819ms)
Aug 13 00:53:49.161: INFO: (6) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 99.934414ms)
Aug 13 00:53:49.252: INFO: (7) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 90.956329ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 90.980091ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 90.952541ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.241213ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.0528ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.165286ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.180834ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.381038ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 91.848625ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.211004ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.082562ms)
Aug 13 00:53:49.253: INFO: (7) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.116657ms)
Aug 13 00:53:49.254: INFO: (7) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 92.590078ms)
Aug 13 00:53:49.254: INFO: (7) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 92.536539ms)
Aug 13 00:53:49.255: INFO: (7) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.415273ms)
Aug 13 00:53:49.256: INFO: (7) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 94.274493ms)
Aug 13 00:53:49.349: INFO: (8) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.759567ms)
Aug 13 00:53:49.349: INFO: (8) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.905164ms)
Aug 13 00:53:49.349: INFO: (8) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.917003ms)
Aug 13 00:53:49.350: INFO: (8) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 93.708545ms)
Aug 13 00:53:49.350: INFO: (8) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 93.659801ms)
Aug 13 00:53:49.350: INFO: (8) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 93.849916ms)
Aug 13 00:53:49.350: INFO: (8) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 93.768431ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 95.056315ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 95.455295ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 95.61582ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 95.497324ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 95.648036ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 95.594119ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 95.765773ms)
Aug 13 00:53:49.351: INFO: (8) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 95.837983ms)
Aug 13 00:53:49.440: INFO: (8) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 184.102414ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.132071ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.269972ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.321844ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.248694ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 92.241539ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.463203ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.388648ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.451496ms)
Aug 13 00:53:49.533: INFO: (9) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.31906ms)
Aug 13 00:53:49.535: INFO: (9) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 94.628983ms)
Aug 13 00:53:49.535: INFO: (9) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 95.009686ms)
Aug 13 00:53:49.535: INFO: (9) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 94.970878ms)
Aug 13 00:53:49.535: INFO: (9) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 95.122936ms)
Aug 13 00:53:49.535: INFO: (9) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 95.015251ms)
Aug 13 00:53:49.536: INFO: (9) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 95.546878ms)
Aug 13 00:53:49.536: INFO: (9) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 95.702081ms)
Aug 13 00:53:49.627: INFO: (10) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.089015ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.283433ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.356202ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.297454ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.424973ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.513125ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.809911ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.731643ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 91.74934ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.730922ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.835911ms)
Aug 13 00:53:49.628: INFO: (10) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 91.857071ms)
Aug 13 00:53:49.629: INFO: (10) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 92.92588ms)
Aug 13 00:53:49.630: INFO: (10) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.607894ms)
Aug 13 00:53:49.630: INFO: (10) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 93.648882ms)
Aug 13 00:53:49.630: INFO: (10) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.788991ms)
Aug 13 00:53:49.722: INFO: (11) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.220101ms)
Aug 13 00:53:49.722: INFO: (11) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.206403ms)
Aug 13 00:53:49.722: INFO: (11) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.203691ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.721755ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.781266ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.510077ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.57255ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.581642ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.574352ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.734819ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.918701ms)
Aug 13 00:53:49.723: INFO: (11) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.792646ms)
Aug 13 00:53:49.724: INFO: (11) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.878469ms)
Aug 13 00:53:49.725: INFO: (11) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 94.716893ms)
Aug 13 00:53:49.725: INFO: (11) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 94.795159ms)
Aug 13 00:53:49.725: INFO: (11) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 94.722662ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.700744ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.789455ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.939519ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.886284ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.820686ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.120038ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.157881ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.305386ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.245735ms)
Aug 13 00:53:49.817: INFO: (12) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.255016ms)
Aug 13 00:53:49.818: INFO: (12) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 93.204724ms)
Aug 13 00:53:49.818: INFO: (12) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 93.230748ms)
Aug 13 00:53:49.818: INFO: (12) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.175881ms)
Aug 13 00:53:49.818: INFO: (12) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 93.168105ms)
Aug 13 00:53:49.819: INFO: (12) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.776273ms)
Aug 13 00:53:49.820: INFO: (12) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 94.737981ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.335687ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.183901ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.222134ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.260646ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.343336ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.427198ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.566409ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.348631ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.538723ms)
Aug 13 00:53:49.911: INFO: (13) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 91.621308ms)
Aug 13 00:53:49.912: INFO: (13) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 91.84683ms)
Aug 13 00:53:49.912: INFO: (13) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 91.861091ms)
Aug 13 00:53:49.912: INFO: (13) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.603685ms)
Aug 13 00:53:49.913: INFO: (13) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.504975ms)
Aug 13 00:53:49.913: INFO: (13) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 93.551684ms)
Aug 13 00:53:49.913: INFO: (13) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.536749ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.496654ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.410878ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.405845ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.42021ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.501788ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.529065ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.460887ms)
Aug 13 00:53:50.006: INFO: (14) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.696643ms)
Aug 13 00:53:50.007: INFO: (14) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.976484ms)
Aug 13 00:53:50.007: INFO: (14) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 93.090058ms)
Aug 13 00:53:50.007: INFO: (14) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 93.25224ms)
Aug 13 00:53:50.007: INFO: (14) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 93.058381ms)
Aug 13 00:53:50.007: INFO: (14) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 93.036717ms)
Aug 13 00:53:50.008: INFO: (14) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.916302ms)
Aug 13 00:53:50.008: INFO: (14) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.963715ms)
Aug 13 00:53:50.008: INFO: (14) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 94.598842ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.311581ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.146612ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.198684ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.22195ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.26366ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.367383ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.241526ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.368701ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.327308ms)
Aug 13 00:53:50.101: INFO: (15) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.482573ms)
Aug 13 00:53:50.105: INFO: (15) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 96.320298ms)
Aug 13 00:53:50.105: INFO: (15) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 96.427609ms)
Aug 13 00:53:50.105: INFO: (15) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 96.46091ms)
Aug 13 00:53:50.105: INFO: (15) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 96.591735ms)
Aug 13 00:53:50.105: INFO: (15) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 96.527298ms)
Aug 13 00:53:50.105: INFO: (15) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 96.600919ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.34979ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.290856ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.490184ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.651277ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.800761ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.920467ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.822129ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.990654ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.811686ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 93.15029ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 93.131566ms)
Aug 13 00:53:50.198: INFO: (16) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.884884ms)
Aug 13 00:53:50.199: INFO: (16) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.97433ms)
Aug 13 00:53:50.200: INFO: (16) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 94.507967ms)
Aug 13 00:53:50.200: INFO: (16) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 94.565161ms)
Aug 13 00:53:50.200: INFO: (16) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 94.627522ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.220548ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 92.422456ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 92.201641ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 92.266949ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.230005ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 92.395596ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 92.376905ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 92.361888ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 92.355165ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 92.571128ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.523749ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.675256ms)
Aug 13 00:53:50.293: INFO: (17) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.513989ms)
Aug 13 00:53:50.294: INFO: (17) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.581359ms)
Aug 13 00:53:50.294: INFO: (17) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.344433ms)
Aug 13 00:53:50.294: INFO: (17) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 93.387799ms)
Aug 13 00:53:50.385: INFO: (18) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 90.829958ms)
Aug 13 00:53:50.385: INFO: (18) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 90.831119ms)
Aug 13 00:53:50.385: INFO: (18) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 90.783264ms)
Aug 13 00:53:50.385: INFO: (18) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 90.94372ms)
Aug 13 00:53:50.385: INFO: (18) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 90.97578ms)
Aug 13 00:53:50.385: INFO: (18) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 90.947296ms)
Aug 13 00:53:50.386: INFO: (18) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.645741ms)
Aug 13 00:53:50.386: INFO: (18) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.741952ms)
Aug 13 00:53:50.386: INFO: (18) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.639039ms)
Aug 13 00:53:50.386: INFO: (18) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.736075ms)
Aug 13 00:53:50.387: INFO: (18) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.648822ms)
Aug 13 00:53:50.387: INFO: (18) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 92.746328ms)
Aug 13 00:53:50.387: INFO: (18) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.636696ms)
Aug 13 00:53:50.387: INFO: (18) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 92.690833ms)
Aug 13 00:53:50.388: INFO: (18) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 93.285281ms)
Aug 13 00:53:50.388: INFO: (18) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 93.90489ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:443/proxy/tlsrewritem... (200; 91.42268ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.558169ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:460/proxy/: tls baz (200; 91.626623ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f/proxy/rewriteme">test</a> (200; 91.615834ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.704043ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/https:proxy-service-xff2k-2td2f:462/proxy/: tls qux (200; 91.706042ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:162/proxy/: bar (200; 91.673752ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/http:proxy-service-xff2k-2td2f:1080/proxy/rewriteme">... (200; 91.699737ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/: <a href="/api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:1080/proxy/rewriteme">test<... (200; 91.688251ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/pods/proxy-service-xff2k-2td2f:160/proxy/: foo (200; 91.891642ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname1/proxy/: tls baz (200; 92.041237ms)
Aug 13 00:53:50.480: INFO: (19) /api/v1/namespaces/proxy-3063/services/https:proxy-service-xff2k:tlsportname2/proxy/: tls qux (200; 92.01696ms)
Aug 13 00:53:50.481: INFO: (19) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname1/proxy/: foo (200; 92.978911ms)
Aug 13 00:53:50.483: INFO: (19) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname1/proxy/: foo (200; 94.186704ms)
Aug 13 00:53:50.483: INFO: (19) /api/v1/namespaces/proxy-3063/services/proxy-service-xff2k:portname2/proxy/: bar (200; 94.324027ms)
Aug 13 00:53:50.483: INFO: (19) /api/v1/namespaces/proxy-3063/services/http:proxy-service-xff2k:portname2/proxy/: bar (200; 94.487564ms)
STEP: deleting ReplicationController proxy-service-xff2k in namespace proxy-3063, will wait for the garbage collector to delete the pods
Aug 13 00:53:50.764: INFO: Deleting ReplicationController proxy-service-xff2k took: 90.933764ms
Aug 13 00:53:51.164: INFO: Terminating ReplicationController proxy-service-xff2k pods took: 400.482767ms
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:54:01.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3063" for this suite.
Aug 13 00:54:07.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:54:11.123: INFO: namespace proxy-3063 deletion completed in 9.668507053s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:54:11.124: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:54:12.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3061" for this suite.
Aug 13 00:54:34.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:54:37.953: INFO: namespace pods-3061 deletion completed in 25.674772339s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:54:37.953: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-6e3b6280-0178-49db-b7db-5d640f7390c7
STEP: Creating a pod to test consume secrets
Aug 13 00:54:38.772: INFO: Waiting up to 5m0s for pod "pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5" in namespace "secrets-4808" to be "success or failure"
Aug 13 00:54:38.862: INFO: Pod "pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.285088ms
Aug 13 00:54:40.952: INFO: Pod "pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17922869s
STEP: Saw pod success
Aug 13 00:54:40.952: INFO: Pod "pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5" satisfied condition "success or failure"
Aug 13 00:54:41.041: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 00:54:41.228: INFO: Waiting for pod pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5 to disappear
Aug 13 00:54:41.317: INFO: Pod pod-secrets-99d9b04f-4216-4d36-8711-093aa3c0b1b5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:54:41.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4808" for this suite.
Aug 13 00:54:47.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:54:51.079: INFO: namespace secrets-4808 deletion completed in 9.67208618s
•SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:54:51.080: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 13 00:54:56.623: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 00:54:56.713: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 00:54:58.713: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 00:54:58.803: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 00:55:00.713: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 00:55:00.803: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:55:00.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3821" for this suite.
Aug 13 00:55:23.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:55:26.576: INFO: namespace container-lifecycle-hook-3821 deletion completed in 25.682296292s
•SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:55:26.576: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-9ddd18e4-5391-4a8b-b639-7c6e16066a1d
STEP: Creating a pod to test consume configMaps
Aug 13 00:55:27.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5" in namespace "projected-7198" to be "success or failure"
Aug 13 00:55:27.485: INFO: Pod "pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.706662ms
Aug 13 00:55:29.575: INFO: Pod "pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179771502s
STEP: Saw pod success
Aug 13 00:55:29.575: INFO: Pod "pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5" satisfied condition "success or failure"
Aug 13 00:55:29.664: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 00:55:29.852: INFO: Waiting for pod pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5 to disappear
Aug 13 00:55:29.941: INFO: Pod pod-projected-configmaps-4a70fbb4-6613-4923-aac0-d58966f645a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:55:29.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7198" for this suite.
Aug 13 00:55:36.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:55:39.710: INFO: namespace projected-7198 deletion completed in 9.678609715s
•SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:55:39.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3168.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3168.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3168.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3168.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3168.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3168.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 13 00:55:43.533: INFO: DNS probes using dns-3168/dns-test-96a631bb-4482-46d1-961e-ab9cc409e904 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:55:43.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3168" for this suite.
Aug 13 00:55:49.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:55:53.385: INFO: namespace dns-3168 deletion completed in 9.667844296s
•
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:55:53.385: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-24c24fbb-fedf-4c68-8247-db9c6f8e1c86
STEP: Creating a pod to test consume configMaps
Aug 13 00:55:54.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5" in namespace "configmap-3019" to be "success or failure"
Aug 13 00:55:54.294: INFO: Pod "pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.804102ms
Aug 13 00:55:56.384: INFO: Pod "pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179850216s
STEP: Saw pod success
Aug 13 00:55:56.385: INFO: Pod "pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5" satisfied condition "success or failure"
Aug 13 00:55:56.475: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 00:55:56.663: INFO: Waiting for pod pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5 to disappear
Aug 13 00:55:56.753: INFO: Pod pod-configmaps-6592855e-0cb1-45bf-9c9f-ef93683f83c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:55:56.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3019" for this suite.
Aug 13 00:56:03.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:56:06.514: INFO: namespace configmap-3019 deletion completed in 9.670853603s
•SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:56:06.514: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5571
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-35ae39b2-0a83-4594-9306-9758de1592eb
STEP: Creating a pod to test consume configMaps
Aug 13 00:56:07.333: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed" in namespace "projected-5571" to be "success or failure"
Aug 13 00:56:07.423: INFO: Pod "pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed": Phase="Pending", Reason="", readiness=false. Elapsed: 89.58309ms
Aug 13 00:56:09.513: INFO: Pod "pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179944013s
STEP: Saw pod success
Aug 13 00:56:09.513: INFO: Pod "pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed" satisfied condition "success or failure"
Aug 13 00:56:09.603: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 00:56:09.792: INFO: Waiting for pod pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed to disappear
Aug 13 00:56:09.882: INFO: Pod pod-projected-configmaps-333b73c0-cc88-428f-9574-8dce6d67e2ed no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:56:09.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5571" for this suite.
Aug 13 00:56:16.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:56:19.646: INFO: namespace projected-5571 deletion completed in 9.672551582s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:56:19.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug 13 00:56:20.376: INFO: Waiting up to 5m0s for pod "client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670" in namespace "containers-679" to be "success or failure"
Aug 13 00:56:20.470: INFO: Pod "client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670": Phase="Pending", Reason="", readiness=false. Elapsed: 93.835903ms
Aug 13 00:56:22.560: INFO: Pod "client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.183954609s
STEP: Saw pod success
Aug 13 00:56:22.560: INFO: Pod "client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670" satisfied condition "success or failure"
Aug 13 00:56:22.650: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670 container test-container: <nil>
STEP: delete the pod
Aug 13 00:56:22.836: INFO: Waiting for pod client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670 to disappear
Aug 13 00:56:22.925: INFO: Pod client-containers-eadce962-f7a2-4061-a8eb-9a8b40a9b670 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:56:22.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-679" for this suite.
Aug 13 00:56:29.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:56:32.693: INFO: namespace containers-679 deletion completed in 9.677498537s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:56:32.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 13 00:56:39.964: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:56:39.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0813 00:56:39.964057    4207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3511" for this suite.
Aug 13 00:56:46.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:56:49.735: INFO: namespace gc-3511 deletion completed in 9.68155352s
•SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:56:49.735: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 13 00:56:53.533: INFO: Successfully updated pod "labelsupdate373eb2cd-9183-4a5d-b466-9da9dcbf6025"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:56:55.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-252" for this suite.
Aug 13 00:57:18.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:57:21.486: INFO: namespace downward-api-252 deletion completed in 25.672749604s
•SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:57:21.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 00:57:24.669: INFO: Waiting up to 5m0s for pod "client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c" in namespace "pods-3982" to be "success or failure"
Aug 13 00:57:24.758: INFO: Pod "client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.219994ms
Aug 13 00:57:26.848: INFO: Pod "client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179310012s
STEP: Saw pod success
Aug 13 00:57:26.848: INFO: Pod "client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c" satisfied condition "success or failure"
Aug 13 00:57:26.938: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c container env3cont: <nil>
STEP: delete the pod
Aug 13 00:57:27.127: INFO: Waiting for pod client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c to disappear
Aug 13 00:57:27.216: INFO: Pod client-envvars-12fa45a6-e06f-4b0b-95f5-e23ac76e171c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:57:27.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3982" for this suite.
Aug 13 00:58:11.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:58:15.028: INFO: namespace pods-3982 deletion completed in 47.721769831s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:58:15.028: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-e059bcc8-dda7-440c-bf3b-32edc7bb2c3e
STEP: Creating secret with name secret-projected-all-test-volume-d70778ce-7ff2-474e-a047-397f8b121035
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 13 00:58:15.953: INFO: Waiting up to 5m0s for pod "projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374" in namespace "projected-5960" to be "success or failure"
Aug 13 00:58:16.042: INFO: Pod "projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374": Phase="Pending", Reason="", readiness=false. Elapsed: 89.205743ms
Aug 13 00:58:18.132: INFO: Pod "projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179539864s
STEP: Saw pod success
Aug 13 00:58:18.132: INFO: Pod "projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374" satisfied condition "success or failure"
Aug 13 00:58:18.222: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 13 00:58:18.410: INFO: Waiting for pod projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374 to disappear
Aug 13 00:58:18.499: INFO: Pod projected-volume-850b8bf3-fdcb-49c3-8aef-e4a97e27c374 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:58:18.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5960" for this suite.
Aug 13 00:58:24.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:58:28.268: INFO: namespace projected-5960 deletion completed in 9.678017778s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:58:28.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-f67w
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 00:58:29.177: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f67w" in namespace "subpath-4927" to be "success or failure"
Aug 13 00:58:29.267: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Pending", Reason="", readiness=false. Elapsed: 89.585594ms
Aug 13 00:58:31.357: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 2.179363325s
Aug 13 00:58:33.446: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 4.269259989s
Aug 13 00:58:35.536: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 6.359098747s
Aug 13 00:58:37.627: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 8.449930873s
Aug 13 00:58:39.717: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 10.539806152s
Aug 13 00:58:41.807: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 12.629870879s
Aug 13 00:58:43.897: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 14.719997254s
Aug 13 00:58:45.987: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 16.810029294s
Aug 13 00:58:48.077: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 18.900082353s
Aug 13 00:58:50.167: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Running", Reason="", readiness=true. Elapsed: 20.990081148s
Aug 13 00:58:52.257: INFO: Pod "pod-subpath-test-configmap-f67w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.080012947s
STEP: Saw pod success
Aug 13 00:58:52.257: INFO: Pod "pod-subpath-test-configmap-f67w" satisfied condition "success or failure"
Aug 13 00:58:52.347: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-subpath-test-configmap-f67w container test-container-subpath-configmap-f67w: <nil>
STEP: delete the pod
Aug 13 00:58:52.533: INFO: Waiting for pod pod-subpath-test-configmap-f67w to disappear
Aug 13 00:58:52.623: INFO: Pod pod-subpath-test-configmap-f67w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f67w
Aug 13 00:58:52.623: INFO: Deleting pod "pod-subpath-test-configmap-f67w" in namespace "subpath-4927"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:58:52.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4927" for this suite.
Aug 13 00:58:59.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:59:02.472: INFO: namespace subpath-4927 deletion completed in 9.66975409s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:59:02.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 13 00:59:07.925: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:08.015: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:10.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:10.106: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:12.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:12.106: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:14.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:14.105: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:16.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:16.105: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:18.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:18.106: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:20.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:20.106: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 00:59:22.016: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 00:59:22.106: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:59:22.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6312" for this suite.
Aug 13 00:59:44.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 00:59:47.960: INFO: namespace container-lifecycle-hook-6312 deletion completed in 25.668478417s
•SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 00:59:47.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 13 00:59:48.689: INFO: Waiting up to 5m0s for pod "downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6" in namespace "downward-api-6778" to be "success or failure"
Aug 13 00:59:48.779: INFO: Pod "downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.633257ms
Aug 13 00:59:50.869: INFO: Pod "downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179819264s
STEP: Saw pod success
Aug 13 00:59:50.869: INFO: Pod "downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6" satisfied condition "success or failure"
Aug 13 00:59:50.959: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6 container dapi-container: <nil>
STEP: delete the pod
Aug 13 00:59:51.148: INFO: Waiting for pod downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6 to disappear
Aug 13 00:59:51.237: INFO: Pod downward-api-846bdf7a-026d-4634-8e2b-03795d4e4bf6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 00:59:51.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6778" for this suite.
Aug 13 00:59:57.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:00:00.995: INFO: namespace downward-api-6778 deletion completed in 9.668527792s
•SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:00:00.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-547
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:00:01.647: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:00:02.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-547" for this suite.
Aug 13 01:00:08.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:00:12.076: INFO: namespace custom-resource-definition-547 deletion completed in 9.696743887s
•SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:00:12.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 13 01:00:12.715: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:00:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7087" for this suite.
Aug 13 01:00:21.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:00:25.280: INFO: namespace init-container-7087 deletion completed in 9.670481288s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:00:25.280: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 13 01:00:26.010: INFO: Waiting up to 5m0s for pod "downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c" in namespace "downward-api-1999" to be "success or failure"
Aug 13 01:00:26.099: INFO: Pod "downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.581117ms
Aug 13 01:00:28.190: INFO: Pod "downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180127234s
STEP: Saw pod success
Aug 13 01:00:28.190: INFO: Pod "downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c" satisfied condition "success or failure"
Aug 13 01:00:28.279: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c container dapi-container: <nil>
STEP: delete the pod
Aug 13 01:00:28.467: INFO: Waiting for pod downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c to disappear
Aug 13 01:00:28.556: INFO: Pod downward-api-1835a972-de82-46fb-baf0-67dd50aeae1c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:00:28.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1999" for this suite.
Aug 13 01:00:34.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:00:38.325: INFO: namespace downward-api-1999 deletion completed in 9.678332252s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:00:38.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8362
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 13 01:00:39.064: INFO: Waiting up to 5m0s for pod "pod-32d61d66-e6d3-46d1-badd-6f8260b702ba" in namespace "emptydir-8362" to be "success or failure"
Aug 13 01:00:39.154: INFO: Pod "pod-32d61d66-e6d3-46d1-badd-6f8260b702ba": Phase="Pending", Reason="", readiness=false. Elapsed: 89.437953ms
Aug 13 01:00:41.244: INFO: Pod "pod-32d61d66-e6d3-46d1-badd-6f8260b702ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179613955s
STEP: Saw pod success
Aug 13 01:00:41.244: INFO: Pod "pod-32d61d66-e6d3-46d1-badd-6f8260b702ba" satisfied condition "success or failure"
Aug 13 01:00:41.333: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-32d61d66-e6d3-46d1-badd-6f8260b702ba container test-container: <nil>
STEP: delete the pod
Aug 13 01:00:41.527: INFO: Waiting for pod pod-32d61d66-e6d3-46d1-badd-6f8260b702ba to disappear
Aug 13 01:00:41.617: INFO: Pod pod-32d61d66-e6d3-46d1-badd-6f8260b702ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:00:41.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8362" for this suite.
Aug 13 01:00:47.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:00:51.379: INFO: namespace emptydir-8362 deletion completed in 9.672322017s
•SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:00:51.379: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-287e227f-75ee-4c86-8225-8bf3ef55e04b
STEP: Creating a pod to test consume secrets
Aug 13 01:00:52.199: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812" in namespace "projected-9155" to be "success or failure"
Aug 13 01:00:52.288: INFO: Pod "pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812": Phase="Pending", Reason="", readiness=false. Elapsed: 89.650545ms
Aug 13 01:00:54.378: INFO: Pod "pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179542926s
STEP: Saw pod success
Aug 13 01:00:54.378: INFO: Pod "pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812" satisfied condition "success or failure"
Aug 13 01:00:54.468: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 01:00:54.655: INFO: Waiting for pod pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812 to disappear
Aug 13 01:00:54.744: INFO: Pod pod-projected-secrets-5c43d5bc-40cb-4f67-a1bb-2d5622ad9812 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:00:54.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9155" for this suite.
Aug 13 01:01:01.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:01:04.514: INFO: namespace projected-9155 deletion completed in 9.680607262s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:01:04.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3588
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:01:05.691: INFO: Create a RollingUpdate DaemonSet
Aug 13 01:01:05.781: INFO: Check that daemon pods launch on every node of the cluster
Aug 13 01:01:05.961: INFO: Number of nodes with available pods: 0
Aug 13 01:01:05.961: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:01:07.140: INFO: Number of nodes with available pods: 2
Aug 13 01:01:07.140: INFO: Number of running nodes: 2, number of available pods: 2
Aug 13 01:01:07.141: INFO: Update the DaemonSet to trigger a rollout
Aug 13 01:01:07.320: INFO: Updating DaemonSet daemon-set
Aug 13 01:01:21.680: INFO: Roll back the DaemonSet before rollout is complete
Aug 13 01:01:21.860: INFO: Updating DaemonSet daemon-set
Aug 13 01:01:21.860: INFO: Make sure DaemonSet rollback is complete
Aug 13 01:01:21.950: INFO: Wrong image for pod: daemon-set-42mgm. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 13 01:01:21.950: INFO: Pod daemon-set-42mgm is not available
Aug 13 01:01:23.130: INFO: Wrong image for pod: daemon-set-42mgm. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 13 01:01:23.130: INFO: Pod daemon-set-42mgm is not available
Aug 13 01:01:24.130: INFO: Pod daemon-set-v8lh9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3588, will wait for the garbage collector to delete the pods
Aug 13 01:01:24.679: INFO: Deleting DaemonSet.extensions daemon-set took: 90.564764ms
Aug 13 01:01:25.079: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.360193ms
Aug 13 01:01:31.869: INFO: Number of nodes with available pods: 0
Aug 13 01:01:31.869: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 01:01:31.960: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3588/daemonsets","resourceVersion":"8738"},"items":null}

Aug 13 01:01:32.050: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3588/pods","resourceVersion":"8738"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:01:32.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3588" for this suite.
Aug 13 01:01:38.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:01:42.096: INFO: namespace daemonsets-3588 deletion completed in 9.687113416s
•SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:01:42.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:01:45.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3356" for this suite.
Aug 13 01:02:23.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:02:27.043: INFO: namespace kubelet-test-3356 deletion completed in 41.749510667s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:02:27.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a51b7936-a72d-46ce-beee-fdcf89e148e0
STEP: Creating a pod to test consume configMaps
Aug 13 01:02:27.863: INFO: Waiting up to 5m0s for pod "pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750" in namespace "configmap-5113" to be "success or failure"
Aug 13 01:02:27.953: INFO: Pod "pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750": Phase="Pending", Reason="", readiness=false. Elapsed: 89.708526ms
Aug 13 01:02:30.043: INFO: Pod "pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179798199s
STEP: Saw pod success
Aug 13 01:02:30.043: INFO: Pod "pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750" satisfied condition "success or failure"
Aug 13 01:02:30.133: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:02:30.327: INFO: Waiting for pod pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750 to disappear
Aug 13 01:02:30.417: INFO: Pod pod-configmaps-4bb74484-7326-4909-a6cd-f547279a4750 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:02:30.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5113" for this suite.
Aug 13 01:02:36.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:02:40.180: INFO: namespace configmap-5113 deletion completed in 9.673288563s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:02:40.181: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug 13 01:02:43.269: INFO: Pod pod-hostip-7a7090fa-824f-486f-8a5d-db0c5fa86b0d has hostIP: 10.250.2.100
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:02:43.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5549" for this suite.
Aug 13 01:03:05.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:03:09.027: INFO: namespace pods-5549 deletion completed in 25.668412283s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:03:09.028: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug 13 01:03:09.666: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Aug 13 01:03:10.611: INFO: stderr: ""
Aug 13 01:03:10.611: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm-h0con.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm-h0con.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm-h0con.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:03:10.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2271" for this suite.
Aug 13 01:03:16.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:03:20.375: INFO: namespace kubectl-2271 deletion completed in 9.674089008s
•SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:03:20.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 13 01:03:21.016: INFO: PodSpec: initContainers in spec.initContainers
Aug 13 01:04:07.938: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c7008bd2-aa56-4316-904c-f9f6644dd1bb", GenerateName:"", Namespace:"init-container-439", SelfLink:"/api/v1/namespaces/init-container-439/pods/pod-init-c7008bd2-aa56-4316-904c-f9f6644dd1bb", UID:"5e0f346b-25f1-4747-bc84-d7c5be132264", ResourceVersion:"9217", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63701255001, loc:(*time.Location)(0x80bfa40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"16086948"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.104/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-5snq7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0023c9d80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5snq7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5snq7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-5snq7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a43798), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-2-100.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002b4b740), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a43830)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a43850)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a43858), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001a4385c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255001, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255001, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255001, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255001, loc:(*time.Location)(0x80bfa40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.2.100", PodIP:"100.96.1.104", StartTime:(*v1.Time)(0xc0022eeca0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002307180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002307260)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://e90249ecc5187d2d8e1d27c4cee44b99740e488f7079a31af41f5a9744491508"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0022eed20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0022eed00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:04:07.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-439" for this suite.
Aug 13 01:04:30.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:04:33.697: INFO: namespace init-container-439 deletion completed in 25.668469392s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:04:33.698: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1809
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 13 01:04:34.428: INFO: Waiting up to 5m0s for pod "pod-569ff0db-9cbe-44d6-b696-10532f3ed825" in namespace "emptydir-1809" to be "success or failure"
Aug 13 01:04:34.518: INFO: Pod "pod-569ff0db-9cbe-44d6-b696-10532f3ed825": Phase="Pending", Reason="", readiness=false. Elapsed: 89.610746ms
Aug 13 01:04:36.608: INFO: Pod "pod-569ff0db-9cbe-44d6-b696-10532f3ed825": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179853105s
STEP: Saw pod success
Aug 13 01:04:36.608: INFO: Pod "pod-569ff0db-9cbe-44d6-b696-10532f3ed825" satisfied condition "success or failure"
Aug 13 01:04:36.698: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-569ff0db-9cbe-44d6-b696-10532f3ed825 container test-container: <nil>
STEP: delete the pod
Aug 13 01:04:36.885: INFO: Waiting for pod pod-569ff0db-9cbe-44d6-b696-10532f3ed825 to disappear
Aug 13 01:04:36.974: INFO: Pod pod-569ff0db-9cbe-44d6-b696-10532f3ed825 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:04:36.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1809" for this suite.
Aug 13 01:04:43.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:04:46.738: INFO: namespace emptydir-1809 deletion completed in 9.668771206s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:04:46.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:04:47.741: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fc9cc596-26e9-4d98-909f-2dd60a20f465", Controller:(*bool)(0xc0031386ca), BlockOwnerDeletion:(*bool)(0xc0031386cb)}}
Aug 13 01:04:47.832: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"39dc8f21-bfd4-45d9-9e5b-54c6845c249b", Controller:(*bool)(0xc002b40076), BlockOwnerDeletion:(*bool)(0xc002b40077)}}
Aug 13 01:04:47.923: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"00e86009-58b9-4d45-9892-f5509c3af7ad", Controller:(*bool)(0xc00232a916), BlockOwnerDeletion:(*bool)(0xc00232a917)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:04:53.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-836" for this suite.
Aug 13 01:04:59.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:05:02.868: INFO: namespace gc-836 deletion completed in 9.675774786s
•
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:05:02.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:05:27.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6913" for this suite.
Aug 13 01:05:33.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:05:37.235: INFO: namespace container-runtime-6913 deletion completed in 9.668132816s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:05:37.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 13 01:05:37.874: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 13 01:05:38.053: INFO: Waiting for terminating namespaces to be deleted...
Aug 13 01:05:38.142: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-2-100.ec2.internal before test
Aug 13 01:05:38.236: INFO: kube-proxy-pncr5 from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.236: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 01:05:38.236: INFO: node-exporter-md2rz from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.236: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 01:05:38.236: INFO: addons-kube2iam-rtfmd from kube-system started at 2019-08-13 00:23:54 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.236: INFO: 	Container kube2iam ready: true, restart count 0
Aug 13 01:05:38.236: INFO: calico-node-8pgth from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.236: INFO: 	Container calico-node ready: true, restart count 0
Aug 13 01:05:38.236: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-2-233.ec2.internal before test
Aug 13 01:05:38.334: INFO: addons-nginx-ingress-controller-66c9ddddb9-5cjjg from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 13 01:05:38.334: INFO: calico-kube-controllers-5f4b46ffb5-wxxxf from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 13 01:05:38.334: INFO: addons-kube2iam-zm6dv from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container kube2iam ready: true, restart count 0
Aug 13 01:05:38.334: INFO: calico-node-qjrkf from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container calico-node ready: true, restart count 0
Aug 13 01:05:38.334: INFO: kube-proxy-wgll2 from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 01:05:38.334: INFO: coredns-85cc454dd8-nrq5t from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container coredns ready: true, restart count 0
Aug 13 01:05:38.334: INFO: vpn-shoot-7f79d69868-7vfs5 from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug 13 01:05:38.334: INFO: addons-kubernetes-dashboard-5c8d9945bc-qr4hm from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 13 01:05:38.334: INFO: metrics-server-7dd7d74c9b-6dvkl from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container metrics-server ready: true, restart count 0
Aug 13 01:05:38.334: INFO: node-exporter-bb6ms from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 01:05:38.334: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-9jm4k from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug 13 01:05:38.334: INFO: coredns-85cc454dd8-mxfbk from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container coredns ready: true, restart count 0
Aug 13 01:05:38.334: INFO: blackbox-exporter-954dd954b-b4c5l from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:05:38.334: INFO: 	Container blackbox-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-10-250-2-100.ec2.internal
STEP: verifying the node has the label node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod addons-kube2iam-rtfmd requesting resource cpu=10m on Node ip-10-250-2-100.ec2.internal
Aug 13 01:05:38.877: INFO: Pod addons-kube2iam-zm6dv requesting resource cpu=10m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod addons-kubernetes-dashboard-5c8d9945bc-qr4hm requesting resource cpu=50m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod addons-nginx-ingress-controller-66c9ddddb9-5cjjg requesting resource cpu=100m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-9jm4k requesting resource cpu=0m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod blackbox-exporter-954dd954b-b4c5l requesting resource cpu=5m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod calico-kube-controllers-5f4b46ffb5-wxxxf requesting resource cpu=0m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod calico-node-8pgth requesting resource cpu=100m on Node ip-10-250-2-100.ec2.internal
Aug 13 01:05:38.877: INFO: Pod calico-node-qjrkf requesting resource cpu=100m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod coredns-85cc454dd8-mxfbk requesting resource cpu=50m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod coredns-85cc454dd8-nrq5t requesting resource cpu=50m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod kube-proxy-pncr5 requesting resource cpu=20m on Node ip-10-250-2-100.ec2.internal
Aug 13 01:05:38.877: INFO: Pod kube-proxy-wgll2 requesting resource cpu=20m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod metrics-server-7dd7d74c9b-6dvkl requesting resource cpu=20m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod node-exporter-bb6ms requesting resource cpu=5m on Node ip-10-250-2-233.ec2.internal
Aug 13 01:05:38.877: INFO: Pod node-exporter-md2rz requesting resource cpu=5m on Node ip-10-250-2-100.ec2.internal
Aug 13 01:05:38.877: INFO: Pod vpn-shoot-7f79d69868-7vfs5 requesting resource cpu=100m on Node ip-10-250-2-233.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe.15ba55f6cee8e951], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7914/filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe to ip-10-250-2-100.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe.15ba55f6f5460b67], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe.15ba55f6f82e1383], Reason = [Created], Message = [Created container filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe.15ba55f6fd077c5d], Reason = [Started], Message = [Started container filler-pod-08b9745e-8c0d-4504-ab8f-1bcf86f384fe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc.15ba55f6d4450d06], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7914/filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc to ip-10-250-2-233.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc.15ba55f6f816e3b1], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc.15ba55f70fa643d2], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc.15ba55f712a7af76], Reason = [Created], Message = [Created container filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc.15ba55f71891d6df], Reason = [Started], Message = [Started container filler-pod-35fdb449-f728-4aea-a5d8-310d73d7cfbc]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ba55f76b8070a2], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-2-100.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-2-233.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:05:43.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7914" for this suite.
Aug 13 01:05:49.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:05:52.986: INFO: namespace sched-pred-7914 deletion completed in 9.6685019s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:05:52.986: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:05:56.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8346" for this suite.
Aug 13 01:06:34.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:06:37.849: INFO: namespace kubelet-test-8346 deletion completed in 41.674612155s
•SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:06:37.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 13 01:06:43.299: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:43.389: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:45.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:45.479: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:47.390: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:47.480: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:49.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:49.479: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:51.390: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:51.479: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:53.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:53.479: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:55.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:55.479: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:57.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:57.480: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:06:59.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:06:59.480: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:07:01.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:07:01.480: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 01:07:03.389: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 01:07:03.479: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:07:03.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1260" for this suite.
Aug 13 01:07:25.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:07:29.347: INFO: namespace container-lifecycle-hook-1260 deletion completed in 25.683067033s
•SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:07:29.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 01:07:29.985: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1889'
Aug 13 01:07:30.427: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 01:07:30.428: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Aug 13 01:07:30.609: INFO: scanned /root for discovery docs: <nil>
Aug 13 01:07:30.609: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1889'
Aug 13 01:07:44.085: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 13 01:07:44.085: INFO: stdout: "Created e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb\nScaling up e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 13 01:07:44.085: INFO: stdout: "Created e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb\nScaling up e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 13 01:07:44.085: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1889'
Aug 13 01:07:44.521: INFO: stderr: ""
Aug 13 01:07:44.521: INFO: stdout: "e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb-gb7kh "
Aug 13 01:07:44.521: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb-gb7kh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1889'
Aug 13 01:07:44.958: INFO: stderr: ""
Aug 13 01:07:44.958: INFO: stdout: "true"
Aug 13 01:07:44.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb-gb7kh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1889'
Aug 13 01:07:45.389: INFO: stderr: ""
Aug 13 01:07:45.389: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 13 01:07:45.389: INFO: e2e-test-nginx-rc-2327576886c9679e6bd489929adb86cb-gb7kh is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug 13 01:07:45.389: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-1889'
Aug 13 01:07:45.913: INFO: stderr: ""
Aug 13 01:07:45.913: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:07:45.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1889" for this suite.
Aug 13 01:07:52.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:07:55.682: INFO: namespace kubectl-1889 deletion completed in 9.679618849s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:07:55.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6594
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug 13 01:07:56.320: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Aug 13 01:07:56.847: INFO: stderr: ""
Aug 13 01:07:56.847: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:07:56.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6594" for this suite.
Aug 13 01:08:03.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:08:06.613: INFO: namespace kubectl-6594 deletion completed in 9.676174886s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:08:06.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:08:07.275: INFO: Creating deployment "test-recreate-deployment"
Aug 13 01:08:07.365: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 13 01:08:07.545: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 13 01:08:07.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255287, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255287, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255287, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255287, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:08:09.724: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 13 01:08:09.904: INFO: Updating deployment test-recreate-deployment
Aug 13 01:08:09.904: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 13 01:08:10.083: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-573,SelfLink:/apis/apps/v1/namespaces/deployment-573/deployments/test-recreate-deployment,UID:ce028e0e-0856-4fac-bc98-5f387ff641fa,ResourceVersion:10127,Generation:2,CreationTimestamp:2019-08-13 01:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-13 01:08:09 +0000 UTC 2019-08-13 01:08:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-13 01:08:09 +0000 UTC 2019-08-13 01:08:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 13 01:08:10.174: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-573,SelfLink:/apis/apps/v1/namespaces/deployment-573/replicasets/test-recreate-deployment-5c8c9cc69d,UID:b203c7a3-33c7-4648-a255-1ab9ab7fc790,ResourceVersion:10125,Generation:1,CreationTimestamp:2019-08-13 01:08:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ce028e0e-0856-4fac-bc98-5f387ff641fa 0xc0028e14d7 0xc0028e14d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 01:08:10.174: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 13 01:08:10.174: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-573,SelfLink:/apis/apps/v1/namespaces/deployment-573/replicasets/test-recreate-deployment-6df85df6b9,UID:b505ebbe-d676-4139-b72b-24486de4d561,ResourceVersion:10119,Generation:2,CreationTimestamp:2019-08-13 01:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ce028e0e-0856-4fac-bc98-5f387ff641fa 0xc0028e1637 0xc0028e1638}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 01:08:10.264: INFO: Pod "test-recreate-deployment-5c8c9cc69d-m5zx4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-m5zx4,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-573,SelfLink:/api/v1/namespaces/deployment-573/pods/test-recreate-deployment-5c8c9cc69d-m5zx4,UID:17c6cf28-6775-4340-8dc7-aa9cb3c8f34c,ResourceVersion:10128,Generation:0,CreationTimestamp:2019-08-13 01:08:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d b203c7a3-33c7-4648-a255-1ab9ab7fc790 0xc00232a077 0xc00232a078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-66rbw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-66rbw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-66rbw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00232a0e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00232a100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:08:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:08:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:08:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:08:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:,StartTime:2019-08-13 01:08:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:08:10.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-573" for this suite.
Aug 13 01:08:16.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:08:20.025: INFO: namespace deployment-573 deletion completed in 9.670757537s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:08:20.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:08:21.111: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 13 01:08:21.380: INFO: Number of nodes with available pods: 0
Aug 13 01:08:21.381: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:08:22.561: INFO: Number of nodes with available pods: 1
Aug 13 01:08:22.561: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 01:08:23.560: INFO: Number of nodes with available pods: 2
Aug 13 01:08:23.560: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 13 01:08:24.191: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:24.191: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:25.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:25.373: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:26.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:26.373: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:26.373: INFO: Pod daemon-set-w2tql is not available
Aug 13 01:08:27.374: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:27.374: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:27.374: INFO: Pod daemon-set-w2tql is not available
Aug 13 01:08:28.374: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:28.374: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:28.374: INFO: Pod daemon-set-w2tql is not available
Aug 13 01:08:29.374: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:29.374: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:29.374: INFO: Pod daemon-set-w2tql is not available
Aug 13 01:08:30.374: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:30.374: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:30.374: INFO: Pod daemon-set-w2tql is not available
Aug 13 01:08:31.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:31.373: INFO: Wrong image for pod: daemon-set-w2tql. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:31.373: INFO: Pod daemon-set-w2tql is not available
Aug 13 01:08:32.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:32.373: INFO: Pod daemon-set-8dgsg is not available
Aug 13 01:08:33.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:33.374: INFO: Pod daemon-set-8dgsg is not available
Aug 13 01:08:34.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:35.373: INFO: Wrong image for pod: daemon-set-4tnqn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 01:08:35.373: INFO: Pod daemon-set-4tnqn is not available
Aug 13 01:08:36.374: INFO: Pod daemon-set-q4svt is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 13 01:08:36.646: INFO: Number of nodes with available pods: 2
Aug 13 01:08:36.646: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1640, will wait for the garbage collector to delete the pods
Aug 13 01:08:37.376: INFO: Deleting DaemonSet.extensions daemon-set took: 90.459848ms
Aug 13 01:08:37.476: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.502275ms
Aug 13 01:08:51.466: INFO: Number of nodes with available pods: 0
Aug 13 01:08:51.466: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 01:08:51.556: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1640/daemonsets","resourceVersion":"10314"},"items":null}

Aug 13 01:08:51.645: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1640/pods","resourceVersion":"10314"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:08:51.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1640" for this suite.
Aug 13 01:08:58.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:09:01.682: INFO: namespace daemonsets-1640 deletion completed in 9.675966047s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:09:01.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 13 01:09:02.412: INFO: Waiting up to 5m0s for pod "pod-421a0eff-a622-4bdc-b224-2b2064542f73" in namespace "emptydir-3568" to be "success or failure"
Aug 13 01:09:02.502: INFO: Pod "pod-421a0eff-a622-4bdc-b224-2b2064542f73": Phase="Pending", Reason="", readiness=false. Elapsed: 89.568138ms
Aug 13 01:09:04.592: INFO: Pod "pod-421a0eff-a622-4bdc-b224-2b2064542f73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179718372s
STEP: Saw pod success
Aug 13 01:09:04.592: INFO: Pod "pod-421a0eff-a622-4bdc-b224-2b2064542f73" satisfied condition "success or failure"
Aug 13 01:09:04.681: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-421a0eff-a622-4bdc-b224-2b2064542f73 container test-container: <nil>
STEP: delete the pod
Aug 13 01:09:04.870: INFO: Waiting for pod pod-421a0eff-a622-4bdc-b224-2b2064542f73 to disappear
Aug 13 01:09:04.959: INFO: Pod pod-421a0eff-a622-4bdc-b224-2b2064542f73 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:09:04.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3568" for this suite.
Aug 13 01:09:11.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:09:14.725: INFO: namespace emptydir-3568 deletion completed in 9.675827917s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:09:14.725: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 13 01:09:16.726: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:09:16.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1115" for this suite.
Aug 13 01:09:23.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:09:26.678: INFO: namespace container-runtime-1115 deletion completed in 9.678121584s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:09:26.679: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-28073723-2b70-4454-96c4-dd480a9bc24b
STEP: Creating a pod to test consume secrets
Aug 13 01:09:27.500: INFO: Waiting up to 5m0s for pod "pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410" in namespace "secrets-3107" to be "success or failure"
Aug 13 01:09:27.590: INFO: Pod "pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410": Phase="Pending", Reason="", readiness=false. Elapsed: 89.490975ms
Aug 13 01:09:29.680: INFO: Pod "pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179580564s
STEP: Saw pod success
Aug 13 01:09:29.680: INFO: Pod "pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410" satisfied condition "success or failure"
Aug 13 01:09:29.770: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 01:09:29.958: INFO: Waiting for pod pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410 to disappear
Aug 13 01:09:30.047: INFO: Pod pod-secrets-f9c7e0ad-c0ae-44f3-af54-98cb8a041410 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:09:30.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3107" for this suite.
Aug 13 01:09:36.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:09:39.819: INFO: namespace secrets-3107 deletion completed in 9.6811979s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:09:39.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7430
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 01:09:40.457: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-7430'
Aug 13 01:09:40.906: INFO: stderr: ""
Aug 13 01:09:40.906: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 13 01:09:46.007: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-7430 -o json'
Aug 13 01:09:46.448: INFO: stderr: ""
Aug 13 01:09:46.448: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.124/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-08-13T01:09:40Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-7430\",\n        \"resourceVersion\": \"10512\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7430/pods/e2e-test-nginx-pod\",\n        \"uid\": \"c55c0be5-b597-41e8-ab58-6a7efa6a014c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5vd4z\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-2-100.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5vd4z\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5vd4z\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T01:09:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T01:09:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T01:09:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T01:09:40Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6e19829be7c7fa4a75997bfd365a45372be251739bd53b5121d2713a50ac9221\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-13T01:09:41Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.2.100\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.124\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-13T01:09:40Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 13 01:09:46.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-7430'
Aug 13 01:09:47.403: INFO: stderr: ""
Aug 13 01:09:47.403: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug 13 01:09:47.493: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-7430'
Aug 13 01:10:00.342: INFO: stderr: ""
Aug 13 01:10:00.342: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:10:00.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7430" for this suite.
Aug 13 01:10:06.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:10:10.105: INFO: namespace kubectl-7430 deletion completed in 9.672379591s
•S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:10:10.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 13 01:10:10.744: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:10:13.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6413" for this suite.
Aug 13 01:10:20.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:10:23.733: INFO: namespace init-container-6413 deletion completed in 9.672926988s
•SSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:10:23.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-1052
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 13 01:10:24.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug 13 01:10:26.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:10:28.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:10:30.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:10:32.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:10:34.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255425, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:10:37.716: INFO: Waited 1.285683028s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:10:40.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1052" for this suite.
Aug 13 01:10:47.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:10:50.582: INFO: namespace aggregator-1052 deletion completed in 9.680556119s
•
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:10:50.582: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-f41982fe-28e5-44ba-b3e6-c7cf63cb53d0
STEP: Creating a pod to test consume configMaps
Aug 13 01:10:51.403: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011" in namespace "configmap-2354" to be "success or failure"
Aug 13 01:10:51.493: INFO: Pod "pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011": Phase="Pending", Reason="", readiness=false. Elapsed: 89.827711ms
Aug 13 01:10:53.583: INFO: Pod "pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180039423s
STEP: Saw pod success
Aug 13 01:10:53.583: INFO: Pod "pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011" satisfied condition "success or failure"
Aug 13 01:10:53.673: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:10:53.860: INFO: Waiting for pod pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011 to disappear
Aug 13 01:10:53.949: INFO: Pod pod-configmaps-0e513ebc-dc55-4260-a6ad-59afeaf22011 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:10:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2354" for this suite.
Aug 13 01:11:00.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:11:03.715: INFO: namespace configmap-2354 deletion completed in 9.675099473s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:11:03.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9712
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 01:11:04.354: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 01:11:27.973: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.128:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9712 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:11:27.973: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:11:28.863: INFO: Found all expected endpoints: [netserver-0]
Aug 13 01:11:28.953: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.34:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9712 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:11:28.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:11:29.808: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:11:29.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9712" for this suite.
Aug 13 01:11:52.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:11:55.570: INFO: namespace pod-network-test-9712 deletion completed in 25.672041669s
•S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:11:55.570: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9470/configmap-test-04264597-df87-4f38-be44-952269ff52b5
STEP: Creating a pod to test consume configMaps
Aug 13 01:11:56.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf" in namespace "configmap-9470" to be "success or failure"
Aug 13 01:11:56.575: INFO: Pod "pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf": Phase="Pending", Reason="", readiness=false. Elapsed: 89.299914ms
Aug 13 01:11:58.665: INFO: Pod "pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17937177s
STEP: Saw pod success
Aug 13 01:11:58.665: INFO: Pod "pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf" satisfied condition "success or failure"
Aug 13 01:11:58.756: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf container env-test: <nil>
STEP: delete the pod
Aug 13 01:11:58.944: INFO: Waiting for pod pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf to disappear
Aug 13 01:11:59.033: INFO: Pod pod-configmaps-c8d62fb7-7fe7-45ad-9be2-64bbf27180cf no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:11:59.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9470" for this suite.
Aug 13 01:12:05.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:12:08.795: INFO: namespace configmap-9470 deletion completed in 9.672239186s
•SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:12:08.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:12:09.882: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 13 01:12:10.062: INFO: Number of nodes with available pods: 0
Aug 13 01:12:10.062: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 13 01:12:10.422: INFO: Number of nodes with available pods: 0
Aug 13 01:12:10.422: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:12:11.512: INFO: Number of nodes with available pods: 0
Aug 13 01:12:11.512: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:12:12.512: INFO: Number of nodes with available pods: 1
Aug 13 01:12:12.512: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 13 01:12:12.871: INFO: Number of nodes with available pods: 0
Aug 13 01:12:12.871: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 13 01:12:13.051: INFO: Number of nodes with available pods: 0
Aug 13 01:12:13.051: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:12:14.141: INFO: Number of nodes with available pods: 0
Aug 13 01:12:14.141: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:12:15.141: INFO: Number of nodes with available pods: 0
Aug 13 01:12:15.142: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:12:16.141: INFO: Number of nodes with available pods: 0
Aug 13 01:12:16.141: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 01:12:17.142: INFO: Number of nodes with available pods: 1
Aug 13 01:12:17.142: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3916, will wait for the garbage collector to delete the pods
Aug 13 01:12:17.602: INFO: Deleting DaemonSet.extensions daemon-set took: 91.053579ms
Aug 13 01:12:18.003: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.501775ms
Aug 13 01:12:31.493: INFO: Number of nodes with available pods: 0
Aug 13 01:12:31.493: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 01:12:31.582: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3916/daemonsets","resourceVersion":"11148"},"items":null}

Aug 13 01:12:31.671: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3916/pods","resourceVersion":"11148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:12:32.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3916" for this suite.
Aug 13 01:12:38.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:12:41.820: INFO: namespace daemonsets-3916 deletion completed in 9.699909396s
•SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:12:41.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9831
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9831 to expose endpoints map[]
Aug 13 01:12:42.643: INFO: successfully validated that service multi-endpoint-test in namespace services-9831 exposes endpoints map[] (89.589742ms elapsed)
STEP: Creating pod pod1 in namespace services-9831
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9831 to expose endpoints map[pod1:[100]]
Aug 13 01:12:45.276: INFO: successfully validated that service multi-endpoint-test in namespace services-9831 exposes endpoints map[pod1:[100]] (2.540441274s elapsed)
STEP: Creating pod pod2 in namespace services-9831
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9831 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 13 01:12:48.173: INFO: successfully validated that service multi-endpoint-test in namespace services-9831 exposes endpoints map[pod1:[100] pod2:[101]] (2.80637746s elapsed)
STEP: Deleting pod pod1 in namespace services-9831
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9831 to expose endpoints map[pod2:[101]]
Aug 13 01:12:48.444: INFO: successfully validated that service multi-endpoint-test in namespace services-9831 exposes endpoints map[pod2:[101]] (179.684494ms elapsed)
STEP: Deleting pod pod2 in namespace services-9831
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9831 to expose endpoints map[]
Aug 13 01:12:48.624: INFO: successfully validated that service multi-endpoint-test in namespace services-9831 exposes endpoints map[] (89.529496ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:12:48.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9831" for this suite.
Aug 13 01:12:55.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:12:58.479: INFO: namespace services-9831 deletion completed in 9.669813291s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:12:58.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8072
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug 13 01:12:59.118: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:12:59.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8072" for this suite.
Aug 13 01:13:05.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:13:09.341: INFO: namespace kubectl-8072 deletion completed in 9.691543594s
•SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:13:09.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-93b376fa-85b6-4d6a-8536-47ea6d2c805a
STEP: Creating a pod to test consume configMaps
Aug 13 01:13:10.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e" in namespace "configmap-5732" to be "success or failure"
Aug 13 01:13:10.252: INFO: Pod "pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.496104ms
Aug 13 01:13:12.343: INFO: Pod "pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179950378s
STEP: Saw pod success
Aug 13 01:13:12.343: INFO: Pod "pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e" satisfied condition "success or failure"
Aug 13 01:13:12.433: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:13:12.629: INFO: Waiting for pod pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e to disappear
Aug 13 01:13:12.719: INFO: Pod pod-configmaps-202b1053-e86c-48bd-813f-6d97f6549e5e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:13:12.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5732" for this suite.
Aug 13 01:13:19.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:13:22.477: INFO: namespace configmap-5732 deletion completed in 9.667574651s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:13:22.477: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug 13 01:13:23.115: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-766'
Aug 13 01:13:24.379: INFO: stderr: ""
Aug 13 01:13:24.379: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug 13 01:13:25.469: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 01:13:25.469: INFO: Found 1 / 1
Aug 13 01:13:25.469: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 13 01:13:25.558: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 01:13:25.558: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 13 01:13:25.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-m7vvr redis-master --namespace=kubectl-766'
Aug 13 01:13:26.098: INFO: stderr: ""
Aug 13 01:13:26.099: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Aug 01:13:25.245 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Aug 01:13:25.245 # Server started, Redis version 3.2.12\n1:M 13 Aug 01:13:25.245 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Aug 01:13:25.245 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 13 01:13:26.099: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-m7vvr redis-master --namespace=kubectl-766 --tail=1'
Aug 13 01:13:26.627: INFO: stderr: ""
Aug 13 01:13:26.627: INFO: stdout: "1:M 13 Aug 01:13:25.245 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 13 01:13:26.627: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-m7vvr redis-master --namespace=kubectl-766 --limit-bytes=1'
Aug 13 01:13:27.157: INFO: stderr: ""
Aug 13 01:13:27.157: INFO: stdout: " "
STEP: exposing timestamps
Aug 13 01:13:27.157: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-m7vvr redis-master --namespace=kubectl-766 --tail=1 --timestamps'
Aug 13 01:13:27.684: INFO: stderr: ""
Aug 13 01:13:27.684: INFO: stdout: "2019-08-13T01:13:25.245526113Z 1:M 13 Aug 01:13:25.245 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 13 01:13:30.185: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-m7vvr redis-master --namespace=kubectl-766 --since=1s'
Aug 13 01:13:30.728: INFO: stderr: ""
Aug 13 01:13:30.728: INFO: stdout: ""
Aug 13 01:13:30.728: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-m7vvr redis-master --namespace=kubectl-766 --since=24h'
Aug 13 01:13:31.266: INFO: stderr: ""
Aug 13 01:13:31.266: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Aug 01:13:25.245 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Aug 01:13:25.245 # Server started, Redis version 3.2.12\n1:M 13 Aug 01:13:25.245 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Aug 01:13:25.245 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug 13 01:13:31.266: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-766'
Aug 13 01:13:31.786: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:13:31.786: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 13 01:13:31.786: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-766'
Aug 13 01:13:32.311: INFO: stderr: "No resources found.\n"
Aug 13 01:13:32.311: INFO: stdout: ""
Aug 13 01:13:32.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-766 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 01:13:32.742: INFO: stderr: ""
Aug 13 01:13:32.742: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:13:32.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-766" for this suite.
Aug 13 01:13:55.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:13:58.503: INFO: namespace kubectl-766 deletion completed in 25.670592334s
•SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:13:58.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-01ae719d-663b-4e9a-a77c-e40d053f1e40 in namespace container-probe-5855
Aug 13 01:14:03.412: INFO: Started pod liveness-01ae719d-663b-4e9a-a77c-e40d053f1e40 in namespace container-probe-5855
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 01:14:03.502: INFO: Initial restart count of pod liveness-01ae719d-663b-4e9a-a77c-e40d053f1e40 is 0
Aug 13 01:14:26.581: INFO: Restart count of pod container-probe-5855/liveness-01ae719d-663b-4e9a-a77c-e40d053f1e40 is now 1 (23.079365123s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:14:26.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5855" for this suite.
Aug 13 01:14:33.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:14:36.444: INFO: namespace container-probe-5855 deletion completed in 9.676243013s
•SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:14:36.444: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6558
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-8708b036-bd68-4c64-bdc4-9f62c50c5b69
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:14:39.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6558" for this suite.
Aug 13 01:15:02.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:15:05.615: INFO: namespace configmap-6558 deletion completed in 25.676566748s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:15:05.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Aug 13 01:15:17.515: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:15:17.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0813 01:15:17.515464    4207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-4089" for this suite.
Aug 13 01:15:23.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:15:27.274: INFO: namespace gc-4089 deletion completed in 9.668980264s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:15:27.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug 13 01:15:28.005: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4610" to be "success or failure"
Aug 13 01:15:28.095: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 89.729192ms
Aug 13 01:15:30.185: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179773967s
STEP: Saw pod success
Aug 13 01:15:30.185: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 13 01:15:30.274: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 13 01:15:30.463: INFO: Waiting for pod pod-host-path-test to disappear
Aug 13 01:15:30.552: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:15:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4610" for this suite.
Aug 13 01:15:36.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:15:40.318: INFO: namespace hostpath-4610 deletion completed in 9.675743132s
•SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:15:40.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-6447/secret-test-eb0c491e-d698-40ec-9645-9df21977eef7
STEP: Creating a pod to test consume secrets
Aug 13 01:15:41.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8" in namespace "secrets-6447" to be "success or failure"
Aug 13 01:15:41.228: INFO: Pod "pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.567532ms
Aug 13 01:15:43.318: INFO: Pod "pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180008066s
STEP: Saw pod success
Aug 13 01:15:43.318: INFO: Pod "pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8" satisfied condition "success or failure"
Aug 13 01:15:43.408: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8 container env-test: <nil>
STEP: delete the pod
Aug 13 01:15:43.594: INFO: Waiting for pod pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8 to disappear
Aug 13 01:15:43.683: INFO: Pod pod-configmaps-1b28036b-6e25-4fa3-a190-736355a070a8 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:15:43.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6447" for this suite.
Aug 13 01:15:50.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:15:53.590: INFO: namespace secrets-6447 deletion completed in 9.817044693s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:15:53.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:15:54.229: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 13 01:15:54.408: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 13 01:15:56.587: INFO: Creating deployment "test-rolling-update-deployment"
Aug 13 01:15:56.677: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 13 01:15:56.857: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 13 01:15:56.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255756, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255756, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255756, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701255756, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:15:59.037: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 13 01:15:59.310: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-9115,SelfLink:/apis/apps/v1/namespaces/deployment-9115/deployments/test-rolling-update-deployment,UID:e0071638-aea6-4429-9a1c-64a57e281ac0,ResourceVersion:11983,Generation:1,CreationTimestamp:2019-08-13 01:15:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-13 01:15:56 +0000 UTC 2019-08-13 01:15:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-13 01:15:58 +0000 UTC 2019-08-13 01:15:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 13 01:15:59.400: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-9115,SelfLink:/apis/apps/v1/namespaces/deployment-9115/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:629c2bc8-0ec5-4d32-ba5b-cf26bd9f4b5a,ResourceVersion:11976,Generation:1,CreationTimestamp:2019-08-13 01:15:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e0071638-aea6-4429-9a1c-64a57e281ac0 0xc0027d5d97 0xc0027d5d98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 13 01:15:59.400: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 13 01:15:59.400: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-9115,SelfLink:/apis/apps/v1/namespaces/deployment-9115/replicasets/test-rolling-update-controller,UID:9c6fb15b-d6c3-4fce-b8fe-828bdf2ccd92,ResourceVersion:11982,Generation:2,CreationTimestamp:2019-08-13 01:15:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e0071638-aea6-4429-9a1c-64a57e281ac0 0xc0027d5cc7 0xc0027d5cc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 01:15:59.490: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-52x6l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-52x6l,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-9115,SelfLink:/api/v1/namespaces/deployment-9115/pods/test-rolling-update-deployment-79f6b9d75c-52x6l,UID:5b39428d-df56-4af3-909d-ecdf29904552,ResourceVersion:11975,Generation:0,CreationTimestamp:2019-08-13 01:15:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.149/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 629c2bc8-0ec5-4d32-ba5b-cf26bd9f4b5a 0xc00101c6a7 0xc00101c6a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dlqvs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dlqvs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-dlqvs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00101c720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00101c740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:15:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:15:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:15:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:15:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.149,StartTime:2019-08-13 01:15:56 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-13 01:15:57 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c9a2b69b9f0bab04c3431e7ca10a00c09526285b646deb1017521740888ab703}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:15:59.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9115" for this suite.
Aug 13 01:16:05.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:16:09.254: INFO: namespace deployment-9115 deletion completed in 9.67393053s
•SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:16:09.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 13 01:16:09.999: INFO: Waiting up to 5m0s for pod "pod-033d21f1-840a-47f0-b35e-dc3973ca886f" in namespace "emptydir-4971" to be "success or failure"
Aug 13 01:16:10.089: INFO: Pod "pod-033d21f1-840a-47f0-b35e-dc3973ca886f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.789475ms
Aug 13 01:16:12.179: INFO: Pod "pod-033d21f1-840a-47f0-b35e-dc3973ca886f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180056931s
STEP: Saw pod success
Aug 13 01:16:12.179: INFO: Pod "pod-033d21f1-840a-47f0-b35e-dc3973ca886f" satisfied condition "success or failure"
Aug 13 01:16:12.269: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-033d21f1-840a-47f0-b35e-dc3973ca886f container test-container: <nil>
STEP: delete the pod
Aug 13 01:16:12.456: INFO: Waiting for pod pod-033d21f1-840a-47f0-b35e-dc3973ca886f to disappear
Aug 13 01:16:12.545: INFO: Pod pod-033d21f1-840a-47f0-b35e-dc3973ca886f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:16:12.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4971" for this suite.
Aug 13 01:16:18.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:16:22.315: INFO: namespace emptydir-4971 deletion completed in 9.679682624s
•SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:16:22.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3305
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 13 01:16:23.587: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3305,SelfLink:/api/v1/namespaces/watch-3305/configmaps/e2e-watch-test-resource-version,UID:f67a69ee-ebda-40fd-9b19-9037e58c56ef,ResourceVersion:12093,Generation:0,CreationTimestamp:2019-08-13 01:16:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 01:16:23.587: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3305,SelfLink:/api/v1/namespaces/watch-3305/configmaps/e2e-watch-test-resource-version,UID:f67a69ee-ebda-40fd-9b19-9037e58c56ef,ResourceVersion:12095,Generation:0,CreationTimestamp:2019-08-13 01:16:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:16:23.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3305" for this suite.
Aug 13 01:16:29.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:16:33.353: INFO: namespace watch-3305 deletion completed in 9.675884786s
•SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:16:33.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6982
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 13 01:16:36.442: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:16:36.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6982" for this suite.
Aug 13 01:16:42.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:16:46.387: INFO: namespace container-runtime-6982 deletion completed in 9.671950362s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:16:46.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 13 01:16:47.135: INFO: Waiting up to 5m0s for pod "pod-c9bd3fa9-ba0e-4531-84fe-53538c403434" in namespace "emptydir-3847" to be "success or failure"
Aug 13 01:16:47.224: INFO: Pod "pod-c9bd3fa9-ba0e-4531-84fe-53538c403434": Phase="Pending", Reason="", readiness=false. Elapsed: 89.592907ms
Aug 13 01:16:49.315: INFO: Pod "pod-c9bd3fa9-ba0e-4531-84fe-53538c403434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.18009427s
STEP: Saw pod success
Aug 13 01:16:49.315: INFO: Pod "pod-c9bd3fa9-ba0e-4531-84fe-53538c403434" satisfied condition "success or failure"
Aug 13 01:16:49.405: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-c9bd3fa9-ba0e-4531-84fe-53538c403434 container test-container: <nil>
STEP: delete the pod
Aug 13 01:16:49.607: INFO: Waiting for pod pod-c9bd3fa9-ba0e-4531-84fe-53538c403434 to disappear
Aug 13 01:16:49.696: INFO: Pod pod-c9bd3fa9-ba0e-4531-84fe-53538c403434 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:16:49.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3847" for this suite.
Aug 13 01:16:56.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:16:59.456: INFO: namespace emptydir-3847 deletion completed in 9.669452486s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:16:59.457: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6681
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 01:17:00.119: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 01:17:21.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.154:8080/dial?request=hostName&protocol=udp&host=100.96.0.38&port=8081&tries=1'] Namespace:pod-network-test-6681 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:17:21.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:17:22.591: INFO: Waiting for endpoints: map[]
Aug 13 01:17:22.681: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.154:8080/dial?request=hostName&protocol=udp&host=100.96.1.153&port=8081&tries=1'] Namespace:pod-network-test-6681 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:17:22.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:17:23.541: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:17:23.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6681" for this suite.
Aug 13 01:17:45.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:17:49.305: INFO: namespace pod-network-test-6681 deletion completed in 25.674148163s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:17:49.306: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 13 01:17:49.944: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 13 01:17:50.124: INFO: Waiting for terminating namespaces to be deleted...
Aug 13 01:17:50.214: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-2-100.ec2.internal before test
Aug 13 01:17:50.308: INFO: kube-proxy-pncr5 from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.308: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 01:17:50.308: INFO: node-exporter-md2rz from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.308: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 01:17:50.308: INFO: addons-kube2iam-rtfmd from kube-system started at 2019-08-13 00:23:54 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.308: INFO: 	Container kube2iam ready: true, restart count 0
Aug 13 01:17:50.308: INFO: calico-node-8pgth from kube-system started at 2019-08-13 00:23:44 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.308: INFO: 	Container calico-node ready: true, restart count 0
Aug 13 01:17:50.308: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-2-233.ec2.internal before test
Aug 13 01:17:50.406: INFO: node-exporter-bb6ms from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 01:17:50.406: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-9jm4k from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug 13 01:17:50.406: INFO: coredns-85cc454dd8-mxfbk from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container coredns ready: true, restart count 0
Aug 13 01:17:50.406: INFO: blackbox-exporter-954dd954b-b4c5l from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 13 01:17:50.406: INFO: addons-nginx-ingress-controller-66c9ddddb9-5cjjg from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 13 01:17:50.406: INFO: calico-kube-controllers-5f4b46ffb5-wxxxf from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 13 01:17:50.406: INFO: addons-kube2iam-zm6dv from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container kube2iam ready: true, restart count 0
Aug 13 01:17:50.406: INFO: calico-node-qjrkf from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container calico-node ready: true, restart count 0
Aug 13 01:17:50.406: INFO: kube-proxy-wgll2 from kube-system started at 2019-08-13 00:23:31 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 01:17:50.406: INFO: coredns-85cc454dd8-nrq5t from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container coredns ready: true, restart count 0
Aug 13 01:17:50.406: INFO: vpn-shoot-7f79d69868-7vfs5 from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug 13 01:17:50.406: INFO: addons-kubernetes-dashboard-5c8d9945bc-qr4hm from kube-system started at 2019-08-13 00:23:41 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 13 01:17:50.406: INFO: metrics-server-7dd7d74c9b-6dvkl from kube-system started at 2019-08-13 00:23:43 +0000 UTC (1 container statuses recorded)
Aug 13 01:17:50.406: INFO: 	Container metrics-server ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ba56a12f314782], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:17:51.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8897" for this suite.
Aug 13 01:17:58.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:18:01.628: INFO: namespace sched-pred-8897 deletion completed in 9.677283606s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:18:01.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:18:02.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb" in namespace "downward-api-9807" to be "success or failure"
Aug 13 01:18:02.451: INFO: Pod "downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.535147ms
Aug 13 01:18:04.542: INFO: Pod "downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179803342s
STEP: Saw pod success
Aug 13 01:18:04.542: INFO: Pod "downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb" satisfied condition "success or failure"
Aug 13 01:18:04.631: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb container client-container: <nil>
STEP: delete the pod
Aug 13 01:18:04.819: INFO: Waiting for pod downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb to disappear
Aug 13 01:18:04.909: INFO: Pod downwardapi-volume-bc7142f3-49b5-4f1d-8b96-75b14d0ff7eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:18:04.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9807" for this suite.
Aug 13 01:18:11.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:18:14.676: INFO: namespace downward-api-9807 deletion completed in 9.676756765s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:18:14.677: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2676
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug 13 01:18:16.264: INFO: created pod pod-service-account-defaultsa
Aug 13 01:18:16.264: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 13 01:18:16.357: INFO: created pod pod-service-account-mountsa
Aug 13 01:18:16.357: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 13 01:18:16.457: INFO: created pod pod-service-account-nomountsa
Aug 13 01:18:16.457: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 13 01:18:16.547: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 13 01:18:16.547: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 13 01:18:16.637: INFO: created pod pod-service-account-mountsa-mountspec
Aug 13 01:18:16.637: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 13 01:18:16.727: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 13 01:18:16.727: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 13 01:18:16.817: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 13 01:18:16.817: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 13 01:18:16.909: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 13 01:18:16.909: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 13 01:18:17.000: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 13 01:18:17.000: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:18:17.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2676" for this suite.
Aug 13 01:18:23.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:18:26.764: INFO: namespace svcaccounts-2676 deletion completed in 9.674449503s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:18:26.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:18:27.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b" in namespace "projected-5604" to be "success or failure"
Aug 13 01:18:27.594: INFO: Pod "downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.789166ms
Aug 13 01:18:29.684: INFO: Pod "downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180094422s
STEP: Saw pod success
Aug 13 01:18:29.684: INFO: Pod "downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b" satisfied condition "success or failure"
Aug 13 01:18:29.773: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b container client-container: <nil>
STEP: delete the pod
Aug 13 01:18:29.959: INFO: Waiting for pod downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b to disappear
Aug 13 01:18:30.050: INFO: Pod downwardapi-volume-5c5d040c-b64b-4853-a3c7-2de544833f8b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:18:30.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5604" for this suite.
Aug 13 01:18:36.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:18:39.809: INFO: namespace projected-5604 deletion completed in 9.668124204s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:18:39.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7c806091-a22b-458f-9ee0-7c6db935967a
STEP: Creating a pod to test consume configMaps
Aug 13 01:18:40.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47" in namespace "configmap-4082" to be "success or failure"
Aug 13 01:18:40.718: INFO: Pod "pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47": Phase="Pending", Reason="", readiness=false. Elapsed: 89.647335ms
Aug 13 01:18:42.808: INFO: Pod "pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179697755s
STEP: Saw pod success
Aug 13 01:18:42.808: INFO: Pod "pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47" satisfied condition "success or failure"
Aug 13 01:18:42.898: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:18:43.084: INFO: Waiting for pod pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47 to disappear
Aug 13 01:18:43.173: INFO: Pod pod-configmaps-24534b7d-5d25-42e4-8d3f-0257cd711b47 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:18:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4082" for this suite.
Aug 13 01:18:49.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:18:52.934: INFO: namespace configmap-4082 deletion completed in 9.670549414s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:18:52.934: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7461
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 13 01:19:04.113: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:19:04.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0813 01:19:04.113197    4207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7461" for this suite.
Aug 13 01:19:10.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:19:13.875: INFO: namespace gc-7461 deletion completed in 9.672575229s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:19:13.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:19:14.513: INFO: Creating ReplicaSet my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58
Aug 13 01:19:14.693: INFO: Pod name my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58: Found 1 pods out of 1
Aug 13 01:19:14.693: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58" is running
Aug 13 01:19:16.874: INFO: Pod "my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58-lvfpk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 01:19:14 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 01:19:14 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 01:19:14 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 01:19:14 +0000 UTC Reason: Message:}])
Aug 13 01:19:16.875: INFO: Trying to dial the pod
Aug 13 01:19:22.232: INFO: Controller my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58: Got expected result from replica 1 [my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58-lvfpk]: "my-hostname-basic-69b0acac-37e0-48eb-9632-a58305640b58-lvfpk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:19:22.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8157" for this suite.
Aug 13 01:19:28.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:19:31.998: INFO: namespace replicaset-8157 deletion completed in 9.675686297s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:19:31.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-f07b1e03-a61b-442f-bc1c-dceff439cdd5
STEP: Creating a pod to test consume secrets
Aug 13 01:19:32.819: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d" in namespace "projected-6329" to be "success or failure"
Aug 13 01:19:32.910: INFO: Pod "pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d": Phase="Pending", Reason="", readiness=false. Elapsed: 90.455302ms
Aug 13 01:19:35.001: INFO: Pod "pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181679416s
STEP: Saw pod success
Aug 13 01:19:35.001: INFO: Pod "pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d" satisfied condition "success or failure"
Aug 13 01:19:35.091: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 01:19:35.280: INFO: Waiting for pod pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d to disappear
Aug 13 01:19:35.369: INFO: Pod pod-projected-secrets-8715b9f4-4b61-4230-bf54-11819ed5de3d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:19:35.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6329" for this suite.
Aug 13 01:19:41.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:19:45.134: INFO: namespace projected-6329 deletion completed in 9.674224647s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:19:45.134: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3102
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:19:45.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3102" for this suite.
Aug 13 01:20:08.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:20:11.749: INFO: namespace kubelet-test-3102 deletion completed in 25.704109113s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:20:11.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:20:12.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232" in namespace "projected-946" to be "success or failure"
Aug 13 01:20:12.570: INFO: Pod "downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232": Phase="Pending", Reason="", readiness=false. Elapsed: 89.743283ms
Aug 13 01:20:14.660: INFO: Pod "downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180020529s
STEP: Saw pod success
Aug 13 01:20:14.660: INFO: Pod "downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232" satisfied condition "success or failure"
Aug 13 01:20:14.750: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232 container client-container: <nil>
STEP: delete the pod
Aug 13 01:20:14.944: INFO: Waiting for pod downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232 to disappear
Aug 13 01:20:15.033: INFO: Pod downwardapi-volume-5d5db358-aeef-4cea-b8d2-329bc2a49232 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:20:15.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-946" for this suite.
Aug 13 01:20:21.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:20:24.793: INFO: namespace projected-946 deletion completed in 9.669142836s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:20:24.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6702
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-550098ae-32ae-456f-8629-22bb2f4e2f40
STEP: Creating configMap with name cm-test-opt-upd-5612d706-989b-4331-bd73-1fe2ad8fe35c
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-550098ae-32ae-456f-8629-22bb2f4e2f40
STEP: Updating configmap cm-test-opt-upd-5612d706-989b-4331-bd73-1fe2ad8fe35c
STEP: Creating configMap with name cm-test-opt-create-11c95896-794b-48a7-aa13-3754866de990
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:22:01.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6702" for this suite.
Aug 13 01:22:23.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:22:26.856: INFO: namespace configmap-6702 deletion completed in 25.671550157s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:22:26.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5315
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 01:22:27.499: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 01:22:51.029: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.173 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5315 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:22:51.029: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:22:52.962: INFO: Found all expected endpoints: [netserver-0]
Aug 13 01:22:53.052: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.40 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5315 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:22:53.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:22:54.880: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:22:54.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5315" for this suite.
Aug 13 01:23:17.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:23:20.651: INFO: namespace pod-network-test-5315 deletion completed in 25.679989544s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:23:20.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-8684
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8684
STEP: Deleting pre-stop pod
Aug 13 01:23:31.191: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:23:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8684" for this suite.
Aug 13 01:24:09.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:24:13.042: INFO: namespace prestop-8684 deletion completed in 41.670012498s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:24:13.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 13 01:24:13.681: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8815'
Aug 13 01:24:15.277: INFO: stderr: ""
Aug 13 01:24:15.277: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 01:24:15.277: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8815'
Aug 13 01:24:15.743: INFO: stderr: ""
Aug 13 01:24:15.743: INFO: stdout: "update-demo-nautilus-l5jxk update-demo-nautilus-vftb6 "
Aug 13 01:24:15.743: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l5jxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:16.191: INFO: stderr: ""
Aug 13 01:24:16.191: INFO: stdout: ""
Aug 13 01:24:16.191: INFO: update-demo-nautilus-l5jxk is created but not running
Aug 13 01:24:21.192: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8815'
Aug 13 01:24:21.672: INFO: stderr: ""
Aug 13 01:24:21.672: INFO: stdout: "update-demo-nautilus-l5jxk update-demo-nautilus-vftb6 "
Aug 13 01:24:21.672: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l5jxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:22.135: INFO: stderr: ""
Aug 13 01:24:22.135: INFO: stdout: "true"
Aug 13 01:24:22.135: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l5jxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:22.595: INFO: stderr: ""
Aug 13 01:24:22.595: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:24:22.595: INFO: validating pod update-demo-nautilus-l5jxk
Aug 13 01:24:22.773: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:24:22.773: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:24:22.773: INFO: update-demo-nautilus-l5jxk is verified up and running
Aug 13 01:24:22.773: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-vftb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:23.248: INFO: stderr: ""
Aug 13 01:24:23.248: INFO: stdout: "true"
Aug 13 01:24:23.248: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-vftb6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:23.720: INFO: stderr: ""
Aug 13 01:24:23.720: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:24:23.720: INFO: validating pod update-demo-nautilus-vftb6
Aug 13 01:24:23.900: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:24:23.900: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:24:23.900: INFO: update-demo-nautilus-vftb6 is verified up and running
STEP: scaling down the replication controller
Aug 13 01:24:23.902: INFO: scanned /root for discovery docs: <nil>
Aug 13 01:24:23.902: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8815'
Aug 13 01:24:24.561: INFO: stderr: ""
Aug 13 01:24:24.561: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 01:24:24.562: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8815'
Aug 13 01:24:25.034: INFO: stderr: ""
Aug 13 01:24:25.034: INFO: stdout: "update-demo-nautilus-l5jxk update-demo-nautilus-vftb6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 13 01:24:30.035: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8815'
Aug 13 01:24:30.494: INFO: stderr: ""
Aug 13 01:24:30.494: INFO: stdout: "update-demo-nautilus-vftb6 "
Aug 13 01:24:30.494: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-vftb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:30.961: INFO: stderr: ""
Aug 13 01:24:30.961: INFO: stdout: "true"
Aug 13 01:24:30.961: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-vftb6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:31.417: INFO: stderr: ""
Aug 13 01:24:31.417: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:24:31.417: INFO: validating pod update-demo-nautilus-vftb6
Aug 13 01:24:31.508: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:24:31.508: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:24:31.508: INFO: update-demo-nautilus-vftb6 is verified up and running
STEP: scaling up the replication controller
Aug 13 01:24:31.510: INFO: scanned /root for discovery docs: <nil>
Aug 13 01:24:31.510: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8815'
Aug 13 01:24:32.150: INFO: stderr: ""
Aug 13 01:24:32.150: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 01:24:32.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8815'
Aug 13 01:24:32.611: INFO: stderr: ""
Aug 13 01:24:32.611: INFO: stdout: "update-demo-nautilus-8g76g update-demo-nautilus-vftb6 "
Aug 13 01:24:32.611: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8g76g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:33.061: INFO: stderr: ""
Aug 13 01:24:33.061: INFO: stdout: "true"
Aug 13 01:24:33.061: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8g76g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:33.526: INFO: stderr: ""
Aug 13 01:24:33.526: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:24:33.526: INFO: validating pod update-demo-nautilus-8g76g
Aug 13 01:24:33.708: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:24:33.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:24:33.708: INFO: update-demo-nautilus-8g76g is verified up and running
Aug 13 01:24:33.709: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-vftb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:34.163: INFO: stderr: ""
Aug 13 01:24:34.163: INFO: stdout: "true"
Aug 13 01:24:34.163: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-vftb6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8815'
Aug 13 01:24:34.628: INFO: stderr: ""
Aug 13 01:24:34.628: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:24:34.628: INFO: validating pod update-demo-nautilus-vftb6
Aug 13 01:24:34.720: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:24:34.720: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:24:34.720: INFO: update-demo-nautilus-vftb6 is verified up and running
STEP: using delete to clean up resources
Aug 13 01:24:34.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8815'
Aug 13 01:24:35.256: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:24:35.256: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 13 01:24:35.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8815'
Aug 13 01:24:35.811: INFO: stderr: "No resources found.\n"
Aug 13 01:24:35.811: INFO: stdout: ""
Aug 13 01:24:35.811: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-8815 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 01:24:36.266: INFO: stderr: ""
Aug 13 01:24:36.266: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:24:36.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8815" for this suite.
Aug 13 01:24:42.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:24:46.022: INFO: namespace kubectl-8815 deletion completed in 9.666499708s
•SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:24:46.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-5fb46e17-5a9d-415f-9afe-93d393fdb86d
STEP: Creating a pod to test consume secrets
Aug 13 01:24:46.972: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3" in namespace "projected-2127" to be "success or failure"
Aug 13 01:24:47.061: INFO: Pod "pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3": Phase="Pending", Reason="", readiness=false. Elapsed: 89.211131ms
Aug 13 01:24:49.151: INFO: Pod "pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178894524s
STEP: Saw pod success
Aug 13 01:24:49.151: INFO: Pod "pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3" satisfied condition "success or failure"
Aug 13 01:24:49.241: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 01:24:49.427: INFO: Waiting for pod pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3 to disappear
Aug 13 01:24:49.516: INFO: Pod pod-projected-secrets-8e25c9f7-c548-4280-af55-7d1ac59009a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:24:49.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2127" for this suite.
Aug 13 01:24:55.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:24:59.282: INFO: namespace projected-2127 deletion completed in 9.676310384s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:24:59.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-ff189436-6491-4ea5-b73c-35758df7a968
STEP: Creating a pod to test consume configMaps
Aug 13 01:25:00.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a" in namespace "configmap-6248" to be "success or failure"
Aug 13 01:25:00.192: INFO: Pod "pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.459941ms
Aug 13 01:25:02.282: INFO: Pod "pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179684431s
STEP: Saw pod success
Aug 13 01:25:02.282: INFO: Pod "pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a" satisfied condition "success or failure"
Aug 13 01:25:02.372: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:25:02.559: INFO: Waiting for pod pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a to disappear
Aug 13 01:25:02.648: INFO: Pod pod-configmaps-f51e208f-de00-481d-9ba4-0356835d8e4a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:25:02.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6248" for this suite.
Aug 13 01:25:07.976: INFO: Error while waiting for namespace to be terminated: Get https://api.tm-h0con.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/configmap-6248: http2: server sent GOAWAY and closed the connection; LastStreamID=23957, ErrCode=NO_ERROR, debug=""
Aug 13 01:25:31.860: INFO: Error while waiting for namespace to be terminated: Get https://api.tm-h0con.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/configmap-6248: http2: server sent GOAWAY and closed the connection; LastStreamID=1, ErrCode=NO_ERROR, debug=""
Aug 13 01:25:43.013: INFO: Error while waiting for namespace to be terminated: Get https://api.tm-h0con.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/configmap-6248: net/http: TLS handshake timeout
Aug 13 01:26:23.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:26:26.405: INFO: namespace configmap-6248 deletion completed in 1m23.666123672s
•SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:26:26.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 13 01:26:31.822: INFO: Pod name wrapped-volume-race-b84f86bb-d583-4bf9-a563-446c5144147e: Found 1 pods out of 5
Aug 13 01:26:37.005: INFO: Pod name wrapped-volume-race-b84f86bb-d583-4bf9-a563-446c5144147e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b84f86bb-d583-4bf9-a563-446c5144147e in namespace emptydir-wrapper-8306, will wait for the garbage collector to delete the pods
Aug 13 01:26:37.737: INFO: Deleting ReplicationController wrapped-volume-race-b84f86bb-d583-4bf9-a563-446c5144147e took: 92.679403ms
Aug 13 01:26:38.337: INFO: Terminating ReplicationController wrapped-volume-race-b84f86bb-d583-4bf9-a563-446c5144147e pods took: 600.417677ms
STEP: Creating RC which spawns configmap-volume pods
Aug 13 01:27:21.824: INFO: Pod name wrapped-volume-race-e184b898-e703-42a8-bf12-64aeab0cbcb2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e184b898-e703-42a8-bf12-64aeab0cbcb2 in namespace emptydir-wrapper-8306, will wait for the garbage collector to delete the pods
Aug 13 01:27:26.644: INFO: Deleting ReplicationController wrapped-volume-race-e184b898-e703-42a8-bf12-64aeab0cbcb2 took: 91.516331ms
Aug 13 01:27:26.745: INFO: Terminating ReplicationController wrapped-volume-race-e184b898-e703-42a8-bf12-64aeab0cbcb2 pods took: 100.595514ms
STEP: Creating RC which spawns configmap-volume pods
Aug 13 01:28:11.724: INFO: Pod name wrapped-volume-race-1bdd43da-b7b1-4abe-96c6-c39e7a3e7dae: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1bdd43da-b7b1-4abe-96c6-c39e7a3e7dae in namespace emptydir-wrapper-8306, will wait for the garbage collector to delete the pods
Aug 13 01:28:16.545: INFO: Deleting ReplicationController wrapped-volume-race-1bdd43da-b7b1-4abe-96c6-c39e7a3e7dae took: 91.297948ms
Aug 13 01:28:16.945: INFO: Terminating ReplicationController wrapped-volume-race-1bdd43da-b7b1-4abe-96c6-c39e7a3e7dae pods took: 400.498743ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:29:06.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8306" for this suite.
Aug 13 01:29:12.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:29:15.770: INFO: namespace emptydir-wrapper-8306 deletion completed in 9.667320033s
•SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:29:15.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 13 01:29:19.735: INFO: Successfully updated pod "annotationupdate23aeb0ec-b9de-4b87-88ba-0322b4287fc6"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:29:21.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4622" for this suite.
Aug 13 01:29:44.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:29:47.697: INFO: namespace downward-api-4622 deletion completed in 25.682775434s
•
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:29:47.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 13 01:29:50.871: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 13 01:29:56.395: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:29:56.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1343" for this suite.
Aug 13 01:30:02.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:30:06.248: INFO: namespace pods-1343 deletion completed in 9.673268807s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:30:06.248: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 01:30:06.884: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8350'
Aug 13 01:30:07.343: INFO: stderr: ""
Aug 13 01:30:07.343: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug 13 01:30:07.433: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-8350'
Aug 13 01:30:21.363: INFO: stderr: ""
Aug 13 01:30:21.364: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:30:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8350" for this suite.
Aug 13 01:30:27.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:30:31.117: INFO: namespace kubectl-8350 deletion completed in 9.662569975s
•SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:30:31.117: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7149
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-51e2a28b-8f1a-44c4-b703-0deb03e54f9c
STEP: Creating configMap with name cm-test-opt-upd-5aae33b4-ec9d-4f90-9b56-adff7d14c153
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-51e2a28b-8f1a-44c4-b703-0deb03e54f9c
STEP: Updating configmap cm-test-opt-upd-5aae33b4-ec9d-4f90-9b56-adff7d14c153
STEP: Creating configMap with name cm-test-opt-create-98f5211d-2457-4515-a827-96ae64a98cd8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:31:46.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7149" for this suite.
Aug 13 01:32:08.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:32:12.221: INFO: namespace projected-7149 deletion completed in 25.67428411s
•SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:32:12.221: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6484
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:32:12.951: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582" in namespace "downward-api-6484" to be "success or failure"
Aug 13 01:32:13.040: INFO: Pod "downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582": Phase="Pending", Reason="", readiness=false. Elapsed: 89.3756ms
Aug 13 01:32:15.133: INFO: Pod "downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182752803s
STEP: Saw pod success
Aug 13 01:32:15.134: INFO: Pod "downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582" satisfied condition "success or failure"
Aug 13 01:32:15.223: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582 container client-container: <nil>
STEP: delete the pod
Aug 13 01:32:15.414: INFO: Waiting for pod downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582 to disappear
Aug 13 01:32:15.503: INFO: Pod downwardapi-volume-89a23faa-cb45-41c7-a9b2-06fd78085582 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:32:15.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6484" for this suite.
Aug 13 01:32:21.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:32:25.284: INFO: namespace downward-api-6484 deletion completed in 9.689870834s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:32:25.285: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5463
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cd6314b5-d36f-4c8c-874e-0f5f8fde98a3
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-cd6314b5-d36f-4c8c-874e-0f5f8fde98a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:33:37.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5463" for this suite.
Aug 13 01:34:00.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:34:03.665: INFO: namespace projected-5463 deletion completed in 25.667540444s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:34:03.666: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 13 01:34:04.660: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-856,SelfLink:/api/v1/namespaces/watch-856/configmaps/e2e-watch-test-watch-closed,UID:e5feac9f-4b62-43d5-a6ce-c93c8083e2ca,ResourceVersion:15556,Generation:0,CreationTimestamp:2019-08-13 01:34:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 01:34:04.661: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-856,SelfLink:/api/v1/namespaces/watch-856/configmaps/e2e-watch-test-watch-closed,UID:e5feac9f-4b62-43d5-a6ce-c93c8083e2ca,ResourceVersion:15559,Generation:0,CreationTimestamp:2019-08-13 01:34:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 13 01:34:05.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-856,SelfLink:/api/v1/namespaces/watch-856/configmaps/e2e-watch-test-watch-closed,UID:e5feac9f-4b62-43d5-a6ce-c93c8083e2ca,ResourceVersion:15560,Generation:0,CreationTimestamp:2019-08-13 01:34:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 01:34:05.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-856,SelfLink:/api/v1/namespaces/watch-856/configmaps/e2e-watch-test-watch-closed,UID:e5feac9f-4b62-43d5-a6ce-c93c8083e2ca,ResourceVersion:15561,Generation:0,CreationTimestamp:2019-08-13 01:34:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:34:05.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-856" for this suite.
Aug 13 01:34:11.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:34:14.776: INFO: namespace watch-856 deletion completed in 9.666532332s
•SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:34:14.776: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug 13 01:34:15.515: INFO: Waiting up to 5m0s for pod "var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e" in namespace "var-expansion-210" to be "success or failure"
Aug 13 01:34:15.605: INFO: Pod "var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.590843ms
Aug 13 01:34:17.695: INFO: Pod "var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179679465s
STEP: Saw pod success
Aug 13 01:34:17.695: INFO: Pod "var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e" satisfied condition "success or failure"
Aug 13 01:34:17.786: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e container dapi-container: <nil>
STEP: delete the pod
Aug 13 01:34:17.975: INFO: Waiting for pod var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e to disappear
Aug 13 01:34:18.064: INFO: Pod var-expansion-fa10687f-87d0-4749-bc08-a3ae4816df6e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:34:18.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-210" for this suite.
Aug 13 01:34:24.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:34:27.824: INFO: namespace var-expansion-210 deletion completed in 9.669575364s
•SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:34:27.825: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-42e8e523-311b-456e-ba88-50693f1baa9a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:34:28.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5936" for this suite.
Aug 13 01:34:34.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:34:38.309: INFO: namespace secrets-5936 deletion completed in 9.669544083s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:34:38.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:34:39.155: INFO: (0) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 118.30212ms)
Aug 13 01:34:39.246: INFO: (1) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.012808ms)
Aug 13 01:34:39.337: INFO: (2) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.026109ms)
Aug 13 01:34:39.428: INFO: (3) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.801658ms)
Aug 13 01:34:39.520: INFO: (4) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.509725ms)
Aug 13 01:34:39.611: INFO: (5) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.160367ms)
Aug 13 01:34:39.702: INFO: (6) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.153376ms)
Aug 13 01:34:39.793: INFO: (7) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.192943ms)
Aug 13 01:34:39.884: INFO: (8) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.971433ms)
Aug 13 01:34:39.975: INFO: (9) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.901176ms)
Aug 13 01:34:40.067: INFO: (10) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.2587ms)
Aug 13 01:34:40.158: INFO: (11) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.629572ms)
Aug 13 01:34:40.250: INFO: (12) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.039946ms)
Aug 13 01:34:40.340: INFO: (13) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.716829ms)
Aug 13 01:34:40.432: INFO: (14) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.291207ms)
Aug 13 01:34:40.525: INFO: (15) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.38152ms)
Aug 13 01:34:40.616: INFO: (16) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.970831ms)
Aug 13 01:34:40.709: INFO: (17) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.496107ms)
Aug 13 01:34:40.800: INFO: (18) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.440462ms)
Aug 13 01:34:40.892: INFO: (19) /api/v1/nodes/ip-10-250-2-100.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.243336ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:34:40.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2108" for this suite.
Aug 13 01:34:47.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:34:50.652: INFO: namespace proxy-2108 deletion completed in 9.671312772s
•SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:34:50.653: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-0ab3aae6-12f1-4079-86f3-81356a255a49 in namespace container-probe-3265
Aug 13 01:34:53.566: INFO: Started pod test-webserver-0ab3aae6-12f1-4079-86f3-81356a255a49 in namespace container-probe-3265
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 01:34:53.656: INFO: Initial restart count of pod test-webserver-0ab3aae6-12f1-4079-86f3-81356a255a49 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:38:54.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3265" for this suite.
Aug 13 01:39:00.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:39:03.900: INFO: namespace container-probe-3265 deletion completed in 9.687660486s
•SS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:39:03.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4973
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4973
I0813 01:39:04.625155    4207 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4973, replica count: 1
I0813 01:39:05.725555    4207 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 01:39:06.725751    4207 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 13 01:39:06.920: INFO: Created: latency-svc-w6pw7
Aug 13 01:39:06.924: INFO: Got endpoints: latency-svc-w6pw7 [98.031964ms]
Aug 13 01:39:07.018: INFO: Created: latency-svc-s7qzz
Aug 13 01:39:07.019: INFO: Got endpoints: latency-svc-s7qzz [94.996441ms]
Aug 13 01:39:07.023: INFO: Created: latency-svc-v9zsp
Aug 13 01:39:07.026: INFO: Got endpoints: latency-svc-v9zsp [102.252287ms]
Aug 13 01:39:07.030: INFO: Created: latency-svc-bddgr
Aug 13 01:39:07.030: INFO: Got endpoints: latency-svc-bddgr [105.848751ms]
Aug 13 01:39:07.104: INFO: Created: latency-svc-brztl
Aug 13 01:39:07.108: INFO: Got endpoints: latency-svc-brztl [184.065073ms]
Aug 13 01:39:07.109: INFO: Created: latency-svc-mbn87
Aug 13 01:39:07.113: INFO: Got endpoints: latency-svc-mbn87 [188.647904ms]
Aug 13 01:39:07.113: INFO: Created: latency-svc-rmpx6
Aug 13 01:39:07.115: INFO: Got endpoints: latency-svc-rmpx6 [189.748631ms]
Aug 13 01:39:07.119: INFO: Created: latency-svc-76jbq
Aug 13 01:39:07.120: INFO: Got endpoints: latency-svc-76jbq [195.434806ms]
Aug 13 01:39:07.124: INFO: Created: latency-svc-98mm5
Aug 13 01:39:07.131: INFO: Got endpoints: latency-svc-98mm5 [206.40388ms]
Aug 13 01:39:07.131: INFO: Created: latency-svc-fpx7w
Aug 13 01:39:07.133: INFO: Got endpoints: latency-svc-fpx7w [208.128126ms]
Aug 13 01:39:07.136: INFO: Created: latency-svc-9qw52
Aug 13 01:39:07.137: INFO: Got endpoints: latency-svc-9qw52 [212.747273ms]
Aug 13 01:39:07.141: INFO: Created: latency-svc-cbtkm
Aug 13 01:39:07.142: INFO: Got endpoints: latency-svc-cbtkm [217.974609ms]
Aug 13 01:39:07.146: INFO: Created: latency-svc-gzq58
Aug 13 01:39:07.147: INFO: Got endpoints: latency-svc-gzq58 [222.075715ms]
Aug 13 01:39:07.151: INFO: Created: latency-svc-khfwf
Aug 13 01:39:07.152: INFO: Got endpoints: latency-svc-khfwf [226.999224ms]
Aug 13 01:39:07.156: INFO: Created: latency-svc-skmz7
Aug 13 01:39:07.158: INFO: Got endpoints: latency-svc-skmz7 [232.697956ms]
Aug 13 01:39:07.161: INFO: Created: latency-svc-x5x2c
Aug 13 01:39:07.168: INFO: Created: latency-svc-hqb5n
Aug 13 01:39:07.168: INFO: Got endpoints: latency-svc-x5x2c [243.018954ms]
Aug 13 01:39:07.168: INFO: Got endpoints: latency-svc-hqb5n [149.405378ms]
Aug 13 01:39:07.171: INFO: Created: latency-svc-5lnwv
Aug 13 01:39:07.173: INFO: Got endpoints: latency-svc-5lnwv [146.511548ms]
Aug 13 01:39:07.176: INFO: Created: latency-svc-6w8z6
Aug 13 01:39:07.178: INFO: Got endpoints: latency-svc-6w8z6 [147.92284ms]
Aug 13 01:39:07.210: INFO: Created: latency-svc-96k2b
Aug 13 01:39:07.211: INFO: Got endpoints: latency-svc-96k2b [102.172679ms]
Aug 13 01:39:07.213: INFO: Created: latency-svc-ps8k9
Aug 13 01:39:07.217: INFO: Got endpoints: latency-svc-ps8k9 [103.52414ms]
Aug 13 01:39:07.218: INFO: Created: latency-svc-w24ff
Aug 13 01:39:07.218: INFO: Got endpoints: latency-svc-w24ff [103.663692ms]
Aug 13 01:39:07.222: INFO: Created: latency-svc-d4c6x
Aug 13 01:39:07.223: INFO: Got endpoints: latency-svc-d4c6x [103.224069ms]
Aug 13 01:39:07.227: INFO: Created: latency-svc-g468q
Aug 13 01:39:07.229: INFO: Got endpoints: latency-svc-g468q [97.354305ms]
Aug 13 01:39:07.239: INFO: Created: latency-svc-rh64r
Aug 13 01:39:07.240: INFO: Got endpoints: latency-svc-rh64r [107.841985ms]
Aug 13 01:39:07.245: INFO: Created: latency-svc-k496r
Aug 13 01:39:07.246: INFO: Got endpoints: latency-svc-k496r [108.611013ms]
Aug 13 01:39:07.250: INFO: Created: latency-svc-z6ljd
Aug 13 01:39:07.251: INFO: Got endpoints: latency-svc-z6ljd [108.803711ms]
Aug 13 01:39:07.289: INFO: Created: latency-svc-dpshc
Aug 13 01:39:07.293: INFO: Got endpoints: latency-svc-dpshc [145.904315ms]
Aug 13 01:39:07.294: INFO: Created: latency-svc-vpjg8
Aug 13 01:39:07.295: INFO: Got endpoints: latency-svc-vpjg8 [142.625761ms]
Aug 13 01:39:07.299: INFO: Created: latency-svc-k2c4g
Aug 13 01:39:07.300: INFO: Got endpoints: latency-svc-k2c4g [131.923305ms]
Aug 13 01:39:07.303: INFO: Created: latency-svc-kxsks
Aug 13 01:39:07.309: INFO: Created: latency-svc-zshs4
Aug 13 01:39:07.309: INFO: Got endpoints: latency-svc-kxsks [130.785394ms]
Aug 13 01:39:07.309: INFO: Got endpoints: latency-svc-zshs4 [151.75057ms]
Aug 13 01:39:07.316: INFO: Created: latency-svc-q4k48
Aug 13 01:39:07.317: INFO: Got endpoints: latency-svc-q4k48 [143.630013ms]
Aug 13 01:39:07.321: INFO: Created: latency-svc-4zwd2
Aug 13 01:39:07.322: INFO: Got endpoints: latency-svc-4zwd2 [154.189165ms]
Aug 13 01:39:07.326: INFO: Created: latency-svc-27vlt
Aug 13 01:39:07.328: INFO: Got endpoints: latency-svc-27vlt [117.241003ms]
Aug 13 01:39:07.332: INFO: Created: latency-svc-rlfp2
Aug 13 01:39:07.333: INFO: Got endpoints: latency-svc-rlfp2 [116.668121ms]
Aug 13 01:39:07.350: INFO: Created: latency-svc-cwk5j
Aug 13 01:39:07.351: INFO: Got endpoints: latency-svc-cwk5j [132.690415ms]
Aug 13 01:39:07.365: INFO: Created: latency-svc-4xj8p
Aug 13 01:39:07.374: INFO: Created: latency-svc-xs2pk
Aug 13 01:39:07.374: INFO: Got endpoints: latency-svc-4xj8p [150.460833ms]
Aug 13 01:39:07.380: INFO: Created: latency-svc-dxbz6
Aug 13 01:39:07.387: INFO: Created: latency-svc-bh4f4
Aug 13 01:39:07.396: INFO: Created: latency-svc-r7cqp
Aug 13 01:39:07.403: INFO: Created: latency-svc-ssdsp
Aug 13 01:39:07.408: INFO: Created: latency-svc-z77fq
Aug 13 01:39:07.418: INFO: Created: latency-svc-wzqls
Aug 13 01:39:07.419: INFO: Got endpoints: latency-svc-xs2pk [190.429827ms]
Aug 13 01:39:07.425: INFO: Created: latency-svc-64fd6
Aug 13 01:39:07.433: INFO: Created: latency-svc-dldh4
Aug 13 01:39:07.439: INFO: Created: latency-svc-bg8l8
Aug 13 01:39:07.446: INFO: Created: latency-svc-pvj62
Aug 13 01:39:07.461: INFO: Created: latency-svc-r4zkp
Aug 13 01:39:07.469: INFO: Created: latency-svc-mg9lq
Aug 13 01:39:07.471: INFO: Got endpoints: latency-svc-dxbz6 [229.974204ms]
Aug 13 01:39:07.477: INFO: Created: latency-svc-cxb6c
Aug 13 01:39:07.483: INFO: Created: latency-svc-m9hzv
Aug 13 01:39:07.513: INFO: Created: latency-svc-wm95n
Aug 13 01:39:07.519: INFO: Got endpoints: latency-svc-bh4f4 [272.977467ms]
Aug 13 01:39:07.566: INFO: Created: latency-svc-5h624
Aug 13 01:39:07.569: INFO: Got endpoints: latency-svc-r7cqp [317.696821ms]
Aug 13 01:39:07.615: INFO: Created: latency-svc-qmtxd
Aug 13 01:39:07.619: INFO: Got endpoints: latency-svc-ssdsp [325.885406ms]
Aug 13 01:39:07.672: INFO: Created: latency-svc-rqnmq
Aug 13 01:39:07.672: INFO: Got endpoints: latency-svc-z77fq [377.257382ms]
Aug 13 01:39:07.715: INFO: Created: latency-svc-htcbb
Aug 13 01:39:07.719: INFO: Got endpoints: latency-svc-wzqls [419.513306ms]
Aug 13 01:39:07.766: INFO: Created: latency-svc-bk25v
Aug 13 01:39:07.769: INFO: Got endpoints: latency-svc-64fd6 [460.182407ms]
Aug 13 01:39:07.813: INFO: Created: latency-svc-rsh7w
Aug 13 01:39:07.819: INFO: Got endpoints: latency-svc-dldh4 [509.807903ms]
Aug 13 01:39:07.863: INFO: Created: latency-svc-4h9cw
Aug 13 01:39:07.869: INFO: Got endpoints: latency-svc-bg8l8 [552.510103ms]
Aug 13 01:39:07.913: INFO: Created: latency-svc-5jfrk
Aug 13 01:39:07.919: INFO: Got endpoints: latency-svc-pvj62 [596.657679ms]
Aug 13 01:39:07.963: INFO: Created: latency-svc-cqt9g
Aug 13 01:39:07.969: INFO: Got endpoints: latency-svc-r4zkp [641.313005ms]
Aug 13 01:39:08.013: INFO: Created: latency-svc-v6rws
Aug 13 01:39:08.019: INFO: Got endpoints: latency-svc-mg9lq [685.696514ms]
Aug 13 01:39:08.064: INFO: Created: latency-svc-dbt2x
Aug 13 01:39:08.069: INFO: Got endpoints: latency-svc-cxb6c [718.144582ms]
Aug 13 01:39:08.122: INFO: Created: latency-svc-sh4qx
Aug 13 01:39:08.122: INFO: Got endpoints: latency-svc-m9hzv [748.322476ms]
Aug 13 01:39:08.163: INFO: Created: latency-svc-69s6k
Aug 13 01:39:08.169: INFO: Got endpoints: latency-svc-wm95n [749.912359ms]
Aug 13 01:39:08.216: INFO: Created: latency-svc-976kp
Aug 13 01:39:08.219: INFO: Got endpoints: latency-svc-5h624 [748.566002ms]
Aug 13 01:39:08.262: INFO: Created: latency-svc-jhctr
Aug 13 01:39:08.269: INFO: Got endpoints: latency-svc-qmtxd [750.246449ms]
Aug 13 01:39:08.313: INFO: Created: latency-svc-cbwhv
Aug 13 01:39:08.319: INFO: Got endpoints: latency-svc-rqnmq [750.107184ms]
Aug 13 01:39:08.363: INFO: Created: latency-svc-9btf9
Aug 13 01:39:08.369: INFO: Got endpoints: latency-svc-htcbb [750.077546ms]
Aug 13 01:39:08.413: INFO: Created: latency-svc-gbwjd
Aug 13 01:39:08.419: INFO: Got endpoints: latency-svc-bk25v [747.167505ms]
Aug 13 01:39:08.464: INFO: Created: latency-svc-wkb84
Aug 13 01:39:08.469: INFO: Got endpoints: latency-svc-rsh7w [749.681776ms]
Aug 13 01:39:08.514: INFO: Created: latency-svc-tps84
Aug 13 01:39:08.519: INFO: Got endpoints: latency-svc-4h9cw [750.244945ms]
Aug 13 01:39:08.564: INFO: Created: latency-svc-4gl7r
Aug 13 01:39:08.569: INFO: Got endpoints: latency-svc-5jfrk [749.992901ms]
Aug 13 01:39:08.617: INFO: Created: latency-svc-bqcpb
Aug 13 01:39:08.619: INFO: Got endpoints: latency-svc-cqt9g [749.959834ms]
Aug 13 01:39:08.663: INFO: Created: latency-svc-jrgpc
Aug 13 01:39:08.669: INFO: Got endpoints: latency-svc-v6rws [750.128672ms]
Aug 13 01:39:08.714: INFO: Created: latency-svc-87nk9
Aug 13 01:39:08.719: INFO: Got endpoints: latency-svc-dbt2x [749.836891ms]
Aug 13 01:39:08.763: INFO: Created: latency-svc-plcp8
Aug 13 01:39:08.769: INFO: Got endpoints: latency-svc-sh4qx [750.031764ms]
Aug 13 01:39:08.822: INFO: Created: latency-svc-vgjnk
Aug 13 01:39:08.822: INFO: Got endpoints: latency-svc-69s6k [752.669689ms]
Aug 13 01:39:08.865: INFO: Created: latency-svc-pf84k
Aug 13 01:39:08.869: INFO: Got endpoints: latency-svc-976kp [746.910133ms]
Aug 13 01:39:08.916: INFO: Created: latency-svc-jjbvj
Aug 13 01:39:08.923: INFO: Got endpoints: latency-svc-jhctr [753.823976ms]
Aug 13 01:39:08.963: INFO: Created: latency-svc-jfn45
Aug 13 01:39:08.969: INFO: Got endpoints: latency-svc-cbwhv [749.948706ms]
Aug 13 01:39:09.017: INFO: Created: latency-svc-crl7h
Aug 13 01:39:09.019: INFO: Got endpoints: latency-svc-9btf9 [749.50504ms]
Aug 13 01:39:09.063: INFO: Created: latency-svc-qcvq8
Aug 13 01:39:09.069: INFO: Got endpoints: latency-svc-gbwjd [750.055394ms]
Aug 13 01:39:09.113: INFO: Created: latency-svc-rn6x4
Aug 13 01:39:09.119: INFO: Got endpoints: latency-svc-wkb84 [749.872376ms]
Aug 13 01:39:09.164: INFO: Created: latency-svc-vmcgs
Aug 13 01:39:09.169: INFO: Got endpoints: latency-svc-tps84 [749.663764ms]
Aug 13 01:39:09.222: INFO: Created: latency-svc-vrbg4
Aug 13 01:39:09.222: INFO: Got endpoints: latency-svc-4gl7r [752.997755ms]
Aug 13 01:39:09.265: INFO: Created: latency-svc-t6m64
Aug 13 01:39:09.269: INFO: Got endpoints: latency-svc-bqcpb [749.757683ms]
Aug 13 01:39:09.320: INFO: Got endpoints: latency-svc-jrgpc [750.816496ms]
Aug 13 01:39:09.321: INFO: Created: latency-svc-flbl7
Aug 13 01:39:09.364: INFO: Created: latency-svc-7w6bf
Aug 13 01:39:09.369: INFO: Got endpoints: latency-svc-87nk9 [750.039318ms]
Aug 13 01:39:09.414: INFO: Created: latency-svc-j2qfs
Aug 13 01:39:09.419: INFO: Got endpoints: latency-svc-plcp8 [749.820168ms]
Aug 13 01:39:09.463: INFO: Created: latency-svc-slv94
Aug 13 01:39:09.469: INFO: Got endpoints: latency-svc-vgjnk [749.920748ms]
Aug 13 01:39:09.513: INFO: Created: latency-svc-jp6hq
Aug 13 01:39:09.519: INFO: Got endpoints: latency-svc-pf84k [749.949198ms]
Aug 13 01:39:09.565: INFO: Created: latency-svc-p5xkg
Aug 13 01:39:09.569: INFO: Got endpoints: latency-svc-jjbvj [746.984532ms]
Aug 13 01:39:09.614: INFO: Created: latency-svc-6gsk5
Aug 13 01:39:09.619: INFO: Got endpoints: latency-svc-jfn45 [749.921434ms]
Aug 13 01:39:09.662: INFO: Created: latency-svc-hm4nv
Aug 13 01:39:09.669: INFO: Got endpoints: latency-svc-crl7h [746.072208ms]
Aug 13 01:39:09.713: INFO: Created: latency-svc-klmf7
Aug 13 01:39:09.719: INFO: Got endpoints: latency-svc-qcvq8 [749.793606ms]
Aug 13 01:39:09.766: INFO: Created: latency-svc-6tgm2
Aug 13 01:39:09.769: INFO: Got endpoints: latency-svc-rn6x4 [750.121557ms]
Aug 13 01:39:09.813: INFO: Created: latency-svc-wsp8h
Aug 13 01:39:09.819: INFO: Got endpoints: latency-svc-vmcgs [749.828319ms]
Aug 13 01:39:09.868: INFO: Created: latency-svc-bwktx
Aug 13 01:39:09.869: INFO: Got endpoints: latency-svc-vrbg4 [749.659211ms]
Aug 13 01:39:09.913: INFO: Created: latency-svc-52rdd
Aug 13 01:39:09.919: INFO: Got endpoints: latency-svc-t6m64 [749.995568ms]
Aug 13 01:39:10.039: INFO: Created: latency-svc-jswh9
Aug 13 01:39:10.039: INFO: Got endpoints: latency-svc-flbl7 [816.516612ms]
Aug 13 01:39:10.039: INFO: Got endpoints: latency-svc-7w6bf [770.203929ms]
Aug 13 01:39:10.044: INFO: Created: latency-svc-tl89m
Aug 13 01:39:10.069: INFO: Got endpoints: latency-svc-j2qfs [748.861417ms]
Aug 13 01:39:10.119: INFO: Got endpoints: latency-svc-slv94 [749.980634ms]
Aug 13 01:39:10.142: INFO: Created: latency-svc-6pv7z
Aug 13 01:39:10.146: INFO: Created: latency-svc-jtnhq
Aug 13 01:39:10.164: INFO: Created: latency-svc-w25pc
Aug 13 01:39:10.169: INFO: Got endpoints: latency-svc-jp6hq [749.777572ms]
Aug 13 01:39:10.213: INFO: Created: latency-svc-lmpkf
Aug 13 01:39:10.219: INFO: Got endpoints: latency-svc-p5xkg [749.816623ms]
Aug 13 01:39:10.263: INFO: Created: latency-svc-px98w
Aug 13 01:39:10.269: INFO: Got endpoints: latency-svc-6gsk5 [749.754834ms]
Aug 13 01:39:10.313: INFO: Created: latency-svc-5825l
Aug 13 01:39:10.319: INFO: Got endpoints: latency-svc-hm4nv [749.973737ms]
Aug 13 01:39:10.363: INFO: Created: latency-svc-jsl2g
Aug 13 01:39:10.369: INFO: Got endpoints: latency-svc-klmf7 [749.755532ms]
Aug 13 01:39:10.417: INFO: Created: latency-svc-7qlqc
Aug 13 01:39:10.419: INFO: Got endpoints: latency-svc-6tgm2 [749.864811ms]
Aug 13 01:39:10.463: INFO: Created: latency-svc-rdmcf
Aug 13 01:39:10.469: INFO: Got endpoints: latency-svc-wsp8h [750.185554ms]
Aug 13 01:39:10.512: INFO: Created: latency-svc-m9jrg
Aug 13 01:39:10.519: INFO: Got endpoints: latency-svc-bwktx [749.934819ms]
Aug 13 01:39:10.563: INFO: Created: latency-svc-hstjf
Aug 13 01:39:10.569: INFO: Got endpoints: latency-svc-52rdd [750.077679ms]
Aug 13 01:39:10.613: INFO: Created: latency-svc-ckw4m
Aug 13 01:39:10.619: INFO: Got endpoints: latency-svc-jswh9 [750.110206ms]
Aug 13 01:39:10.664: INFO: Created: latency-svc-dxqrt
Aug 13 01:39:10.669: INFO: Got endpoints: latency-svc-tl89m [750.090563ms]
Aug 13 01:39:10.713: INFO: Created: latency-svc-5z992
Aug 13 01:39:10.719: INFO: Got endpoints: latency-svc-6pv7z [680.44953ms]
Aug 13 01:39:10.763: INFO: Created: latency-svc-5x5qq
Aug 13 01:39:10.769: INFO: Got endpoints: latency-svc-jtnhq [729.524288ms]
Aug 13 01:39:10.814: INFO: Created: latency-svc-6wv5v
Aug 13 01:39:10.819: INFO: Got endpoints: latency-svc-w25pc [749.692387ms]
Aug 13 01:39:10.863: INFO: Created: latency-svc-kcw7t
Aug 13 01:39:10.869: INFO: Got endpoints: latency-svc-lmpkf [749.803232ms]
Aug 13 01:39:10.915: INFO: Created: latency-svc-6wn2v
Aug 13 01:39:10.919: INFO: Got endpoints: latency-svc-px98w [749.762874ms]
Aug 13 01:39:10.963: INFO: Created: latency-svc-lszfd
Aug 13 01:39:10.969: INFO: Got endpoints: latency-svc-5825l [750.189807ms]
Aug 13 01:39:11.012: INFO: Created: latency-svc-rqfdf
Aug 13 01:39:11.019: INFO: Got endpoints: latency-svc-jsl2g [749.765544ms]
Aug 13 01:39:11.063: INFO: Created: latency-svc-j7q5t
Aug 13 01:39:11.069: INFO: Got endpoints: latency-svc-7qlqc [750.016416ms]
Aug 13 01:39:11.114: INFO: Created: latency-svc-77ph4
Aug 13 01:39:11.124: INFO: Got endpoints: latency-svc-rdmcf [754.649251ms]
Aug 13 01:39:11.163: INFO: Created: latency-svc-fdqcg
Aug 13 01:39:11.169: INFO: Got endpoints: latency-svc-m9jrg [750.030681ms]
Aug 13 01:39:11.218: INFO: Created: latency-svc-v9flh
Aug 13 01:39:11.219: INFO: Got endpoints: latency-svc-hstjf [749.687866ms]
Aug 13 01:39:11.267: INFO: Created: latency-svc-ctdp7
Aug 13 01:39:11.269: INFO: Got endpoints: latency-svc-ckw4m [749.809763ms]
Aug 13 01:39:11.314: INFO: Created: latency-svc-66xrx
Aug 13 01:39:11.319: INFO: Got endpoints: latency-svc-dxqrt [749.600904ms]
Aug 13 01:39:11.363: INFO: Created: latency-svc-n7dsr
Aug 13 01:39:11.369: INFO: Got endpoints: latency-svc-5z992 [750.001588ms]
Aug 13 01:39:11.413: INFO: Created: latency-svc-lslxk
Aug 13 01:39:11.419: INFO: Got endpoints: latency-svc-5x5qq [749.841987ms]
Aug 13 01:39:11.467: INFO: Created: latency-svc-p6lrm
Aug 13 01:39:11.469: INFO: Got endpoints: latency-svc-6wv5v [749.712103ms]
Aug 13 01:39:11.513: INFO: Created: latency-svc-mg596
Aug 13 01:39:11.519: INFO: Got endpoints: latency-svc-kcw7t [749.85668ms]
Aug 13 01:39:11.562: INFO: Created: latency-svc-kws7h
Aug 13 01:39:11.569: INFO: Got endpoints: latency-svc-6wn2v [749.929153ms]
Aug 13 01:39:11.613: INFO: Created: latency-svc-b429r
Aug 13 01:39:11.619: INFO: Got endpoints: latency-svc-lszfd [750.054991ms]
Aug 13 01:39:11.663: INFO: Created: latency-svc-mrbgd
Aug 13 01:39:11.669: INFO: Got endpoints: latency-svc-rqfdf [750.419925ms]
Aug 13 01:39:11.714: INFO: Created: latency-svc-9jtxf
Aug 13 01:39:11.719: INFO: Got endpoints: latency-svc-j7q5t [749.817126ms]
Aug 13 01:39:11.763: INFO: Created: latency-svc-8pb7s
Aug 13 01:39:11.769: INFO: Got endpoints: latency-svc-77ph4 [750.360016ms]
Aug 13 01:39:11.813: INFO: Created: latency-svc-gdzsh
Aug 13 01:39:11.819: INFO: Got endpoints: latency-svc-fdqcg [749.805412ms]
Aug 13 01:39:11.863: INFO: Created: latency-svc-vl9jk
Aug 13 01:39:11.869: INFO: Got endpoints: latency-svc-v9flh [745.447573ms]
Aug 13 01:39:11.913: INFO: Created: latency-svc-7pgtr
Aug 13 01:39:11.919: INFO: Got endpoints: latency-svc-ctdp7 [750.138014ms]
Aug 13 01:39:11.967: INFO: Created: latency-svc-gk549
Aug 13 01:39:11.969: INFO: Got endpoints: latency-svc-66xrx [749.88873ms]
Aug 13 01:39:12.014: INFO: Created: latency-svc-4v5x8
Aug 13 01:39:12.019: INFO: Got endpoints: latency-svc-n7dsr [750.139711ms]
Aug 13 01:39:12.068: INFO: Created: latency-svc-n9rfc
Aug 13 01:39:12.070: INFO: Got endpoints: latency-svc-lslxk [750.378903ms]
Aug 13 01:39:12.114: INFO: Created: latency-svc-cv56h
Aug 13 01:39:12.120: INFO: Got endpoints: latency-svc-p6lrm [750.162661ms]
Aug 13 01:39:12.165: INFO: Created: latency-svc-jjlfn
Aug 13 01:39:12.173: INFO: Got endpoints: latency-svc-mg596 [753.372741ms]
Aug 13 01:39:12.214: INFO: Created: latency-svc-z79p4
Aug 13 01:39:12.219: INFO: Got endpoints: latency-svc-kws7h [749.938383ms]
Aug 13 01:39:12.269: INFO: Created: latency-svc-bvb6f
Aug 13 01:39:12.269: INFO: Got endpoints: latency-svc-b429r [750.206721ms]
Aug 13 01:39:12.313: INFO: Created: latency-svc-r4jlk
Aug 13 01:39:12.319: INFO: Got endpoints: latency-svc-mrbgd [750.125678ms]
Aug 13 01:39:12.363: INFO: Created: latency-svc-l7qsg
Aug 13 01:39:12.369: INFO: Got endpoints: latency-svc-9jtxf [749.700055ms]
Aug 13 01:39:12.413: INFO: Created: latency-svc-c6qd6
Aug 13 01:39:12.419: INFO: Got endpoints: latency-svc-8pb7s [749.682554ms]
Aug 13 01:39:12.463: INFO: Created: latency-svc-96qnp
Aug 13 01:39:12.469: INFO: Got endpoints: latency-svc-gdzsh [750.258147ms]
Aug 13 01:39:12.514: INFO: Created: latency-svc-zqnnn
Aug 13 01:39:12.520: INFO: Got endpoints: latency-svc-vl9jk [749.995987ms]
Aug 13 01:39:12.564: INFO: Created: latency-svc-rwwk8
Aug 13 01:39:12.570: INFO: Got endpoints: latency-svc-7pgtr [750.460342ms]
Aug 13 01:39:12.614: INFO: Created: latency-svc-g7mg7
Aug 13 01:39:12.619: INFO: Got endpoints: latency-svc-gk549 [750.138952ms]
Aug 13 01:39:12.663: INFO: Created: latency-svc-l8vfw
Aug 13 01:39:12.669: INFO: Got endpoints: latency-svc-4v5x8 [749.679706ms]
Aug 13 01:39:12.714: INFO: Created: latency-svc-dgbqq
Aug 13 01:39:12.719: INFO: Got endpoints: latency-svc-n9rfc [749.944774ms]
Aug 13 01:39:12.764: INFO: Created: latency-svc-6f7xl
Aug 13 01:39:12.769: INFO: Got endpoints: latency-svc-cv56h [749.737787ms]
Aug 13 01:39:12.813: INFO: Created: latency-svc-srrgd
Aug 13 01:39:12.819: INFO: Got endpoints: latency-svc-jjlfn [749.724963ms]
Aug 13 01:39:12.863: INFO: Created: latency-svc-px6bw
Aug 13 01:39:12.869: INFO: Got endpoints: latency-svc-z79p4 [749.66999ms]
Aug 13 01:39:12.915: INFO: Created: latency-svc-lbfnk
Aug 13 01:39:12.921: INFO: Got endpoints: latency-svc-bvb6f [748.05585ms]
Aug 13 01:39:12.964: INFO: Created: latency-svc-zf6p9
Aug 13 01:39:12.969: INFO: Got endpoints: latency-svc-r4jlk [750.183625ms]
Aug 13 01:39:13.016: INFO: Created: latency-svc-2m4qv
Aug 13 01:39:13.020: INFO: Got endpoints: latency-svc-l7qsg [750.298475ms]
Aug 13 01:39:13.064: INFO: Created: latency-svc-68l8h
Aug 13 01:39:13.069: INFO: Got endpoints: latency-svc-c6qd6 [749.953974ms]
Aug 13 01:39:13.115: INFO: Created: latency-svc-jr8xt
Aug 13 01:39:13.119: INFO: Got endpoints: latency-svc-96qnp [750.027545ms]
Aug 13 01:39:13.164: INFO: Created: latency-svc-9h4hw
Aug 13 01:39:13.169: INFO: Got endpoints: latency-svc-zqnnn [749.976587ms]
Aug 13 01:39:13.222: INFO: Got endpoints: latency-svc-rwwk8 [752.694402ms]
Aug 13 01:39:13.223: INFO: Created: latency-svc-5tnk2
Aug 13 01:39:13.263: INFO: Created: latency-svc-9n7zg
Aug 13 01:39:13.270: INFO: Got endpoints: latency-svc-g7mg7 [750.854308ms]
Aug 13 01:39:13.318: INFO: Created: latency-svc-txzhb
Aug 13 01:39:13.319: INFO: Got endpoints: latency-svc-l8vfw [749.537323ms]
Aug 13 01:39:13.365: INFO: Created: latency-svc-5k8d8
Aug 13 01:39:13.369: INFO: Got endpoints: latency-svc-dgbqq [749.804163ms]
Aug 13 01:39:13.414: INFO: Created: latency-svc-pn2jn
Aug 13 01:39:13.419: INFO: Got endpoints: latency-svc-6f7xl [749.939865ms]
Aug 13 01:39:13.463: INFO: Created: latency-svc-8hmzl
Aug 13 01:39:13.469: INFO: Got endpoints: latency-svc-srrgd [750.082247ms]
Aug 13 01:39:13.513: INFO: Created: latency-svc-tddgq
Aug 13 01:39:13.519: INFO: Got endpoints: latency-svc-px6bw [749.915906ms]
Aug 13 01:39:13.563: INFO: Created: latency-svc-zmwqk
Aug 13 01:39:13.569: INFO: Got endpoints: latency-svc-lbfnk [749.602461ms]
Aug 13 01:39:13.613: INFO: Created: latency-svc-hwd64
Aug 13 01:39:13.630: INFO: Got endpoints: latency-svc-zf6p9 [760.20401ms]
Aug 13 01:39:13.664: INFO: Created: latency-svc-97sk7
Aug 13 01:39:13.669: INFO: Got endpoints: latency-svc-2m4qv [748.473312ms]
Aug 13 01:39:13.719: INFO: Got endpoints: latency-svc-68l8h [749.611686ms]
Aug 13 01:39:13.725: INFO: Created: latency-svc-l7p8s
Aug 13 01:39:13.764: INFO: Created: latency-svc-69gff
Aug 13 01:39:13.769: INFO: Got endpoints: latency-svc-jr8xt [749.625855ms]
Aug 13 01:39:13.814: INFO: Created: latency-svc-qmplm
Aug 13 01:39:13.819: INFO: Got endpoints: latency-svc-9h4hw [749.545822ms]
Aug 13 01:39:13.863: INFO: Created: latency-svc-q2tdc
Aug 13 01:39:13.869: INFO: Got endpoints: latency-svc-5tnk2 [750.071168ms]
Aug 13 01:39:13.913: INFO: Created: latency-svc-5qjfd
Aug 13 01:39:13.924: INFO: Got endpoints: latency-svc-9n7zg [754.351392ms]
Aug 13 01:39:13.963: INFO: Created: latency-svc-d9wl8
Aug 13 01:39:13.969: INFO: Got endpoints: latency-svc-txzhb [746.864442ms]
Aug 13 01:39:14.019: INFO: Created: latency-svc-kgcmb
Aug 13 01:39:14.019: INFO: Got endpoints: latency-svc-5k8d8 [748.52318ms]
Aug 13 01:39:14.067: INFO: Created: latency-svc-6chm4
Aug 13 01:39:14.069: INFO: Got endpoints: latency-svc-pn2jn [749.687399ms]
Aug 13 01:39:14.116: INFO: Created: latency-svc-z82ch
Aug 13 01:39:14.119: INFO: Got endpoints: latency-svc-8hmzl [749.685794ms]
Aug 13 01:39:14.163: INFO: Created: latency-svc-rtbfs
Aug 13 01:39:14.169: INFO: Got endpoints: latency-svc-tddgq [749.965514ms]
Aug 13 01:39:14.214: INFO: Created: latency-svc-skqpp
Aug 13 01:39:14.219: INFO: Got endpoints: latency-svc-zmwqk [749.624467ms]
Aug 13 01:39:14.263: INFO: Created: latency-svc-mbhd8
Aug 13 01:39:14.273: INFO: Got endpoints: latency-svc-hwd64 [753.252275ms]
Aug 13 01:39:14.313: INFO: Created: latency-svc-lj5cg
Aug 13 01:39:14.319: INFO: Got endpoints: latency-svc-97sk7 [749.99427ms]
Aug 13 01:39:14.380: INFO: Created: latency-svc-dchbs
Aug 13 01:39:14.380: INFO: Got endpoints: latency-svc-l7p8s [750.703777ms]
Aug 13 01:39:14.413: INFO: Created: latency-svc-p82s7
Aug 13 01:39:14.419: INFO: Got endpoints: latency-svc-69gff [749.723888ms]
Aug 13 01:39:14.469: INFO: Got endpoints: latency-svc-qmplm [749.767419ms]
Aug 13 01:39:14.474: INFO: Created: latency-svc-wkhsx
Aug 13 01:39:14.514: INFO: Created: latency-svc-tbvk9
Aug 13 01:39:14.519: INFO: Got endpoints: latency-svc-q2tdc [749.839997ms]
Aug 13 01:39:14.563: INFO: Created: latency-svc-gq588
Aug 13 01:39:14.569: INFO: Got endpoints: latency-svc-5qjfd [750.173056ms]
Aug 13 01:39:14.613: INFO: Created: latency-svc-wh25t
Aug 13 01:39:14.619: INFO: Got endpoints: latency-svc-d9wl8 [749.699054ms]
Aug 13 01:39:14.663: INFO: Created: latency-svc-ft6kk
Aug 13 01:39:14.669: INFO: Got endpoints: latency-svc-kgcmb [745.136217ms]
Aug 13 01:39:14.713: INFO: Created: latency-svc-hrj6g
Aug 13 01:39:14.719: INFO: Got endpoints: latency-svc-6chm4 [749.586777ms]
Aug 13 01:39:14.763: INFO: Created: latency-svc-dj6vd
Aug 13 01:39:14.769: INFO: Got endpoints: latency-svc-z82ch [750.124594ms]
Aug 13 01:39:14.814: INFO: Created: latency-svc-xgndg
Aug 13 01:39:14.819: INFO: Got endpoints: latency-svc-rtbfs [750.097608ms]
Aug 13 01:39:14.864: INFO: Created: latency-svc-44gzz
Aug 13 01:39:14.869: INFO: Got endpoints: latency-svc-skqpp [749.997578ms]
Aug 13 01:39:14.919: INFO: Got endpoints: latency-svc-mbhd8 [750.211237ms]
Aug 13 01:39:14.970: INFO: Got endpoints: latency-svc-lj5cg [750.452474ms]
Aug 13 01:39:15.020: INFO: Got endpoints: latency-svc-dchbs [747.815473ms]
Aug 13 01:39:15.069: INFO: Got endpoints: latency-svc-p82s7 [749.997607ms]
Aug 13 01:39:15.120: INFO: Got endpoints: latency-svc-wkhsx [739.12466ms]
Aug 13 01:39:15.172: INFO: Got endpoints: latency-svc-tbvk9 [752.531765ms]
Aug 13 01:39:15.219: INFO: Got endpoints: latency-svc-gq588 [750.250009ms]
Aug 13 01:39:15.270: INFO: Got endpoints: latency-svc-wh25t [750.141196ms]
Aug 13 01:39:15.320: INFO: Got endpoints: latency-svc-ft6kk [750.408394ms]
Aug 13 01:39:15.369: INFO: Got endpoints: latency-svc-hrj6g [750.215397ms]
Aug 13 01:39:15.420: INFO: Got endpoints: latency-svc-dj6vd [750.457235ms]
Aug 13 01:39:15.469: INFO: Got endpoints: latency-svc-xgndg [750.246919ms]
Aug 13 01:39:15.521: INFO: Got endpoints: latency-svc-44gzz [751.443072ms]
Aug 13 01:39:15.521: INFO: Latencies: [94.996441ms 97.354305ms 102.172679ms 102.252287ms 103.224069ms 103.52414ms 103.663692ms 105.848751ms 107.841985ms 108.611013ms 108.803711ms 116.668121ms 117.241003ms 130.785394ms 131.923305ms 132.690415ms 142.625761ms 143.630013ms 145.904315ms 146.511548ms 147.92284ms 149.405378ms 150.460833ms 151.75057ms 154.189165ms 184.065073ms 188.647904ms 189.748631ms 190.429827ms 195.434806ms 206.40388ms 208.128126ms 212.747273ms 217.974609ms 222.075715ms 226.999224ms 229.974204ms 232.697956ms 243.018954ms 272.977467ms 317.696821ms 325.885406ms 377.257382ms 419.513306ms 460.182407ms 509.807903ms 552.510103ms 596.657679ms 641.313005ms 680.44953ms 685.696514ms 718.144582ms 729.524288ms 739.12466ms 745.136217ms 745.447573ms 746.072208ms 746.864442ms 746.910133ms 746.984532ms 747.167505ms 747.815473ms 748.05585ms 748.322476ms 748.473312ms 748.52318ms 748.566002ms 748.861417ms 749.50504ms 749.537323ms 749.545822ms 749.586777ms 749.600904ms 749.602461ms 749.611686ms 749.624467ms 749.625855ms 749.659211ms 749.663764ms 749.66999ms 749.679706ms 749.681776ms 749.682554ms 749.685794ms 749.687399ms 749.687866ms 749.692387ms 749.699054ms 749.700055ms 749.712103ms 749.723888ms 749.724963ms 749.737787ms 749.754834ms 749.755532ms 749.757683ms 749.762874ms 749.765544ms 749.767419ms 749.777572ms 749.793606ms 749.803232ms 749.804163ms 749.805412ms 749.809763ms 749.816623ms 749.817126ms 749.820168ms 749.828319ms 749.836891ms 749.839997ms 749.841987ms 749.85668ms 749.864811ms 749.872376ms 749.88873ms 749.912359ms 749.915906ms 749.920748ms 749.921434ms 749.929153ms 749.934819ms 749.938383ms 749.939865ms 749.944774ms 749.948706ms 749.949198ms 749.953974ms 749.959834ms 749.965514ms 749.973737ms 749.976587ms 749.980634ms 749.992901ms 749.99427ms 749.995568ms 749.995987ms 749.997578ms 749.997607ms 750.001588ms 750.016416ms 750.027545ms 750.030681ms 750.031764ms 750.039318ms 750.054991ms 750.055394ms 750.071168ms 750.077546ms 750.077679ms 750.082247ms 750.090563ms 750.097608ms 750.107184ms 750.110206ms 750.121557ms 750.124594ms 750.125678ms 750.128672ms 750.138014ms 750.138952ms 750.139711ms 750.141196ms 750.162661ms 750.173056ms 750.183625ms 750.185554ms 750.189807ms 750.206721ms 750.211237ms 750.215397ms 750.244945ms 750.246449ms 750.246919ms 750.250009ms 750.258147ms 750.298475ms 750.360016ms 750.378903ms 750.408394ms 750.419925ms 750.452474ms 750.457235ms 750.460342ms 750.703777ms 750.816496ms 750.854308ms 751.443072ms 752.531765ms 752.669689ms 752.694402ms 752.997755ms 753.252275ms 753.372741ms 753.823976ms 754.351392ms 754.649251ms 760.20401ms 770.203929ms 816.516612ms]
Aug 13 01:39:15.522: INFO: 50 %ile: 749.793606ms
Aug 13 01:39:15.522: INFO: 90 %ile: 750.419925ms
Aug 13 01:39:15.522: INFO: 99 %ile: 770.203929ms
Aug 13 01:39:15.522: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:39:15.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4973" for this suite.
Aug 13 01:39:37.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:39:41.268: INFO: namespace svc-latency-4973 deletion completed in 25.656585983s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:39:41.269: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-2472
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 01:39:42.175: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2472" in namespace "subpath-1861" to be "success or failure"
Aug 13 01:39:42.264: INFO: Pod "pod-subpath-test-secret-2472": Phase="Pending", Reason="", readiness=false. Elapsed: 89.100381ms
Aug 13 01:39:44.354: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 2.178830437s
Aug 13 01:39:46.443: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 4.268122067s
Aug 13 01:39:48.533: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 6.358263455s
Aug 13 01:39:50.623: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 8.448110073s
Aug 13 01:39:52.713: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 10.538044383s
Aug 13 01:39:54.803: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 12.627894929s
Aug 13 01:39:56.892: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 14.717592954s
Aug 13 01:39:58.982: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 16.807265301s
Aug 13 01:40:01.072: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 18.897144918s
Aug 13 01:40:03.162: INFO: Pod "pod-subpath-test-secret-2472": Phase="Running", Reason="", readiness=true. Elapsed: 20.986785577s
Aug 13 01:40:05.251: INFO: Pod "pod-subpath-test-secret-2472": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.076593983s
STEP: Saw pod success
Aug 13 01:40:05.251: INFO: Pod "pod-subpath-test-secret-2472" satisfied condition "success or failure"
Aug 13 01:40:05.341: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-subpath-test-secret-2472 container test-container-subpath-secret-2472: <nil>
STEP: delete the pod
Aug 13 01:40:05.530: INFO: Waiting for pod pod-subpath-test-secret-2472 to disappear
Aug 13 01:40:05.619: INFO: Pod pod-subpath-test-secret-2472 no longer exists
STEP: Deleting pod pod-subpath-test-secret-2472
Aug 13 01:40:05.619: INFO: Deleting pod "pod-subpath-test-secret-2472" in namespace "subpath-1861"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:40:05.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1861" for this suite.
Aug 13 01:40:12.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:40:15.386: INFO: namespace subpath-1861 deletion completed in 9.585203605s
•SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:40:15.386: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:40:16.214: INFO: (0) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.587113ms)
Aug 13 01:40:16.305: INFO: (1) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.007586ms)
Aug 13 01:40:16.396: INFO: (2) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.18104ms)
Aug 13 01:40:16.488: INFO: (3) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.139938ms)
Aug 13 01:40:16.579: INFO: (4) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.959634ms)
Aug 13 01:40:16.670: INFO: (5) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.319429ms)
Aug 13 01:40:16.761: INFO: (6) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.923659ms)
Aug 13 01:40:16.852: INFO: (7) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.255924ms)
Aug 13 01:40:16.943: INFO: (8) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.995616ms)
Aug 13 01:40:17.035: INFO: (9) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.191796ms)
Aug 13 01:40:17.125: INFO: (10) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.743998ms)
Aug 13 01:40:17.218: INFO: (11) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.299247ms)
Aug 13 01:40:17.308: INFO: (12) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.64755ms)
Aug 13 01:40:17.399: INFO: (13) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.884421ms)
Aug 13 01:40:17.490: INFO: (14) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.920923ms)
Aug 13 01:40:17.581: INFO: (15) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.034173ms)
Aug 13 01:40:17.673: INFO: (16) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.055725ms)
Aug 13 01:40:17.763: INFO: (17) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 90.888091ms)
Aug 13 01:40:17.855: INFO: (18) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.484669ms)
Aug 13 01:40:17.946: INFO: (19) /api/v1/nodes/ip-10-250-2-100.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.193048ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:40:17.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6806" for this suite.
Aug 13 01:40:24.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:40:27.712: INFO: namespace proxy-6806 deletion completed in 9.67521737s
•SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:40:27.713: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a0c47b80-0191-4bd3-87a5-cc5e625f4f82
STEP: Creating a pod to test consume secrets
Aug 13 01:40:28.540: INFO: Waiting up to 5m0s for pod "pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208" in namespace "secrets-4746" to be "success or failure"
Aug 13 01:40:28.629: INFO: Pod "pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208": Phase="Pending", Reason="", readiness=false. Elapsed: 89.219925ms
Aug 13 01:40:30.719: INFO: Pod "pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179187469s
STEP: Saw pod success
Aug 13 01:40:30.719: INFO: Pod "pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208" satisfied condition "success or failure"
Aug 13 01:40:30.808: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 01:40:30.996: INFO: Waiting for pod pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208 to disappear
Aug 13 01:40:31.087: INFO: Pod pod-secrets-c498368e-2d04-45b7-a2ff-7fb015399208 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:40:31.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4746" for this suite.
Aug 13 01:40:37.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:40:40.851: INFO: namespace secrets-4746 deletion completed in 9.674556945s
•SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:40:40.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:40:43.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1377" for this suite.
Aug 13 01:41:34.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:41:37.685: INFO: namespace kubelet-test-1377 deletion completed in 53.654070591s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:41:37.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4451
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug 13 01:41:38.322: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix550304893/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:41:38.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4451" for this suite.
Aug 13 01:41:44.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:41:48.149: INFO: namespace kubectl-4451 deletion completed in 9.654919917s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:41:48.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7001
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-c72733b5-945c-443e-810a-6ac51b426def
STEP: Creating secret with name s-test-opt-upd-698baa7a-f0a5-4ffa-96c0-f363105a77ba
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c72733b5-945c-443e-810a-6ac51b426def
STEP: Updating secret s-test-opt-upd-698baa7a-f0a5-4ffa-96c0-f363105a77ba
STEP: Creating secret with name s-test-opt-create-7dc49479-0e25-4e63-b617-f1e8fdb1e990
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:41:54.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7001" for this suite.
Aug 13 01:42:16.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:42:20.086: INFO: namespace secrets-7001 deletion completed in 25.655170708s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:42:20.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4272
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 13 01:42:20.994: INFO: Found 1 stateful pods, waiting for 3
Aug 13 01:42:31.084: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:42:31.084: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:42:31.084: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:42:31.352: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4272 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 01:42:33.225: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 01:42:33.225: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 01:42:33.225: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 13 01:42:43.773: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 13 01:42:44.042: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4272 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:42:45.395: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 01:42:45.395: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 01:42:45.395: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 01:43:05.933: INFO: Waiting for StatefulSet statefulset-4272/ss2 to complete update
Aug 13 01:43:05.933: INFO: Waiting for Pod statefulset-4272/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Aug 13 01:43:16.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4272 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 01:43:17.445: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 01:43:17.445: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 01:43:17.445: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 01:43:27.992: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 13 01:43:28.260: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4272 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:43:29.602: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 01:43:29.602: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 01:43:29.602: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 01:43:40.138: INFO: Waiting for StatefulSet statefulset-4272/ss2 to complete update
Aug 13 01:43:40.138: INFO: Waiting for Pod statefulset-4272/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 13 01:43:50.317: INFO: Deleting all statefulset in ns statefulset-4272
Aug 13 01:43:50.407: INFO: Scaling statefulset ss2 to 0
Aug 13 01:44:00.766: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 01:44:00.855: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:44:01.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4272" for this suite.
Aug 13 01:44:07.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:44:10.885: INFO: namespace statefulset-4272 deletion completed in 9.671632273s
•SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:44:10.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8036
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:44:11.623: INFO: Waiting up to 5m0s for pod "downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e" in namespace "downward-api-8036" to be "success or failure"
Aug 13 01:44:11.712: INFO: Pod "downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.147207ms
Aug 13 01:44:13.801: INFO: Pod "downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178826359s
STEP: Saw pod success
Aug 13 01:44:13.802: INFO: Pod "downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e" satisfied condition "success or failure"
Aug 13 01:44:13.891: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e container client-container: <nil>
STEP: delete the pod
Aug 13 01:44:14.078: INFO: Waiting for pod downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e to disappear
Aug 13 01:44:14.167: INFO: Pod downwardapi-volume-705f86bd-c965-4658-8a47-070f5f532e6e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:44:14.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8036" for this suite.
Aug 13 01:44:20.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:44:23.912: INFO: namespace downward-api-8036 deletion completed in 9.655319966s
•SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:44:23.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:44:24.732: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 13 01:44:26.911: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 13 01:44:29.631: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-780,SelfLink:/apis/apps/v1/namespaces/deployment-780/deployments/test-cleanup-deployment,UID:210f79cc-5bbc-4ce0-8e2f-47e037f05957,ResourceVersion:18818,Generation:1,CreationTimestamp:2019-08-13 01:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-13 01:44:27 +0000 UTC 2019-08-13 01:44:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-13 01:44:28 +0000 UTC 2019-08-13 01:44:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 13 01:44:29.720: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-780,SelfLink:/apis/apps/v1/namespaces/deployment-780/replicasets/test-cleanup-deployment-55bbcbc84c,UID:351676be-11c2-4a87-915a-ce7603268910,ResourceVersion:18811,Generation:1,CreationTimestamp:2019-08-13 01:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 210f79cc-5bbc-4ce0-8e2f-47e037f05957 0xc002dad337 0xc002dad338}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 13 01:44:29.810: INFO: Pod "test-cleanup-deployment-55bbcbc84c-lb4gv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-lb4gv,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-780,SelfLink:/api/v1/namespaces/deployment-780/pods/test-cleanup-deployment-55bbcbc84c-lb4gv,UID:9bf33504-c262-4ae5-b091-a946aba42152,ResourceVersion:18810,Generation:0,CreationTimestamp:2019-08-13 01:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.218/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 351676be-11c2-4a87-915a-ce7603268910 0xc0037fac97 0xc0037fac98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8dmrh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8dmrh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8dmrh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037fad00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037fad20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:44:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:44:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:44:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:44:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.218,StartTime:2019-08-13 01:44:27 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-13 01:44:27 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://951ba4fc8027e631907c6a4eb03e237a77569b6b469679f2cb9575fa9bc1cf7f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:44:29.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-780" for this suite.
Aug 13 01:44:36.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:44:39.557: INFO: namespace deployment-780 deletion completed in 9.65699668s
•SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:44:39.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:44:56.554: INFO: Container started at 2019-08-13 01:44:41 +0000 UTC, pod became ready at 2019-08-13 01:44:56 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:44:56.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2413" for this suite.
Aug 13 01:45:18.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:45:22.301: INFO: namespace container-probe-2413 deletion completed in 25.65670513s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:45:22.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-e2fd590a-d13b-4ab3-a843-f27c8eb9f370
STEP: Creating a pod to test consume configMaps
Aug 13 01:45:23.118: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7" in namespace "projected-5479" to be "success or failure"
Aug 13 01:45:23.210: INFO: Pod "pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 92.581358ms
Aug 13 01:45:25.303: INFO: Pod "pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.185233132s
STEP: Saw pod success
Aug 13 01:45:25.303: INFO: Pod "pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7" satisfied condition "success or failure"
Aug 13 01:45:25.393: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:45:25.581: INFO: Waiting for pod pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7 to disappear
Aug 13 01:45:25.670: INFO: Pod pod-projected-configmaps-fc536593-4166-41ba-b9f3-fb20ce70a5d7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:45:25.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5479" for this suite.
Aug 13 01:45:32.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:45:35.415: INFO: namespace projected-5479 deletion completed in 9.655087275s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:45:35.416: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:45:36.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c" in namespace "downward-api-8771" to be "success or failure"
Aug 13 01:45:36.232: INFO: Pod "downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.146047ms
Aug 13 01:45:38.321: INFO: Pod "downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178447529s
STEP: Saw pod success
Aug 13 01:45:38.321: INFO: Pod "downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c" satisfied condition "success or failure"
Aug 13 01:45:38.411: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c container client-container: <nil>
STEP: delete the pod
Aug 13 01:45:38.600: INFO: Waiting for pod downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c to disappear
Aug 13 01:45:38.689: INFO: Pod downwardapi-volume-fccf28d0-d5ef-437f-a938-b820397fc75c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:45:38.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8771" for this suite.
Aug 13 01:45:45.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:45:48.433: INFO: namespace downward-api-8771 deletion completed in 9.654491651s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:45:48.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8010
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-1fc66d4d-50d9-4baa-95b0-44a9feebad34
STEP: Creating a pod to test consume configMaps
Aug 13 01:45:49.252: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8" in namespace "projected-8010" to be "success or failure"
Aug 13 01:45:49.342: INFO: Pod "pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.423774ms
Aug 13 01:45:51.432: INFO: Pod "pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179248625s
STEP: Saw pod success
Aug 13 01:45:51.432: INFO: Pod "pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8" satisfied condition "success or failure"
Aug 13 01:45:51.521: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 01:45:51.708: INFO: Waiting for pod pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8 to disappear
Aug 13 01:45:51.797: INFO: Pod pod-projected-configmaps-a0d1ffff-70c1-4f34-b27d-9d4e1560efa8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:45:51.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8010" for this suite.
Aug 13 01:45:58.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:46:01.549: INFO: namespace projected-8010 deletion completed in 9.661464767s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:46:01.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:46:02.278: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77" in namespace "projected-6802" to be "success or failure"
Aug 13 01:46:02.367: INFO: Pod "downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77": Phase="Pending", Reason="", readiness=false. Elapsed: 89.322187ms
Aug 13 01:46:04.457: INFO: Pod "downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178815957s
STEP: Saw pod success
Aug 13 01:46:04.457: INFO: Pod "downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77" satisfied condition "success or failure"
Aug 13 01:46:04.546: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77 container client-container: <nil>
STEP: delete the pod
Aug 13 01:46:04.733: INFO: Waiting for pod downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77 to disappear
Aug 13 01:46:04.822: INFO: Pod downwardapi-volume-ec6b3b45-2a5a-453f-945e-1c3b34edae77 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:46:04.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6802" for this suite.
Aug 13 01:46:11.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:46:14.567: INFO: namespace projected-6802 deletion completed in 9.655656748s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:46:14.567: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 13 01:46:15.202: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1029'
Aug 13 01:46:16.262: INFO: stderr: ""
Aug 13 01:46:16.262: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 13 01:46:17.352: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 01:46:17.352: INFO: Found 0 / 1
Aug 13 01:46:18.352: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 01:46:18.352: INFO: Found 1 / 1
Aug 13 01:46:18.352: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 13 01:46:18.441: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 01:46:18.441: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 13 01:46:18.442: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-xmmsp --namespace=kubectl-1029 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 13 01:46:18.984: INFO: stderr: ""
Aug 13 01:46:18.984: INFO: stdout: "pod/redis-master-xmmsp patched\n"
STEP: checking annotations
Aug 13 01:46:19.074: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 01:46:19.074: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:46:19.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1029" for this suite.
Aug 13 01:46:41.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:46:44.836: INFO: namespace kubectl-1029 deletion completed in 25.671720185s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:46:44.837: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 13 01:46:45.473: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6293'
Aug 13 01:46:46.492: INFO: stderr: ""
Aug 13 01:46:46.492: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 01:46:46.492: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6293'
Aug 13 01:46:46.962: INFO: stderr: ""
Aug 13 01:46:46.962: INFO: stdout: "update-demo-nautilus-lflvq update-demo-nautilus-svrhn "
Aug 13 01:46:46.963: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-lflvq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6293'
Aug 13 01:46:47.433: INFO: stderr: ""
Aug 13 01:46:47.433: INFO: stdout: ""
Aug 13 01:46:47.433: INFO: update-demo-nautilus-lflvq is created but not running
Aug 13 01:46:52.434: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6293'
Aug 13 01:46:52.883: INFO: stderr: ""
Aug 13 01:46:52.883: INFO: stdout: "update-demo-nautilus-lflvq update-demo-nautilus-svrhn "
Aug 13 01:46:52.883: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-lflvq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6293'
Aug 13 01:46:53.352: INFO: stderr: ""
Aug 13 01:46:53.352: INFO: stdout: "true"
Aug 13 01:46:53.352: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-lflvq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6293'
Aug 13 01:46:53.834: INFO: stderr: ""
Aug 13 01:46:53.834: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:46:53.834: INFO: validating pod update-demo-nautilus-lflvq
Aug 13 01:46:54.012: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:46:54.012: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:46:54.012: INFO: update-demo-nautilus-lflvq is verified up and running
Aug 13 01:46:54.012: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-svrhn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6293'
Aug 13 01:46:54.483: INFO: stderr: ""
Aug 13 01:46:54.483: INFO: stdout: "true"
Aug 13 01:46:54.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-svrhn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6293'
Aug 13 01:46:54.956: INFO: stderr: ""
Aug 13 01:46:54.956: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:46:54.956: INFO: validating pod update-demo-nautilus-svrhn
Aug 13 01:46:55.135: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:46:55.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:46:55.135: INFO: update-demo-nautilus-svrhn is verified up and running
STEP: using delete to clean up resources
Aug 13 01:46:55.136: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6293'
Aug 13 01:46:55.667: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:46:55.667: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 13 01:46:55.667: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6293'
Aug 13 01:46:56.223: INFO: stderr: "No resources found.\n"
Aug 13 01:46:56.223: INFO: stdout: ""
Aug 13 01:46:56.223: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-6293 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 01:46:56.695: INFO: stderr: ""
Aug 13 01:46:56.695: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:46:56.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6293" for this suite.
Aug 13 01:47:03.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:47:06.453: INFO: namespace kubectl-6293 deletion completed in 9.668583533s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:47:06.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-p7p7
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 01:47:07.366: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-p7p7" in namespace "subpath-9859" to be "success or failure"
Aug 13 01:47:07.455: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.053743ms
Aug 13 01:47:09.545: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 2.178639974s
Aug 13 01:47:11.635: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 4.268271294s
Aug 13 01:47:13.725: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 6.358620252s
Aug 13 01:47:15.815: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 8.449102048s
Aug 13 01:47:17.908: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 10.541420682s
Aug 13 01:47:19.997: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 12.630982573s
Aug 13 01:47:22.087: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 14.720843557s
Aug 13 01:47:24.177: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 16.81070117s
Aug 13 01:47:26.267: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 18.900359963s
Aug 13 01:47:28.357: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Running", Reason="", readiness=true. Elapsed: 20.990474321s
Aug 13 01:47:30.447: INFO: Pod "pod-subpath-test-downwardapi-p7p7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.080207032s
STEP: Saw pod success
Aug 13 01:47:30.447: INFO: Pod "pod-subpath-test-downwardapi-p7p7" satisfied condition "success or failure"
Aug 13 01:47:30.536: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-subpath-test-downwardapi-p7p7 container test-container-subpath-downwardapi-p7p7: <nil>
STEP: delete the pod
Aug 13 01:47:30.722: INFO: Waiting for pod pod-subpath-test-downwardapi-p7p7 to disappear
Aug 13 01:47:30.811: INFO: Pod pod-subpath-test-downwardapi-p7p7 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-p7p7
Aug 13 01:47:30.811: INFO: Deleting pod "pod-subpath-test-downwardapi-p7p7" in namespace "subpath-9859"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:47:30.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9859" for this suite.
Aug 13 01:47:37.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:47:40.646: INFO: namespace subpath-9859 deletion completed in 9.655925819s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:47:40.646: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1899.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1899.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 227.215.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.215.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.215.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.215.227_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1899.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1899.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1899.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1899.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1899.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 227.215.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.215.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.215.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.215.227_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 13 01:47:44.010: INFO: Unable to read wheezy_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:44.101: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:44.191: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:44.282: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:44.930: INFO: Unable to read jessie_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:45.021: INFO: Unable to read jessie_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:45.111: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:45.202: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:45.751: INFO: Lookups using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 failed for: [wheezy_udp@dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_udp@dns-test-service.dns-1899.svc.cluster.local jessie_tcp@dns-test-service.dns-1899.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local]

Aug 13 01:47:50.843: INFO: Unable to read wheezy_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:50.934: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:51.024: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:51.115: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:51.755: INFO: Unable to read jessie_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:51.846: INFO: Unable to read jessie_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:51.937: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:52.028: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:52.582: INFO: Lookups using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 failed for: [wheezy_udp@dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_udp@dns-test-service.dns-1899.svc.cluster.local jessie_tcp@dns-test-service.dns-1899.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local]

Aug 13 01:47:55.842: INFO: Unable to read wheezy_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:55.933: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:56.024: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:56.115: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:56.759: INFO: Unable to read jessie_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:56.850: INFO: Unable to read jessie_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:56.940: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:57.031: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:47:57.609: INFO: Lookups using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 failed for: [wheezy_udp@dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_udp@dns-test-service.dns-1899.svc.cluster.local jessie_tcp@dns-test-service.dns-1899.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local]

Aug 13 01:48:00.842: INFO: Unable to read wheezy_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:00.933: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:01.024: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:01.115: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:01.769: INFO: Unable to read jessie_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:01.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:01.952: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:02.043: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:02.592: INFO: Lookups using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 failed for: [wheezy_udp@dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_udp@dns-test-service.dns-1899.svc.cluster.local jessie_tcp@dns-test-service.dns-1899.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local]

Aug 13 01:48:05.842: INFO: Unable to read wheezy_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:05.933: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:06.024: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:06.115: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:06.760: INFO: Unable to read jessie_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:06.851: INFO: Unable to read jessie_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:06.942: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:07.033: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:07.582: INFO: Lookups using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 failed for: [wheezy_udp@dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_udp@dns-test-service.dns-1899.svc.cluster.local jessie_tcp@dns-test-service.dns-1899.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local]

Aug 13 01:48:10.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:10.936: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:11.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:11.117: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:11.758: INFO: Unable to read jessie_udp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:11.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:11.942: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:12.033: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local from pod dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9: the server could not find the requested resource (get pods dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9)
Aug 13 01:48:12.584: INFO: Lookups using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 failed for: [wheezy_udp@dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@dns-test-service.dns-1899.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_udp@dns-test-service.dns-1899.svc.cluster.local jessie_tcp@dns-test-service.dns-1899.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1899.svc.cluster.local]

Aug 13 01:48:17.587: INFO: DNS probes using dns-1899/dns-test-1cbb75d6-bbe0-415f-89cb-5596982d18b9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:48:17.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1899" for this suite.
Aug 13 01:48:24.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:48:27.614: INFO: namespace dns-1899 deletion completed in 9.653409316s
•SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:48:27.614: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2290
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug 13 01:48:28.341: INFO: Waiting up to 5m0s for pod "client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab" in namespace "containers-2290" to be "success or failure"
Aug 13 01:48:28.430: INFO: Pod "client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 88.816127ms
Aug 13 01:48:30.519: INFO: Pod "client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178143987s
STEP: Saw pod success
Aug 13 01:48:30.519: INFO: Pod "client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab" satisfied condition "success or failure"
Aug 13 01:48:30.608: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab container test-container: <nil>
STEP: delete the pod
Aug 13 01:48:30.820: INFO: Waiting for pod client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab to disappear
Aug 13 01:48:30.909: INFO: Pod client-containers-e9268a56-7f96-4bd9-8943-9bdc4301d0ab no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:48:30.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2290" for this suite.
Aug 13 01:48:37.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:48:40.657: INFO: namespace containers-2290 deletion completed in 9.658428815s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:48:40.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 13 01:48:41.410: INFO: Waiting up to 5m0s for pod "pod-303ed704-d8da-4d85-87e4-10c6fc3975fd" in namespace "emptydir-4733" to be "success or failure"
Aug 13 01:48:41.500: INFO: Pod "pod-303ed704-d8da-4d85-87e4-10c6fc3975fd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.653081ms
Aug 13 01:48:43.590: INFO: Pod "pod-303ed704-d8da-4d85-87e4-10c6fc3975fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179838009s
STEP: Saw pod success
Aug 13 01:48:43.590: INFO: Pod "pod-303ed704-d8da-4d85-87e4-10c6fc3975fd" satisfied condition "success or failure"
Aug 13 01:48:43.680: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-303ed704-d8da-4d85-87e4-10c6fc3975fd container test-container: <nil>
STEP: delete the pod
Aug 13 01:48:43.870: INFO: Waiting for pod pod-303ed704-d8da-4d85-87e4-10c6fc3975fd to disappear
Aug 13 01:48:43.959: INFO: Pod pod-303ed704-d8da-4d85-87e4-10c6fc3975fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:48:43.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4733" for this suite.
Aug 13 01:48:50.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:48:53.705: INFO: namespace emptydir-4733 deletion completed in 9.655758918s
•SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:48:53.705: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6515
STEP: Creating secret with name secret-test-3e58609f-3289-40d3-8769-7dd03cdc74d6
STEP: Creating a pod to test consume secrets
Aug 13 01:48:55.153: INFO: Waiting up to 5m0s for pod "pod-secrets-2b806316-78df-4e58-b1f5-45524742d165" in namespace "secrets-4193" to be "success or failure"
Aug 13 01:48:55.242: INFO: Pod "pod-secrets-2b806316-78df-4e58-b1f5-45524742d165": Phase="Pending", Reason="", readiness=false. Elapsed: 88.966324ms
Aug 13 01:48:57.332: INFO: Pod "pod-secrets-2b806316-78df-4e58-b1f5-45524742d165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178621058s
STEP: Saw pod success
Aug 13 01:48:57.332: INFO: Pod "pod-secrets-2b806316-78df-4e58-b1f5-45524742d165" satisfied condition "success or failure"
Aug 13 01:48:57.421: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-2b806316-78df-4e58-b1f5-45524742d165 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 01:48:57.611: INFO: Waiting for pod pod-secrets-2b806316-78df-4e58-b1f5-45524742d165 to disappear
Aug 13 01:48:57.700: INFO: Pod pod-secrets-2b806316-78df-4e58-b1f5-45524742d165 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:48:57.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4193" for this suite.
Aug 13 01:49:04.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:49:07.445: INFO: namespace secrets-4193 deletion completed in 9.655612105s
STEP: Destroying namespace "secret-namespace-6515" for this suite.
Aug 13 01:49:13.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:49:17.110: INFO: namespace secret-namespace-6515 deletion completed in 9.66530924s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:49:17.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:49:17.925: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 13 01:49:20.104: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 13 01:49:22.195: INFO: Creating deployment "test-rollover-deployment"
Aug 13 01:49:22.374: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 13 01:49:22.463: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 13 01:49:22.644: INFO: Ensure that both replica sets have 1 created replica
Aug 13 01:49:22.823: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 13 01:49:23.001: INFO: Updating deployment test-rollover-deployment
Aug 13 01:49:23.001: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 13 01:49:23.090: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 13 01:49:23.269: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 13 01:49:23.447: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 01:49:23.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257763, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:49:25.627: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 01:49:25.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257764, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:49:27.635: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 01:49:27.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257764, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:49:29.627: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 01:49:29.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257764, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:49:31.626: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 01:49:31.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257764, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:49:33.627: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 01:49:33.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257764, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701257762, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 01:49:35.627: INFO: 
Aug 13 01:49:35.627: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 13 01:49:35.895: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5549,SelfLink:/apis/apps/v1/namespaces/deployment-5549/deployments/test-rollover-deployment,UID:e07e0af9-b74c-4150-8ff3-98f8966a09b5,ResourceVersion:19925,Generation:2,CreationTimestamp:2019-08-13 01:49:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-13 01:49:22 +0000 UTC 2019-08-13 01:49:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-13 01:49:34 +0000 UTC 2019-08-13 01:49:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 13 01:49:35.985: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-5549,SelfLink:/apis/apps/v1/namespaces/deployment-5549/replicasets/test-rollover-deployment-854595fc44,UID:639d800a-4708-4782-951b-023d10e8e6bf,ResourceVersion:19918,Generation:2,CreationTimestamp:2019-08-13 01:49:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e07e0af9-b74c-4150-8ff3-98f8966a09b5 0xc001622d07 0xc001622d08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 13 01:49:35.985: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 13 01:49:35.985: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5549,SelfLink:/apis/apps/v1/namespaces/deployment-5549/replicasets/test-rollover-controller,UID:017caec9-6437-4c8c-9f26-2449d1675e5c,ResourceVersion:19924,Generation:2,CreationTimestamp:2019-08-13 01:49:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e07e0af9-b74c-4150-8ff3-98f8966a09b5 0xc001622c37 0xc001622c38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 01:49:35.985: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-5549,SelfLink:/apis/apps/v1/namespaces/deployment-5549/replicasets/test-rollover-deployment-9b8b997cf,UID:dc64cb25-30f2-4ef4-be6c-cd88e766d4c6,ResourceVersion:19890,Generation:2,CreationTimestamp:2019-08-13 01:49:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e07e0af9-b74c-4150-8ff3-98f8966a09b5 0xc001622dd0 0xc001622dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 01:49:36.075: INFO: Pod "test-rollover-deployment-854595fc44-5hvtt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-5hvtt,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-5549,SelfLink:/api/v1/namespaces/deployment-5549/pods/test-rollover-deployment-854595fc44-5hvtt,UID:01f11971-ffd4-4176-b869-d193fa09fe92,ResourceVersion:19894,Generation:0,CreationTimestamp:2019-08-13 01:49:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.234/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 639d800a-4708-4782-951b-023d10e8e6bf 0xc0016239e7 0xc0016239e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z7826 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z7826,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-z7826 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-2-100.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001623a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001623a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:49:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:49:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:49:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 01:49:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.2.100,PodIP:100.96.1.234,StartTime:2019-08-13 01:49:23 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-13 01:49:23 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://d289fcbeb9da41fd6cdbc1c2d3d4991e28aba0dcb9ada6aadf8c748ea4489ce5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:49:36.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5549" for this suite.
Aug 13 01:49:42.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:49:45.821: INFO: namespace deployment-5549 deletion completed in 9.65647732s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:49:45.822: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-18
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 13 01:49:46.548: INFO: Waiting up to 5m0s for pod "pod-bf341904-a27a-4b67-9637-f9e482f1f376" in namespace "emptydir-18" to be "success or failure"
Aug 13 01:49:46.637: INFO: Pod "pod-bf341904-a27a-4b67-9637-f9e482f1f376": Phase="Pending", Reason="", readiness=false. Elapsed: 88.965662ms
Aug 13 01:49:48.726: INFO: Pod "pod-bf341904-a27a-4b67-9637-f9e482f1f376": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178595212s
STEP: Saw pod success
Aug 13 01:49:48.726: INFO: Pod "pod-bf341904-a27a-4b67-9637-f9e482f1f376" satisfied condition "success or failure"
Aug 13 01:49:48.816: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-bf341904-a27a-4b67-9637-f9e482f1f376 container test-container: <nil>
STEP: delete the pod
Aug 13 01:49:49.004: INFO: Waiting for pod pod-bf341904-a27a-4b67-9637-f9e482f1f376 to disappear
Aug 13 01:49:49.093: INFO: Pod pod-bf341904-a27a-4b67-9637-f9e482f1f376 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:49:49.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-18" for this suite.
Aug 13 01:49:55.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:49:58.847: INFO: namespace emptydir-18 deletion completed in 9.665174361s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:49:58.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7281
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 13 01:49:59.750: INFO: Found 1 stateful pods, waiting for 3
Aug 13 01:50:09.842: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:50:09.842: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:50:09.842: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 13 01:50:10.301: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 13 01:50:10.675: INFO: Updating stateful set ss2
Aug 13 01:50:10.853: INFO: Waiting for Pod statefulset-7281/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 13 01:50:21.311: INFO: Found 2 stateful pods, waiting for 3
Aug 13 01:50:31.402: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:50:31.402: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:50:31.402: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 13 01:50:31.771: INFO: Updating stateful set ss2
Aug 13 01:50:31.949: INFO: Waiting for Pod statefulset-7281/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 13 01:50:42.318: INFO: Updating stateful set ss2
Aug 13 01:50:42.496: INFO: Waiting for StatefulSet statefulset-7281/ss2 to complete update
Aug 13 01:50:42.496: INFO: Waiting for Pod statefulset-7281/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 13 01:50:52.676: INFO: Deleting all statefulset in ns statefulset-7281
Aug 13 01:50:52.765: INFO: Scaling statefulset ss2 to 0
Aug 13 01:51:13.137: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 01:51:13.226: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:51:13.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7281" for this suite.
Aug 13 01:51:19.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:51:23.251: INFO: namespace statefulset-7281 deletion completed in 9.663164809s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:51:23.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:51:23.979: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6" in namespace "downward-api-4313" to be "success or failure"
Aug 13 01:51:24.068: INFO: Pod "downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.480327ms
Aug 13 01:51:26.158: INFO: Pod "downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179463336s
STEP: Saw pod success
Aug 13 01:51:26.158: INFO: Pod "downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6" satisfied condition "success or failure"
Aug 13 01:51:26.248: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6 container client-container: <nil>
STEP: delete the pod
Aug 13 01:51:26.437: INFO: Waiting for pod downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6 to disappear
Aug 13 01:51:26.526: INFO: Pod downwardapi-volume-e57a76e2-9f8b-4ea2-83ed-018cd36c72d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:51:26.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4313" for this suite.
Aug 13 01:51:32.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:51:36.274: INFO: namespace downward-api-4313 deletion completed in 9.65728518s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:51:36.275: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6370
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 13 01:51:39.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-4597607d-e415-4d3c-a4d2-1a6a32114b69 -c busybox-main-container --namespace=emptydir-6370 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 13 01:51:40.735: INFO: stderr: ""
Aug 13 01:51:40.735: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:51:40.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6370" for this suite.
Aug 13 01:51:47.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:51:50.480: INFO: namespace emptydir-6370 deletion completed in 9.654712607s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:51:50.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6388
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 01:51:51.126: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 01:52:10.654: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.244:8080/dial?request=hostName&protocol=http&host=100.96.0.47&port=8080&tries=1'] Namespace:pod-network-test-6388 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:52:10.654: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:52:11.589: INFO: Waiting for endpoints: map[]
Aug 13 01:52:11.679: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.244:8080/dial?request=hostName&protocol=http&host=100.96.1.243&port=8080&tries=1'] Namespace:pod-network-test-6388 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 01:52:11.679: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 13 01:52:12.504: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:52:12.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6388" for this suite.
Aug 13 01:52:34.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:52:38.250: INFO: namespace pod-network-test-6388 deletion completed in 25.655393817s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:52:38.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 13 01:52:42.015: INFO: Successfully updated pod "pod-update-0f7a8312-991d-4fa6-bc9b-5aa666f6909e"
STEP: verifying the updated pod is in kubernetes
Aug 13 01:52:42.195: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:52:42.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-152" for this suite.
Aug 13 01:53:04.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:53:07.942: INFO: namespace pods-152 deletion completed in 25.656812071s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:53:07.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:53:08.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a" in namespace "projected-226" to be "success or failure"
Aug 13 01:53:08.758: INFO: Pod "downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.316923ms
Aug 13 01:53:10.848: INFO: Pod "downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179366391s
STEP: Saw pod success
Aug 13 01:53:10.848: INFO: Pod "downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a" satisfied condition "success or failure"
Aug 13 01:53:10.937: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a container client-container: <nil>
STEP: delete the pod
Aug 13 01:53:11.125: INFO: Waiting for pod downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a to disappear
Aug 13 01:53:11.214: INFO: Pod downwardapi-volume-e16c0fed-43d0-4ee1-b374-a8d97c1cef0a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:53:11.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-226" for this suite.
Aug 13 01:53:17.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:53:20.954: INFO: namespace projected-226 deletion completed in 9.650253721s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:53:20.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug 13 01:53:21.590: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8938'
Aug 13 01:53:23.029: INFO: stderr: ""
Aug 13 01:53:23.029: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 01:53:23.029: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8938'
Aug 13 01:53:23.471: INFO: stderr: ""
Aug 13 01:53:23.471: INFO: stdout: "update-demo-nautilus-hxsq4 update-demo-nautilus-wgv5l "
Aug 13 01:53:23.471: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-hxsq4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:23.904: INFO: stderr: ""
Aug 13 01:53:23.904: INFO: stdout: ""
Aug 13 01:53:23.904: INFO: update-demo-nautilus-hxsq4 is created but not running
Aug 13 01:53:28.905: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8938'
Aug 13 01:53:29.339: INFO: stderr: ""
Aug 13 01:53:29.339: INFO: stdout: "update-demo-nautilus-hxsq4 update-demo-nautilus-wgv5l "
Aug 13 01:53:29.339: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-hxsq4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:29.771: INFO: stderr: ""
Aug 13 01:53:29.771: INFO: stdout: "true"
Aug 13 01:53:29.771: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-hxsq4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:30.206: INFO: stderr: ""
Aug 13 01:53:30.206: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:53:30.206: INFO: validating pod update-demo-nautilus-hxsq4
Aug 13 01:53:30.384: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:53:30.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:53:30.384: INFO: update-demo-nautilus-hxsq4 is verified up and running
Aug 13 01:53:30.384: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wgv5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:30.820: INFO: stderr: ""
Aug 13 01:53:30.820: INFO: stdout: "true"
Aug 13 01:53:30.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wgv5l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:31.252: INFO: stderr: ""
Aug 13 01:53:31.252: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 01:53:31.252: INFO: validating pod update-demo-nautilus-wgv5l
Aug 13 01:53:31.428: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 01:53:31.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 01:53:31.428: INFO: update-demo-nautilus-wgv5l is verified up and running
STEP: rolling-update to new replication controller
Aug 13 01:53:31.430: INFO: scanned /root for discovery docs: <nil>
Aug 13 01:53:31.430: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8938'
Aug 13 01:53:47.772: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 13 01:53:47.772: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 01:53:47.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8938'
Aug 13 01:53:48.203: INFO: stderr: ""
Aug 13 01:53:48.203: INFO: stdout: "update-demo-kitten-ll4z2 update-demo-kitten-tvs5v "
Aug 13 01:53:48.204: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-ll4z2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:48.639: INFO: stderr: ""
Aug 13 01:53:48.639: INFO: stdout: "true"
Aug 13 01:53:48.639: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-ll4z2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:49.090: INFO: stderr: ""
Aug 13 01:53:49.090: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 13 01:53:49.090: INFO: validating pod update-demo-kitten-ll4z2
Aug 13 01:53:49.268: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 13 01:53:49.268: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 13 01:53:49.268: INFO: update-demo-kitten-ll4z2 is verified up and running
Aug 13 01:53:49.269: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-tvs5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:49.708: INFO: stderr: ""
Aug 13 01:53:49.709: INFO: stdout: "true"
Aug 13 01:53:49.709: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-tvs5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8938'
Aug 13 01:53:50.155: INFO: stderr: ""
Aug 13 01:53:50.155: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 13 01:53:50.155: INFO: validating pod update-demo-kitten-tvs5v
Aug 13 01:53:50.332: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 13 01:53:50.333: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 13 01:53:50.333: INFO: update-demo-kitten-tvs5v is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:53:50.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8938" for this suite.
Aug 13 01:54:12.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:54:16.089: INFO: namespace kubectl-8938 deletion completed in 25.66651354s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:54:16.089: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug 13 01:54:16.730: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 13 01:54:16.730: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7162'
Aug 13 01:54:17.695: INFO: stderr: ""
Aug 13 01:54:17.695: INFO: stdout: "service/redis-slave created\n"
Aug 13 01:54:17.695: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 13 01:54:17.695: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7162'
Aug 13 01:54:18.330: INFO: stderr: ""
Aug 13 01:54:18.330: INFO: stdout: "service/redis-master created\n"
Aug 13 01:54:18.330: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 13 01:54:18.330: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7162'
Aug 13 01:54:19.340: INFO: stderr: ""
Aug 13 01:54:19.340: INFO: stdout: "service/frontend created\n"
Aug 13 01:54:19.340: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 13 01:54:19.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7162'
Aug 13 01:54:20.315: INFO: stderr: ""
Aug 13 01:54:20.315: INFO: stdout: "deployment.apps/frontend created\n"
Aug 13 01:54:20.315: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 13 01:54:20.315: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7162'
Aug 13 01:54:21.267: INFO: stderr: ""
Aug 13 01:54:21.267: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 13 01:54:21.267: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 13 01:54:21.267: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7162'
Aug 13 01:54:22.237: INFO: stderr: ""
Aug 13 01:54:22.237: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 13 01:54:22.237: INFO: Waiting for all frontend pods to be Running.
Aug 13 01:54:37.339: INFO: Waiting for frontend to serve content.
Aug 13 01:54:37.516: INFO: Trying to add a new entry to the guestbook.
Aug 13 01:54:37.696: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 13 01:54:37.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 01:54:38.325: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:54:38.325: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 01:54:38.325: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 01:54:38.857: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:54:38.857: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 01:54:38.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 01:54:39.378: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:54:39.378: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 01:54:39.378: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 01:54:39.900: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:54:39.900: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 01:54:39.900: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 01:54:40.425: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:54:40.425: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 01:54:40.426: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 01:54:40.943: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:54:40.943: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:54:40.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7162" for this suite.
Aug 13 01:55:23.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:55:26.711: INFO: namespace kubectl-7162 deletion completed in 45.677281858s
•SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:55:26.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug 13 01:55:27.438: INFO: Waiting up to 5m0s for pod "client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149" in namespace "containers-9059" to be "success or failure"
Aug 13 01:55:27.527: INFO: Pod "client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149": Phase="Pending", Reason="", readiness=false. Elapsed: 89.203416ms
Aug 13 01:55:29.617: INFO: Pod "client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178969556s
STEP: Saw pod success
Aug 13 01:55:29.617: INFO: Pod "client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149" satisfied condition "success or failure"
Aug 13 01:55:29.707: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149 container test-container: <nil>
STEP: delete the pod
Aug 13 01:55:29.894: INFO: Waiting for pod client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149 to disappear
Aug 13 01:55:29.983: INFO: Pod client-containers-f1fff469-6bda-4ccf-b0cc-6a548f805149 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:55:29.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9059" for this suite.
Aug 13 01:55:36.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:55:39.734: INFO: namespace containers-9059 deletion completed in 9.661110587s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:55:39.735: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug 13 01:55:40.369: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-249'
Aug 13 01:55:41.005: INFO: stderr: ""
Aug 13 01:55:41.005: INFO: stdout: "pod/pause created\n"
Aug 13 01:55:41.005: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 13 01:55:41.005: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-249" to be "running and ready"
Aug 13 01:55:41.094: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 89.221414ms
Aug 13 01:55:43.185: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.179466048s
Aug 13 01:55:43.185: INFO: Pod "pause" satisfied condition "running and ready"
Aug 13 01:55:43.185: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 13 01:55:43.185: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-249'
Aug 13 01:55:43.718: INFO: stderr: ""
Aug 13 01:55:43.718: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 13 01:55:43.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-249'
Aug 13 01:55:44.152: INFO: stderr: ""
Aug 13 01:55:44.152: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 13 01:55:44.152: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-249'
Aug 13 01:55:44.673: INFO: stderr: ""
Aug 13 01:55:44.673: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 13 01:55:44.673: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-249'
Aug 13 01:55:45.104: INFO: stderr: ""
Aug 13 01:55:45.104: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug 13 01:55:45.104: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-249'
Aug 13 01:55:45.621: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 01:55:45.621: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 13 01:55:45.621: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-249'
Aug 13 01:55:46.146: INFO: stderr: "No resources found.\n"
Aug 13 01:55:46.146: INFO: stdout: ""
Aug 13 01:55:46.146: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-249 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 01:55:46.581: INFO: stderr: ""
Aug 13 01:55:46.581: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:55:46.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-249" for this suite.
Aug 13 01:55:52.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:55:56.326: INFO: namespace kubectl-249 deletion completed in 9.655207421s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:55:56.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:55:57.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6186" for this suite.
Aug 13 01:56:03.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:56:06.803: INFO: namespace services-6186 deletion completed in 9.659877967s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:56:06.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 13 01:56:07.531: INFO: Waiting up to 5m0s for pod "pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6" in namespace "emptydir-7947" to be "success or failure"
Aug 13 01:56:07.620: INFO: Pod "pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.293312ms
Aug 13 01:56:09.710: INFO: Pod "pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179270811s
STEP: Saw pod success
Aug 13 01:56:09.710: INFO: Pod "pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6" satisfied condition "success or failure"
Aug 13 01:56:09.800: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6 container test-container: <nil>
STEP: delete the pod
Aug 13 01:56:09.988: INFO: Waiting for pod pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6 to disappear
Aug 13 01:56:10.077: INFO: Pod pod-9e6802d1-dde5-49b5-b5f1-2c83d0ae56a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:56:10.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7947" for this suite.
Aug 13 01:56:16.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:56:19.827: INFO: namespace emptydir-7947 deletion completed in 9.659028854s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:56:19.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 13 01:56:20.554: INFO: Waiting up to 5m0s for pod "pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5" in namespace "emptydir-1496" to be "success or failure"
Aug 13 01:56:20.644: INFO: Pod "pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.854676ms
Aug 13 01:56:22.734: INFO: Pod "pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179603942s
STEP: Saw pod success
Aug 13 01:56:22.734: INFO: Pod "pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5" satisfied condition "success or failure"
Aug 13 01:56:22.823: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5 container test-container: <nil>
STEP: delete the pod
Aug 13 01:56:23.011: INFO: Waiting for pod pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5 to disappear
Aug 13 01:56:23.100: INFO: Pod pod-4c8eb5a1-11d4-432f-9a47-102f2fa730c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:56:23.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1496" for this suite.
Aug 13 01:56:29.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:56:32.866: INFO: namespace emptydir-1496 deletion completed in 9.673103667s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:56:32.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 13 01:56:36.133: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:56:36.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4694" for this suite.
Aug 13 01:56:58.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:57:02.159: INFO: namespace replicaset-4694 deletion completed in 25.666276813s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:57:02.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 13 01:57:02.797: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Aug 13 01:57:03.229: INFO: stderr: ""
Aug 13 01:57:03.229: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:23:26Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:15:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:57:03.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8416" for this suite.
Aug 13 01:57:09.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:57:12.964: INFO: namespace kubectl-8416 deletion completed in 9.645547779s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:57:12.965: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:57:17.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1708" for this suite.
Aug 13 01:57:24.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:57:27.609: INFO: namespace kubelet-test-1708 deletion completed in 9.647676946s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:57:27.609: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 01:57:28.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-1419'
Aug 13 01:57:28.691: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 01:57:28.691: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug 13 01:57:30.870: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-1419'
Aug 13 01:57:31.393: INFO: stderr: ""
Aug 13 01:57:31.393: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:57:31.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1419" for this suite.
Aug 13 01:57:37.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:57:41.128: INFO: namespace kubectl-1419 deletion completed in 9.645298287s
•SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:57:41.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-844
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 13 01:57:41.855: INFO: Waiting up to 5m0s for pod "pod-3197eb98-f098-49e5-99e4-3463ff9f8356" in namespace "emptydir-844" to be "success or failure"
Aug 13 01:57:41.944: INFO: Pod "pod-3197eb98-f098-49e5-99e4-3463ff9f8356": Phase="Pending", Reason="", readiness=false. Elapsed: 88.818601ms
Aug 13 01:57:44.034: INFO: Pod "pod-3197eb98-f098-49e5-99e4-3463ff9f8356": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178428567s
STEP: Saw pod success
Aug 13 01:57:44.034: INFO: Pod "pod-3197eb98-f098-49e5-99e4-3463ff9f8356" satisfied condition "success or failure"
Aug 13 01:57:44.123: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-3197eb98-f098-49e5-99e4-3463ff9f8356 container test-container: <nil>
STEP: delete the pod
Aug 13 01:57:44.441: INFO: Waiting for pod pod-3197eb98-f098-49e5-99e4-3463ff9f8356 to disappear
Aug 13 01:57:44.530: INFO: Pod pod-3197eb98-f098-49e5-99e4-3463ff9f8356 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:57:44.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-844" for this suite.
Aug 13 01:57:50.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:57:54.269: INFO: namespace emptydir-844 deletion completed in 9.648997028s
•SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:57:54.269: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 01:57:54.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae" in namespace "projected-3581" to be "success or failure"
Aug 13 01:57:55.085: INFO: Pod "downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae": Phase="Pending", Reason="", readiness=false. Elapsed: 89.363917ms
Aug 13 01:57:57.175: INFO: Pod "downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179041039s
STEP: Saw pod success
Aug 13 01:57:57.175: INFO: Pod "downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae" satisfied condition "success or failure"
Aug 13 01:57:57.264: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae container client-container: <nil>
STEP: delete the pod
Aug 13 01:57:57.453: INFO: Waiting for pod downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae to disappear
Aug 13 01:57:57.542: INFO: Pod downwardapi-volume-48055412-994d-448a-b8b7-5807138263ae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 01:57:57.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3581" for this suite.
Aug 13 01:58:03.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 01:58:07.294: INFO: namespace projected-3581 deletion completed in 9.66211133s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 01:58:07.294: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1951
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1951
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1951
Aug 13 01:58:08.291: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Aug 13 01:58:18.381: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 13 01:58:18.471: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 01:58:19.768: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 01:58:19.768: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 01:58:19.768: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 01:58:19.857: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 01:58:19.857: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 01:58:20.218: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999486s
Aug 13 01:58:21.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.909894079s
Aug 13 01:58:22.398: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.81993946s
Aug 13 01:58:23.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.730058494s
Aug 13 01:58:24.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.640379073s
Aug 13 01:58:25.667: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.550234336s
Aug 13 01:58:26.757: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.460154189s
Aug 13 01:58:27.847: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.370345731s
Aug 13 01:58:28.937: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.280586146s
Aug 13 01:58:30.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 190.625641ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1951
Aug 13 01:58:31.116: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:58:32.431: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 01:58:32.431: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 01:58:32.431: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 01:58:32.519: INFO: Found 1 stateful pods, waiting for 3
Aug 13 01:58:42.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:58:42.609: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 01:58:42.609: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 13 01:58:42.787: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 01:58:44.027: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 01:58:44.027: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 01:58:44.027: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 01:58:44.027: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 01:58:45.294: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 01:58:45.295: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 01:58:45.295: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 01:58:45.295: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 01:58:46.554: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 01:58:46.554: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 01:58:46.554: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 01:58:46.554: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 01:58:46.643: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 13 01:58:56.822: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 01:58:56.822: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 01:58:56.822: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 01:58:57.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999574s
Aug 13 01:58:58.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.909937452s
Aug 13 01:58:59.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.819338131s
Aug 13 01:59:00.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.72918877s
Aug 13 01:59:01.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.639133249s
Aug 13 01:59:02.542: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.548304153s
Aug 13 01:59:03.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.458595414s
Aug 13 01:59:04.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.368423753s
Aug 13 01:59:05.812: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.278768212s
Aug 13 01:59:06.902: INFO: Verifying statefulset ss doesn't scale past 3 for another 188.735543ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1951
Aug 13 01:59:07.993: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:09.294: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 01:59:09.295: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 01:59:09.295: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 01:59:09.295: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:10.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 01:59:10.613: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 01:59:10.613: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 01:59:10.613: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:11.529: INFO: rc: 1
Aug 13 01:59:11.530: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0027283f0 exit status 1 <nil> <nil> true [0xc0003b3cd8 0xc0003b3d60 0xc0003b3da0] [0xc0003b3cd8 0xc0003b3d60 0xc0003b3da0] [0xc0003b3d28 0xc0003b3d88] [0x9d17b0 0x9d17b0] 0xc0023e21e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Aug 13 01:59:21.530: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:22.060: INFO: rc: 1
Aug 13 01:59:22.060: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1a5a0 exit status 1 <nil> <nil> true [0xc00050a2d0 0xc00050b080 0xc00050bcc0] [0xc00050a2d0 0xc00050b080 0xc00050bcc0] [0xc00050a520 0xc00050bc18] [0x9d17b0 0x9d17b0] 0xc00249c7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 01:59:32.061: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:32.588: INFO: rc: 1
Aug 13 01:59:32.588: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1ab70 exit status 1 <nil> <nil> true [0xc00050bfd0 0xc001b000a8 0xc001b00170] [0xc00050bfd0 0xc001b000a8 0xc001b00170] [0xc00032dee8 0xc001b00138] [0x9d17b0 0x9d17b0] 0xc00249ce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 01:59:42.589: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:43.112: INFO: rc: 1
Aug 13 01:59:43.112: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0e5d0 exit status 1 <nil> <nil> true [0xc00209c010 0xc00209c038 0xc00209c078] [0xc00209c010 0xc00209c038 0xc00209c078] [0xc00209c030 0xc00209c060] [0x9d17b0 0x9d17b0] 0xc002962f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 01:59:53.112: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 01:59:53.631: INFO: rc: 1
Aug 13 01:59:53.631: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1b110 exit status 1 <nil> <nil> true [0xc001b00210 0xc001b003a8 0xc001b004a8] [0xc001b00210 0xc001b003a8 0xc001b004a8] [0xc001b00328 0xc001b00490] [0x9d17b0 0x9d17b0] 0xc00249daa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:00:03.631: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:00:04.152: INFO: rc: 1
Aug 13 02:00:04.152: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1b6b0 exit status 1 <nil> <nil> true [0xc001b004b0 0xc001b00610 0xc001b006d0] [0xc001b004b0 0xc001b00610 0xc001b006d0] [0xc001b005b0 0xc001b006b0] [0x9d17b0 0x9d17b0] 0xc001fde9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:00:14.152: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:00:14.685: INFO: rc: 1
Aug 13 02:00:14.685: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0ebd0 exit status 1 <nil> <nil> true [0xc00209c090 0xc00209c0b8 0xc00209c0d0] [0xc00209c090 0xc00209c0b8 0xc00209c0d0] [0xc00209c0b0 0xc00209c0c8] [0x9d17b0 0x9d17b0] 0xc0024c4360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:00:24.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:00:25.212: INFO: rc: 1
Aug 13 02:00:25.212: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1bce0 exit status 1 <nil> <nil> true [0xc001b006e0 0xc001b00728 0xc001b007a8] [0xc001b006e0 0xc001b00728 0xc001b007a8] [0xc001b00708 0xc001b00788] [0x9d17b0 0x9d17b0] 0xc0024aa420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:00:35.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:00:35.730: INFO: rc: 1
Aug 13 02:00:35.730: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002cd2600 exit status 1 <nil> <nil> true [0xc002aec000 0xc002aec018 0xc002aec030] [0xc002aec000 0xc002aec018 0xc002aec030] [0xc002aec010 0xc002aec028] [0x9d17b0 0x9d17b0] 0xc002c755c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:00:45.730: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:00:46.255: INFO: rc: 1
Aug 13 02:00:46.255: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002cd2ba0 exit status 1 <nil> <nil> true [0xc002aec038 0xc002aec050 0xc002aec068] [0xc002aec038 0xc002aec050 0xc002aec068] [0xc002aec048 0xc002aec060] [0x9d17b0 0x9d17b0] 0xc002c75e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:00:56.255: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:00:56.771: INFO: rc: 1
Aug 13 02:00:56.771: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032722d0 exit status 1 <nil> <nil> true [0xc001b007c8 0xc001b00880 0xc001b00960] [0xc001b007c8 0xc001b00880 0xc001b00960] [0xc001b00838 0xc001b008e0] [0x9d17b0 0x9d17b0] 0xc0024aa9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:01:06.771: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:01:07.299: INFO: rc: 1
Aug 13 02:01:07.299: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032728a0 exit status 1 <nil> <nil> true [0xc001b009a8 0xc001b00a28 0xc001b00a68] [0xc001b009a8 0xc001b00a28 0xc001b00a68] [0xc001b00a20 0xc001b00a40] [0x9d17b0 0x9d17b0] 0xc0024aaf60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:01:17.300: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:01:17.828: INFO: rc: 1
Aug 13 02:01:17.828: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002cd3170 exit status 1 <nil> <nil> true [0xc002aec070 0xc002aec0a8 0xc002aec0e0] [0xc002aec070 0xc002aec0a8 0xc002aec0e0] [0xc002aec088 0xc002aec0c8] [0x9d17b0 0x9d17b0] 0xc001cd3f80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:01:27.829: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:01:28.355: INFO: rc: 1
Aug 13 02:01:28.355: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1a5d0 exit status 1 <nil> <nil> true [0xc00032dee8 0xc00050a448 0xc00050b7a8] [0xc00032dee8 0xc00050a448 0xc00050b7a8] [0xc00050a2d0 0xc00050b080] [0x9d17b0 0x9d17b0] 0xc002c754a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:01:38.355: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:01:38.867: INFO: rc: 1
Aug 13 02:01:38.867: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0e600 exit status 1 <nil> <nil> true [0xc00209c010 0xc00209c038 0xc00209c078] [0xc00209c010 0xc00209c038 0xc00209c078] [0xc00209c030 0xc00209c060] [0x9d17b0 0x9d17b0] 0xc001fdf440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:01:48.867: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:01:49.396: INFO: rc: 1
Aug 13 02:01:49.396: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002cd25a0 exit status 1 <nil> <nil> true [0xc001b000a8 0xc001b00170 0xc001b00328] [0xc001b000a8 0xc001b00170 0xc001b00328] [0xc001b00138 0xc001b002c0] [0x9d17b0 0x9d17b0] 0xc002962f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:01:59.397: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:01:59.917: INFO: rc: 1
Aug 13 02:01:59.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0ec00 exit status 1 <nil> <nil> true [0xc00209c090 0xc00209c0b8 0xc00209c0d0] [0xc00209c090 0xc00209c0b8 0xc00209c0d0] [0xc00209c0b0 0xc00209c0c8] [0x9d17b0 0x9d17b0] 0xc00249c7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:02:09.917: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:02:10.444: INFO: rc: 1
Aug 13 02:02:10.444: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002cd2b70 exit status 1 <nil> <nil> true [0xc001b003a8 0xc001b004a8 0xc001b005b0] [0xc001b003a8 0xc001b004a8 0xc001b005b0] [0xc001b00490 0xc001b00540] [0x9d17b0 0x9d17b0] 0xc0024c4360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:02:20.444: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:02:20.976: INFO: rc: 1
Aug 13 02:02:20.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0f200 exit status 1 <nil> <nil> true [0xc00209c0e8 0xc00209c118 0xc00209c140] [0xc00209c0e8 0xc00209c118 0xc00209c140] [0xc00209c0f8 0xc00209c138] [0x9d17b0 0x9d17b0] 0xc00249ce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:02:30.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:02:31.507: INFO: rc: 1
Aug 13 02:02:31.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0f7d0 exit status 1 <nil> <nil> true [0xc00209c168 0xc00209c190 0xc00209c1d0] [0xc00209c168 0xc00209c190 0xc00209c1d0] [0xc00209c188 0xc00209c1b8] [0x9d17b0 0x9d17b0] 0xc00249daa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:02:41.508: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:02:42.028: INFO: rc: 1
Aug 13 02:02:42.028: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1abd0 exit status 1 <nil> <nil> true [0xc00050bc18 0xc002aec000 0xc002aec018] [0xc00050bc18 0xc002aec000 0xc002aec018] [0xc00050bfd0 0xc002aec010] [0x9d17b0 0x9d17b0] 0xc002c75aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:02:52.029: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:02:52.561: INFO: rc: 1
Aug 13 02:02:52.561: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003272600 exit status 1 <nil> <nil> true [0xc00257a000 0xc00257a018 0xc00257a030] [0xc00257a000 0xc00257a018 0xc00257a030] [0xc00257a010 0xc00257a028] [0x9d17b0 0x9d17b0] 0xc0024aa4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:03:02.562: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:03:03.086: INFO: rc: 1
Aug 13 02:03:03.086: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003272c90 exit status 1 <nil> <nil> true [0xc00257a038 0xc00257a050 0xc00257a068] [0xc00257a038 0xc00257a050 0xc00257a068] [0xc00257a048 0xc00257a060] [0x9d17b0 0x9d17b0] 0xc0024aaa80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:03:13.087: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:03:13.615: INFO: rc: 1
Aug 13 02:03:13.615: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1b230 exit status 1 <nil> <nil> true [0xc002aec020 0xc002aec038 0xc002aec050] [0xc002aec020 0xc002aec038 0xc002aec050] [0xc002aec030 0xc002aec048] [0x9d17b0 0x9d17b0] 0xc001c534a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:03:23.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:03:24.698: INFO: rc: 1
Aug 13 02:03:24.698: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002cd25d0 exit status 1 <nil> <nil> true [0xc00050a2d0 0xc00050b080 0xc00050bcc0] [0xc00050a2d0 0xc00050b080 0xc00050bcc0] [0xc00050a520 0xc00050bc18] [0x9d17b0 0x9d17b0] 0xc002962a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:03:34.699: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:03:35.228: INFO: rc: 1
Aug 13 02:03:35.229: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0e5a0 exit status 1 <nil> <nil> true [0xc00032d948 0xc001b00108 0xc001b00210] [0xc00032d948 0xc001b00108 0xc001b00210] [0xc001b000a8 0xc001b00170] [0x9d17b0 0x9d17b0] 0xc0018274a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:03:45.229: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:03:45.753: INFO: rc: 1
Aug 13 02:03:45.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a1a5a0 exit status 1 <nil> <nil> true [0xc00209c010 0xc00209c038 0xc00209c078] [0xc00209c010 0xc00209c038 0xc00209c078] [0xc00209c030 0xc00209c060] [0x9d17b0 0x9d17b0] 0xc002c758c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:03:55.753: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:03:56.278: INFO: rc: 1
Aug 13 02:03:56.278: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d0eb70 exit status 1 <nil> <nil> true [0xc001b002c0 0xc001b00410 0xc001b004b0] [0xc001b002c0 0xc001b00410 0xc001b004b0] [0xc001b003a8 0xc001b004a8] [0x9d17b0 0x9d17b0] 0xc001cb5da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:04:06.279: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:04:06.811: INFO: rc: 1
Aug 13 02:04:06.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003272570 exit status 1 <nil> <nil> true [0xc002aec000 0xc002aec018 0xc002aec030] [0xc002aec000 0xc002aec018 0xc002aec030] [0xc002aec010 0xc002aec028] [0x9d17b0 0x9d17b0] 0xc0024c4780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 13 02:04:16.811: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1951 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 02:04:17.342: INFO: rc: 1
Aug 13 02:04:17.342: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Aug 13 02:04:17.342: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 13 02:04:17.611: INFO: Deleting all statefulset in ns statefulset-1951
Aug 13 02:04:17.700: INFO: Scaling statefulset ss to 0
Aug 13 02:04:17.968: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 02:04:18.057: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:04:18.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1951" for this suite.
Aug 13 02:04:24.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:04:28.069: INFO: namespace statefulset-1951 deletion completed in 9.651764777s

• [SLOW TEST:380.774 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:04:28.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 13 02:04:28.816: INFO: Waiting up to 5m0s for pod "pod-499b5f94-288b-4125-af91-af6648005135" in namespace "emptydir-445" to be "success or failure"
Aug 13 02:04:28.905: INFO: Pod "pod-499b5f94-288b-4125-af91-af6648005135": Phase="Pending", Reason="", readiness=false. Elapsed: 89.098531ms
Aug 13 02:04:30.995: INFO: Pod "pod-499b5f94-288b-4125-af91-af6648005135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178666396s
STEP: Saw pod success
Aug 13 02:04:30.995: INFO: Pod "pod-499b5f94-288b-4125-af91-af6648005135" satisfied condition "success or failure"
Aug 13 02:04:31.084: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-499b5f94-288b-4125-af91-af6648005135 container test-container: <nil>
STEP: delete the pod
Aug 13 02:04:31.273: INFO: Waiting for pod pod-499b5f94-288b-4125-af91-af6648005135 to disappear
Aug 13 02:04:31.362: INFO: Pod pod-499b5f94-288b-4125-af91-af6648005135 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:04:31.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-445" for this suite.
Aug 13 02:04:37.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:04:41.094: INFO: namespace emptydir-445 deletion completed in 9.643401284s
•SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:04:41.095: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 in namespace container-probe-6323
Aug 13 02:04:43.999: INFO: Started pod liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 in namespace container-probe-6323
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 02:04:44.088: INFO: Initial restart count of pod liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 is 0
Aug 13 02:05:05.072: INFO: Restart count of pod container-probe-6323/liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 is now 1 (20.984091256s elapsed)
Aug 13 02:05:23.876: INFO: Restart count of pod container-probe-6323/liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 is now 2 (39.788574062s elapsed)
Aug 13 02:05:44.771: INFO: Restart count of pod container-probe-6323/liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 is now 3 (1m0.683386137s elapsed)
Aug 13 02:06:03.576: INFO: Restart count of pod container-probe-6323/liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 is now 4 (1m19.488305997s elapsed)
Aug 13 02:07:12.529: INFO: Restart count of pod container-probe-6323/liveness-bd8f1c14-9cf3-43da-b6bd-b23673a28685 is now 5 (2m28.441377989s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:07:12.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6323" for this suite.
Aug 13 02:07:18.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:07:22.361: INFO: namespace container-probe-6323 deletion completed in 9.648934501s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:07:22.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug 13 02:07:22.997: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4669 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 13 02:07:30.918: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 13 02:07:30.918: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:07:33.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4669" for this suite.
Aug 13 02:07:43.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:07:46.839: INFO: namespace kubectl-4669 deletion completed in 13.653287474s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:07:46.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 13 02:07:47.565: INFO: Waiting up to 5m0s for pod "downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d" in namespace "downward-api-4428" to be "success or failure"
Aug 13 02:07:47.654: INFO: Pod "downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.191907ms
Aug 13 02:07:49.743: INFO: Pod "downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17863409s
STEP: Saw pod success
Aug 13 02:07:49.743: INFO: Pod "downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d" satisfied condition "success or failure"
Aug 13 02:07:49.832: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d container dapi-container: <nil>
STEP: delete the pod
Aug 13 02:07:50.020: INFO: Waiting for pod downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d to disappear
Aug 13 02:07:50.109: INFO: Pod downward-api-653a3f05-6aa3-4d0b-bd40-2190fef8134d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:07:50.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4428" for this suite.
Aug 13 02:07:56.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:07:59.853: INFO: namespace downward-api-4428 deletion completed in 9.654293454s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:07:59.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 13 02:08:00.581: INFO: Waiting up to 5m0s for pod "downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286" in namespace "downward-api-9162" to be "success or failure"
Aug 13 02:08:00.670: INFO: Pod "downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286": Phase="Pending", Reason="", readiness=false. Elapsed: 89.203955ms
Aug 13 02:08:02.760: INFO: Pod "downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178799624s
STEP: Saw pod success
Aug 13 02:08:02.760: INFO: Pod "downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286" satisfied condition "success or failure"
Aug 13 02:08:02.849: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286 container dapi-container: <nil>
STEP: delete the pod
Aug 13 02:08:03.036: INFO: Waiting for pod downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286 to disappear
Aug 13 02:08:03.125: INFO: Pod downward-api-506811d9-ac3d-4caf-ae0c-e2571b31f286 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:08:03.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9162" for this suite.
Aug 13 02:08:09.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:08:12.872: INFO: namespace downward-api-9162 deletion completed in 9.656985468s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:08:12.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 13 02:08:14.072: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3917,SelfLink:/api/v1/namespaces/watch-3917/configmaps/e2e-watch-test-label-changed,UID:42ca97f3-2c31-495a-9fea-313003d5ba3e,ResourceVersion:23629,Generation:0,CreationTimestamp:2019-08-13 02:08:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 02:08:14.073: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3917,SelfLink:/api/v1/namespaces/watch-3917/configmaps/e2e-watch-test-label-changed,UID:42ca97f3-2c31-495a-9fea-313003d5ba3e,ResourceVersion:23631,Generation:0,CreationTimestamp:2019-08-13 02:08:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 13 02:08:14.073: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3917,SelfLink:/api/v1/namespaces/watch-3917/configmaps/e2e-watch-test-label-changed,UID:42ca97f3-2c31-495a-9fea-313003d5ba3e,ResourceVersion:23632,Generation:0,CreationTimestamp:2019-08-13 02:08:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 13 02:08:24.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3917,SelfLink:/api/v1/namespaces/watch-3917/configmaps/e2e-watch-test-label-changed,UID:42ca97f3-2c31-495a-9fea-313003d5ba3e,ResourceVersion:23682,Generation:0,CreationTimestamp:2019-08-13 02:08:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 02:08:24.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3917,SelfLink:/api/v1/namespaces/watch-3917/configmaps/e2e-watch-test-label-changed,UID:42ca97f3-2c31-495a-9fea-313003d5ba3e,ResourceVersion:23683,Generation:0,CreationTimestamp:2019-08-13 02:08:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 13 02:08:24.701: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3917,SelfLink:/api/v1/namespaces/watch-3917/configmaps/e2e-watch-test-label-changed,UID:42ca97f3-2c31-495a-9fea-313003d5ba3e,ResourceVersion:23684,Generation:0,CreationTimestamp:2019-08-13 02:08:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:08:24.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3917" for this suite.
Aug 13 02:08:31.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:08:34.440: INFO: namespace watch-3917 deletion completed in 9.647724718s
•SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:08:34.441: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2201
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-2201
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2201 to expose endpoints map[]
Aug 13 02:08:35.257: INFO: successfully validated that service endpoint-test2 in namespace services-2201 exposes endpoints map[] (88.905877ms elapsed)
STEP: Creating pod pod1 in namespace services-2201
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2201 to expose endpoints map[pod1:[80]]
Aug 13 02:08:37.883: INFO: successfully validated that service endpoint-test2 in namespace services-2201 exposes endpoints map[pod1:[80]] (2.535371087s elapsed)
STEP: Creating pod pod2 in namespace services-2201
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2201 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 13 02:08:39.507: INFO: successfully validated that service endpoint-test2 in namespace services-2201 exposes endpoints map[pod1:[80] pod2:[80]] (1.533739911s elapsed)
STEP: Deleting pod pod1 in namespace services-2201
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2201 to expose endpoints map[pod2:[80]]
Aug 13 02:08:39.775: INFO: successfully validated that service endpoint-test2 in namespace services-2201 exposes endpoints map[pod2:[80]] (178.07456ms elapsed)
STEP: Deleting pod pod2 in namespace services-2201
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2201 to expose endpoints map[]
Aug 13 02:08:39.954: INFO: successfully validated that service endpoint-test2 in namespace services-2201 exposes endpoints map[] (89.213009ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:08:40.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2201" for this suite.
Aug 13 02:09:02.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:09:05.790: INFO: namespace services-2201 deletion completed in 25.65159194s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:09:05.790: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-f6d5744b-15e1-438d-b56b-195d5df1bb9d in namespace container-probe-3522
Aug 13 02:09:08.699: INFO: Started pod busybox-f6d5744b-15e1-438d-b56b-195d5df1bb9d in namespace container-probe-3522
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 02:09:08.788: INFO: Initial restart count of pod busybox-f6d5744b-15e1-438d-b56b-195d5df1bb9d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:13:09.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3522" for this suite.
Aug 13 02:13:15.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:13:18.951: INFO: namespace container-probe-3522 deletion completed in 9.673024369s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:13:18.952: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 02:13:19.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263" in namespace "projected-8915" to be "success or failure"
Aug 13 02:13:19.771: INFO: Pod "downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263": Phase="Pending", Reason="", readiness=false. Elapsed: 89.31748ms
Aug 13 02:13:21.861: INFO: Pod "downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179111013s
STEP: Saw pod success
Aug 13 02:13:21.861: INFO: Pod "downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263" satisfied condition "success or failure"
Aug 13 02:13:21.950: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263 container client-container: <nil>
STEP: delete the pod
Aug 13 02:13:22.139: INFO: Waiting for pod downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263 to disappear
Aug 13 02:13:22.230: INFO: Pod downwardapi-volume-a19a6f69-de8c-499e-9a32-796f1301b263 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:13:22.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8915" for this suite.
Aug 13 02:13:28.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:13:31.974: INFO: namespace projected-8915 deletion completed in 9.653911678s
•SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:13:31.974: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 02:13:32.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28" in namespace "projected-9947" to be "success or failure"
Aug 13 02:13:32.807: INFO: Pod "downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28": Phase="Pending", Reason="", readiness=false. Elapsed: 89.387606ms
Aug 13 02:13:34.897: INFO: Pod "downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179066351s
STEP: Saw pod success
Aug 13 02:13:34.897: INFO: Pod "downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28" satisfied condition "success or failure"
Aug 13 02:13:34.986: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28 container client-container: <nil>
STEP: delete the pod
Aug 13 02:13:35.177: INFO: Waiting for pod downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28 to disappear
Aug 13 02:13:35.267: INFO: Pod downwardapi-volume-de77f816-ced9-403c-8527-ebd376a95b28 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:13:35.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9947" for this suite.
Aug 13 02:13:41.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:13:45.021: INFO: namespace projected-9947 deletion completed in 9.664858295s
•SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:13:45.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 02:13:45.749: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00" in namespace "downward-api-454" to be "success or failure"
Aug 13 02:13:45.838: INFO: Pod "downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00": Phase="Pending", Reason="", readiness=false. Elapsed: 89.032618ms
Aug 13 02:13:47.928: INFO: Pod "downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178824509s
STEP: Saw pod success
Aug 13 02:13:47.928: INFO: Pod "downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00" satisfied condition "success or failure"
Aug 13 02:13:48.018: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00 container client-container: <nil>
STEP: delete the pod
Aug 13 02:13:48.206: INFO: Waiting for pod downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00 to disappear
Aug 13 02:13:48.295: INFO: Pod downwardapi-volume-73944762-7593-4008-9f17-e4ce273c8e00 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:13:48.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-454" for this suite.
Aug 13 02:13:54.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:13:58.037: INFO: namespace downward-api-454 deletion completed in 9.651712092s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:13:58.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug 13 02:13:58.764: INFO: Waiting up to 5m0s for pod "client-containers-e88f3f58-a048-4729-81e4-e60881c85991" in namespace "containers-5533" to be "success or failure"
Aug 13 02:13:58.853: INFO: Pod "client-containers-e88f3f58-a048-4729-81e4-e60881c85991": Phase="Pending", Reason="", readiness=false. Elapsed: 88.919564ms
Aug 13 02:14:00.943: INFO: Pod "client-containers-e88f3f58-a048-4729-81e4-e60881c85991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178444325s
STEP: Saw pod success
Aug 13 02:14:00.943: INFO: Pod "client-containers-e88f3f58-a048-4729-81e4-e60881c85991" satisfied condition "success or failure"
Aug 13 02:14:01.032: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod client-containers-e88f3f58-a048-4729-81e4-e60881c85991 container test-container: <nil>
STEP: delete the pod
Aug 13 02:14:01.218: INFO: Waiting for pod client-containers-e88f3f58-a048-4729-81e4-e60881c85991 to disappear
Aug 13 02:14:01.307: INFO: Pod client-containers-e88f3f58-a048-4729-81e4-e60881c85991 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:14:01.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5533" for this suite.
Aug 13 02:14:07.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:14:11.052: INFO: namespace containers-5533 deletion completed in 9.655712223s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:14:11.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 13 02:14:52.320: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0813 02:14:52.319937    4207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 02:14:52.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2980" for this suite.
Aug 13 02:14:58.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:15:02.060: INFO: namespace gc-2980 deletion completed in 9.65115667s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:15:02.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4551
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-f38405a0-4c8a-472b-aa29-eea9af0813b4
STEP: Creating a pod to test consume secrets
Aug 13 02:15:02.879: INFO: Waiting up to 5m0s for pod "pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca" in namespace "secrets-4551" to be "success or failure"
Aug 13 02:15:02.968: INFO: Pod "pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca": Phase="Pending", Reason="", readiness=false. Elapsed: 89.185567ms
Aug 13 02:15:05.058: INFO: Pod "pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178765415s
STEP: Saw pod success
Aug 13 02:15:05.058: INFO: Pod "pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca" satisfied condition "success or failure"
Aug 13 02:15:05.147: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 02:15:05.335: INFO: Waiting for pod pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca to disappear
Aug 13 02:15:05.424: INFO: Pod pod-secrets-b8a1d2ec-953b-4946-85f9-048ee72652ca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:15:05.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4551" for this suite.
Aug 13 02:15:11.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:15:15.179: INFO: namespace secrets-4551 deletion completed in 9.665278699s
•
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:15:15.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f858d98c-0d49-4942-a833-09462981064f
STEP: Creating a pod to test consume secrets
Aug 13 02:15:16.006: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c" in namespace "projected-9267" to be "success or failure"
Aug 13 02:15:16.096: INFO: Pod "pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.320844ms
Aug 13 02:15:18.185: INFO: Pod "pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178664152s
STEP: Saw pod success
Aug 13 02:15:18.185: INFO: Pod "pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c" satisfied condition "success or failure"
Aug 13 02:15:18.274: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 02:15:18.463: INFO: Waiting for pod pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c to disappear
Aug 13 02:15:18.552: INFO: Pod pod-projected-secrets-8530fc11-50ba-412b-8c23-98635129689c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:15:18.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9267" for this suite.
Aug 13 02:15:24.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:15:28.298: INFO: namespace projected-9267 deletion completed in 9.656474284s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:15:28.298: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-20bd97df-b074-4c40-81e0-4248c3eb2fec
STEP: Creating a pod to test consume configMaps
Aug 13 02:15:29.117: INFO: Waiting up to 5m0s for pod "pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20" in namespace "configmap-3817" to be "success or failure"
Aug 13 02:15:29.206: INFO: Pod "pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20": Phase="Pending", Reason="", readiness=false. Elapsed: 89.154314ms
Aug 13 02:15:31.296: INFO: Pod "pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178892466s
STEP: Saw pod success
Aug 13 02:15:31.296: INFO: Pod "pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20" satisfied condition "success or failure"
Aug 13 02:15:31.385: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 02:15:31.574: INFO: Waiting for pod pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20 to disappear
Aug 13 02:15:31.663: INFO: Pod pod-configmaps-647e07d0-8e47-4e11-9003-1e665783cd20 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:15:31.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3817" for this suite.
Aug 13 02:15:38.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:15:41.403: INFO: namespace configmap-3817 deletion completed in 9.649344225s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:15:41.403: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7610
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-cadc2f83-7027-4eb7-8005-b106df0c87d4
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:15:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7610" for this suite.
Aug 13 02:15:48.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:15:51.964: INFO: namespace configmap-7610 deletion completed in 9.744016114s
•S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:15:51.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 13 02:15:52.802: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:16:01.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4688" for this suite.
Aug 13 02:16:07.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:16:11.232: INFO: namespace pods-4688 deletion completed in 9.649571976s
•SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:16:11.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 02:16:11.931: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-531'
Aug 13 02:16:12.936: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 02:16:12.936: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug 13 02:16:13.025: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-531'
Aug 13 02:16:13.574: INFO: stderr: ""
Aug 13 02:16:13.574: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:16:13.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-531" for this suite.
Aug 13 02:16:19.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:16:23.312: INFO: namespace kubectl-531 deletion completed in 9.649458332s
•SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:16:23.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 13 02:16:24.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758" in namespace "downward-api-5265" to be "success or failure"
Aug 13 02:16:24.128: INFO: Pod "downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758": Phase="Pending", Reason="", readiness=false. Elapsed: 88.959021ms
Aug 13 02:16:26.218: INFO: Pod "downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178281427s
STEP: Saw pod success
Aug 13 02:16:26.218: INFO: Pod "downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758" satisfied condition "success or failure"
Aug 13 02:16:26.307: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758 container client-container: <nil>
STEP: delete the pod
Aug 13 02:16:26.493: INFO: Waiting for pod downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758 to disappear
Aug 13 02:16:26.582: INFO: Pod downwardapi-volume-4d650161-8523-4935-a0c1-fcacf6931758 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:16:26.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5265" for this suite.
Aug 13 02:16:32.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:16:36.322: INFO: namespace downward-api-5265 deletion completed in 9.650791398s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:16:36.323: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5209
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4184
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6161
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:17:02.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5209" for this suite.
Aug 13 02:17:09.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:17:12.423: INFO: namespace namespaces-5209 deletion completed in 9.653628872s
STEP: Destroying namespace "nsdeletetest-4184" for this suite.
Aug 13 02:17:12.511: INFO: Namespace nsdeletetest-4184 was already deleted
STEP: Destroying namespace "nsdeletetest-6161" for this suite.
Aug 13 02:17:18.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:17:22.159: INFO: namespace nsdeletetest-6161 deletion completed in 9.647221943s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:17:22.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4239
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 13 02:17:53.510: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0813 02:17:53.510911    4207 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 02:17:53.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4239" for this suite.
Aug 13 02:17:59.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:18:03.263: INFO: namespace gc-4239 deletion completed in 9.663143937s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:18:03.263: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9976
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 13 02:18:07.122: INFO: Successfully updated pod "annotationupdate17ebbc8f-a845-4a32-8265-92966ec73ed1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:18:09.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9976" for this suite.
Aug 13 02:18:31.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:18:35.047: INFO: namespace projected-9976 deletion completed in 25.646529363s
•SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:18:35.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 13 02:18:36.398: INFO: Number of nodes with available pods: 0
Aug 13 02:18:36.398: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 02:18:37.577: INFO: Number of nodes with available pods: 2
Aug 13 02:18:37.577: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 13 02:18:38.025: INFO: Number of nodes with available pods: 1
Aug 13 02:18:38.025: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:39.206: INFO: Number of nodes with available pods: 1
Aug 13 02:18:39.206: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:40.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:40.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:41.207: INFO: Number of nodes with available pods: 1
Aug 13 02:18:41.207: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:42.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:42.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:43.205: INFO: Number of nodes with available pods: 1
Aug 13 02:18:43.205: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:44.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:44.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:45.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:45.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:46.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:46.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:47.203: INFO: Number of nodes with available pods: 1
Aug 13 02:18:47.203: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:48.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:48.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:49.205: INFO: Number of nodes with available pods: 1
Aug 13 02:18:49.205: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:50.204: INFO: Number of nodes with available pods: 1
Aug 13 02:18:50.204: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:51.207: INFO: Number of nodes with available pods: 1
Aug 13 02:18:51.207: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:52.203: INFO: Number of nodes with available pods: 1
Aug 13 02:18:52.203: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:18:53.204: INFO: Number of nodes with available pods: 2
Aug 13 02:18:53.204: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2000, will wait for the garbage collector to delete the pods
Aug 13 02:18:53.572: INFO: Deleting DaemonSet.extensions daemon-set took: 90.741952ms
Aug 13 02:18:53.973: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.385373ms
Aug 13 02:18:56.762: INFO: Number of nodes with available pods: 0
Aug 13 02:18:56.762: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 02:18:56.851: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2000/daemonsets","resourceVersion":"25789"},"items":null}

Aug 13 02:18:56.940: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2000/pods","resourceVersion":"25789"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:18:57.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2000" for this suite.
Aug 13 02:19:03.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:19:06.951: INFO: namespace daemonsets-2000 deletion completed in 9.652272741s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:19:06.951: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3126
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-dbe35f33-988c-4844-bb9e-46911d1087f3
STEP: Creating a pod to test consume secrets
Aug 13 02:19:07.769: INFO: Waiting up to 5m0s for pod "pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c" in namespace "secrets-3126" to be "success or failure"
Aug 13 02:19:07.858: INFO: Pod "pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.276866ms
Aug 13 02:19:09.948: INFO: Pod "pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178930407s
STEP: Saw pod success
Aug 13 02:19:09.948: INFO: Pod "pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c" satisfied condition "success or failure"
Aug 13 02:19:10.037: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c container secret-env-test: <nil>
STEP: delete the pod
Aug 13 02:19:10.229: INFO: Waiting for pod pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c to disappear
Aug 13 02:19:10.317: INFO: Pod pod-secrets-69cfac69-ab54-46f9-8db4-e372a19ba83c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:19:10.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3126" for this suite.
Aug 13 02:19:16.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:19:20.057: INFO: namespace secrets-3126 deletion completed in 9.649533594s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:19:20.057: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 13 02:19:20.694: INFO: namespace kubectl-9416
Aug 13 02:19:20.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9416'
Aug 13 02:19:21.676: INFO: stderr: ""
Aug 13 02:19:21.676: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 13 02:19:22.765: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 02:19:22.765: INFO: Found 1 / 1
Aug 13 02:19:22.765: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 13 02:19:22.855: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 02:19:22.855: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 13 02:19:22.855: INFO: wait on redis-master startup in kubectl-9416 
Aug 13 02:19:22.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-t5l2d redis-master --namespace=kubectl-9416'
Aug 13 02:19:23.500: INFO: stderr: ""
Aug 13 02:19:23.500: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Aug 02:19:22.447 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Aug 02:19:22.447 # Server started, Redis version 3.2.12\n1:M 13 Aug 02:19:22.447 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Aug 02:19:22.447 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 13 02:19:23.500: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9416'
Aug 13 02:19:24.048: INFO: stderr: ""
Aug 13 02:19:24.048: INFO: stdout: "service/rm2 exposed\n"
Aug 13 02:19:24.136: INFO: Service rm2 in namespace kubectl-9416 found.
STEP: exposing service
Aug 13 02:19:26.315: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9416'
Aug 13 02:19:26.844: INFO: stderr: ""
Aug 13 02:19:26.844: INFO: stdout: "service/rm3 exposed\n"
Aug 13 02:19:26.933: INFO: Service rm3 in namespace kubectl-9416 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:19:29.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9416" for this suite.
Aug 13 02:19:51.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:19:54.895: INFO: namespace kubectl-9416 deletion completed in 25.693517044s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:19:54.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 13 02:19:57.984: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:19:58.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6381" for this suite.
Aug 13 02:20:04.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:20:07.916: INFO: namespace container-runtime-6381 deletion completed in 9.660527632s
•SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:20:07.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 13 02:20:08.643: INFO: Waiting up to 5m0s for pod "pod-ae4db374-c458-401a-84b9-73a0435213c6" in namespace "emptydir-5919" to be "success or failure"
Aug 13 02:20:08.733: INFO: Pod "pod-ae4db374-c458-401a-84b9-73a0435213c6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.410773ms
Aug 13 02:20:10.822: INFO: Pod "pod-ae4db374-c458-401a-84b9-73a0435213c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178995984s
STEP: Saw pod success
Aug 13 02:20:10.823: INFO: Pod "pod-ae4db374-c458-401a-84b9-73a0435213c6" satisfied condition "success or failure"
Aug 13 02:20:10.912: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-ae4db374-c458-401a-84b9-73a0435213c6 container test-container: <nil>
STEP: delete the pod
Aug 13 02:20:11.098: INFO: Waiting for pod pod-ae4db374-c458-401a-84b9-73a0435213c6 to disappear
Aug 13 02:20:11.187: INFO: Pod pod-ae4db374-c458-401a-84b9-73a0435213c6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:20:11.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5919" for this suite.
Aug 13 02:20:17.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:20:20.926: INFO: namespace emptydir-5919 deletion completed in 9.64878453s
•SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:20:20.926: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6397
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 13 02:20:22.275: INFO: Number of nodes with available pods: 0
Aug 13 02:20:22.275: INFO: Node ip-10-250-2-100.ec2.internal is running more than one daemon pod
Aug 13 02:20:23.454: INFO: Number of nodes with available pods: 2
Aug 13 02:20:23.454: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 13 02:20:23.900: INFO: Number of nodes with available pods: 1
Aug 13 02:20:23.900: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:20:25.078: INFO: Number of nodes with available pods: 1
Aug 13 02:20:25.078: INFO: Node ip-10-250-2-233.ec2.internal is running more than one daemon pod
Aug 13 02:20:26.080: INFO: Number of nodes with available pods: 2
Aug 13 02:20:26.080: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6397, will wait for the garbage collector to delete the pods
Aug 13 02:20:26.537: INFO: Deleting DaemonSet.extensions daemon-set took: 89.900995ms
Aug 13 02:20:26.637: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.209087ms
Aug 13 02:20:31.926: INFO: Number of nodes with available pods: 0
Aug 13 02:20:31.926: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 02:20:32.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6397/daemonsets","resourceVersion":"26178"},"items":null}

Aug 13 02:20:32.104: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6397/pods","resourceVersion":"26179"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:20:32.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6397" for this suite.
Aug 13 02:20:38.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:20:42.117: INFO: namespace daemonsets-6397 deletion completed in 9.65539817s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:20:42.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:20:45.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5506" for this suite.
Aug 13 02:21:07.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:21:11.031: INFO: namespace replication-controller-5506 deletion completed in 25.644955126s
•SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:21:11.032: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-r9c6
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 02:21:11.935: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r9c6" in namespace "subpath-2411" to be "success or failure"
Aug 13 02:21:12.024: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.015815ms
Aug 13 02:21:14.114: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 2.178408046s
Aug 13 02:21:16.253: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 4.317387157s
Aug 13 02:21:18.342: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 6.406658164s
Aug 13 02:21:20.432: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 8.496315613s
Aug 13 02:21:22.522: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 10.586309064s
Aug 13 02:21:24.611: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 12.675981466s
Aug 13 02:21:26.701: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 14.765744745s
Aug 13 02:21:28.791: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 16.855534796s
Aug 13 02:21:30.880: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 18.944523422s
Aug 13 02:21:32.970: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Running", Reason="", readiness=true. Elapsed: 21.034224563s
Aug 13 02:21:35.059: INFO: Pod "pod-subpath-test-configmap-r9c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.123430441s
STEP: Saw pod success
Aug 13 02:21:35.059: INFO: Pod "pod-subpath-test-configmap-r9c6" satisfied condition "success or failure"
Aug 13 02:21:35.148: INFO: Trying to get logs from node ip-10-250-2-100.ec2.internal pod pod-subpath-test-configmap-r9c6 container test-container-subpath-configmap-r9c6: <nil>
STEP: delete the pod
Aug 13 02:21:35.334: INFO: Waiting for pod pod-subpath-test-configmap-r9c6 to disappear
Aug 13 02:21:35.423: INFO: Pod pod-subpath-test-configmap-r9c6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r9c6
Aug 13 02:21:35.423: INFO: Deleting pod "pod-subpath-test-configmap-r9c6" in namespace "subpath-2411"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:21:35.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2411" for this suite.
Aug 13 02:21:41.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:21:45.246: INFO: namespace subpath-2411 deletion completed in 9.64433884s
•SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:21:45.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2424
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8830
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:21:53.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8631" for this suite.
Aug 13 02:21:59.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:22:03.192: INFO: namespace namespaces-8631 deletion completed in 9.664407867s
STEP: Destroying namespace "nsdeletetest-2424" for this suite.
Aug 13 02:22:03.281: INFO: Namespace nsdeletetest-2424 was already deleted
STEP: Destroying namespace "nsdeletetest-8830" for this suite.
Aug 13 02:22:09.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:22:12.937: INFO: namespace nsdeletetest-8830 deletion completed in 9.656094546s
•SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:22:12.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:22:27.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3647" for this suite.
Aug 13 02:22:33.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:22:37.119: INFO: namespace watch-3647 deletion completed in 9.664888954s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:22:37.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 02:22:37.756: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1152'
Aug 13 02:22:38.285: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 02:22:38.285: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug 13 02:22:38.376: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-h0con.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-1152'
Aug 13 02:22:38.900: INFO: stderr: ""
Aug 13 02:22:38.900: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:22:38.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1152" for this suite.
Aug 13 02:23:01.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:23:04.643: INFO: namespace kubectl-1152 deletion completed in 25.653161655s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:23:04.643: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845
Aug 13 02:23:05.533: INFO: Pod name my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845: Found 1 pods out of 1
Aug 13 02:23:05.533: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845" are running
Aug 13 02:23:07.713: INFO: Pod "my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845-x9ldf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 02:23:05 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 02:23:05 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 02:23:05 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 02:23:05 +0000 UTC Reason: Message:}])
Aug 13 02:23:07.713: INFO: Trying to dial the pod
Aug 13 02:23:13.073: INFO: Controller my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845: Got expected result from replica 1 [my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845-x9ldf]: "my-hostname-basic-89d3ed79-9e65-41a7-9a50-c579c325e845-x9ldf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:23:13.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8640" for this suite.
Aug 13 02:23:19.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:23:22.817: INFO: namespace replication-controller-8640 deletion completed in 9.65362122s
•SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 13 02:23:22.817: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6172
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-c1a1d6cd-2659-4e26-8ee0-39bdc7b3f7a1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c1a1d6cd-2659-4e26-8ee0-39bdc7b3f7a1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 13 02:23:28.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6172" for this suite.
Aug 13 02:23:50.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 02:23:54.103: INFO: namespace configmap-6172 deletion completed in 25.648399689s
•SSAug 13 02:23:54.103: INFO: Running AfterSuite actions on all nodes
Aug 13 02:23:54.103: INFO: Running AfterSuite actions on node 1
Aug 13 02:23:54.103: INFO: Skipping dumping logs from cluster

Ran 212 of 4413 Specs in 6657.176 seconds
SUCCESS! -- 212 Passed | 0 Failed | 0 Flaked | 0 Pending | 4201 Skipped
PASS

Ginkgo ran 1 suite in 1h51m38.793876829s
Test Suite Passed
