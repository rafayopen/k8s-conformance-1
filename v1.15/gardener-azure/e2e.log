Conformance test: not doing test setup.
I1203 14:48:03.688254    5074 e2e.go:243] Starting e2e run "510d427d-36ca-4b58-895f-acd5419778a2" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575384482 - Will randomize all specs
Will run 215 of 4413 specs

Dec  3 14:48:36.398: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Deleting namespaces
STEP: Waiting for namespaces to vanish
I1203 14:48:36.477798    5074 e2e.go:98] Waiting for deletion of the following namespaces: []
Dec  3 14:48:38.496: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  3 14:48:38.550: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  3 14:48:38.641: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  3 14:48:38.641: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Dec  3 14:48:38.641: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  3 14:48:38.664: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  3 14:48:38.664: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  3 14:48:38.664: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec  3 14:48:38.664: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Dec  3 14:48:38.664: INFO: e2e test version: v1.15.6
Dec  3 14:48:38.681: INFO: kube-apiserver version: v1.15.6
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:48:38.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
Dec  3 14:48:38.774: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec  3 14:48:38.835: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 14:48:51.339: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:48:51.356: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:48:53.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:48:53.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:48:55.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:48:55.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:48:57.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:48:57.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:48:59.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:48:59.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:49:01.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:49:01.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:49:03.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:49:03.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:49:05.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:49:05.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:49:07.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:49:07.375: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 14:49:09.357: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 14:49:09.375: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:49:09.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8033" for this suite.
Dec  3 14:49:29.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:30.186: INFO: namespace container-lifecycle-hook-8033 deletion completed in 20.776104725s
•SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:49:30.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-84c8d8d2-c5de-49d6-ac72-72999568e9be
STEP: Creating a pod to test consume configMaps
Dec  3 14:49:30.458: INFO: Waiting up to 5m0s for pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095" in namespace "configmap-1905" to be "success or failure"
Dec  3 14:49:30.476: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Pending", Reason="", readiness=false. Elapsed: 17.433772ms
Dec  3 14:49:32.494: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035508799s
Dec  3 14:49:34.512: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053970273s
Dec  3 14:49:36.531: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072660028s
Dec  3 14:49:38.550: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091238515s
Dec  3 14:49:40.572: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113829212s
Dec  3 14:49:42.590: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.132066601s
STEP: Saw pod success
Dec  3 14:49:42.591: INFO: Pod "pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095" satisfied condition "success or failure"
Dec  3 14:49:42.609: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 14:49:42.684: INFO: Waiting for pod pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095 to disappear
Dec  3 14:49:42.702: INFO: Pod pod-configmaps-20db946f-c362-4fa1-9480-ba4a6cdaf095 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:49:42.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1905" for this suite.
Dec  3 14:49:48.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:49.499: INFO: namespace configmap-1905 deletion completed in 6.76326315s
•SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:49:49.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2896
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 14:49:49.744: INFO: Waiting up to 5m0s for pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc" in namespace "downward-api-2896" to be "success or failure"
Dec  3 14:49:49.765: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.795817ms
Dec  3 14:49:51.784: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04003464s
Dec  3 14:49:53.802: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058740809s
Dec  3 14:49:55.824: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079980151s
Dec  3 14:49:57.842: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097881165s
Dec  3 14:49:59.860: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.11635834s
STEP: Saw pod success
Dec  3 14:49:59.860: INFO: Pod "downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc" satisfied condition "success or failure"
Dec  3 14:49:59.878: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc container dapi-container: <nil>
STEP: delete the pod
Dec  3 14:49:59.931: INFO: Waiting for pod downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc to disappear
Dec  3 14:49:59.949: INFO: Pod downward-api-7008bc64-f510-4d10-bb9f-2df36432d9cc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:49:59.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2896" for this suite.
Dec  3 14:50:06.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:50:06.740: INFO: namespace downward-api-2896 deletion completed in 6.758550565s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:50:06.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 14:50:06.974: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-803'
Dec  3 14:50:07.127: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 14:50:07.127: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Dec  3 14:50:07.165: INFO: scanned /root for discovery docs: <nil>
Dec  3 14:50:07.165: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-803'
Dec  3 14:50:23.551: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 14:50:23.551: INFO: stdout: "Created e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117\nScaling up e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  3 14:50:23.551: INFO: stdout: "Created e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117\nScaling up e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  3 14:50:23.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-803'
Dec  3 14:50:23.689: INFO: stderr: ""
Dec  3 14:50:23.689: INFO: stdout: "e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117-7mvkd e2e-test-nginx-rc-dxzmr "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Dec  3 14:50:28.689: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-803'
Dec  3 14:50:28.825: INFO: stderr: ""
Dec  3 14:50:28.825: INFO: stdout: "e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117-7mvkd "
Dec  3 14:50:28.825: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117-7mvkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-803'
Dec  3 14:50:28.957: INFO: stderr: ""
Dec  3 14:50:28.957: INFO: stdout: "true"
Dec  3 14:50:28.957: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117-7mvkd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-803'
Dec  3 14:50:29.093: INFO: stderr: ""
Dec  3 14:50:29.093: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  3 14:50:29.093: INFO: e2e-test-nginx-rc-8a1d96c9dfbfe0c0496e860f6bb63117-7mvkd is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Dec  3 14:50:29.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-803'
Dec  3 14:50:29.244: INFO: stderr: ""
Dec  3 14:50:29.244: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:50:29.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-803" for this suite.
Dec  3 14:50:51.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:50:52.024: INFO: namespace kubectl-803 deletion completed in 22.745915006s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:50:52.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec  3 14:50:52.243: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Dec  3 14:50:52.436: INFO: stderr: ""
Dec  3 14:50:52.436: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:50:52.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4287" for this suite.
Dec  3 14:50:58.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:50:59.212: INFO: namespace kubectl-4287 deletion completed in 6.756653374s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:50:59.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-3bc14173-5e6b-4ddb-8c38-7cd23e4f8aec
STEP: Creating a pod to test consume configMaps
Dec  3 14:50:59.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e" in namespace "configmap-7854" to be "success or failure"
Dec  3 14:50:59.495: INFO: Pod "pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.345743ms
Dec  3 14:51:01.513: INFO: Pod "pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035743124s
Dec  3 14:51:03.532: INFO: Pod "pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054499618s
STEP: Saw pod success
Dec  3 14:51:03.532: INFO: Pod "pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e" satisfied condition "success or failure"
Dec  3 14:51:03.550: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 14:51:03.604: INFO: Waiting for pod pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e to disappear
Dec  3 14:51:03.621: INFO: Pod pod-configmaps-55fcd9e9-22ec-4012-8c92-f056c133b84e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:51:03.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7854" for this suite.
Dec  3 14:51:09.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:10.423: INFO: namespace configmap-7854 deletion completed in 6.768504859s
•SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:51:10.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec  3 14:51:14.759: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  3 14:51:19.931: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:51:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-905" for this suite.
Dec  3 14:51:26.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:26.762: INFO: namespace pods-905 deletion completed in 6.79320426s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:51:26.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec  3 14:51:27.007: INFO: Waiting up to 5m0s for pod "var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5" in namespace "var-expansion-6736" to be "success or failure"
Dec  3 14:51:27.027: INFO: Pod "var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.392049ms
Dec  3 14:51:29.046: INFO: Pod "var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039616325s
Dec  3 14:51:31.065: INFO: Pod "var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058008134s
STEP: Saw pod success
Dec  3 14:51:31.065: INFO: Pod "var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5" satisfied condition "success or failure"
Dec  3 14:51:31.082: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5 container dapi-container: <nil>
STEP: delete the pod
Dec  3 14:51:31.135: INFO: Waiting for pod var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5 to disappear
Dec  3 14:51:31.153: INFO: Pod var-expansion-295f7c68-7bc0-4c2d-ab8a-2cafd66403d5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:51:31.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6736" for this suite.
Dec  3 14:51:37.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:37.955: INFO: namespace var-expansion-6736 deletion completed in 6.768567157s
•SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:51:37.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-96b9de93-25ff-4ae5-9e56-0f515afefc0c
STEP: Creating a pod to test consume secrets
Dec  3 14:51:38.241: INFO: Waiting up to 5m0s for pod "pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562" in namespace "secrets-1159" to be "success or failure"
Dec  3 14:51:38.259: INFO: Pod "pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562": Phase="Pending", Reason="", readiness=false. Elapsed: 17.800855ms
Dec  3 14:51:40.278: INFO: Pod "pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036693513s
Dec  3 14:51:42.296: INFO: Pod "pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05495064s
STEP: Saw pod success
Dec  3 14:51:42.296: INFO: Pod "pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562" satisfied condition "success or failure"
Dec  3 14:51:42.314: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 14:51:42.362: INFO: Waiting for pod pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562 to disappear
Dec  3 14:51:42.386: INFO: Pod pod-secrets-a86c543e-f826-4520-b68b-7c983ae93562 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:51:42.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1159" for this suite.
Dec  3 14:51:48.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:49.208: INFO: namespace secrets-1159 deletion completed in 6.788429419s
•S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:51:49.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:51:49.515: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"096d7bd0-d25c-405e-ad6d-b8fc5950245e", Controller:(*bool)(0xc002689bca), BlockOwnerDeletion:(*bool)(0xc002689bcb)}}
Dec  3 14:51:49.533: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0e426179-3f77-467a-a853-aa440a42e76d", Controller:(*bool)(0xc002c25896), BlockOwnerDeletion:(*bool)(0xc002c25897)}}
Dec  3 14:51:49.563: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"bbad67aa-5cb6-49b6-b84f-832e5b2cb165", Controller:(*bool)(0xc002c25a56), BlockOwnerDeletion:(*bool)(0xc002c25a57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:51:54.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4953" for this suite.
Dec  3 14:52:00.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:01.404: INFO: namespace gc-4953 deletion completed in 6.769135274s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:52:01.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6356
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec  3 14:52:01.654: INFO: Waiting up to 5m0s for pod "var-expansion-cee706fd-7710-4b64-b955-e696f693c74f" in namespace "var-expansion-6356" to be "success or failure"
Dec  3 14:52:01.672: INFO: Pod "var-expansion-cee706fd-7710-4b64-b955-e696f693c74f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.991755ms
Dec  3 14:52:03.691: INFO: Pod "var-expansion-cee706fd-7710-4b64-b955-e696f693c74f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036451647s
Dec  3 14:52:05.709: INFO: Pod "var-expansion-cee706fd-7710-4b64-b955-e696f693c74f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054929866s
STEP: Saw pod success
Dec  3 14:52:05.709: INFO: Pod "var-expansion-cee706fd-7710-4b64-b955-e696f693c74f" satisfied condition "success or failure"
Dec  3 14:52:05.726: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod var-expansion-cee706fd-7710-4b64-b955-e696f693c74f container dapi-container: <nil>
STEP: delete the pod
Dec  3 14:52:05.773: INFO: Waiting for pod var-expansion-cee706fd-7710-4b64-b955-e696f693c74f to disappear
Dec  3 14:52:05.790: INFO: Pod var-expansion-cee706fd-7710-4b64-b955-e696f693c74f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:52:05.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6356" for this suite.
Dec  3 14:52:11.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:12.580: INFO: namespace var-expansion-6356 deletion completed in 6.755655574s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:52:12.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec  3 14:52:12.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-2903'
Dec  3 14:52:13.115: INFO: stderr: ""
Dec  3 14:52:13.115: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 14:52:14.134: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:14.134: INFO: Found 0 / 1
Dec  3 14:52:15.134: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:15.134: INFO: Found 0 / 1
Dec  3 14:52:16.135: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:16.135: INFO: Found 0 / 1
Dec  3 14:52:17.134: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:17.134: INFO: Found 0 / 1
Dec  3 14:52:18.134: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:18.134: INFO: Found 1 / 1
Dec  3 14:52:18.134: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  3 14:52:18.152: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:18.152: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 14:52:18.152: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-g4xkf --namespace=kubectl-2903 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  3 14:52:18.305: INFO: stderr: ""
Dec  3 14:52:18.305: INFO: stdout: "pod/redis-master-g4xkf patched\n"
STEP: checking annotations
Dec  3 14:52:18.454: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 14:52:18.454: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:52:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2903" for this suite.
Dec  3 14:52:40.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:41.271: INFO: namespace kubectl-2903 deletion completed in 22.783090818s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:52:41.272: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5f33f459-3052-4539-8aa9-597cef3bf62a
STEP: Creating a pod to test consume secrets
Dec  3 14:52:41.534: INFO: Waiting up to 5m0s for pod "pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3" in namespace "secrets-9526" to be "success or failure"
Dec  3 14:52:41.557: INFO: Pod "pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.749643ms
Dec  3 14:52:43.576: INFO: Pod "pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04151478s
Dec  3 14:52:45.595: INFO: Pod "pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060559658s
STEP: Saw pod success
Dec  3 14:52:45.595: INFO: Pod "pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3" satisfied condition "success or failure"
Dec  3 14:52:45.613: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 14:52:45.667: INFO: Waiting for pod pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3 to disappear
Dec  3 14:52:45.685: INFO: Pod pod-secrets-6195a18a-a310-42f7-8d2e-67501780b2c3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:52:45.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9526" for this suite.
Dec  3 14:52:51.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:52.471: INFO: namespace secrets-9526 deletion completed in 6.75256758s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:52:52.471: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:52:52.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2582" for this suite.
Dec  3 14:52:58.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:59.496: INFO: namespace kubelet-test-2582 deletion completed in 6.73676817s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:52:59.497: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-e6c7282e-7a05-413c-837e-1c010a64f2ee
STEP: Creating a pod to test consume configMaps
Dec  3 14:52:59.768: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce" in namespace "configmap-2895" to be "success or failure"
Dec  3 14:52:59.785: INFO: Pod "pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce": Phase="Pending", Reason="", readiness=false. Elapsed: 17.356961ms
Dec  3 14:53:01.803: INFO: Pod "pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035482486s
Dec  3 14:53:03.824: INFO: Pod "pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055796506s
STEP: Saw pod success
Dec  3 14:53:03.824: INFO: Pod "pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce" satisfied condition "success or failure"
Dec  3 14:53:03.843: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 14:53:03.894: INFO: Waiting for pod pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce to disappear
Dec  3 14:53:03.914: INFO: Pod pod-configmaps-e2078cf9-1106-485d-9c31-1dd80ed527ce no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:53:03.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2895" for this suite.
Dec  3 14:53:10.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:10.686: INFO: namespace configmap-2895 deletion completed in 6.738474566s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:53:10.687: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:53:10.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709" in namespace "downward-api-6333" to be "success or failure"
Dec  3 14:53:10.953: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709": Phase="Pending", Reason="", readiness=false. Elapsed: 17.131938ms
Dec  3 14:53:12.971: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035604119s
Dec  3 14:53:14.990: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054146229s
Dec  3 14:53:17.015: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079372998s
Dec  3 14:53:19.034: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097989441s
Dec  3 14:53:21.052: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.116384545s
STEP: Saw pod success
Dec  3 14:53:21.052: INFO: Pod "downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709" satisfied condition "success or failure"
Dec  3 14:53:21.070: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709 container client-container: <nil>
STEP: delete the pod
Dec  3 14:53:21.131: INFO: Waiting for pod downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709 to disappear
Dec  3 14:53:21.148: INFO: Pod downwardapi-volume-c5d67756-d2a9-498d-9df7-fcd63f74a709 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:53:21.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6333" for this suite.
Dec  3 14:53:27.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:27.945: INFO: namespace downward-api-6333 deletion completed in 6.763195472s
•SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:53:27.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 14:53:31.289: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:53:31.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-430" for this suite.
Dec  3 14:53:37.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:38.098: INFO: namespace container-runtime-430 deletion completed in 6.728667707s
•S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:53:38.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  3 14:53:38.441: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8137,SelfLink:/api/v1/namespaces/watch-8137/configmaps/e2e-watch-test-label-changed,UID:4c901a61-e455-46e9-9fdd-18d8cf624930,ResourceVersion:4494,Generation:0,CreationTimestamp:2019-12-03 14:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 14:53:38.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8137,SelfLink:/api/v1/namespaces/watch-8137/configmaps/e2e-watch-test-label-changed,UID:4c901a61-e455-46e9-9fdd-18d8cf624930,ResourceVersion:4495,Generation:0,CreationTimestamp:2019-12-03 14:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 14:53:38.442: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8137,SelfLink:/api/v1/namespaces/watch-8137/configmaps/e2e-watch-test-label-changed,UID:4c901a61-e455-46e9-9fdd-18d8cf624930,ResourceVersion:4496,Generation:0,CreationTimestamp:2019-12-03 14:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  3 14:53:48.568: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8137,SelfLink:/api/v1/namespaces/watch-8137/configmaps/e2e-watch-test-label-changed,UID:4c901a61-e455-46e9-9fdd-18d8cf624930,ResourceVersion:4523,Generation:0,CreationTimestamp:2019-12-03 14:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 14:53:48.568: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8137,SelfLink:/api/v1/namespaces/watch-8137/configmaps/e2e-watch-test-label-changed,UID:4c901a61-e455-46e9-9fdd-18d8cf624930,ResourceVersion:4524,Generation:0,CreationTimestamp:2019-12-03 14:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  3 14:53:48.569: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8137,SelfLink:/api/v1/namespaces/watch-8137/configmaps/e2e-watch-test-label-changed,UID:4c901a61-e455-46e9-9fdd-18d8cf624930,ResourceVersion:4525,Generation:0,CreationTimestamp:2019-12-03 14:53:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:53:48.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8137" for this suite.
Dec  3 14:53:54.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:55.358: INFO: namespace watch-8137 deletion completed in 6.755932803s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:53:55.359: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:53:55.609: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af" in namespace "downward-api-6044" to be "success or failure"
Dec  3 14:53:55.627: INFO: Pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af": Phase="Pending", Reason="", readiness=false. Elapsed: 17.320259ms
Dec  3 14:53:57.646: INFO: Pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036482905s
Dec  3 14:53:59.665: INFO: Pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055139341s
Dec  3 14:54:01.683: INFO: Pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073880521s
Dec  3 14:54:03.702: INFO: Pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.092156313s
STEP: Saw pod success
Dec  3 14:54:03.702: INFO: Pod "downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af" satisfied condition "success or failure"
Dec  3 14:54:03.719: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af container client-container: <nil>
STEP: delete the pod
Dec  3 14:54:03.776: INFO: Waiting for pod downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af to disappear
Dec  3 14:54:03.794: INFO: Pod downwardapi-volume-0989ddf4-349f-4fb2-ac69-404800e984af no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:54:03.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6044" for this suite.
Dec  3 14:54:09.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:54:10.604: INFO: namespace downward-api-6044 deletion completed in 6.764430381s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:54:10.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 14:54:10.850: INFO: Waiting up to 5m0s for pod "pod-6639e852-4597-4640-8b1d-60e3abc10aa4" in namespace "emptydir-447" to be "success or failure"
Dec  3 14:54:10.867: INFO: Pod "pod-6639e852-4597-4640-8b1d-60e3abc10aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.291254ms
Dec  3 14:54:12.885: INFO: Pod "pod-6639e852-4597-4640-8b1d-60e3abc10aa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035381398s
Dec  3 14:54:14.904: INFO: Pod "pod-6639e852-4597-4640-8b1d-60e3abc10aa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054513266s
STEP: Saw pod success
Dec  3 14:54:14.905: INFO: Pod "pod-6639e852-4597-4640-8b1d-60e3abc10aa4" satisfied condition "success or failure"
Dec  3 14:54:14.922: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-6639e852-4597-4640-8b1d-60e3abc10aa4 container test-container: <nil>
STEP: delete the pod
Dec  3 14:54:14.977: INFO: Waiting for pod pod-6639e852-4597-4640-8b1d-60e3abc10aa4 to disappear
Dec  3 14:54:14.994: INFO: Pod pod-6639e852-4597-4640-8b1d-60e3abc10aa4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:54:14.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-447" for this suite.
Dec  3 14:54:21.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:54:21.792: INFO: namespace emptydir-447 deletion completed in 6.764350541s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:54:21.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:54:22.116: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  3 14:54:22.161: INFO: Number of nodes with available pods: 0
Dec  3 14:54:22.161: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  3 14:54:22.240: INFO: Number of nodes with available pods: 0
Dec  3 14:54:22.240: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:23.259: INFO: Number of nodes with available pods: 0
Dec  3 14:54:23.259: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:24.259: INFO: Number of nodes with available pods: 0
Dec  3 14:54:24.259: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:25.260: INFO: Number of nodes with available pods: 0
Dec  3 14:54:25.260: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:26.259: INFO: Number of nodes with available pods: 0
Dec  3 14:54:26.259: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:27.258: INFO: Number of nodes with available pods: 0
Dec  3 14:54:27.258: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:28.258: INFO: Number of nodes with available pods: 0
Dec  3 14:54:28.258: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:29.258: INFO: Number of nodes with available pods: 1
Dec  3 14:54:29.258: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  3 14:54:29.334: INFO: Number of nodes with available pods: 0
Dec  3 14:54:29.334: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  3 14:54:29.373: INFO: Number of nodes with available pods: 0
Dec  3 14:54:29.373: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:30.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:30.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:31.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:31.392: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:32.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:32.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:33.392: INFO: Number of nodes with available pods: 0
Dec  3 14:54:33.392: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:34.396: INFO: Number of nodes with available pods: 0
Dec  3 14:54:34.396: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:35.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:35.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:36.392: INFO: Number of nodes with available pods: 0
Dec  3 14:54:36.392: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:37.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:37.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:38.392: INFO: Number of nodes with available pods: 0
Dec  3 14:54:38.392: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:39.393: INFO: Number of nodes with available pods: 0
Dec  3 14:54:39.393: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:40.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:40.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:41.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:41.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:42.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:42.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:43.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:43.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:44.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:44.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:45.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:45.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:46.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:46.392: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:47.391: INFO: Number of nodes with available pods: 0
Dec  3 14:54:47.391: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 14:54:48.391: INFO: Number of nodes with available pods: 1
Dec  3 14:54:48.391: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3947, will wait for the garbage collector to delete the pods
Dec  3 14:54:48.515: INFO: Deleting DaemonSet.extensions daemon-set took: 20.12092ms
Dec  3 14:54:48.915: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.29757ms
Dec  3 14:54:54.833: INFO: Number of nodes with available pods: 0
Dec  3 14:54:54.833: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 14:54:54.851: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3947/daemonsets","resourceVersion":"4762"},"items":null}

Dec  3 14:54:54.872: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3947/pods","resourceVersion":"4762"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:54:54.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3947" for this suite.
Dec  3 14:55:01.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:55:01.731: INFO: namespace daemonsets-3947 deletion completed in 6.74516457s
•SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:55:01.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe in namespace container-probe-2205
Dec  3 14:55:08.015: INFO: Started pod liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe in namespace container-probe-2205
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 14:55:08.033: INFO: Initial restart count of pod liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe is 0
Dec  3 14:55:24.200: INFO: Restart count of pod container-probe-2205/liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe is now 1 (16.166982017s elapsed)
Dec  3 14:55:44.384: INFO: Restart count of pod container-probe-2205/liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe is now 2 (36.351114097s elapsed)
Dec  3 14:56:04.569: INFO: Restart count of pod container-probe-2205/liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe is now 3 (56.53579051s elapsed)
Dec  3 14:56:24.754: INFO: Restart count of pod container-probe-2205/liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe is now 4 (1m16.720827812s elapsed)
Dec  3 14:57:25.308: INFO: Restart count of pod container-probe-2205/liveness-eeb1e79e-bbcb-4b43-839e-e2b3f0d9d9fe is now 5 (2m17.275101929s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:57:25.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2205" for this suite.
Dec  3 14:57:31.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:57:32.142: INFO: namespace container-probe-2205 deletion completed in 6.776266755s
•S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:57:32.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4664
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-ac804a40-eb89-40c3-a441-ba093a9062a8
STEP: Creating a pod to test consume configMaps
Dec  3 14:57:32.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef" in namespace "configmap-4664" to be "success or failure"
Dec  3 14:57:32.423: INFO: Pod "pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef": Phase="Pending", Reason="", readiness=false. Elapsed: 17.14049ms
Dec  3 14:57:34.441: INFO: Pod "pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035569431s
STEP: Saw pod success
Dec  3 14:57:34.441: INFO: Pod "pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef" satisfied condition "success or failure"
Dec  3 14:57:34.458: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 14:57:34.510: INFO: Waiting for pod pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef to disappear
Dec  3 14:57:34.528: INFO: Pod pod-configmaps-406172e2-1036-45f5-80af-bac7056142ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:57:34.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4664" for this suite.
Dec  3 14:57:40.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:57:41.317: INFO: namespace configmap-4664 deletion completed in 6.755683549s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:57:41.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-033b7d50-8c8f-4e79-a88d-078fde25e3e1
STEP: Creating a pod to test consume secrets
Dec  3 14:57:41.587: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e" in namespace "projected-1986" to be "success or failure"
Dec  3 14:57:41.604: INFO: Pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.577133ms
Dec  3 14:57:43.623: INFO: Pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035915419s
Dec  3 14:57:45.641: INFO: Pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05425974s
Dec  3 14:57:47.659: INFO: Pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072481043s
Dec  3 14:57:49.678: INFO: Pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.090785917s
STEP: Saw pod success
Dec  3 14:57:49.678: INFO: Pod "pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e" satisfied condition "success or failure"
Dec  3 14:57:49.695: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 14:57:49.743: INFO: Waiting for pod pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e to disappear
Dec  3 14:57:49.761: INFO: Pod pod-projected-secrets-f4b491f4-6148-41d1-8bb9-eab0f3f4df1e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:57:49.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1986" for this suite.
Dec  3 14:57:55.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:57:56.576: INFO: namespace projected-1986 deletion completed in 6.782000824s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:57:56.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:58:02.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5723" for this suite.
Dec  3 14:58:08.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:09.194: INFO: namespace watch-5723 deletion completed in 6.800014697s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:58:09.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-nfjp
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 14:58:09.520: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nfjp" in namespace "subpath-1969" to be "success or failure"
Dec  3 14:58:09.538: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Pending", Reason="", readiness=false. Elapsed: 17.284842ms
Dec  3 14:58:11.556: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036112442s
Dec  3 14:58:13.575: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 4.055086162s
Dec  3 14:58:15.594: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 6.073899128s
Dec  3 14:58:17.613: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 8.092667277s
Dec  3 14:58:19.632: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 10.111911157s
Dec  3 14:58:21.651: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 12.130576089s
Dec  3 14:58:23.671: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 14.151087138s
Dec  3 14:58:25.690: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 16.169528472s
Dec  3 14:58:27.708: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 18.188017172s
Dec  3 14:58:29.726: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 20.206123057s
Dec  3 14:58:31.745: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Running", Reason="", readiness=true. Elapsed: 22.224938799s
Dec  3 14:58:33.764: INFO: Pod "pod-subpath-test-configmap-nfjp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.243192413s
STEP: Saw pod success
Dec  3 14:58:33.764: INFO: Pod "pod-subpath-test-configmap-nfjp" satisfied condition "success or failure"
Dec  3 14:58:33.781: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-subpath-test-configmap-nfjp container test-container-subpath-configmap-nfjp: <nil>
STEP: delete the pod
Dec  3 14:58:33.829: INFO: Waiting for pod pod-subpath-test-configmap-nfjp to disappear
Dec  3 14:58:33.847: INFO: Pod pod-subpath-test-configmap-nfjp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nfjp
Dec  3 14:58:33.847: INFO: Deleting pod "pod-subpath-test-configmap-nfjp" in namespace "subpath-1969"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:58:33.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1969" for this suite.
Dec  3 14:58:39.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:40.662: INFO: namespace subpath-1969 deletion completed in 6.764209209s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:58:40.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 14:58:40.901: INFO: Waiting up to 5m0s for pod "pod-b4d950e2-88ef-4d41-9b22-08835e62a92a" in namespace "emptydir-2953" to be "success or failure"
Dec  3 14:58:40.919: INFO: Pod "pod-b4d950e2-88ef-4d41-9b22-08835e62a92a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.743615ms
Dec  3 14:58:42.938: INFO: Pod "pod-b4d950e2-88ef-4d41-9b22-08835e62a92a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036236851s
Dec  3 14:58:44.956: INFO: Pod "pod-b4d950e2-88ef-4d41-9b22-08835e62a92a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05464111s
Dec  3 14:58:46.974: INFO: Pod "pod-b4d950e2-88ef-4d41-9b22-08835e62a92a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.072792248s
STEP: Saw pod success
Dec  3 14:58:46.974: INFO: Pod "pod-b4d950e2-88ef-4d41-9b22-08835e62a92a" satisfied condition "success or failure"
Dec  3 14:58:46.992: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-b4d950e2-88ef-4d41-9b22-08835e62a92a container test-container: <nil>
STEP: delete the pod
Dec  3 14:58:47.056: INFO: Waiting for pod pod-b4d950e2-88ef-4d41-9b22-08835e62a92a to disappear
Dec  3 14:58:47.074: INFO: Pod pod-b4d950e2-88ef-4d41-9b22-08835e62a92a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:58:47.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2953" for this suite.
Dec  3 14:58:53.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:53.888: INFO: namespace emptydir-2953 deletion completed in 6.781207814s
•SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:58:53.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 14:58:54.120: INFO: PodSpec: initContainers in spec.initContainers
Dec  3 14:59:43.816: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-cd79463e-e9b9-4244-9e7d-d661e0442a7f", GenerateName:"", Namespace:"init-container-9178", SelfLink:"/api/v1/namespaces/init-container-9178/pods/pod-init-cd79463e-e9b9-4244-9e7d-d661e0442a7f", UID:"61cd0a64-be78-4f58-bcbe-507055f222f1", ResourceVersion:"5685", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63710981934, loc:(*time.Location)(0x7ed1a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"119989703"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.64.1.26/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wrn5h", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0020ebc80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wrn5h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wrn5h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wrn5h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001cadee8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00252b380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001cadf60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001cadf80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001cadf88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001cadf8c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981934, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981934, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981934, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981934, loc:(*time.Location)(0x7ed1a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.4", PodIP:"100.64.1.26", StartTime:(*v1.Time)(0xc000ecf0a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002752d90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002752fc0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://077cd3d7322f02d48442feffdf73c88ebefe2ebc15414350e6bc7266e2531ec4"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000ecf120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000ecf0e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:59:43.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9178" for this suite.
Dec  3 15:00:05.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:06.601: INFO: namespace init-container-9178 deletion completed in 22.748484518s
•SSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:00:06.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-c6ed6f15-2154-4057-95c4-baa738ed7c44 in namespace container-probe-8697
Dec  3 15:00:12.895: INFO: Started pod test-webserver-c6ed6f15-2154-4057-95c4-baa738ed7c44 in namespace container-probe-8697
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:00:12.913: INFO: Initial restart count of pod test-webserver-c6ed6f15-2154-4057-95c4-baa738ed7c44 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:04:13.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8697" for this suite.
Dec  3 15:04:19.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:19.937: INFO: namespace container-probe-8697 deletion completed in 6.743929745s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:04:19.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-gbl5
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:04:20.234: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-gbl5" in namespace "subpath-5608" to be "success or failure"
Dec  3 15:04:20.252: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.909811ms
Dec  3 15:04:22.270: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036539922s
Dec  3 15:04:24.289: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 4.055312459s
Dec  3 15:04:26.307: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 6.073363665s
Dec  3 15:04:28.325: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 8.091525254s
Dec  3 15:04:30.344: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 10.109871412s
Dec  3 15:04:32.362: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 12.128326366s
Dec  3 15:04:34.381: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 14.146738999s
Dec  3 15:04:36.399: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 16.165476871s
Dec  3 15:04:38.417: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 18.183702277s
Dec  3 15:04:40.450: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 20.215925263s
Dec  3 15:04:42.468: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Running", Reason="", readiness=true. Elapsed: 22.234718742s
Dec  3 15:04:44.487: INFO: Pod "pod-subpath-test-downwardapi-gbl5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.252952948s
STEP: Saw pod success
Dec  3 15:04:44.487: INFO: Pod "pod-subpath-test-downwardapi-gbl5" satisfied condition "success or failure"
Dec  3 15:04:44.505: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-subpath-test-downwardapi-gbl5 container test-container-subpath-downwardapi-gbl5: <nil>
STEP: delete the pod
Dec  3 15:04:44.560: INFO: Waiting for pod pod-subpath-test-downwardapi-gbl5 to disappear
Dec  3 15:04:44.577: INFO: Pod pod-subpath-test-downwardapi-gbl5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-gbl5
Dec  3 15:04:44.577: INFO: Deleting pod "pod-subpath-test-downwardapi-gbl5" in namespace "subpath-5608"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:04:44.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5608" for this suite.
Dec  3 15:04:50.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:51.427: INFO: namespace subpath-5608 deletion completed in 6.797601104s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:04:51.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:04:51.695: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 15:04:55.731: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  3 15:04:57.750: INFO: Creating deployment "test-rollover-deployment"
Dec  3 15:04:57.794: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  3 15:04:59.830: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  3 15:04:59.866: INFO: Ensure that both replica sets have 1 created replica
Dec  3 15:04:59.901: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  3 15:04:59.938: INFO: Updating deployment test-rollover-deployment
Dec  3 15:04:59.938: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  3 15:05:01.974: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  3 15:05:02.010: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  3 15:05:02.046: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:05:02.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982300, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:05:04.083: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:05:04.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982302, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:05:06.084: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:05:06.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982302, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:05:08.084: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:05:08.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982302, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:05:10.083: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:05:10.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982302, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:05:12.083: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 15:05:12.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982302, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982297, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:05:14.083: INFO: 
Dec  3 15:05:14.083: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 15:05:14.139: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7373,SelfLink:/apis/apps/v1/namespaces/deployment-7373/deployments/test-rollover-deployment,UID:ca2c191f-9f33-4216-b2b8-6a3b38d8dc65,ResourceVersion:6571,Generation:2,CreationTimestamp:2019-12-03 15:04:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 15:04:57 +0000 UTC 2019-12-03 15:04:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 15:05:12 +0000 UTC 2019-12-03 15:04:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 15:05:14.157: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7373,SelfLink:/apis/apps/v1/namespaces/deployment-7373/replicasets/test-rollover-deployment-854595fc44,UID:95a7769b-d47c-44f3-b30d-dc1f98e6fabc,ResourceVersion:6564,Generation:2,CreationTimestamp:2019-12-03 15:04:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ca2c191f-9f33-4216-b2b8-6a3b38d8dc65 0xc003829a47 0xc003829a48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 15:05:14.157: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  3 15:05:14.157: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7373,SelfLink:/apis/apps/v1/namespaces/deployment-7373/replicasets/test-rollover-controller,UID:0464edf0-8dfe-46f2-a3bc-20a8846ab793,ResourceVersion:6570,Generation:2,CreationTimestamp:2019-12-03 15:04:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ca2c191f-9f33-4216-b2b8-6a3b38d8dc65 0xc00382996f 0xc003829980}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:05:14.158: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7373,SelfLink:/apis/apps/v1/namespaces/deployment-7373/replicasets/test-rollover-deployment-9b8b997cf,UID:a455aea9-6bd6-41ae-b925-6928de3fea2f,ResourceVersion:6530,Generation:2,CreationTimestamp:2019-12-03 15:04:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ca2c191f-9f33-4216-b2b8-6a3b38d8dc65 0xc003829b00 0xc003829b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:05:14.176: INFO: Pod "test-rollover-deployment-854595fc44-6b75w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-6b75w,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7373,SelfLink:/api/v1/namespaces/deployment-7373/pods/test-rollover-deployment-854595fc44-6b75w,UID:3f00dc2f-155d-448e-ba0c-5a7f62cc96c9,ResourceVersion:6537,Generation:0,CreationTimestamp:2019-12-03 15:04:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.31/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 95a7769b-d47c-44f3-b30d-dc1f98e6fabc 0xc001b966c7 0xc001b966c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qvbs9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qvbs9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-qvbs9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b96730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b96750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:04:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:05:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:05:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:04:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.31,StartTime:2019-12-03 15:04:59 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 15:05:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4d9c281d3d090f4c8008275024e55a36683fe7e38a8c598acacd42045154ca69}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:14.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7373" for this suite.
Dec  3 15:05:20.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:20.980: INFO: namespace deployment-7373 deletion completed in 6.768236891s
•SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:05:20.980: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-ad6e80d2-77b6-4026-8136-54b2e95bcdd0
STEP: Creating a pod to test consume configMaps
Dec  3 15:05:21.241: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404" in namespace "projected-6314" to be "success or failure"
Dec  3 15:05:21.258: INFO: Pod "pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404": Phase="Pending", Reason="", readiness=false. Elapsed: 17.727229ms
Dec  3 15:05:23.277: INFO: Pod "pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036358217s
Dec  3 15:05:25.295: INFO: Pod "pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05459659s
STEP: Saw pod success
Dec  3 15:05:25.295: INFO: Pod "pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404" satisfied condition "success or failure"
Dec  3 15:05:25.313: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:05:25.369: INFO: Waiting for pod pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404 to disappear
Dec  3 15:05:25.387: INFO: Pod pod-projected-configmaps-041ce8c3-0712-4c50-9476-f7cb42c0a404 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:25.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6314" for this suite.
Dec  3 15:05:31.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:32.171: INFO: namespace projected-6314 deletion completed in 6.749759621s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:05:32.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7912
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9a3fb45c-84a3-426a-a387-aeed9464c670
STEP: Creating secret with name s-test-opt-upd-5649a4bc-e766-47d4-bd8d-8f298baa0db8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9a3fb45c-84a3-426a-a387-aeed9464c670
STEP: Updating secret s-test-opt-upd-5649a4bc-e766-47d4-bd8d-8f298baa0db8
STEP: Creating secret with name s-test-opt-create-edb98a6a-b598-4317-93e2-54110c33279a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:07:00.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7912" for this suite.
Dec  3 15:07:22.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:22.986: INFO: namespace projected-7912 deletion completed in 22.748771995s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:07:22.986: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:07:23.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53" in namespace "projected-7530" to be "success or failure"
Dec  3 15:07:23.271: INFO: Pod "downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53": Phase="Pending", Reason="", readiness=false. Elapsed: 17.979063ms
Dec  3 15:07:25.289: INFO: Pod "downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036421238s
Dec  3 15:07:27.308: INFO: Pod "downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055012363s
STEP: Saw pod success
Dec  3 15:07:27.308: INFO: Pod "downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53" satisfied condition "success or failure"
Dec  3 15:07:27.326: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53 container client-container: <nil>
STEP: delete the pod
Dec  3 15:07:27.379: INFO: Waiting for pod downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53 to disappear
Dec  3 15:07:27.396: INFO: Pod downwardapi-volume-bfbc298f-dfe1-43f7-93d4-e95c86e22b53 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:07:27.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7530" for this suite.
Dec  3 15:07:33.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:34.174: INFO: namespace projected-7530 deletion completed in 6.743768402s
•SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:07:34.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:07:39.029: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c4fc5886-0a72-4770-8d8e-90239ae99fcc"
Dec  3 15:07:39.029: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c4fc5886-0a72-4770-8d8e-90239ae99fcc" in namespace "pods-5233" to be "terminated due to deadline exceeded"
Dec  3 15:07:39.047: INFO: Pod "pod-update-activedeadlineseconds-c4fc5886-0a72-4770-8d8e-90239ae99fcc": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 17.3478ms
Dec  3 15:07:39.047: INFO: Pod "pod-update-activedeadlineseconds-c4fc5886-0a72-4770-8d8e-90239ae99fcc" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:07:39.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5233" for this suite.
Dec  3 15:07:45.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:45.821: INFO: namespace pods-5233 deletion completed in 6.740435472s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:07:45.822: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6203
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-lt2r
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:07:46.108: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lt2r" in namespace "subpath-6203" to be "success or failure"
Dec  3 15:07:46.125: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Pending", Reason="", readiness=false. Elapsed: 17.647677ms
Dec  3 15:07:48.144: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 2.036010498s
Dec  3 15:07:50.162: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 4.054749892s
Dec  3 15:07:52.181: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 6.072954202s
Dec  3 15:07:54.199: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 8.091499433s
Dec  3 15:07:56.218: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 10.110081915s
Dec  3 15:07:58.237: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 12.129073974s
Dec  3 15:08:00.255: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 14.147649043s
Dec  3 15:08:02.275: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 16.167291493s
Dec  3 15:08:04.294: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 18.186762438s
Dec  3 15:08:06.313: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Running", Reason="", readiness=true. Elapsed: 20.205411288s
Dec  3 15:08:08.331: INFO: Pod "pod-subpath-test-secret-lt2r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.223610538s
STEP: Saw pod success
Dec  3 15:08:08.331: INFO: Pod "pod-subpath-test-secret-lt2r" satisfied condition "success or failure"
Dec  3 15:08:08.349: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-subpath-test-secret-lt2r container test-container-subpath-secret-lt2r: <nil>
STEP: delete the pod
Dec  3 15:08:08.405: INFO: Waiting for pod pod-subpath-test-secret-lt2r to disappear
Dec  3 15:08:08.423: INFO: Pod pod-subpath-test-secret-lt2r no longer exists
STEP: Deleting pod pod-subpath-test-secret-lt2r
Dec  3 15:08:08.423: INFO: Deleting pod "pod-subpath-test-secret-lt2r" in namespace "subpath-6203"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:08:08.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6203" for this suite.
Dec  3 15:08:14.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:15.214: INFO: namespace subpath-6203 deletion completed in 6.739484341s
•SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:08:15.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d885eb7d-ce0d-433e-bb63-b64d16b2b85c in namespace container-probe-3811
Dec  3 15:08:17.505: INFO: Started pod busybox-d885eb7d-ce0d-433e-bb63-b64d16b2b85c in namespace container-probe-3811
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:08:17.523: INFO: Initial restart count of pod busybox-d885eb7d-ce0d-433e-bb63-b64d16b2b85c is 0
Dec  3 15:09:10.034: INFO: Restart count of pod container-probe-3811/busybox-d885eb7d-ce0d-433e-bb63-b64d16b2b85c is now 1 (52.511337428s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:10.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3811" for this suite.
Dec  3 15:09:16.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:16.855: INFO: namespace container-probe-3811 deletion completed in 6.763301162s
•SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:16.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-395540eb-3d59-4b15-b7f1-87c54da8e8f8
STEP: Creating a pod to test consume secrets
Dec  3 15:09:17.130: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9" in namespace "projected-838" to be "success or failure"
Dec  3 15:09:17.148: INFO: Pod "pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.778704ms
Dec  3 15:09:19.167: INFO: Pod "pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03682514s
Dec  3 15:09:21.186: INFO: Pod "pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055374929s
STEP: Saw pod success
Dec  3 15:09:21.186: INFO: Pod "pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9" satisfied condition "success or failure"
Dec  3 15:09:21.205: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:09:21.253: INFO: Waiting for pod pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9 to disappear
Dec  3 15:09:21.271: INFO: Pod pod-projected-secrets-a4a9e9d2-843b-415f-813d-cb83e636fcd9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:21.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-838" for this suite.
Dec  3 15:09:27.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:28.054: INFO: namespace projected-838 deletion completed in 6.749640036s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:28.054: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 15:09:28.320: INFO: Waiting up to 5m0s for pod "pod-05a8c86a-950d-4e83-94aa-4766ced4538b" in namespace "emptydir-8403" to be "success or failure"
Dec  3 15:09:28.338: INFO: Pod "pod-05a8c86a-950d-4e83-94aa-4766ced4538b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.95316ms
Dec  3 15:09:30.359: INFO: Pod "pod-05a8c86a-950d-4e83-94aa-4766ced4538b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038938136s
Dec  3 15:09:32.378: INFO: Pod "pod-05a8c86a-950d-4e83-94aa-4766ced4538b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057618737s
STEP: Saw pod success
Dec  3 15:09:32.378: INFO: Pod "pod-05a8c86a-950d-4e83-94aa-4766ced4538b" satisfied condition "success or failure"
Dec  3 15:09:32.396: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-05a8c86a-950d-4e83-94aa-4766ced4538b container test-container: <nil>
STEP: delete the pod
Dec  3 15:09:32.465: INFO: Waiting for pod pod-05a8c86a-950d-4e83-94aa-4766ced4538b to disappear
Dec  3 15:09:32.482: INFO: Pod pod-05a8c86a-950d-4e83-94aa-4766ced4538b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:32.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8403" for this suite.
Dec  3 15:09:38.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:39.273: INFO: namespace emptydir-8403 deletion completed in 6.75637562s
•SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:39.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-7183e9ee-86a8-4907-890d-d043dddcb2ec
STEP: Creating a pod to test consume configMaps
Dec  3 15:09:39.539: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a" in namespace "projected-9871" to be "success or failure"
Dec  3 15:09:39.557: INFO: Pod "pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.513735ms
Dec  3 15:09:41.575: INFO: Pod "pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035747385s
Dec  3 15:09:43.594: INFO: Pod "pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054696428s
STEP: Saw pod success
Dec  3 15:09:43.594: INFO: Pod "pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a" satisfied condition "success or failure"
Dec  3 15:09:43.611: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:09:43.666: INFO: Waiting for pod pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a to disappear
Dec  3 15:09:43.684: INFO: Pod pod-projected-configmaps-58663887-aef6-456c-afde-086f6250042a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:43.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9871" for this suite.
Dec  3 15:09:49.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:50.461: INFO: namespace projected-9871 deletion completed in 6.744045806s
•SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:50.462: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3276
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-4fb331de-6b53-491c-85db-4e3dd89286e6
STEP: Creating a pod to test consume secrets
Dec  3 15:09:50.725: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4" in namespace "projected-3276" to be "success or failure"
Dec  3 15:09:50.743: INFO: Pod "pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.854508ms
Dec  3 15:09:52.762: INFO: Pod "pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037034484s
Dec  3 15:09:54.780: INFO: Pod "pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055218771s
STEP: Saw pod success
Dec  3 15:09:54.780: INFO: Pod "pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4" satisfied condition "success or failure"
Dec  3 15:09:54.798: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:09:54.859: INFO: Waiting for pod pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4 to disappear
Dec  3 15:09:54.877: INFO: Pod pod-projected-secrets-2e3d2a1e-be24-4545-b7f3-567e49d4dff4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:54.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3276" for this suite.
Dec  3 15:10:00.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:01.725: INFO: namespace projected-3276 deletion completed in 6.813934518s
•SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:10:01.725: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:10:01.982: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7" in namespace "downward-api-5618" to be "success or failure"
Dec  3 15:10:02.003: INFO: Pod "downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.159704ms
Dec  3 15:10:04.021: INFO: Pod "downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038316298s
Dec  3 15:10:06.040: INFO: Pod "downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05728104s
STEP: Saw pod success
Dec  3 15:10:06.040: INFO: Pod "downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7" satisfied condition "success or failure"
Dec  3 15:10:06.058: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7 container client-container: <nil>
STEP: delete the pod
Dec  3 15:10:06.117: INFO: Waiting for pod downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7 to disappear
Dec  3 15:10:06.134: INFO: Pod downwardapi-volume-dc1b39f9-7f51-4757-b01d-f12bf59edcc7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:10:06.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5618" for this suite.
Dec  3 15:10:12.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:12.914: INFO: namespace downward-api-5618 deletion completed in 6.745639187s
•SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:10:12.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9497
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  3 15:10:13.140: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:10:13.177: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:10:13.194: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 before test
Dec  3 15:10:13.238: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-82b4r from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:10:13.238: INFO: coredns-858b686868-8npzw from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:10:13.238: INFO: kube-proxy-m56xr from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:10:13.238: INFO: calico-typha-vertical-autoscaler-656557779f-9ms99 from kube-system started at 2019-12-03 14:34:49 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container autoscaler ready: true, restart count 6
Dec  3 15:10:13.238: INFO: blackbox-exporter-c87bdd467-6qxtm from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:10:13.238: INFO: coredns-858b686868-vmthw from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:10:13.238: INFO: metrics-server-6c6b44bdf-6fqbp from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:10:13.238: INFO: addons-nginx-ingress-controller-8468678b64-k2xb6 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:10:13.238: INFO: calico-node-8578h from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:10:13.238: INFO: node-exporter-9bgsx from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:10:13.238: INFO: calico-kube-controllers-5d785bc598-vwwh6 from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Dec  3 15:10:13.238: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-t2scl from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:10:13.238: INFO: addons-kubernetes-dashboard-5c8d9945bc-q67s2 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:10:13.238: INFO: vpn-shoot-76b5996f55-sb8l6 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:10:13.238: INFO: node-problem-detector-9qtgm from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.238: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:10:13.238: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf before test
Dec  3 15:10:13.276: INFO: kube-proxy-xrd2g from kube-system started at 2019-12-03 14:34:17 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.276: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:10:13.276: INFO: node-problem-detector-phmbb from kube-system started at 2019-12-03 14:34:17 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.276: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:10:13.276: INFO: calico-typha-deploy-5547c4cdc6-6ngrt from kube-system started at 2019-12-03 14:43:06 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.276: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:10:13.276: INFO: calico-node-nddkb from kube-system started at 2019-12-03 14:34:16 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.276: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:10:13.276: INFO: node-exporter-m8zkr from kube-system started at 2019-12-03 14:34:16 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:13.276: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7630c133-bfce-4ee8-b83b-a5219ff29e4d 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7630c133-bfce-4ee8-b83b-a5219ff29e4d off the node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7630c133-bfce-4ee8-b83b-a5219ff29e4d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:10:21.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9497" for this suite.
Dec  3 15:10:39.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:40.302: INFO: namespace sched-pred-9497 deletion completed in 18.738255375s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:10:40.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  3 15:10:40.521: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:10:40.561: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:10:40.578: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 before test
Dec  3 15:10:40.619: INFO: addons-nginx-ingress-controller-8468678b64-k2xb6 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:10:40.619: INFO: calico-node-8578h from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:10:40.619: INFO: coredns-858b686868-vmthw from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:10:40.619: INFO: metrics-server-6c6b44bdf-6fqbp from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:10:40.619: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-t2scl from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:10:40.619: INFO: addons-kubernetes-dashboard-5c8d9945bc-q67s2 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:10:40.619: INFO: vpn-shoot-76b5996f55-sb8l6 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:10:40.619: INFO: node-problem-detector-9qtgm from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:10:40.619: INFO: node-exporter-9bgsx from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:10:40.619: INFO: calico-kube-controllers-5d785bc598-vwwh6 from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Dec  3 15:10:40.619: INFO: coredns-858b686868-8npzw from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:10:40.619: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-82b4r from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:10:40.619: INFO: blackbox-exporter-c87bdd467-6qxtm from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:10:40.619: INFO: kube-proxy-m56xr from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:10:40.619: INFO: calico-typha-vertical-autoscaler-656557779f-9ms99 from kube-system started at 2019-12-03 14:34:49 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.619: INFO: 	Container autoscaler ready: true, restart count 6
Dec  3 15:10:40.619: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf before test
Dec  3 15:10:40.659: INFO: calico-node-nddkb from kube-system started at 2019-12-03 14:34:16 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.659: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:10:40.659: INFO: node-exporter-m8zkr from kube-system started at 2019-12-03 14:34:16 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.659: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:10:40.659: INFO: kube-proxy-xrd2g from kube-system started at 2019-12-03 14:34:17 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.659: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:10:40.659: INFO: node-problem-detector-phmbb from kube-system started at 2019-12-03 14:34:17 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.659: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:10:40.659: INFO: calico-typha-deploy-5547c4cdc6-6ngrt from kube-system started at 2019-12-03 14:43:06 +0000 UTC (1 container statuses recorded)
Dec  3 15:10:40.659: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
STEP: verifying the node has the label node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
Dec  3 15:10:40.777: INFO: Pod addons-kubernetes-dashboard-5c8d9945bc-q67s2 requesting resource cpu=50m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod addons-nginx-ingress-controller-8468678b64-k2xb6 requesting resource cpu=100m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-t2scl requesting resource cpu=0m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod blackbox-exporter-c87bdd467-6qxtm requesting resource cpu=5m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod calico-kube-controllers-5d785bc598-vwwh6 requesting resource cpu=0m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod calico-node-8578h requesting resource cpu=100m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod calico-node-nddkb requesting resource cpu=100m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
Dec  3 15:10:40.777: INFO: Pod calico-typha-deploy-5547c4cdc6-6ngrt requesting resource cpu=0m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
Dec  3 15:10:40.777: INFO: Pod calico-typha-horizontal-autoscaler-554dfbfdd7-82b4r requesting resource cpu=10m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod calico-typha-vertical-autoscaler-656557779f-9ms99 requesting resource cpu=0m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod coredns-858b686868-8npzw requesting resource cpu=50m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod coredns-858b686868-vmthw requesting resource cpu=50m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod kube-proxy-m56xr requesting resource cpu=20m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod kube-proxy-xrd2g requesting resource cpu=20m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
Dec  3 15:10:40.777: INFO: Pod metrics-server-6c6b44bdf-6fqbp requesting resource cpu=20m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod node-exporter-9bgsx requesting resource cpu=5m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod node-exporter-m8zkr requesting resource cpu=5m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
Dec  3 15:10:40.777: INFO: Pod node-problem-detector-9qtgm requesting resource cpu=20m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
Dec  3 15:10:40.777: INFO: Pod node-problem-detector-phmbb requesting resource cpu=20m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
Dec  3 15:10:40.777: INFO: Pod vpn-shoot-76b5996f55-sb8l6 requesting resource cpu=100m on Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c.15dce5135138cc90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1218/filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c to shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c.15dce51399bd34ce], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c.15dce513bb5f204b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c.15dce513d3388946], Reason = [Created], Message = [Created container filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c.15dce513e89f5cbc], Reason = [Started], Message = [Started container filler-pod-ad9e159f-a298-4019-b6eb-c6c4656aa49c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041.15dce5135294c150], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1218/filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041 to shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041.15dce513a61a23c2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041.15dce513cf73228f], Reason = [Created], Message = [Created container filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041.15dce513e2e25fc8], Reason = [Started], Message = [Started container filler-pod-c83603b1-087d-48c7-b7b0-c846bfc5f041]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dce51446b277da], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:10:46.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1218" for this suite.
Dec  3 15:10:52.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:52.807: INFO: namespace sched-pred-1218 deletion completed in 6.735162658s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:10:52.807: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-86b967db-93e1-43d6-a036-43c4bbd0f255 in namespace container-probe-9844
Dec  3 15:10:57.088: INFO: Started pod busybox-86b967db-93e1-43d6-a036-43c4bbd0f255 in namespace container-probe-9844
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:10:57.106: INFO: Initial restart count of pod busybox-86b967db-93e1-43d6-a036-43c4bbd0f255 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:14:57.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9844" for this suite.
Dec  3 15:15:03.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:15:04.303: INFO: namespace container-probe-9844 deletion completed in 6.739747678s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:15:04.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:15:04.530: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-5179'
Dec  3 15:15:04.952: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:15:04.952: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Dec  3 15:15:06.992: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-5179'
Dec  3 15:15:07.152: INFO: stderr: ""
Dec  3 15:15:07.152: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:15:07.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5179" for this suite.
Dec  3 15:17:09.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:17:09.956: INFO: namespace kubectl-5179 deletion completed in 2m2.77009156s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:17:09.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec  3 15:17:14.279: INFO: Pod pod-hostip-8fa145ad-22c1-48fe-9844-71252c716d1e has hostIP: 10.250.0.4
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:17:14.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4472" for this suite.
Dec  3 15:17:36.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:17:37.061: INFO: namespace pods-4472 deletion completed in 22.748933453s
•SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:17:37.061: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:18:08.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4805" for this suite.
Dec  3 15:18:14.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:18:15.129: INFO: namespace container-runtime-4805 deletion completed in 6.755605136s
•SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:18:15.130: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2147
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec  3 15:18:15.416: INFO: Found 1 stateful pods, waiting for 3
Dec  3 15:18:25.436: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:18:25.437: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:18:25.437: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 15:18:35.435: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:18:35.435: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:18:35.435: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  3 15:18:35.535: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  3 15:18:35.616: INFO: Updating stateful set ss2
Dec  3 15:18:35.652: INFO: Waiting for Pod statefulset-2147/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:18:45.689: INFO: Waiting for Pod statefulset-2147/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec  3 15:18:55.811: INFO: Found 2 stateful pods, waiting for 3
Dec  3 15:19:05.837: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:19:05.837: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:19:05.837: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  3 15:19:05.922: INFO: Updating stateful set ss2
Dec  3 15:19:05.962: INFO: Waiting for Pod statefulset-2147/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:19:16.046: INFO: Updating stateful set ss2
Dec  3 15:19:16.081: INFO: Waiting for StatefulSet statefulset-2147/ss2 to complete update
Dec  3 15:19:16.082: INFO: Waiting for Pod statefulset-2147/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:19:26.120: INFO: Waiting for StatefulSet statefulset-2147/ss2 to complete update
Dec  3 15:19:26.120: INFO: Waiting for Pod statefulset-2147/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:19:36.119: INFO: Waiting for StatefulSet statefulset-2147/ss2 to complete update
Dec  3 15:19:36.119: INFO: Waiting for Pod statefulset-2147/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 15:19:46.119: INFO: Deleting all statefulset in ns statefulset-2147
Dec  3 15:19:46.137: INFO: Scaling statefulset ss2 to 0
Dec  3 15:20:26.219: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:20:26.236: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:20:26.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2147" for this suite.
Dec  3 15:20:32.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:33.082: INFO: namespace statefulset-2147 deletion completed in 6.756834125s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:20:33.083: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec  3 15:20:33.313: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:20:33.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5855" for this suite.
Dec  3 15:20:39.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:40.231: INFO: namespace kubectl-5855 deletion completed in 6.769474797s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:20:40.231: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3090
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e8d43e2c-c553-4ab0-be35-9fbbae76591a
STEP: Creating configMap with name cm-test-opt-upd-1f5d4ea3-ab8e-4a35-a622-ec25afb6af51
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e8d43e2c-c553-4ab0-be35-9fbbae76591a
STEP: Updating configmap cm-test-opt-upd-1f5d4ea3-ab8e-4a35-a622-ec25afb6af51
STEP: Creating configMap with name cm-test-opt-create-d3e4d68b-e91c-46cc-861f-6b3b1882b2e7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:20:53.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3090" for this suite.
Dec  3 15:21:15.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:15.896: INFO: namespace configmap-3090 deletion completed in 22.746083551s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:21:15.897: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-67g2
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:21:16.192: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-67g2" in namespace "subpath-4275" to be "success or failure"
Dec  3 15:21:16.209: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.226715ms
Dec  3 15:21:18.228: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036035673s
Dec  3 15:21:20.246: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054717221s
Dec  3 15:21:22.265: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 6.073557265s
Dec  3 15:21:24.292: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 8.100109413s
Dec  3 15:21:26.311: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 10.119213567s
Dec  3 15:21:28.329: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 12.137214697s
Dec  3 15:21:30.347: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 14.155417268s
Dec  3 15:21:32.365: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 16.173769636s
Dec  3 15:21:34.384: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 18.192210157s
Dec  3 15:21:36.404: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 20.212659401s
Dec  3 15:21:38.422: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 22.230755191s
Dec  3 15:21:40.441: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Running", Reason="", readiness=true. Elapsed: 24.249220421s
Dec  3 15:21:42.460: INFO: Pod "pod-subpath-test-projected-67g2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.268229547s
STEP: Saw pod success
Dec  3 15:21:42.460: INFO: Pod "pod-subpath-test-projected-67g2" satisfied condition "success or failure"
Dec  3 15:21:42.478: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-subpath-test-projected-67g2 container test-container-subpath-projected-67g2: <nil>
STEP: delete the pod
Dec  3 15:21:42.525: INFO: Waiting for pod pod-subpath-test-projected-67g2 to disappear
Dec  3 15:21:42.543: INFO: Pod pod-subpath-test-projected-67g2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-67g2
Dec  3 15:21:42.543: INFO: Deleting pod "pod-subpath-test-projected-67g2" in namespace "subpath-4275"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:21:42.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4275" for this suite.
Dec  3 15:21:48.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:49.358: INFO: namespace subpath-4275 deletion completed in 6.763195521s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:21:49.358: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-76c30b9e-27a4-4c47-bb67-fbe71d0c9232
STEP: Creating a pod to test consume secrets
Dec  3 15:21:49.621: INFO: Waiting up to 5m0s for pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f" in namespace "secrets-1588" to be "success or failure"
Dec  3 15:21:49.639: INFO: Pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.441328ms
Dec  3 15:21:51.657: INFO: Pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036052567s
Dec  3 15:21:53.675: INFO: Pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054301273s
Dec  3 15:21:55.693: INFO: Pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072317586s
Dec  3 15:21:57.712: INFO: Pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.090764076s
STEP: Saw pod success
Dec  3 15:21:57.712: INFO: Pod "pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f" satisfied condition "success or failure"
Dec  3 15:21:57.729: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:21:57.786: INFO: Waiting for pod pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f to disappear
Dec  3 15:21:57.804: INFO: Pod pod-secrets-5ac42a4f-1de3-4903-87d9-817979e19c2f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:21:57.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1588" for this suite.
Dec  3 15:22:03.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:22:04.636: INFO: namespace secrets-1588 deletion completed in 6.798703229s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:22:04.636: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 15:22:13.528: INFO: Successfully updated pod "annotationupdatef4e83955-c98b-4d5f-826e-6ba4bc0fc8c7"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:22:15.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4000" for this suite.
Dec  3 15:22:37.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:22:38.363: INFO: namespace downward-api-4000 deletion completed in 22.734848563s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:22:38.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-tlb9
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:22:38.651: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tlb9" in namespace "subpath-6150" to be "success or failure"
Dec  3 15:22:38.668: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.071626ms
Dec  3 15:22:40.687: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036031443s
Dec  3 15:22:42.706: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054868767s
Dec  3 15:22:44.724: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 6.073376869s
Dec  3 15:22:46.743: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 8.09241101s
Dec  3 15:22:48.762: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 10.111286099s
Dec  3 15:22:50.781: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 12.129957917s
Dec  3 15:22:52.799: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 14.148389565s
Dec  3 15:22:54.818: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 16.167389118s
Dec  3 15:22:56.837: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 18.185898568s
Dec  3 15:22:58.855: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 20.20463614s
Dec  3 15:23:00.874: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 22.223623146s
Dec  3 15:23:02.893: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Running", Reason="", readiness=true. Elapsed: 24.242548986s
Dec  3 15:23:04.912: INFO: Pod "pod-subpath-test-configmap-tlb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.261334359s
STEP: Saw pod success
Dec  3 15:23:04.912: INFO: Pod "pod-subpath-test-configmap-tlb9" satisfied condition "success or failure"
Dec  3 15:23:04.929: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-subpath-test-configmap-tlb9 container test-container-subpath-configmap-tlb9: <nil>
STEP: delete the pod
Dec  3 15:23:04.983: INFO: Waiting for pod pod-subpath-test-configmap-tlb9 to disappear
Dec  3 15:23:05.000: INFO: Pod pod-subpath-test-configmap-tlb9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tlb9
Dec  3 15:23:05.001: INFO: Deleting pod "pod-subpath-test-configmap-tlb9" in namespace "subpath-6150"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:23:05.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6150" for this suite.
Dec  3 15:23:11.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:11.791: INFO: namespace subpath-6150 deletion completed in 6.739594739s
•SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:23:11.792: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:23:12.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97" in namespace "projected-3669" to be "success or failure"
Dec  3 15:23:12.058: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 17.336876ms
Dec  3 15:23:14.076: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035824025s
Dec  3 15:23:16.095: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054317343s
Dec  3 15:23:18.113: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073132989s
Dec  3 15:23:20.132: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091896138s
Dec  3 15:23:22.151: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110809668s
Dec  3 15:23:24.169: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Pending", Reason="", readiness=false. Elapsed: 12.1291783s
Dec  3 15:23:26.188: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.147735038s
STEP: Saw pod success
Dec  3 15:23:26.188: INFO: Pod "downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97" satisfied condition "success or failure"
Dec  3 15:23:26.207: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97 container client-container: <nil>
STEP: delete the pod
Dec  3 15:23:26.267: INFO: Waiting for pod downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97 to disappear
Dec  3 15:23:26.285: INFO: Pod downwardapi-volume-04cb1c57-92de-415a-aa8b-ba6b94969a97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:23:26.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3669" for this suite.
Dec  3 15:23:32.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:33.088: INFO: namespace projected-3669 deletion completed in 6.76895472s
•SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:23:33.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lnfpz in namespace proxy-2998
I1203 15:23:33.365976    5074 runners.go:180] Created replication controller with name: proxy-service-lnfpz, namespace: proxy-2998, replica count: 1
I1203 15:23:34.416546    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:35.416800    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:36.417048    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:37.417282    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:38.417545    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:39.417894    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:40.418160    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:41.418372    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:23:42.418724    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:23:43.418946    5074 runners.go:180] proxy-service-lnfpz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:23:43.440: INFO: setup took 10.124250399s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  3 15:23:43.464: INFO: (0) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 22.891177ms)
Dec  3 15:23:43.464: INFO: (0) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 23.063379ms)
Dec  3 15:23:43.464: INFO: (0) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 22.892116ms)
Dec  3 15:23:43.464: INFO: (0) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 23.063702ms)
Dec  3 15:23:43.464: INFO: (0) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 23.056667ms)
Dec  3 15:23:43.464: INFO: (0) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 23.091466ms)
Dec  3 15:23:43.469: INFO: (0) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 28.360477ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 38.386587ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 38.415123ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 38.494238ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 38.440856ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 38.513067ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 38.402742ms)
Dec  3 15:23:43.479: INFO: (0) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 38.436065ms)
Dec  3 15:23:43.481: INFO: (0) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 40.471991ms)
Dec  3 15:23:43.527: INFO: (0) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 86.34295ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.637105ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.817605ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.698031ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.808756ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.79422ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.891744ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.869201ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.191527ms)
Dec  3 15:23:43.547: INFO: (1) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.052194ms)
Dec  3 15:23:43.548: INFO: (1) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.38219ms)
Dec  3 15:23:43.548: INFO: (1) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 20.355893ms)
Dec  3 15:23:43.548: INFO: (1) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 20.770278ms)
Dec  3 15:23:43.564: INFO: (1) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 36.257623ms)
Dec  3 15:23:43.564: INFO: (1) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 36.171487ms)
Dec  3 15:23:43.564: INFO: (1) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 36.235956ms)
Dec  3 15:23:43.566: INFO: (1) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 38.054897ms)
Dec  3 15:23:43.585: INFO: (2) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 19.682711ms)
Dec  3 15:23:43.585: INFO: (2) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.591903ms)
Dec  3 15:23:43.585: INFO: (2) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.629844ms)
Dec  3 15:23:43.585: INFO: (2) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.633494ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.98708ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 20.25396ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.15711ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.220374ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.117683ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.15829ms)
Dec  3 15:23:43.586: INFO: (2) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.325757ms)
Dec  3 15:23:43.587: INFO: (2) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 20.88132ms)
Dec  3 15:23:43.589: INFO: (2) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 22.929799ms)
Dec  3 15:23:43.668: INFO: (2) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 102.352863ms)
Dec  3 15:23:43.668: INFO: (2) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 102.369903ms)
Dec  3 15:23:43.668: INFO: (2) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 102.400747ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.107699ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.917957ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.014213ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 19.988525ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.127972ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.105284ms)
Dec  3 15:23:43.688: INFO: (3) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.992837ms)
Dec  3 15:23:43.689: INFO: (3) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.751477ms)
Dec  3 15:23:43.689: INFO: (3) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 20.783712ms)
Dec  3 15:23:43.689: INFO: (3) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.713198ms)
Dec  3 15:23:43.689: INFO: (3) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.064511ms)
Dec  3 15:23:43.690: INFO: (3) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 21.553719ms)
Dec  3 15:23:43.690: INFO: (3) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 21.502767ms)
Dec  3 15:23:43.690: INFO: (3) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 21.647668ms)
Dec  3 15:23:43.734: INFO: (3) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 65.903515ms)
Dec  3 15:23:43.734: INFO: (3) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 65.914501ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.862297ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.786379ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.830159ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.81831ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.96842ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 19.926462ms)
Dec  3 15:23:43.754: INFO: (4) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.856609ms)
Dec  3 15:23:43.755: INFO: (4) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.075266ms)
Dec  3 15:23:43.755: INFO: (4) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 19.989307ms)
Dec  3 15:23:43.796: INFO: (4) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 61.676369ms)
Dec  3 15:23:43.796: INFO: (4) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 61.645541ms)
Dec  3 15:23:43.796: INFO: (4) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 61.690792ms)
Dec  3 15:23:43.796: INFO: (4) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 61.635354ms)
Dec  3 15:23:43.796: INFO: (4) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 61.858077ms)
Dec  3 15:23:43.797: INFO: (4) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 62.157781ms)
Dec  3 15:23:43.797: INFO: (4) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 62.157173ms)
Dec  3 15:23:43.817: INFO: (5) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.581239ms)
Dec  3 15:23:43.817: INFO: (5) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.582868ms)
Dec  3 15:23:43.817: INFO: (5) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.647708ms)
Dec  3 15:23:43.817: INFO: (5) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.582951ms)
Dec  3 15:23:43.817: INFO: (5) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.239748ms)
Dec  3 15:23:43.817: INFO: (5) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.400406ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 20.604165ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.341734ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.401146ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.493809ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.447271ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 21.141925ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 21.041615ms)
Dec  3 15:23:43.818: INFO: (5) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 21.050503ms)
Dec  3 15:23:43.819: INFO: (5) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 21.710978ms)
Dec  3 15:23:43.819: INFO: (5) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 22.126109ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 21.194306ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 21.182628ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 21.219739ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 21.197879ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 21.241269ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 21.175261ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 21.276534ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 21.306446ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 21.30509ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 21.141292ms)
Dec  3 15:23:43.841: INFO: (6) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.882248ms)
Dec  3 15:23:43.842: INFO: (6) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 22.436662ms)
Dec  3 15:23:43.843: INFO: (6) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 23.487992ms)
Dec  3 15:23:43.843: INFO: (6) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 23.530274ms)
Dec  3 15:23:43.843: INFO: (6) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 23.615432ms)
Dec  3 15:23:43.843: INFO: (6) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 23.613072ms)
Dec  3 15:23:43.863: INFO: (7) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.433171ms)
Dec  3 15:23:43.863: INFO: (7) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.562017ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 21.587909ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 21.443793ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 21.503209ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 21.598388ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 21.811239ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.851533ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 21.866596ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 21.872077ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 22.044323ms)
Dec  3 15:23:43.865: INFO: (7) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 22.13489ms)
Dec  3 15:23:43.866: INFO: (7) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 22.220175ms)
Dec  3 15:23:43.907: INFO: (7) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 63.311712ms)
Dec  3 15:23:43.907: INFO: (7) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 63.278615ms)
Dec  3 15:23:43.907: INFO: (7) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 63.359666ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.71745ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.861165ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.736267ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.735068ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.048412ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 19.968163ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.008028ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.86323ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.804305ms)
Dec  3 15:23:43.927: INFO: (8) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.945369ms)
Dec  3 15:23:43.928: INFO: (8) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.017974ms)
Dec  3 15:23:43.928: INFO: (8) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.915311ms)
Dec  3 15:23:43.928: INFO: (8) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 21.047489ms)
Dec  3 15:23:43.928: INFO: (8) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 21.078372ms)
Dec  3 15:23:43.929: INFO: (8) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 21.725665ms)
Dec  3 15:23:43.929: INFO: (8) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 21.723868ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.956355ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.985286ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.018536ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.134992ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.179173ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 20.038721ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 20.170218ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.105422ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.093446ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.153633ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.154772ms)
Dec  3 15:23:43.949: INFO: (9) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.077849ms)
Dec  3 15:23:43.950: INFO: (9) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.725797ms)
Dec  3 15:23:43.950: INFO: (9) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 20.937348ms)
Dec  3 15:23:43.951: INFO: (9) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 21.643496ms)
Dec  3 15:23:43.951: INFO: (9) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 21.726583ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.051622ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.076801ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 20.044223ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.048866ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.11561ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.02818ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.056516ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.212427ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.231663ms)
Dec  3 15:23:43.971: INFO: (10) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.598366ms)
Dec  3 15:23:43.972: INFO: (10) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.48429ms)
Dec  3 15:23:43.972: INFO: (10) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 21.416938ms)
Dec  3 15:23:43.973: INFO: (10) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 22.27802ms)
Dec  3 15:23:43.973: INFO: (10) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 22.304771ms)
Dec  3 15:23:43.973: INFO: (10) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 22.287916ms)
Dec  3 15:23:43.973: INFO: (10) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 22.340476ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.39837ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.502135ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.48798ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.341042ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.551512ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.380748ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.447551ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 20.612706ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.415609ms)
Dec  3 15:23:43.994: INFO: (11) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.693238ms)
Dec  3 15:23:43.995: INFO: (11) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.762394ms)
Dec  3 15:23:43.996: INFO: (11) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 22.196525ms)
Dec  3 15:23:43.996: INFO: (11) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 22.176929ms)
Dec  3 15:23:43.996: INFO: (11) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 22.58542ms)
Dec  3 15:23:44.034: INFO: (11) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 61.104612ms)
Dec  3 15:23:44.034: INFO: (11) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 61.16636ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.339476ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 19.462865ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.38149ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.505912ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.296047ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.30409ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 19.376362ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.488659ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 19.432521ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.404567ms)
Dec  3 15:23:44.054: INFO: (12) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.363564ms)
Dec  3 15:23:44.055: INFO: (12) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 20.155005ms)
Dec  3 15:23:44.058: INFO: (12) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 23.019614ms)
Dec  3 15:23:44.058: INFO: (12) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 23.068516ms)
Dec  3 15:23:44.058: INFO: (12) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 23.007453ms)
Dec  3 15:23:44.058: INFO: (12) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 23.070998ms)
Dec  3 15:23:44.077: INFO: (13) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.406678ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.382668ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.373096ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.401704ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.470077ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.421046ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.541802ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 20.560065ms)
Dec  3 15:23:44.079: INFO: (13) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.511157ms)
Dec  3 15:23:44.078: INFO: (13) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.450117ms)
Dec  3 15:23:44.079: INFO: (13) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.646312ms)
Dec  3 15:23:44.079: INFO: (13) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.247054ms)
Dec  3 15:23:44.080: INFO: (13) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 21.564526ms)
Dec  3 15:23:44.081: INFO: (13) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 22.769832ms)
Dec  3 15:23:44.081: INFO: (13) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 22.783997ms)
Dec  3 15:23:44.081: INFO: (13) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 22.786267ms)
Dec  3 15:23:44.101: INFO: (14) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.172223ms)
Dec  3 15:23:44.101: INFO: (14) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.39596ms)
Dec  3 15:23:44.101: INFO: (14) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.231055ms)
Dec  3 15:23:44.101: INFO: (14) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 20.172226ms)
Dec  3 15:23:44.101: INFO: (14) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.249421ms)
Dec  3 15:23:44.101: INFO: (14) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.235992ms)
Dec  3 15:23:44.102: INFO: (14) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.396384ms)
Dec  3 15:23:44.102: INFO: (14) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.509337ms)
Dec  3 15:23:44.102: INFO: (14) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.355828ms)
Dec  3 15:23:44.102: INFO: (14) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.332924ms)
Dec  3 15:23:44.103: INFO: (14) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 22.205211ms)
Dec  3 15:23:44.103: INFO: (14) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 22.273939ms)
Dec  3 15:23:44.103: INFO: (14) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 22.262942ms)
Dec  3 15:23:44.103: INFO: (14) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 22.239496ms)
Dec  3 15:23:44.103: INFO: (14) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 22.39156ms)
Dec  3 15:23:44.105: INFO: (14) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 24.075966ms)
Dec  3 15:23:44.124: INFO: (15) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.201703ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.46544ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.551249ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.480756ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 19.599806ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.507457ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 19.807344ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.786655ms)
Dec  3 15:23:44.125: INFO: (15) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.853258ms)
Dec  3 15:23:44.127: INFO: (15) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 21.688984ms)
Dec  3 15:23:44.127: INFO: (15) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.825543ms)
Dec  3 15:23:44.127: INFO: (15) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 21.719484ms)
Dec  3 15:23:44.127: INFO: (15) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 21.698619ms)
Dec  3 15:23:44.127: INFO: (15) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 21.720492ms)
Dec  3 15:23:44.127: INFO: (15) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 21.770069ms)
Dec  3 15:23:44.145: INFO: (15) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 39.926245ms)
Dec  3 15:23:44.165: INFO: (16) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.713629ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.216664ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.223854ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.250008ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.290318ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.303729ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.39692ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.302414ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.355611ms)
Dec  3 15:23:44.166: INFO: (16) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.269316ms)
Dec  3 15:23:44.169: INFO: (16) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 23.151885ms)
Dec  3 15:23:44.207: INFO: (16) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 61.151306ms)
Dec  3 15:23:44.207: INFO: (16) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 61.119773ms)
Dec  3 15:23:44.207: INFO: (16) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 61.245923ms)
Dec  3 15:23:44.207: INFO: (16) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 61.315476ms)
Dec  3 15:23:44.207: INFO: (16) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 61.107145ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.905759ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.008959ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 20.245266ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 20.259356ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 20.253684ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.394919ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 20.437635ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 20.397099ms)
Dec  3 15:23:44.227: INFO: (17) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 20.54786ms)
Dec  3 15:23:44.228: INFO: (17) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 20.784655ms)
Dec  3 15:23:44.228: INFO: (17) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 20.871697ms)
Dec  3 15:23:44.228: INFO: (17) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 20.84735ms)
Dec  3 15:23:44.229: INFO: (17) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 22.222156ms)
Dec  3 15:23:44.229: INFO: (17) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 22.156419ms)
Dec  3 15:23:44.229: INFO: (17) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 22.250562ms)
Dec  3 15:23:44.229: INFO: (17) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 22.286518ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.885264ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 19.888138ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.89882ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.986433ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.850282ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.98296ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.988117ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.892584ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.961537ms)
Dec  3 15:23:44.249: INFO: (18) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 20.044017ms)
Dec  3 15:23:44.251: INFO: (18) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 21.537975ms)
Dec  3 15:23:44.251: INFO: (18) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 21.60442ms)
Dec  3 15:23:44.251: INFO: (18) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 21.640794ms)
Dec  3 15:23:44.251: INFO: (18) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 21.527662ms)
Dec  3 15:23:44.251: INFO: (18) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 22.055171ms)
Dec  3 15:23:44.252: INFO: (18) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 22.077029ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.658691ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:1080/proxy/rewriteme">... (200; 19.817842ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:460/proxy/: tls baz (200; 19.737195ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:160/proxy/: foo (200; 19.728653ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67/proxy/rewriteme">test</a> (200; 19.728141ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.768544ms)
Dec  3 15:23:44.271: INFO: (19) /api/v1/namespaces/proxy-2998/pods/http:proxy-service-lnfpz-snr67:162/proxy/: bar (200; 19.861161ms)
Dec  3 15:23:44.272: INFO: (19) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:443/proxy/tlsrewritem... (200; 19.889027ms)
Dec  3 15:23:44.272: INFO: (19) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname2/proxy/: tls qux (200; 19.803351ms)
Dec  3 15:23:44.272: INFO: (19) /api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/: <a href="/api/v1/namespaces/proxy-2998/pods/proxy-service-lnfpz-snr67:1080/proxy/rewriteme">test<... (200; 19.942487ms)
Dec  3 15:23:44.272: INFO: (19) /api/v1/namespaces/proxy-2998/pods/https:proxy-service-lnfpz-snr67:462/proxy/: tls qux (200; 19.830192ms)
Dec  3 15:23:44.272: INFO: (19) /api/v1/namespaces/proxy-2998/services/https:proxy-service-lnfpz:tlsportname1/proxy/: tls baz (200; 19.852639ms)
Dec  3 15:23:44.272: INFO: (19) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname1/proxy/: foo (200; 20.165423ms)
Dec  3 15:23:44.273: INFO: (19) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname1/proxy/: foo (200; 20.884057ms)
Dec  3 15:23:44.273: INFO: (19) /api/v1/namespaces/proxy-2998/services/proxy-service-lnfpz:portname2/proxy/: bar (200; 20.888919ms)
Dec  3 15:23:44.273: INFO: (19) /api/v1/namespaces/proxy-2998/services/http:proxy-service-lnfpz:portname2/proxy/: bar (200; 20.965466ms)
STEP: deleting ReplicationController proxy-service-lnfpz in namespace proxy-2998, will wait for the garbage collector to delete the pods
Dec  3 15:23:44.371: INFO: Deleting ReplicationController proxy-service-lnfpz took: 30.507289ms
Dec  3 15:23:44.471: INFO: Terminating ReplicationController proxy-service-lnfpz pods took: 100.363827ms
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:23:48.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2998" for this suite.
Dec  3 15:23:55.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:55.786: INFO: namespace proxy-2998 deletion completed in 6.780818715s
•SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:23:55.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-980.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-980.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-980.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-980.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:24:22.335: INFO: DNS probes using dns-test-9bdaa95c-c959-46e2-bec0-74746372f133 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-980.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-980.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-980.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-980.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:24:42.708: INFO: DNS probes using dns-test-4741c897-c27d-4d62-acfd-773ebba1dff5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-980.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-980.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-980.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-980.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:24:51.125: INFO: DNS probes using dns-test-64ef9005-c9a8-4fe1-b352-0c183f63f926 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:24:51.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-980" for this suite.
Dec  3 15:24:57.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:57.960: INFO: namespace dns-980 deletion completed in 6.744373239s
•SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:24:57.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 15:24:58.230: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:18.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2019" for this suite.
Dec  3 15:25:41.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:41.773: INFO: namespace init-container-2019 deletion completed in 22.736521003s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:25:41.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-590
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-d1ec3184-fa90-41a8-bde3-2fb5d004381d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:48.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-590" for this suite.
Dec  3 15:26:10.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:11.004: INFO: namespace configmap-590 deletion completed in 22.732807804s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:26:11.004: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:26:11.255: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05" in namespace "projected-2328" to be "success or failure"
Dec  3 15:26:11.273: INFO: Pod "downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05": Phase="Pending", Reason="", readiness=false. Elapsed: 17.613794ms
Dec  3 15:26:13.292: INFO: Pod "downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036154416s
Dec  3 15:26:15.311: INFO: Pod "downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055741394s
STEP: Saw pod success
Dec  3 15:26:15.311: INFO: Pod "downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05" satisfied condition "success or failure"
Dec  3 15:26:15.329: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05 container client-container: <nil>
STEP: delete the pod
Dec  3 15:26:15.380: INFO: Waiting for pod downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05 to disappear
Dec  3 15:26:15.397: INFO: Pod downwardapi-volume-d0a18f26-becc-4012-9a5a-71c605a86a05 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:26:15.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2328" for this suite.
Dec  3 15:26:21.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:22.182: INFO: namespace projected-2328 deletion completed in 6.751654049s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:26:22.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1203 15:27:02.553863    5074 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:27:02.553: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:27:02.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2195" for this suite.
Dec  3 15:27:08.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:09.306: INFO: namespace gc-2195 deletion completed in 6.73407127s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:27:09.306: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:27:09.574: INFO: (0) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.88164ms)
Dec  3 15:27:09.618: INFO: (1) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 43.870439ms)
Dec  3 15:27:09.637: INFO: (2) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.433938ms)
Dec  3 15:27:09.657: INFO: (3) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.915959ms)
Dec  3 15:27:09.677: INFO: (4) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.622693ms)
Dec  3 15:27:09.696: INFO: (5) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.397274ms)
Dec  3 15:27:09.716: INFO: (6) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.312134ms)
Dec  3 15:27:09.735: INFO: (7) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.938234ms)
Dec  3 15:27:09.754: INFO: (8) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.874524ms)
Dec  3 15:27:09.773: INFO: (9) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.980912ms)
Dec  3 15:27:09.794: INFO: (10) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 20.923961ms)
Dec  3 15:27:09.812: INFO: (11) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.572239ms)
Dec  3 15:27:09.831: INFO: (12) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.907268ms)
Dec  3 15:27:09.852: INFO: (13) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 20.513131ms)
Dec  3 15:27:09.871: INFO: (14) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.360847ms)
Dec  3 15:27:09.895: INFO: (15) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.980723ms)
Dec  3 15:27:09.916: INFO: (16) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 20.038645ms)
Dec  3 15:27:09.935: INFO: (17) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 19.213253ms)
Dec  3 15:27:09.955: INFO: (18) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 20.559219ms)
Dec  3 15:27:09.979: INFO: (19) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 23.763785ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:27:09.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4730" for this suite.
Dec  3 15:27:16.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:16.735: INFO: namespace proxy-4730 deletion completed in 6.737209005s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:27:16.736: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-5342
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5342 to expose endpoints map[]
Dec  3 15:27:17.008: INFO: successfully validated that service endpoint-test2 in namespace services-5342 exposes endpoints map[] (17.406149ms elapsed)
STEP: Creating pod pod1 in namespace services-5342
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5342 to expose endpoints map[pod1:[80]]
Dec  3 15:27:21.205: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.176610179s elapsed, will retry)
Dec  3 15:27:24.313: INFO: successfully validated that service endpoint-test2 in namespace services-5342 exposes endpoints map[pod1:[80]] (7.284306337s elapsed)
STEP: Creating pod pod2 in namespace services-5342
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5342 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  3 15:27:28.600: INFO: successfully validated that service endpoint-test2 in namespace services-5342 exposes endpoints map[pod1:[80] pod2:[80]] (4.268100745s elapsed)
STEP: Deleting pod pod1 in namespace services-5342
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5342 to expose endpoints map[pod2:[80]]
Dec  3 15:27:28.656: INFO: successfully validated that service endpoint-test2 in namespace services-5342 exposes endpoints map[pod2:[80]] (36.886938ms elapsed)
STEP: Deleting pod pod2 in namespace services-5342
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5342 to expose endpoints map[]
Dec  3 15:27:28.692: INFO: successfully validated that service endpoint-test2 in namespace services-5342 exposes endpoints map[] (17.030202ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:27:28.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5342" for this suite.
Dec  3 15:27:50.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:51.517: INFO: namespace services-5342 deletion completed in 22.756862387s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•S
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:27:51.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1991
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:27:51.738: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  3 15:27:51.776: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 15:27:55.812: INFO: Creating deployment "test-rolling-update-deployment"
Dec  3 15:27:55.831: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  3 15:27:55.869: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  3 15:27:55.887: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:57.905: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983675, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:59.905: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 15:27:59.958: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1991,SelfLink:/apis/apps/v1/namespaces/deployment-1991/deployments/test-rolling-update-deployment,UID:17d7ec34-036c-4f8f-9856-93f3b73e4f18,ResourceVersion:10829,Generation:1,CreationTimestamp:2019-12-03 15:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 15:27:55 +0000 UTC 2019-12-03 15:27:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 15:27:59 +0000 UTC 2019-12-03 15:27:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 15:27:59.976: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-1991,SelfLink:/apis/apps/v1/namespaces/deployment-1991/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:d700b57f-f113-4c40-b133-dfb350e699cb,ResourceVersion:10822,Generation:1,CreationTimestamp:2019-12-03 15:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 17d7ec34-036c-4f8f-9856-93f3b73e4f18 0xc002688497 0xc002688498}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 15:27:59.976: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  3 15:27:59.976: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1991,SelfLink:/apis/apps/v1/namespaces/deployment-1991/replicasets/test-rolling-update-controller,UID:6f4b9fe9-18c0-4439-aa5e-ab493c15da77,ResourceVersion:10828,Generation:2,CreationTimestamp:2019-12-03 15:27:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 17d7ec34-036c-4f8f-9856-93f3b73e4f18 0xc0026883c7 0xc0026883c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:27:59.995: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-9bf6k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-9bf6k,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-1991,SelfLink:/api/v1/namespaces/deployment-1991/pods/test-rolling-update-deployment-79f6b9d75c-9bf6k,UID:61ea9c95-356e-48c4-84e2-abb3aa5f308a,ResourceVersion:10821,Generation:0,CreationTimestamp:2019-12-03 15:27:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.79/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c d700b57f-f113-4c40-b133-dfb350e699cb 0xc002688da7 0xc002688da8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-m2bj2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-m2bj2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-m2bj2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002688e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002688e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:27:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:27:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:27:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:27:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.79,StartTime:2019-12-03 15:27:55 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 15:27:58 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://148ba36346bdfbc35e2186f84d9c1d4402bd90aa8da28e9812c092e16e12062d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:27:59.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1991" for this suite.
Dec  3 15:28:06.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:06.772: INFO: namespace deployment-1991 deletion completed in 6.74332348s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:28:06.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  3 15:28:06.997: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:28:07.037: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:28:07.055: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 before test
Dec  3 15:28:07.098: INFO: coredns-858b686868-8npzw from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:28:07.098: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-82b4r from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:28:07.098: INFO: blackbox-exporter-c87bdd467-6qxtm from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:28:07.098: INFO: kube-proxy-m56xr from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:28:07.098: INFO: calico-typha-vertical-autoscaler-656557779f-9ms99 from kube-system started at 2019-12-03 14:34:49 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container autoscaler ready: true, restart count 6
Dec  3 15:28:07.098: INFO: calico-node-8578h from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:28:07.098: INFO: coredns-858b686868-vmthw from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:28:07.098: INFO: metrics-server-6c6b44bdf-6fqbp from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:28:07.098: INFO: addons-nginx-ingress-controller-8468678b64-k2xb6 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:28:07.098: INFO: addons-kubernetes-dashboard-5c8d9945bc-q67s2 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:28:07.098: INFO: vpn-shoot-76b5996f55-sb8l6 from kube-system started at 2019-12-03 14:34:48 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:28:07.098: INFO: node-problem-detector-9qtgm from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:28:07.098: INFO: node-exporter-9bgsx from kube-system started at 2019-12-03 14:34:05 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:28:07.098: INFO: calico-kube-controllers-5d785bc598-vwwh6 from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container calico-kube-controllers ready: true, restart count 4
Dec  3 15:28:07.098: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-t2scl from kube-system started at 2019-12-03 14:34:47 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.098: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:28:07.098: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf before test
Dec  3 15:28:07.136: INFO: calico-typha-deploy-5547c4cdc6-6ngrt from kube-system started at 2019-12-03 14:43:06 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.136: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:28:07.136: INFO: calico-node-nddkb from kube-system started at 2019-12-03 14:34:16 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.136: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:28:07.136: INFO: kube-proxy-xrd2g from kube-system started at 2019-12-03 14:34:17 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.136: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:28:07.136: INFO: node-exporter-m8zkr from kube-system started at 2019-12-03 14:34:16 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.136: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:28:07.136: INFO: node-problem-detector-phmbb from kube-system started at 2019-12-03 14:34:17 +0000 UTC (1 container statuses recorded)
Dec  3 15:28:07.136: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dce606f3e1fd88], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:28:08.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1917" for this suite.
Dec  3 15:28:14.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:15.005: INFO: namespace sched-pred-1917 deletion completed in 6.740381319s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:28:15.005: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-674.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-674.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-674.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-674.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-674.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-674.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-674.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 214.102.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.102.214_udp@PTR;check="$$(dig +tcp +noall +answer +search 214.102.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.102.214_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-674.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-674.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-674.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-674.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-674.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-674.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-674.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-674.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 214.102.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.102.214_udp@PTR;check="$$(dig +tcp +noall +answer +search 214.102.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.102.214_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:28:21.469: INFO: Unable to read wheezy_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:21.512: INFO: Unable to read wheezy_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:21.531: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:21.551: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:21.989: INFO: Unable to read jessie_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:22.009: INFO: Unable to read jessie_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:22.028: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:22.048: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:22.405: INFO: Lookups using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee failed for: [wheezy_udp@dns-test-service.dns-674.svc.cluster.local wheezy_tcp@dns-test-service.dns-674.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_udp@dns-test-service.dns-674.svc.cluster.local jessie_tcp@dns-test-service.dns-674.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local]

Dec  3 15:28:27.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.463: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.482: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.885: INFO: Unable to read jessie_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.905: INFO: Unable to read jessie_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.927: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:27.946: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:28.304: INFO: Lookups using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee failed for: [wheezy_udp@dns-test-service.dns-674.svc.cluster.local wheezy_tcp@dns-test-service.dns-674.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_udp@dns-test-service.dns-674.svc.cluster.local jessie_tcp@dns-test-service.dns-674.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local]

Dec  3 15:28:32.466: INFO: Unable to read wheezy_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.492: INFO: Unable to read wheezy_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.511: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.530: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.932: INFO: Unable to read jessie_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.951: INFO: Unable to read jessie_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.971: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:32.991: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:33.386: INFO: Lookups using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee failed for: [wheezy_udp@dns-test-service.dns-674.svc.cluster.local wheezy_tcp@dns-test-service.dns-674.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_udp@dns-test-service.dns-674.svc.cluster.local jessie_tcp@dns-test-service.dns-674.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local]

Dec  3 15:28:37.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.463: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.483: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.926: INFO: Unable to read jessie_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.945: INFO: Unable to read jessie_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.964: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:37.983: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:38.383: INFO: Lookups using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee failed for: [wheezy_udp@dns-test-service.dns-674.svc.cluster.local wheezy_tcp@dns-test-service.dns-674.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_udp@dns-test-service.dns-674.svc.cluster.local jessie_tcp@dns-test-service.dns-674.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local]

Dec  3 15:28:42.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:42.469: INFO: Unable to read wheezy_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:42.491: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:42.510: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:42.954: INFO: Unable to read jessie_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:42.973: INFO: Unable to read jessie_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:43.001: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:43.024: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:43.384: INFO: Lookups using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee failed for: [wheezy_udp@dns-test-service.dns-674.svc.cluster.local wheezy_tcp@dns-test-service.dns-674.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_udp@dns-test-service.dns-674.svc.cluster.local jessie_tcp@dns-test-service.dns-674.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local]

Dec  3 15:28:47.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.444: INFO: Unable to read wheezy_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.464: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.484: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.926: INFO: Unable to read jessie_udp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.946: INFO: Unable to read jessie_tcp@dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.965: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:47.984: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local from pod dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee: the server could not find the requested resource (get pods dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee)
Dec  3 15:28:48.383: INFO: Lookups using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee failed for: [wheezy_udp@dns-test-service.dns-674.svc.cluster.local wheezy_tcp@dns-test-service.dns-674.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_udp@dns-test-service.dns-674.svc.cluster.local jessie_tcp@dns-test-service.dns-674.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-674.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-674.svc.cluster.local]

Dec  3 15:28:53.752: INFO: DNS probes using dns-674/dns-test-afcd12f9-dd9c-4aa3-8b3e-d462b8bff5ee succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:28:53.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-674" for this suite.
Dec  3 15:28:59.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:00.625: INFO: namespace dns-674 deletion completed in 6.752414195s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:29:00.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 15:29:05.507: INFO: Successfully updated pod "annotationupdate836d6b16-770d-4d2b-8c99-b3415a441568"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:29:09.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2716" for this suite.
Dec  3 15:29:29.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:30.370: INFO: namespace projected-2716 deletion completed in 20.754371538s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:29:30.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:29:30.621: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9" in namespace "projected-8545" to be "success or failure"
Dec  3 15:29:30.638: INFO: Pod "downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.239033ms
Dec  3 15:29:32.656: INFO: Pod "downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035726447s
Dec  3 15:29:34.675: INFO: Pod "downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054673338s
STEP: Saw pod success
Dec  3 15:29:34.675: INFO: Pod "downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9" satisfied condition "success or failure"
Dec  3 15:29:34.699: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9 container client-container: <nil>
STEP: delete the pod
Dec  3 15:29:34.749: INFO: Waiting for pod downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9 to disappear
Dec  3 15:29:34.766: INFO: Pod downwardapi-volume-d8eeda33-dda8-42f0-88cf-6cfbc24885c9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:29:34.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8545" for this suite.
Dec  3 15:29:40.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:41.539: INFO: namespace projected-8545 deletion completed in 6.738544005s
•SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:29:41.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:29:41.849: INFO: Waiting up to 5m0s for pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa" in namespace "emptydir-1960" to be "success or failure"
Dec  3 15:29:41.866: INFO: Pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 17.573837ms
Dec  3 15:29:43.885: INFO: Pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035858667s
Dec  3 15:29:45.903: INFO: Pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054518427s
Dec  3 15:29:47.922: INFO: Pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073084452s
Dec  3 15:29:49.940: INFO: Pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.091688181s
STEP: Saw pod success
Dec  3 15:29:49.940: INFO: Pod "pod-c9f8c88b-1236-4686-bd1a-a283493fddaa" satisfied condition "success or failure"
Dec  3 15:29:49.958: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-c9f8c88b-1236-4686-bd1a-a283493fddaa container test-container: <nil>
STEP: delete the pod
Dec  3 15:29:50.012: INFO: Waiting for pod pod-c9f8c88b-1236-4686-bd1a-a283493fddaa to disappear
Dec  3 15:29:50.030: INFO: Pod pod-c9f8c88b-1236-4686-bd1a-a283493fddaa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:29:50.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1960" for this suite.
Dec  3 15:29:56.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:56.815: INFO: namespace emptydir-1960 deletion completed in 6.751019381s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:29:56.815: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:30:07.167: INFO: Waiting up to 5m0s for pod "client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383" in namespace "pods-7435" to be "success or failure"
Dec  3 15:30:07.185: INFO: Pod "client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383": Phase="Pending", Reason="", readiness=false. Elapsed: 17.242718ms
Dec  3 15:30:09.203: INFO: Pod "client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035522661s
Dec  3 15:30:11.222: INFO: Pod "client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05410237s
Dec  3 15:30:13.239: INFO: Pod "client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071846275s
STEP: Saw pod success
Dec  3 15:30:13.240: INFO: Pod "client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383" satisfied condition "success or failure"
Dec  3 15:30:13.257: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383 container env3cont: <nil>
STEP: delete the pod
Dec  3 15:30:13.305: INFO: Waiting for pod client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383 to disappear
Dec  3 15:30:13.323: INFO: Pod client-envvars-9ac89f97-223f-4675-b636-43bb07a4a383 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:30:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7435" for this suite.
Dec  3 15:30:51.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:52.115: INFO: namespace pods-7435 deletion completed in 38.759492752s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:30:52.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-fd2c7ffe-9669-4929-b4ba-705efc54c90c
STEP: Creating a pod to test consume secrets
Dec  3 15:30:52.378: INFO: Waiting up to 5m0s for pod "pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596" in namespace "secrets-9508" to be "success or failure"
Dec  3 15:30:52.396: INFO: Pod "pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596": Phase="Pending", Reason="", readiness=false. Elapsed: 17.484679ms
Dec  3 15:30:54.415: INFO: Pod "pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036639192s
Dec  3 15:30:56.433: INFO: Pod "pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054720815s
Dec  3 15:30:58.452: INFO: Pod "pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073617474s
STEP: Saw pod success
Dec  3 15:30:58.452: INFO: Pod "pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596" satisfied condition "success or failure"
Dec  3 15:30:58.469: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:30:58.526: INFO: Waiting for pod pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596 to disappear
Dec  3 15:30:58.543: INFO: Pod pod-secrets-c0babb5d-f22a-4365-974a-eefa576ab596 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:30:58.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9508" for this suite.
Dec  3 15:31:04.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:05.319: INFO: namespace secrets-9508 deletion completed in 6.742119521s
•SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:31:05.319: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7291
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:31:05.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7291'
Dec  3 15:31:05.969: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:31:05.969: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Dec  3 15:31:05.986: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-7291'
Dec  3 15:31:06.151: INFO: stderr: ""
Dec  3 15:31:06.151: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:31:06.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7291" for this suite.
Dec  3 15:31:28.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:28.956: INFO: namespace kubectl-7291 deletion completed in 22.769328106s
•SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:31:28.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-664596a3-1315-41a4-800b-380ba398062d
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:31:29.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5105" for this suite.
Dec  3 15:31:35.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:35.970: INFO: namespace secrets-5105 deletion completed in 6.75524289s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:31:35.971: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:31:40.834: INFO: Successfully updated pod "pod-update-7bfd36c0-4821-49eb-859e-fe05f8435584"
STEP: verifying the updated pod is in kubernetes
Dec  3 15:31:40.869: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:31:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7147" for this suite.
Dec  3 15:32:02.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:03.641: INFO: namespace pods-7147 deletion completed in 22.738405324s
•SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:32:03.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:32:03.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:32:08.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6619" for this suite.
Dec  3 15:32:50.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:51.011: INFO: namespace pods-6619 deletion completed in 42.762457119s
•SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:32:51.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:32:51.233: INFO: Creating ReplicaSet my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b
Dec  3 15:32:51.273: INFO: Pod name my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b: Found 1 pods out of 1
Dec  3 15:32:51.273: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b" is running
Dec  3 15:33:01.319: INFO: Pod "my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b-w8tbc" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:32:51 +0000 UTC Reason: Message:}])
Dec  3 15:33:01.319: INFO: Trying to dial the pod
Dec  3 15:33:06.460: INFO: Controller my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b: Got expected result from replica 1 [my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b-w8tbc]: "my-hostname-basic-57040e59-8bee-4406-bc0e-e2734c6eac3b-w8tbc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:33:06.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1473" for this suite.
Dec  3 15:33:12.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:13.262: INFO: namespace replicaset-1473 deletion completed in 6.768353599s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:33:13.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-3105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec  3 15:33:13.525: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3105" to be "success or failure"
Dec  3 15:33:13.543: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.316594ms
Dec  3 15:33:15.562: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036676031s
Dec  3 15:33:17.583: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058354165s
Dec  3 15:33:19.602: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076794961s
Dec  3 15:33:21.620: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.095266082s
STEP: Saw pod success
Dec  3 15:33:21.620: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  3 15:33:21.638: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  3 15:33:21.685: INFO: Waiting for pod pod-host-path-test to disappear
Dec  3 15:33:21.702: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:33:21.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3105" for this suite.
Dec  3 15:33:27.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:28.495: INFO: namespace hostpath-3105 deletion completed in 6.759359543s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:33:28.496: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:33:36.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2285" for this suite.
Dec  3 15:33:43.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:43.672: INFO: namespace emptydir-wrapper-2285 deletion completed in 6.724041592s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:33:43.672: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 15:33:54.089: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:33:54.106: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:33:56.107: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:33:56.124: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:33:58.106: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:33:58.125: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:34:00.106: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:34:00.125: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:34:02.106: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:34:02.125: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:34:04.106: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:34:04.124: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:34:06.107: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:34:06.125: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 15:34:08.106: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 15:34:08.125: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:08.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8331" for this suite.
Dec  3 15:34:30.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:30.901: INFO: namespace container-lifecycle-hook-8331 deletion completed in 22.742295902s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:30.903: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 15:34:31.150: INFO: Waiting up to 5m0s for pod "pod-59eb49d6-5073-4150-aa6a-8d71494f4d69" in namespace "emptydir-4902" to be "success or failure"
Dec  3 15:34:31.168: INFO: Pod "pod-59eb49d6-5073-4150-aa6a-8d71494f4d69": Phase="Pending", Reason="", readiness=false. Elapsed: 17.235781ms
Dec  3 15:34:33.186: INFO: Pod "pod-59eb49d6-5073-4150-aa6a-8d71494f4d69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035426597s
Dec  3 15:34:35.204: INFO: Pod "pod-59eb49d6-5073-4150-aa6a-8d71494f4d69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05397845s
STEP: Saw pod success
Dec  3 15:34:35.204: INFO: Pod "pod-59eb49d6-5073-4150-aa6a-8d71494f4d69" satisfied condition "success or failure"
Dec  3 15:34:35.222: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-59eb49d6-5073-4150-aa6a-8d71494f4d69 container test-container: <nil>
STEP: delete the pod
Dec  3 15:34:35.294: INFO: Waiting for pod pod-59eb49d6-5073-4150-aa6a-8d71494f4d69 to disappear
Dec  3 15:34:35.312: INFO: Pod pod-59eb49d6-5073-4150-aa6a-8d71494f4d69 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:35.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4902" for this suite.
Dec  3 15:34:41.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:42.092: INFO: namespace emptydir-4902 deletion completed in 6.746944671s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:42.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7220
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  3 15:34:42.461: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7220,SelfLink:/api/v1/namespaces/watch-7220/configmaps/e2e-watch-test-resource-version,UID:31274115-e20d-4311-b865-379be7061d96,ResourceVersion:12140,Generation:0,CreationTimestamp:2019-12-03 15:34:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:34:42.461: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7220,SelfLink:/api/v1/namespaces/watch-7220/configmaps/e2e-watch-test-resource-version,UID:31274115-e20d-4311-b865-379be7061d96,ResourceVersion:12141,Generation:0,CreationTimestamp:2019-12-03 15:34:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:42.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7220" for this suite.
Dec  3 15:34:48.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:49.267: INFO: namespace watch-7220 deletion completed in 6.786879971s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:49.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4276
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:34:50.446: INFO: Pod name wrapped-volume-race-d1182003-7fa6-401f-8627-233a01037fce: Found 1 pods out of 5
Dec  3 15:34:55.483: INFO: Pod name wrapped-volume-race-d1182003-7fa6-401f-8627-233a01037fce: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d1182003-7fa6-401f-8627-233a01037fce in namespace emptydir-wrapper-4276, will wait for the garbage collector to delete the pods
Dec  3 15:35:01.776: INFO: Deleting ReplicationController wrapped-volume-race-d1182003-7fa6-401f-8627-233a01037fce took: 21.440734ms
Dec  3 15:35:01.877: INFO: Terminating ReplicationController wrapped-volume-race-d1182003-7fa6-401f-8627-233a01037fce pods took: 100.290209ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:35:44.955: INFO: Pod name wrapped-volume-race-2cefeb66-722a-48a8-9fb5-ab910fa904df: Found 3 pods out of 5
Dec  3 15:35:49.992: INFO: Pod name wrapped-volume-race-2cefeb66-722a-48a8-9fb5-ab910fa904df: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2cefeb66-722a-48a8-9fb5-ab910fa904df in namespace emptydir-wrapper-4276, will wait for the garbage collector to delete the pods
Dec  3 15:35:56.191: INFO: Deleting ReplicationController wrapped-volume-race-2cefeb66-722a-48a8-9fb5-ab910fa904df took: 20.838697ms
Dec  3 15:35:56.291: INFO: Terminating ReplicationController wrapped-volume-race-2cefeb66-722a-48a8-9fb5-ab910fa904df pods took: 100.330404ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:36:35.066: INFO: Pod name wrapped-volume-race-fe31d7b7-f12c-487d-b5dc-b3036ef7a769: Found 1 pods out of 5
Dec  3 15:36:40.102: INFO: Pod name wrapped-volume-race-fe31d7b7-f12c-487d-b5dc-b3036ef7a769: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fe31d7b7-f12c-487d-b5dc-b3036ef7a769 in namespace emptydir-wrapper-4276, will wait for the garbage collector to delete the pods
Dec  3 15:36:44.296: INFO: Deleting ReplicationController wrapped-volume-race-fe31d7b7-f12c-487d-b5dc-b3036ef7a769 took: 20.427128ms
Dec  3 15:36:44.397: INFO: Terminating ReplicationController wrapped-volume-race-fe31d7b7-f12c-487d-b5dc-b3036ef7a769 pods took: 100.292101ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:37:25.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4276" for this suite.
Dec  3 15:37:32.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:37:32.743: INFO: namespace emptydir-wrapper-4276 deletion completed in 6.737042939s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:37:32.744: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-1b58d9c9-c63c-4f83-af4e-767daa57acf0
STEP: Creating a pod to test consume configMaps
Dec  3 15:37:32.999: INFO: Waiting up to 5m0s for pod "pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149" in namespace "configmap-260" to be "success or failure"
Dec  3 15:37:33.016: INFO: Pod "pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149": Phase="Pending", Reason="", readiness=false. Elapsed: 17.246954ms
Dec  3 15:37:35.034: INFO: Pod "pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035572426s
Dec  3 15:37:37.053: INFO: Pod "pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054306143s
STEP: Saw pod success
Dec  3 15:37:37.053: INFO: Pod "pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149" satisfied condition "success or failure"
Dec  3 15:37:37.071: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:37:37.131: INFO: Waiting for pod pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149 to disappear
Dec  3 15:37:37.149: INFO: Pod pod-configmaps-d78a8605-771d-416c-b04f-d70d3d2af149 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:37:37.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-260" for this suite.
Dec  3 15:37:43.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:37:43.936: INFO: namespace configmap-260 deletion completed in 6.744741906s
•
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:37:43.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2261
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-606
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:10.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1817" for this suite.
Dec  3 15:38:16.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:17.475: INFO: namespace namespaces-1817 deletion completed in 6.736308702s
STEP: Destroying namespace "nsdeletetest-2261" for this suite.
Dec  3 15:38:17.492: INFO: Namespace nsdeletetest-2261 was already deleted
STEP: Destroying namespace "nsdeletetest-606" for this suite.
Dec  3 15:38:23.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:24.257: INFO: namespace nsdeletetest-606 deletion completed in 6.764752701s
•S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:24.257: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:24.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8263" for this suite.
Dec  3 15:38:30.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:31.264: INFO: namespace services-8263 deletion completed in 6.74728602s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:31.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:38:31.493: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-3096'
Dec  3 15:38:31.653: INFO: stderr: ""
Dec  3 15:38:31.654: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  3 15:38:36.704: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-3096 -o json'
Dec  3 15:38:36.848: INFO: stderr: ""
Dec  3 15:38:36.848: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.64.1.97/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-12-03T15:38:31Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-3096\",\n        \"resourceVersion\": \"13052\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3096/pods/e2e-test-nginx-pod\",\n        \"uid\": \"ec588d6d-be1c-4871-87e7-054419720fe7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5dv8f\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5dv8f\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5dv8f\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:38:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:38:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:38:34Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T15:38:31Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://4b6e8d53579589fcb23f1b0b33165e08e0a69440579d043f8873f12862e16595\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-03T15:38:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.97\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-03T15:38:31Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  3 15:38:36.849: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-3096'
Dec  3 15:38:37.101: INFO: stderr: ""
Dec  3 15:38:37.101: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Dec  3 15:38:37.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-3096'
Dec  3 15:38:46.434: INFO: stderr: ""
Dec  3 15:38:46.434: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:46.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3096" for this suite.
Dec  3 15:38:52.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:53.247: INFO: namespace kubectl-3096 deletion completed in 6.779895082s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:53.247: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:57.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4327" for this suite.
Dec  3 15:39:51.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:39:52.349: INFO: namespace kubelet-test-4327 deletion completed in 54.737163279s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:39:52.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec  3 15:39:52.592: INFO: Waiting up to 5m0s for pod "client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208" in namespace "containers-5481" to be "success or failure"
Dec  3 15:39:52.610: INFO: Pod "client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208": Phase="Pending", Reason="", readiness=false. Elapsed: 17.79721ms
Dec  3 15:39:54.628: INFO: Pod "client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036687213s
Dec  3 15:39:56.647: INFO: Pod "client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054879441s
Dec  3 15:39:58.667: INFO: Pod "client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075491572s
STEP: Saw pod success
Dec  3 15:39:58.667: INFO: Pod "client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208" satisfied condition "success or failure"
Dec  3 15:39:58.687: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208 container test-container: <nil>
STEP: delete the pod
Dec  3 15:39:58.735: INFO: Waiting for pod client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208 to disappear
Dec  3 15:39:58.752: INFO: Pod client-containers-65d26086-1b25-4da8-935f-a93b1e3b9208 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:39:58.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5481" for this suite.
Dec  3 15:40:04.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:05.542: INFO: namespace containers-5481 deletion completed in 6.755927205s
•SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:40:05.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Dec  3 15:40:05.949: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W1203 15:40:05.949398    5074 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:40:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8201" for this suite.
Dec  3 15:40:12.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:12.726: INFO: namespace gc-8201 deletion completed in 6.758719688s
•SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:40:12.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-2214
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[]
Dec  3 15:40:13.019: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[] (27.13846ms elapsed)
STEP: Creating pod pod1 in namespace services-2214
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod1:[100]]
Dec  3 15:40:15.148: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod1:[100]] (2.107610277s elapsed)
STEP: Creating pod pod2 in namespace services-2214
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  3 15:40:19.433: INFO: Unexpected endpoints: found map[79e69817-310e-4b10-bffa-4e30f67fde08:[100]], expected map[pod1:[100] pod2:[101]] (4.265650355s elapsed, will retry)
Dec  3 15:40:20.486: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod1:[100] pod2:[101]] (5.318834726s elapsed)
STEP: Deleting pod pod1 in namespace services-2214
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[pod2:[101]]
Dec  3 15:40:20.540: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[pod2:[101]] (34.091873ms elapsed)
STEP: Deleting pod pod2 in namespace services-2214
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2214 to expose endpoints map[]
Dec  3 15:40:20.577: INFO: successfully validated that service multi-endpoint-test in namespace services-2214 exposes endpoints map[] (17.173058ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:40:20.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2214" for this suite.
Dec  3 15:40:42.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:43.417: INFO: namespace services-2214 deletion completed in 22.764443043s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:40:43.417: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec  3 15:40:43.644: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8626 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  3 15:40:46.732: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  3 15:40:46.732: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:40:48.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8626" for this suite.
Dec  3 15:40:54.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:55.563: INFO: namespace kubectl-8626 deletion completed in 6.759808377s
•SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:40:55.563: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8233
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:40:55.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:40:56.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8233" for this suite.
Dec  3 15:41:02.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:03.250: INFO: namespace custom-resource-definition-8233 deletion completed in 6.742710175s
•SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:41:03.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 15:41:03.624: INFO: Number of nodes with available pods: 0
Dec  3 15:41:03.624: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 15:41:04.677: INFO: Number of nodes with available pods: 0
Dec  3 15:41:04.677: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 15:41:05.676: INFO: Number of nodes with available pods: 0
Dec  3 15:41:05.676: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 15:41:06.679: INFO: Number of nodes with available pods: 2
Dec  3 15:41:06.679: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  3 15:41:06.777: INFO: Number of nodes with available pods: 1
Dec  3 15:41:06.777: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:07.829: INFO: Number of nodes with available pods: 1
Dec  3 15:41:07.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:08.829: INFO: Number of nodes with available pods: 1
Dec  3 15:41:08.829: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:09.829: INFO: Number of nodes with available pods: 1
Dec  3 15:41:09.829: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:10.830: INFO: Number of nodes with available pods: 1
Dec  3 15:41:10.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:11.831: INFO: Number of nodes with available pods: 1
Dec  3 15:41:11.831: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:12.830: INFO: Number of nodes with available pods: 1
Dec  3 15:41:12.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:13.830: INFO: Number of nodes with available pods: 1
Dec  3 15:41:13.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:14.829: INFO: Number of nodes with available pods: 1
Dec  3 15:41:14.829: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:15.834: INFO: Number of nodes with available pods: 1
Dec  3 15:41:15.834: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:16.830: INFO: Number of nodes with available pods: 1
Dec  3 15:41:16.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:17.830: INFO: Number of nodes with available pods: 1
Dec  3 15:41:17.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:18.830: INFO: Number of nodes with available pods: 1
Dec  3 15:41:18.830: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 15:41:19.830: INFO: Number of nodes with available pods: 2
Dec  3 15:41:19.830: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8185, will wait for the garbage collector to delete the pods
Dec  3 15:41:19.936: INFO: Deleting DaemonSet.extensions daemon-set took: 20.477914ms
Dec  3 15:41:20.336: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.403126ms
Dec  3 15:41:23.355: INFO: Number of nodes with available pods: 0
Dec  3 15:41:23.355: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:41:23.379: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8185/daemonsets","resourceVersion":"13652"},"items":null}

Dec  3 15:41:23.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8185/pods","resourceVersion":"13652"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:41:23.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8185" for this suite.
Dec  3 15:41:29.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:30.239: INFO: namespace daemonsets-8185 deletion completed in 6.745271653s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:41:30.240: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:41:30.458: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:41:32.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5423" for this suite.
Dec  3 15:42:18.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:42:19.522: INFO: namespace pods-5423 deletion completed in 46.742991961s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:42:19.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-318
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec  3 15:42:23.913: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-0f4724ce-8c54-48c0-9c44-ebda5f4654d4 -c busybox-main-container --namespace=emptydir-318 -- cat /usr/share/volumeshare/shareddata.txt'
Dec  3 15:42:24.773: INFO: stderr: ""
Dec  3 15:42:24.773: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:42:24.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-318" for this suite.
Dec  3 15:42:30.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:42:31.560: INFO: namespace emptydir-318 deletion completed in 6.753459432s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:42:31.561: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2882
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:42:31.778: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:42:58.091: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.107:8080/dial?request=hostName&protocol=udp&host=100.64.1.106&port=8081&tries=1'] Namespace:pod-network-test-2882 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:42:58.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:42:58.635: INFO: Waiting for endpoints: map[]
Dec  3 15:42:58.653: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.107:8080/dial?request=hostName&protocol=udp&host=100.64.0.41&port=8081&tries=1'] Namespace:pod-network-test-2882 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:42:58.653: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:42:59.160: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:42:59.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2882" for this suite.
Dec  3 15:43:21.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:21.936: INFO: namespace pod-network-test-2882 deletion completed in 22.742115269s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:43:21.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec  3 15:43:22.187: INFO: Waiting up to 5m0s for pod "client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e" in namespace "containers-3850" to be "success or failure"
Dec  3 15:43:22.205: INFO: Pod "client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.179387ms
Dec  3 15:43:24.224: INFO: Pod "client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036391996s
Dec  3 15:43:26.242: INFO: Pod "client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054805275s
STEP: Saw pod success
Dec  3 15:43:26.242: INFO: Pod "client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e" satisfied condition "success or failure"
Dec  3 15:43:26.261: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e container test-container: <nil>
STEP: delete the pod
Dec  3 15:43:26.319: INFO: Waiting for pod client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e to disappear
Dec  3 15:43:26.336: INFO: Pod client-containers-80cd043b-f1f8-46a0-9b68-6ba7fc2d675e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:43:26.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3850" for this suite.
Dec  3 15:43:32.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:33.128: INFO: namespace containers-3850 deletion completed in 6.758098633s
•SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:43:33.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 15:43:33.358: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:43:39.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7683" for this suite.
Dec  3 15:43:45.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:45.784: INFO: namespace init-container-7683 deletion completed in 6.741944467s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:43:45.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:43:46.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c" in namespace "projected-9578" to be "success or failure"
Dec  3 15:43:46.056: INFO: Pod "downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.528353ms
Dec  3 15:43:48.075: INFO: Pod "downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035970485s
Dec  3 15:43:50.093: INFO: Pod "downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054524494s
STEP: Saw pod success
Dec  3 15:43:50.093: INFO: Pod "downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c" satisfied condition "success or failure"
Dec  3 15:43:50.111: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c container client-container: <nil>
STEP: delete the pod
Dec  3 15:43:50.163: INFO: Waiting for pod downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c to disappear
Dec  3 15:43:50.181: INFO: Pod downwardapi-volume-27152889-c931-48ba-8ef1-bb9dfa2a6e5c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:43:50.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9578" for this suite.
Dec  3 15:43:56.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:56.955: INFO: namespace projected-9578 deletion completed in 6.741134107s
•SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:43:56.956: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-67
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-67
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-67
STEP: Creating statefulset with conflicting port in namespace statefulset-67
STEP: Waiting until pod test-pod will start running in namespace statefulset-67
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-67
Dec  3 15:44:01.315: INFO: Observed stateful pod in namespace: statefulset-67, name: ss-0, uid: 42b10663-2aa3-4e27-aad4-79d21297df4c, status phase: Pending. Waiting for statefulset controller to delete.
Dec  3 15:44:01.391: INFO: Observed stateful pod in namespace: statefulset-67, name: ss-0, uid: 42b10663-2aa3-4e27-aad4-79d21297df4c, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:44:01.395: INFO: Observed stateful pod in namespace: statefulset-67, name: ss-0, uid: 42b10663-2aa3-4e27-aad4-79d21297df4c, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:44:01.409: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-67
STEP: Removing pod with conflicting port in namespace statefulset-67
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-67 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 15:44:05.538: INFO: Deleting all statefulset in ns statefulset-67
Dec  3 15:44:05.593: INFO: Scaling statefulset ss to 0
Dec  3 15:44:15.794: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:44:15.811: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:44:15.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-67" for this suite.
Dec  3 15:44:21.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:22.643: INFO: namespace statefulset-67 deletion completed in 6.739094838s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:44:22.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:44:22.866: INFO: Creating deployment "test-recreate-deployment"
Dec  3 15:44:22.885: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  3 15:44:22.923: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  3 15:44:22.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:44:24.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984662, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:44:26.965: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  3 15:44:27.001: INFO: Updating deployment test-recreate-deployment
Dec  3 15:44:27.001: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 15:44:27.108: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1404,SelfLink:/apis/apps/v1/namespaces/deployment-1404/deployments/test-recreate-deployment,UID:8aca36a8-993c-4ed2-93e5-61ae04c7baa8,ResourceVersion:14417,Generation:2,CreationTimestamp:2019-12-03 15:44:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-03 15:44:27 +0000 UTC 2019-12-03 15:44:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-03 15:44:27 +0000 UTC 2019-12-03 15:44:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  3 15:44:27.127: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-1404,SelfLink:/apis/apps/v1/namespaces/deployment-1404/replicasets/test-recreate-deployment-5c8c9cc69d,UID:8e48e512-b5ca-40bd-94fc-7d91cf320246,ResourceVersion:14416,Generation:1,CreationTimestamp:2019-12-03 15:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8aca36a8-993c-4ed2-93e5-61ae04c7baa8 0xc002c630f7 0xc002c630f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:44:27.127: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  3 15:44:27.127: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-1404,SelfLink:/apis/apps/v1/namespaces/deployment-1404/replicasets/test-recreate-deployment-6df85df6b9,UID:88306eee-ea65-46b2-a966-159acfb98e43,ResourceVersion:14409,Generation:2,CreationTimestamp:2019-12-03 15:44:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8aca36a8-993c-4ed2-93e5-61ae04c7baa8 0xc002c631c7 0xc002c631c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:44:27.145: INFO: Pod "test-recreate-deployment-5c8c9cc69d-x6cs6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-x6cs6,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-1404,SelfLink:/api/v1/namespaces/deployment-1404/pods/test-recreate-deployment-5c8c9cc69d-x6cs6,UID:8257fdab-1e7b-4617-8e91-7f65d2c61a30,ResourceVersion:14418,Generation:0,CreationTimestamp:2019-12-03 15:44:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 8e48e512-b5ca-40bd-94fc-7d91cf320246 0xc002c63a97 0xc002c63a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wdntm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdntm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdntm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c63b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c63b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:27 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 15:44:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:44:27.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1404" for this suite.
Dec  3 15:44:33.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:33.919: INFO: namespace deployment-1404 deletion completed in 6.737186592s
•SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:44:33.919: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:44:34.166: INFO: Waiting up to 5m0s for pod "pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785" in namespace "emptydir-3589" to be "success or failure"
Dec  3 15:44:34.183: INFO: Pod "pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785": Phase="Pending", Reason="", readiness=false. Elapsed: 17.03665ms
Dec  3 15:44:36.201: INFO: Pod "pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035561869s
Dec  3 15:44:38.219: INFO: Pod "pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053269382s
STEP: Saw pod success
Dec  3 15:44:38.219: INFO: Pod "pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785" satisfied condition "success or failure"
Dec  3 15:44:38.236: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785 container test-container: <nil>
STEP: delete the pod
Dec  3 15:44:38.288: INFO: Waiting for pod pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785 to disappear
Dec  3 15:44:38.305: INFO: Pod pod-0bad28a6-2dd7-4d23-8d41-cf2616f2e785 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:44:38.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3589" for this suite.
Dec  3 15:44:44.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:45.111: INFO: namespace emptydir-3589 deletion completed in 6.77180901s
•
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:44:45.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 15:44:49.459: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:44:49.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5250" for this suite.
Dec  3 15:44:55.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:56.276: INFO: namespace container-runtime-5250 deletion completed in 6.735435493s
•SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:44:56.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8714
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-8714
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8714
Dec  3 15:44:56.560: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 15:45:06.580: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  3 15:45:06.598: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:45:07.279: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:45:07.279: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:45:07.279: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:45:07.302: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 15:45:17.321: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:45:17.322: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:45:17.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998948s
Dec  3 15:45:18.412: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980573344s
Dec  3 15:45:19.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.962263185s
Dec  3 15:45:20.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.939442471s
Dec  3 15:45:21.473: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.920542775s
Dec  3 15:45:22.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.901219892s
Dec  3 15:45:23.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.882198147s
Dec  3 15:45:24.533: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.863257972s
Dec  3 15:45:25.562: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.841371916s
Dec  3 15:45:26.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 811.977469ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8714
Dec  3 15:45:27.633: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:45:28.670: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:45:28.671: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:45:28.671: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:45:28.671: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:45:29.791: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 15:45:29.791: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:45:29.791: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:45:29.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:45:30.826: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 15:45:30.827: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:45:30.827: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:45:30.897: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:45:30.897: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:45:30.897: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  3 15:45:30.964: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:45:32.080: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:45:32.080: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:45:32.080: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:45:32.081: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:45:33.293: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:45:33.294: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:45:33.294: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:45:33.294: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:45:34.356: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:45:34.357: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:45:34.357: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:45:34.357: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:45:34.393: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 15:45:44.430: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:45:44.430: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:45:44.430: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 15:45:44.487: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:44.487: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:44.487: INFO: ss-1  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:44.487: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:44.487: INFO: 
Dec  3 15:45:44.487: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 15:45:45.506: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:45.506: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:45.506: INFO: ss-1  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:45.506: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:45.506: INFO: 
Dec  3 15:45:45.506: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 15:45:46.529: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:46.530: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:46.530: INFO: ss-1  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:46.530: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:46.530: INFO: 
Dec  3 15:45:46.530: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 15:45:47.548: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:47.548: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:47.548: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:47.549: INFO: 
Dec  3 15:45:47.549: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 15:45:48.567: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:48.567: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:48.567: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:48.567: INFO: 
Dec  3 15:45:48.567: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 15:45:49.586: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:49.586: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:49.586: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:49.587: INFO: 
Dec  3 15:45:49.587: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 15:45:50.605: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:50.605: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:50.605: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:50.605: INFO: 
Dec  3 15:45:50.605: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 15:45:51.628: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:51.628: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:51.628: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:51.628: INFO: 
Dec  3 15:45:51.628: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 15:45:52.647: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:52.647: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:52.647: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:52.647: INFO: 
Dec  3 15:45:52.647: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 15:45:53.669: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Dec  3 15:45:53.669: INFO: ss-0  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:44:56 +0000 UTC  }]
Dec  3 15:45:53.669: INFO: ss-2  shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:45:17 +0000 UTC  }]
Dec  3 15:45:53.669: INFO: 
Dec  3 15:45:53.669: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8714
Dec  3 15:45:54.689: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:45:55.161: INFO: rc: 1
Dec  3 15:45:55.161: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00364a5d0 exit status 1 <nil> <nil> true [0xc002a3c388 0xc002a3c3a0 0xc002a3c3b8] [0xc002a3c388 0xc002a3c3a0 0xc002a3c3b8] [0xc002a3c398 0xc002a3c3b0] [0xba6c10 0xba6c10] 0xc00378afc0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Dec  3 15:46:05.161: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:46:05.386: INFO: rc: 1
Dec  3 15:46:05.386: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00364abd0 exit status 1 <nil> <nil> true [0xc002a3c3c0 0xc002a3c3d8 0xc002a3c3f0] [0xc002a3c3c0 0xc002a3c3d8 0xc002a3c3f0] [0xc002a3c3d0 0xc002a3c3e8] [0xba6c10 0xba6c10] 0xc00378b2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:46:15.386: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:46:15.643: INFO: rc: 1
Dec  3 15:46:15.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00364b200 exit status 1 <nil> <nil> true [0xc002a3c3f8 0xc002a3c410 0xc002a3c428] [0xc002a3c3f8 0xc002a3c410 0xc002a3c428] [0xc002a3c408 0xc002a3c420] [0xba6c10 0xba6c10] 0xc00378b680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:46:25.643: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:46:25.881: INFO: rc: 1
Dec  3 15:46:25.882: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00364b7d0 exit status 1 <nil> <nil> true [0xc002a3c430 0xc002a3c448 0xc002a3c460] [0xc002a3c430 0xc002a3c448 0xc002a3c460] [0xc002a3c440 0xc002a3c458] [0xba6c10 0xba6c10] 0xc00378b9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:46:35.882: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:46:36.106: INFO: rc: 1
Dec  3 15:46:36.106: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e3cb0 exit status 1 <nil> <nil> true [0xc000195770 0xc0001957c8 0xc0001957e8] [0xc000195770 0xc0001957c8 0xc0001957e8] [0xc000195790 0xc0001957e0] [0xba6c10 0xba6c10] 0xc00026bd40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:46:46.107: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:46:46.289: INFO: rc: 1
Dec  3 15:46:46.290: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0018bc2a0 exit status 1 <nil> <nil> true [0xc000195858 0xc000195940 0xc000195a10] [0xc000195858 0xc000195940 0xc000195a10] [0xc000195920 0xc0001959b8] [0xba6c10 0xba6c10] 0xc0024be0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:46:56.290: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:46:56.464: INFO: rc: 1
Dec  3 15:46:56.464: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023a8780 exit status 1 <nil> <nil> true [0xc0000deae8 0xc0000decf0 0xc0000dee58] [0xc0000deae8 0xc0000decf0 0xc0000dee58] [0xc0000debe8 0xc0000dee08] [0xba6c10 0xba6c10] 0xc00380c300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:47:06.464: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:47:11.619: INFO: rc: 1
Dec  3 15:47:11.619: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e27b0 exit status 1 <nil> <nil> true [0xc0006cf640 0xc0006cf778 0xc0006cf958] [0xc0006cf640 0xc0006cf778 0xc0006cf958] [0xc0006cf748 0xc0006cf850] [0xba6c10 0xba6c10] 0xc0008f0de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:47:21.619: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:47:21.773: INFO: rc: 1
Dec  3 15:47:21.773: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e2d80 exit status 1 <nil> <nil> true [0xc0006cf978 0xc0006cfc78 0xc0006cff40] [0xc0006cf978 0xc0006cfc78 0xc0006cff40] [0xc0006cfb48 0xc0006cff18] [0xba6c10 0xba6c10] 0xc0008f10e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:47:31.774: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:47:31.945: INFO: rc: 1
Dec  3 15:47:31.945: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00236e6c0 exit status 1 <nil> <nil> true [0xc0023fc008 0xc0023fc040 0xc0023fc070] [0xc0023fc008 0xc0023fc040 0xc0023fc070] [0xc0023fc030 0xc0023fc060] [0xba6c10 0xba6c10] 0xc0036a94a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:47:41.945: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:47:42.132: INFO: rc: 1
Dec  3 15:47:42.132: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023a8d50 exit status 1 <nil> <nil> true [0xc0000dee70 0xc0000def70 0xc0000df198] [0xc0000dee70 0xc0000def70 0xc0000df198] [0xc0000deee0 0xc0000df0b8] [0xba6c10 0xba6c10] 0xc00380c660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:47:52.132: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:47:52.285: INFO: rc: 1
Dec  3 15:47:52.285: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00236ee10 exit status 1 <nil> <nil> true [0xc0023fc088 0xc0023fc0b8 0xc0023fc0f0] [0xc0023fc088 0xc0023fc0b8 0xc0023fc0f0] [0xc0023fc0a8 0xc0023fc0e0] [0xba6c10 0xba6c10] 0xc0008f4120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:48:02.285: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:48:02.448: INFO: rc: 1
Dec  3 15:48:02.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c2690 exit status 1 <nil> <nil> true [0xc001970000 0xc001970018 0xc001970030] [0xc001970000 0xc001970018 0xc001970030] [0xc001970010 0xc001970028] [0xba6c10 0xba6c10] 0xc000b9a2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:48:12.448: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:48:12.626: INFO: rc: 1
Dec  3 15:48:12.626: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c2c90 exit status 1 <nil> <nil> true [0xc001970038 0xc001970050 0xc001970070] [0xc001970038 0xc001970050 0xc001970070] [0xc001970048 0xc001970060] [0xba6c10 0xba6c10] 0xc000b9a5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:48:22.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:48:22.803: INFO: rc: 1
Dec  3 15:48:22.803: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00236f530 exit status 1 <nil> <nil> true [0xc0023fc108 0xc0023fc160 0xc0023fc178] [0xc0023fc108 0xc0023fc160 0xc0023fc178] [0xc0023fc140 0xc0023fc170] [0xba6c10 0xba6c10] 0xc0008f4480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:48:32.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:48:32.967: INFO: rc: 1
Dec  3 15:48:32.967: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023a9320 exit status 1 <nil> <nil> true [0xc0000df1b0 0xc0000df470 0xc0000df7a8] [0xc0000df1b0 0xc0000df470 0xc0000df7a8] [0xc0000df308 0xc0000df6c0] [0xba6c10 0xba6c10] 0xc00380c960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:48:42.967: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:48:43.145: INFO: rc: 1
Dec  3 15:48:43.145: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023a9950 exit status 1 <nil> <nil> true [0xc0000df880 0xc0000dfb98 0xc0000dfd00] [0xc0000df880 0xc0000dfb98 0xc0000dfd00] [0xc0000dfa58 0xc0000dfc58] [0xba6c10 0xba6c10] 0xc00380cc60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:48:53.146: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:48:53.315: INFO: rc: 1
Dec  3 15:48:53.315: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e3560 exit status 1 <nil> <nil> true [0xc002a3c008 0xc002a3c020 0xc002a3c038] [0xc002a3c008 0xc002a3c020 0xc002a3c038] [0xc002a3c018 0xc002a3c030] [0xba6c10 0xba6c10] 0xc0008f18c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:49:03.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:49:03.559: INFO: rc: 1
Dec  3 15:49:03.559: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00236e690 exit status 1 <nil> <nil> true [0xc0006cf640 0xc0006cf778 0xc0006cf958] [0xc0006cf640 0xc0006cf778 0xc0006cf958] [0xc0006cf748 0xc0006cf850] [0xba6c10 0xba6c10] 0xc0036a94a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:49:13.560: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:49:13.715: INFO: rc: 1
Dec  3 15:49:13.715: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0029c26c0 exit status 1 <nil> <nil> true [0xc0023fc008 0xc0023fc040 0xc0023fc070] [0xc0023fc008 0xc0023fc040 0xc0023fc070] [0xc0023fc030 0xc0023fc060] [0xba6c10 0xba6c10] 0xc0008f40c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:49:23.715: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:49:23.861: INFO: rc: 1
Dec  3 15:49:23.861: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00236edb0 exit status 1 <nil> <nil> true [0xc0006cf978 0xc0006cfc78 0xc0006cff40] [0xc0006cf978 0xc0006cfc78 0xc0006cff40] [0xc0006cfb48 0xc0006cff18] [0xba6c10 0xba6c10] 0xc000b9a120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:49:33.862: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:49:34.048: INFO: rc: 1
Dec  3 15:49:34.048: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023a87b0 exit status 1 <nil> <nil> true [0xc001970000 0xc001970018 0xc001970030] [0xc001970000 0xc001970018 0xc001970030] [0xc001970010 0xc001970028] [0xba6c10 0xba6c10] 0xc00380c300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:49:44.049: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:49:44.219: INFO: rc: 1
Dec  3 15:49:44.219: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e2780 exit status 1 <nil> <nil> true [0xc0000de938 0xc0000debe8 0xc0000dee08] [0xc0000de938 0xc0000debe8 0xc0000dee08] [0xc0000deb90 0xc0000ded98] [0xba6c10 0xba6c10] 0xc0008f0f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:49:54.220: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:49:54.371: INFO: rc: 1
Dec  3 15:49:54.371: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e2db0 exit status 1 <nil> <nil> true [0xc0000dee58 0xc0000deee0 0xc0000df0b8] [0xc0000dee58 0xc0000deee0 0xc0000df0b8] [0xc0000deec0 0xc0000def98] [0xba6c10 0xba6c10] 0xc0008f1b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:50:04.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:50:04.547: INFO: rc: 1
Dec  3 15:50:04.548: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0008e3b30 exit status 1 <nil> <nil> true [0xc0000df198 0xc0000df308 0xc0000df6c0] [0xc0000df198 0xc0000df308 0xc0000df6c0] [0xc0000df1e0 0xc0000df478] [0xba6c10 0xba6c10] 0xc0008f1ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:50:14.548: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:50:14.709: INFO: rc: 1
Dec  3 15:50:14.710: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d46120 exit status 1 <nil> <nil> true [0xc0000df7a8 0xc0000dfa58 0xc0000dfc58] [0xc0000df7a8 0xc0000dfa58 0xc0000dfc58] [0xc0000df908 0xc0000dfbd0] [0xba6c10 0xba6c10] 0xc0038c0780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:50:24.710: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:50:24.859: INFO: rc: 1
Dec  3 15:50:24.859: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0023a8e40 exit status 1 <nil> <nil> true [0xc001970038 0xc001970050 0xc001970070] [0xc001970038 0xc001970050 0xc001970070] [0xc001970048 0xc001970060] [0xba6c10 0xba6c10] 0xc00380c660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:50:34.859: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:50:35.008: INFO: rc: 1
Dec  3 15:50:35.008: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d466f0 exit status 1 <nil> <nil> true [0xc0000dfd00 0xc0000dfe30 0xc0000dffe8] [0xc0000dfd00 0xc0000dfe30 0xc0000dffe8] [0xc0000dfd48 0xc0000dffd0] [0xba6c10 0xba6c10] 0xc0038c0a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:50:45.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:50:45.169: INFO: rc: 1
Dec  3 15:50:45.169: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d46d50 exit status 1 <nil> <nil> true [0xc002a3c040 0xc002a3c058 0xc002a3c070] [0xc002a3c040 0xc002a3c058 0xc002a3c070] [0xc002a3c050 0xc002a3c068] [0xba6c10 0xba6c10] 0xc0038c0f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 15:50:55.169: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-8714 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:50:55.321: INFO: rc: 1
Dec  3 15:50:55.321: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Dec  3 15:50:55.321: INFO: Scaling statefulset ss to 0
Dec  3 15:50:55.386: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 15:50:55.407: INFO: Deleting all statefulset in ns statefulset-8714
Dec  3 15:50:55.428: INFO: Scaling statefulset ss to 0
Dec  3 15:50:55.493: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:50:55.514: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:50:55.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8714" for this suite.
Dec  3 15:51:01.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:02.529: INFO: namespace statefulset-8714 deletion completed in 6.906979975s

• [SLOW TEST:366.252 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:02.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:51:02.772: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7754'
Dec  3 15:51:02.957: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:51:02.957: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Dec  3 15:51:02.979: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-7754'
Dec  3 15:51:03.162: INFO: stderr: ""
Dec  3 15:51:03.162: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:03.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7754" for this suite.
Dec  3 15:51:09.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:10.107: INFO: namespace kubectl-7754 deletion completed in 6.921863713s
•SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:10.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-f295560d-75e8-4cd4-9a76-62ebe40c4d4f
STEP: Creating a pod to test consume configMaps
Dec  3 15:51:10.388: INFO: Waiting up to 5m0s for pod "pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e" in namespace "configmap-8506" to be "success or failure"
Dec  3 15:51:10.411: INFO: Pod "pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.588521ms
Dec  3 15:51:12.433: INFO: Pod "pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044896171s
Dec  3 15:51:14.455: INFO: Pod "pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066526806s
STEP: Saw pod success
Dec  3 15:51:14.455: INFO: Pod "pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e" satisfied condition "success or failure"
Dec  3 15:51:14.476: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:51:14.534: INFO: Waiting for pod pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e to disappear
Dec  3 15:51:14.555: INFO: Pod pod-configmaps-4d3b859c-0d87-4ac8-ba79-1c18371cef3e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:14.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8506" for this suite.
Dec  3 15:51:20.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:21.498: INFO: namespace configmap-8506 deletion completed in 6.900209837s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:21.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:51:21.778: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150" in namespace "projected-1336" to be "success or failure"
Dec  3 15:51:21.802: INFO: Pod "downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150": Phase="Pending", Reason="", readiness=false. Elapsed: 23.935493ms
Dec  3 15:51:23.824: INFO: Pod "downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045867911s
Dec  3 15:51:25.847: INFO: Pod "downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068087254s
STEP: Saw pod success
Dec  3 15:51:25.847: INFO: Pod "downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150" satisfied condition "success or failure"
Dec  3 15:51:25.868: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150 container client-container: <nil>
STEP: delete the pod
Dec  3 15:51:25.933: INFO: Waiting for pod downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150 to disappear
Dec  3 15:51:25.954: INFO: Pod downwardapi-volume-e0562ab6-95ff-4c47-b799-e6354ad83150 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:25.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1336" for this suite.
Dec  3 15:51:32.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:32.900: INFO: namespace projected-1336 deletion completed in 6.904723994s
•SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:32.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 15:51:33.173: INFO: Waiting up to 5m0s for pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b" in namespace "emptydir-2745" to be "success or failure"
Dec  3 15:51:33.195: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.15227ms
Dec  3 15:51:35.217: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043331966s
Dec  3 15:51:37.238: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064927996s
Dec  3 15:51:39.260: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086517673s
Dec  3 15:51:41.284: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.110359611s
Dec  3 15:51:43.306: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.132485184s
STEP: Saw pod success
Dec  3 15:51:43.306: INFO: Pod "pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b" satisfied condition "success or failure"
Dec  3 15:51:43.327: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b container test-container: <nil>
STEP: delete the pod
Dec  3 15:51:43.389: INFO: Waiting for pod pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b to disappear
Dec  3 15:51:43.410: INFO: Pod pod-08bfdbc0-be55-43cd-ba0c-3d57143e349b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:43.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2745" for this suite.
Dec  3 15:51:49.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:50.397: INFO: namespace emptydir-2745 deletion completed in 6.946288683s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:50.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  3 15:51:50.728: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5906,SelfLink:/api/v1/namespaces/watch-5906/configmaps/e2e-watch-test-watch-closed,UID:e96f6943-8d57-4903-9761-ccae7efec65b,ResourceVersion:15690,Generation:0,CreationTimestamp:2019-12-03 15:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:51:50.728: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5906,SelfLink:/api/v1/namespaces/watch-5906/configmaps/e2e-watch-test-watch-closed,UID:e96f6943-8d57-4903-9761-ccae7efec65b,ResourceVersion:15691,Generation:0,CreationTimestamp:2019-12-03 15:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  3 15:51:50.814: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5906,SelfLink:/api/v1/namespaces/watch-5906/configmaps/e2e-watch-test-watch-closed,UID:e96f6943-8d57-4903-9761-ccae7efec65b,ResourceVersion:15692,Generation:0,CreationTimestamp:2019-12-03 15:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:51:50.814: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5906,SelfLink:/api/v1/namespaces/watch-5906/configmaps/e2e-watch-test-watch-closed,UID:e96f6943-8d57-4903-9761-ccae7efec65b,ResourceVersion:15694,Generation:0,CreationTimestamp:2019-12-03 15:51:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:50.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5906" for this suite.
Dec  3 15:51:56.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:57.735: INFO: namespace watch-5906 deletion completed in 6.897055685s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:57.736: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  3 15:52:08.325: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 15:52:08.325008    5074 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:52:08.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1299" for this suite.
Dec  3 15:52:14.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:15.236: INFO: namespace gc-1299 deletion completed in 6.889834923s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:52:15.237: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9072
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-acc4962d-e115-4fa4-8460-60f23279632e
STEP: Creating secret with name s-test-opt-upd-61da467e-cb3f-4d84-9843-2d5b23097e68
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-acc4962d-e115-4fa4-8460-60f23279632e
STEP: Updating secret s-test-opt-upd-61da467e-cb3f-4d84-9843-2d5b23097e68
STEP: Creating secret with name s-test-opt-create-161dba40-9d2e-41ca-9abb-9547737676d6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:52:22.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9072" for this suite.
Dec  3 15:52:44.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:45.057: INFO: namespace secrets-9072 deletion completed in 22.917093254s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:52:45.058: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8709
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:52:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8709" for this suite.
Dec  3 15:53:19.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:20.376: INFO: namespace replication-controller-8709 deletion completed in 22.894031086s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:53:20.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 15:53:20.653: INFO: Waiting up to 5m0s for pod "pod-917fff03-775b-4b6d-beff-613caf8cd4e0" in namespace "emptydir-4828" to be "success or failure"
Dec  3 15:53:20.675: INFO: Pod "pod-917fff03-775b-4b6d-beff-613caf8cd4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.294104ms
Dec  3 15:53:22.697: INFO: Pod "pod-917fff03-775b-4b6d-beff-613caf8cd4e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043410827s
Dec  3 15:53:24.719: INFO: Pod "pod-917fff03-775b-4b6d-beff-613caf8cd4e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065763843s
STEP: Saw pod success
Dec  3 15:53:24.719: INFO: Pod "pod-917fff03-775b-4b6d-beff-613caf8cd4e0" satisfied condition "success or failure"
Dec  3 15:53:24.740: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-917fff03-775b-4b6d-beff-613caf8cd4e0 container test-container: <nil>
STEP: delete the pod
Dec  3 15:53:24.807: INFO: Waiting for pod pod-917fff03-775b-4b6d-beff-613caf8cd4e0 to disappear
Dec  3 15:53:24.828: INFO: Pod pod-917fff03-775b-4b6d-beff-613caf8cd4e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:53:24.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4828" for this suite.
Dec  3 15:53:30.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:31.804: INFO: namespace emptydir-4828 deletion completed in 6.935534417s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:53:31.805: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  3 15:53:36.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6157 pod-service-account-fd198a60-367c-4151-b272-e93d338970e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  3 15:53:37.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6157 pod-service-account-fd198a60-367c-4151-b272-e93d338970e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  3 15:53:38.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-6157 pod-service-account-fd198a60-367c-4151-b272-e93d338970e2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:53:38.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6157" for this suite.
Dec  3 15:53:44.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:45.741: INFO: namespace svcaccounts-6157 deletion completed in 6.927533495s
•SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:53:45.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9548
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c3feed40-296d-4d0a-9708-2d67c046e019
STEP: Creating a pod to test consume secrets
Dec  3 15:53:46.035: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e" in namespace "projected-9548" to be "success or failure"
Dec  3 15:53:46.056: INFO: Pod "pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.967573ms
Dec  3 15:53:48.078: INFO: Pod "pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04308261s
Dec  3 15:53:50.100: INFO: Pod "pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065143096s
STEP: Saw pod success
Dec  3 15:53:50.100: INFO: Pod "pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e" satisfied condition "success or failure"
Dec  3 15:53:50.121: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:53:50.182: INFO: Waiting for pod pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e to disappear
Dec  3 15:53:50.203: INFO: Pod pod-projected-secrets-42915dfd-c8fe-4438-88c5-1d518e88ef2e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:53:50.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9548" for this suite.
Dec  3 15:53:56.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:57.175: INFO: namespace projected-9548 deletion completed in 6.930882013s
•SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:53:57.176: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5cc5d2f5-a55f-4ebc-8e32-6f2fefdb610c
STEP: Creating a pod to test consume secrets
Dec  3 15:53:57.464: INFO: Waiting up to 5m0s for pod "pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc" in namespace "secrets-9526" to be "success or failure"
Dec  3 15:53:57.485: INFO: Pod "pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.854469ms
Dec  3 15:53:59.508: INFO: Pod "pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043384315s
Dec  3 15:54:01.530: INFO: Pod "pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065635958s
STEP: Saw pod success
Dec  3 15:54:01.530: INFO: Pod "pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc" satisfied condition "success or failure"
Dec  3 15:54:01.552: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:54:01.608: INFO: Waiting for pod pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc to disappear
Dec  3 15:54:01.642: INFO: Pod pod-secrets-66937d82-619f-4f21-a85a-210b1546a9dc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:54:01.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9526" for this suite.
Dec  3 15:54:07.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:54:08.573: INFO: namespace secrets-9526 deletion completed in 6.890239897s
•SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:54:08.574: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-4033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4033
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4033
STEP: Deleting pre-stop pod
Dec  3 15:54:24.107: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:54:24.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4033" for this suite.
Dec  3 15:55:10.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:11.062: INFO: namespace prestop-4033 deletion completed in 46.887127313s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:55:11.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Dec  3 15:55:11.305: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8794'
Dec  3 15:55:11.627: INFO: stderr: ""
Dec  3 15:55:11.627: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec  3 15:55:12.650: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:55:12.650: INFO: Found 0 / 1
Dec  3 15:55:13.650: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:55:13.650: INFO: Found 0 / 1
Dec  3 15:55:14.650: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:55:14.650: INFO: Found 1 / 1
Dec  3 15:55:14.650: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 15:55:14.671: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:55:14.671: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  3 15:55:14.671: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mfn48 redis-master --namespace=kubectl-8794'
Dec  3 15:55:14.931: INFO: stderr: ""
Dec  3 15:55:14.931: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:55:13.433 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:55:13.433 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:55:13.433 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:55:13.433 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  3 15:55:14.931: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mfn48 redis-master --namespace=kubectl-8794 --tail=1'
Dec  3 15:55:15.094: INFO: stderr: ""
Dec  3 15:55:15.094: INFO: stdout: "1:M 03 Dec 15:55:13.433 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  3 15:55:15.094: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mfn48 redis-master --namespace=kubectl-8794 --limit-bytes=1'
Dec  3 15:55:15.257: INFO: stderr: ""
Dec  3 15:55:15.257: INFO: stdout: " "
STEP: exposing timestamps
Dec  3 15:55:15.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mfn48 redis-master --namespace=kubectl-8794 --tail=1 --timestamps'
Dec  3 15:55:15.428: INFO: stderr: ""
Dec  3 15:55:15.428: INFO: stdout: "2019-12-03T15:55:13.433932993Z 1:M 03 Dec 15:55:13.433 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  3 15:55:17.928: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mfn48 redis-master --namespace=kubectl-8794 --since=1s'
Dec  3 15:55:18.092: INFO: stderr: ""
Dec  3 15:55:18.092: INFO: stdout: ""
Dec  3 15:55:18.092: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mfn48 redis-master --namespace=kubectl-8794 --since=24h'
Dec  3 15:55:18.255: INFO: stderr: ""
Dec  3 15:55:18.255: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:55:13.433 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:55:13.433 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:55:13.433 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:55:13.433 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Dec  3 15:55:18.255: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8794'
Dec  3 15:55:18.405: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:55:18.405: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  3 15:55:18.405: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-8794'
Dec  3 15:55:18.560: INFO: stderr: "No resources found.\n"
Dec  3 15:55:18.560: INFO: stdout: ""
Dec  3 15:55:18.560: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-8794 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:55:18.693: INFO: stderr: ""
Dec  3 15:55:18.693: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:55:18.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8794" for this suite.
Dec  3 15:55:24.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:25.667: INFO: namespace kubectl-8794 deletion completed in 6.932843198s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:55:25.667: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:55:25.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede" in namespace "downward-api-6810" to be "success or failure"
Dec  3 15:55:25.954: INFO: Pod "downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede": Phase="Pending", Reason="", readiness=false. Elapsed: 21.121341ms
Dec  3 15:55:27.976: INFO: Pod "downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043692301s
Dec  3 15:55:29.999: INFO: Pod "downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065983945s
STEP: Saw pod success
Dec  3 15:55:29.999: INFO: Pod "downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede" satisfied condition "success or failure"
Dec  3 15:55:30.020: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede container client-container: <nil>
STEP: delete the pod
Dec  3 15:55:30.084: INFO: Waiting for pod downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede to disappear
Dec  3 15:55:30.106: INFO: Pod downwardapi-volume-2769389c-a399-4091-8af3-f66ae6f41ede no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:55:30.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6810" for this suite.
Dec  3 15:55:36.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:37.049: INFO: namespace downward-api-6810 deletion completed in 6.900882424s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:55:37.049: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:55:37.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8" in namespace "projected-4781" to be "success or failure"
Dec  3 15:55:37.342: INFO: Pod "downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 21.140712ms
Dec  3 15:55:39.364: INFO: Pod "downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043097966s
Dec  3 15:55:41.387: INFO: Pod "downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066759849s
STEP: Saw pod success
Dec  3 15:55:41.387: INFO: Pod "downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8" satisfied condition "success or failure"
Dec  3 15:55:41.409: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8 container client-container: <nil>
STEP: delete the pod
Dec  3 15:55:41.463: INFO: Waiting for pod downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8 to disappear
Dec  3 15:55:41.484: INFO: Pod downwardapi-volume-1bb8af64-34a9-406e-a1ef-63daecacc1d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:55:41.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4781" for this suite.
Dec  3 15:55:47.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:48.459: INFO: namespace projected-4781 deletion completed in 6.932841466s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:55:48.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9816
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:55:48.696: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9816'
Dec  3 15:55:48.852: INFO: stderr: ""
Dec  3 15:55:48.852: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec  3 15:55:48.873: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-9816'
Dec  3 15:56:01.198: INFO: stderr: ""
Dec  3 15:56:01.198: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:56:01.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9816" for this suite.
Dec  3 15:56:07.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:08.132: INFO: namespace kubectl-9816 deletion completed in 6.891480227s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:56:08.132: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1799
I1203 15:56:08.395093    5074 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1799, replica count: 1
I1203 15:56:09.445569    5074 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:56:10.445796    5074 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:56:10.585: INFO: Created: latency-svc-l58kg
Dec  3 15:56:10.591: INFO: Got endpoints: latency-svc-l58kg [45.774372ms]
Dec  3 15:56:10.625: INFO: Created: latency-svc-nqnbd
Dec  3 15:56:10.628: INFO: Got endpoints: latency-svc-nqnbd [36.250365ms]
Dec  3 15:56:10.640: INFO: Created: latency-svc-vn7bx
Dec  3 15:56:10.642: INFO: Got endpoints: latency-svc-vn7bx [50.29488ms]
Dec  3 15:56:10.654: INFO: Created: latency-svc-5dp69
Dec  3 15:56:10.656: INFO: Got endpoints: latency-svc-5dp69 [64.776586ms]
Dec  3 15:56:10.669: INFO: Created: latency-svc-7tv65
Dec  3 15:56:10.677: INFO: Got endpoints: latency-svc-7tv65 [84.970609ms]
Dec  3 15:56:10.713: INFO: Created: latency-svc-xt85n
Dec  3 15:56:10.715: INFO: Got endpoints: latency-svc-xt85n [123.014277ms]
Dec  3 15:56:10.727: INFO: Created: latency-svc-q44bc
Dec  3 15:56:10.756: INFO: Got endpoints: latency-svc-q44bc [163.936397ms]
Dec  3 15:56:10.765: INFO: Created: latency-svc-rrdb5
Dec  3 15:56:10.767: INFO: Got endpoints: latency-svc-rrdb5 [175.607563ms]
Dec  3 15:56:10.779: INFO: Created: latency-svc-xnmfs
Dec  3 15:56:10.782: INFO: Got endpoints: latency-svc-xnmfs [190.547205ms]
Dec  3 15:56:10.806: INFO: Created: latency-svc-frvcl
Dec  3 15:56:10.809: INFO: Got endpoints: latency-svc-frvcl [216.936365ms]
Dec  3 15:56:10.856: INFO: Created: latency-svc-fjsl2
Dec  3 15:56:10.858: INFO: Got endpoints: latency-svc-fjsl2 [266.158807ms]
Dec  3 15:56:10.871: INFO: Created: latency-svc-5hch7
Dec  3 15:56:10.878: INFO: Got endpoints: latency-svc-5hch7 [286.669341ms]
Dec  3 15:56:10.887: INFO: Created: latency-svc-jm25t
Dec  3 15:56:10.913: INFO: Created: latency-svc-mc8tc
Dec  3 15:56:10.913: INFO: Got endpoints: latency-svc-jm25t [321.013595ms]
Dec  3 15:56:10.916: INFO: Got endpoints: latency-svc-mc8tc [324.817759ms]
Dec  3 15:56:10.928: INFO: Created: latency-svc-tzhms
Dec  3 15:56:10.936: INFO: Got endpoints: latency-svc-tzhms [343.968078ms]
Dec  3 15:56:10.967: INFO: Created: latency-svc-nr22d
Dec  3 15:56:10.969: INFO: Got endpoints: latency-svc-nr22d [376.90669ms]
Dec  3 15:56:11.012: INFO: Created: latency-svc-89zp6
Dec  3 15:56:11.013: INFO: Got endpoints: latency-svc-89zp6 [385.616526ms]
Dec  3 15:56:11.026: INFO: Created: latency-svc-f22kw
Dec  3 15:56:11.029: INFO: Got endpoints: latency-svc-f22kw [386.690331ms]
Dec  3 15:56:11.057: INFO: Created: latency-svc-dgfzz
Dec  3 15:56:11.059: INFO: Got endpoints: latency-svc-dgfzz [403.079191ms]
Dec  3 15:56:11.111: INFO: Created: latency-svc-wbf77
Dec  3 15:56:11.115: INFO: Got endpoints: latency-svc-wbf77 [438.259525ms]
Dec  3 15:56:11.126: INFO: Created: latency-svc-j5k2n
Dec  3 15:56:11.129: INFO: Got endpoints: latency-svc-j5k2n [414.574852ms]
Dec  3 15:56:11.141: INFO: Created: latency-svc-7nkfv
Dec  3 15:56:11.143: INFO: Got endpoints: latency-svc-7nkfv [387.146925ms]
Dec  3 15:56:11.169: INFO: Created: latency-svc-tk44s
Dec  3 15:56:11.172: INFO: Got endpoints: latency-svc-tk44s [404.52098ms]
Dec  3 15:56:11.187: INFO: Created: latency-svc-zvj8w
Dec  3 15:56:11.190: INFO: Got endpoints: latency-svc-zvj8w [408.075849ms]
Dec  3 15:56:11.211: INFO: Created: latency-svc-k8jqg
Dec  3 15:56:11.255: INFO: Got endpoints: latency-svc-k8jqg [446.464252ms]
Dec  3 15:56:11.261: INFO: Created: latency-svc-9hqxq
Dec  3 15:56:11.264: INFO: Got endpoints: latency-svc-9hqxq [406.28944ms]
Dec  3 15:56:11.276: INFO: Created: latency-svc-rl8rl
Dec  3 15:56:11.306: INFO: Got endpoints: latency-svc-rl8rl [427.038213ms]
Dec  3 15:56:11.306: INFO: Created: latency-svc-47zj4
Dec  3 15:56:11.313: INFO: Got endpoints: latency-svc-47zj4 [400.44835ms]
Dec  3 15:56:11.322: INFO: Created: latency-svc-7292w
Dec  3 15:56:11.325: INFO: Got endpoints: latency-svc-7292w [408.544229ms]
Dec  3 15:56:11.352: INFO: Created: latency-svc-dcwdk
Dec  3 15:56:11.369: INFO: Got endpoints: latency-svc-dcwdk [433.105407ms]
Dec  3 15:56:11.370: INFO: Created: latency-svc-4dtd2
Dec  3 15:56:11.373: INFO: Got endpoints: latency-svc-4dtd2 [404.538442ms]
Dec  3 15:56:11.412: INFO: Created: latency-svc-9qbrs
Dec  3 15:56:11.420: INFO: Got endpoints: latency-svc-9qbrs [406.061645ms]
Dec  3 15:56:11.429: INFO: Created: latency-svc-wsns7
Dec  3 15:56:11.432: INFO: Got endpoints: latency-svc-wsns7 [402.882594ms]
Dec  3 15:56:11.458: INFO: Created: latency-svc-slfjl
Dec  3 15:56:11.512: INFO: Got endpoints: latency-svc-slfjl [451.980667ms]
Dec  3 15:56:11.512: INFO: Created: latency-svc-db4mf
Dec  3 15:56:11.524: INFO: Created: latency-svc-wxhb4
Dec  3 15:56:11.536: INFO: Created: latency-svc-72mr7
Dec  3 15:56:11.555: INFO: Got endpoints: latency-svc-72mr7 [411.765482ms]
Dec  3 15:56:11.559: INFO: Got endpoints: latency-svc-wxhb4 [430.037324ms]
Dec  3 15:56:11.559: INFO: Got endpoints: latency-svc-db4mf [444.426183ms]
Dec  3 15:56:11.565: INFO: Created: latency-svc-sz9hg
Dec  3 15:56:11.573: INFO: Got endpoints: latency-svc-sz9hg [401.253163ms]
Dec  3 15:56:11.582: INFO: Created: latency-svc-4lhbs
Dec  3 15:56:11.584: INFO: Got endpoints: latency-svc-4lhbs [394.051083ms]
Dec  3 15:56:11.629: INFO: Created: latency-svc-g5g7v
Dec  3 15:56:11.662: INFO: Created: latency-svc-4wlv4
Dec  3 15:56:11.662: INFO: Got endpoints: latency-svc-g5g7v [407.007658ms]
Dec  3 15:56:11.671: INFO: Got endpoints: latency-svc-4wlv4 [406.434218ms]
Dec  3 15:56:11.680: INFO: Created: latency-svc-xv9qc
Dec  3 15:56:11.682: INFO: Got endpoints: latency-svc-xv9qc [376.295762ms]
Dec  3 15:56:11.706: INFO: Created: latency-svc-z8fd4
Dec  3 15:56:11.709: INFO: Got endpoints: latency-svc-z8fd4 [395.388992ms]
Dec  3 15:56:11.721: INFO: Created: latency-svc-n7wcf
Dec  3 15:56:11.723: INFO: Got endpoints: latency-svc-n7wcf [398.183078ms]
Dec  3 15:56:11.768: INFO: Created: latency-svc-rsrgv
Dec  3 15:56:11.768: INFO: Got endpoints: latency-svc-rsrgv [399.422068ms]
Dec  3 15:56:11.809: INFO: Created: latency-svc-4xvm5
Dec  3 15:56:11.820: INFO: Got endpoints: latency-svc-4xvm5 [446.492191ms]
Dec  3 15:56:11.820: INFO: Created: latency-svc-95dp2
Dec  3 15:56:11.823: INFO: Got endpoints: latency-svc-95dp2 [402.942334ms]
Dec  3 15:56:11.832: INFO: Created: latency-svc-zdb28
Dec  3 15:56:11.855: INFO: Got endpoints: latency-svc-zdb28 [423.80617ms]
Dec  3 15:56:11.864: INFO: Created: latency-svc-n57vq
Dec  3 15:56:11.909: INFO: Got endpoints: latency-svc-n57vq [396.874053ms]
Dec  3 15:56:11.910: INFO: Created: latency-svc-8lfhf
Dec  3 15:56:11.913: INFO: Got endpoints: latency-svc-8lfhf [358.421439ms]
Dec  3 15:56:11.924: INFO: Created: latency-svc-bb5c8
Dec  3 15:56:11.938: INFO: Got endpoints: latency-svc-bb5c8 [378.133319ms]
Dec  3 15:56:11.947: INFO: Created: latency-svc-9556l
Dec  3 15:56:11.958: INFO: Got endpoints: latency-svc-9556l [398.109509ms]
Dec  3 15:56:11.967: INFO: Created: latency-svc-cs7ph
Dec  3 15:56:11.970: INFO: Got endpoints: latency-svc-cs7ph [396.531623ms]
Dec  3 15:56:11.979: INFO: Created: latency-svc-67tbs
Dec  3 15:56:11.983: INFO: Got endpoints: latency-svc-67tbs [398.247905ms]
Dec  3 15:56:11.993: INFO: Created: latency-svc-kkp49
Dec  3 15:56:12.058: INFO: Got endpoints: latency-svc-kkp49 [395.608253ms]
Dec  3 15:56:12.073: INFO: Created: latency-svc-4m4gf
Dec  3 15:56:12.073: INFO: Created: latency-svc-fg4z7
Dec  3 15:56:12.085: INFO: Got endpoints: latency-svc-4m4gf [413.765329ms]
Dec  3 15:56:12.085: INFO: Got endpoints: latency-svc-fg4z7 [402.638413ms]
Dec  3 15:56:12.085: INFO: Created: latency-svc-lw4nb
Dec  3 15:56:12.088: INFO: Got endpoints: latency-svc-lw4nb [379.726323ms]
Dec  3 15:56:12.108: INFO: Created: latency-svc-tzvk7
Dec  3 15:56:12.125: INFO: Created: latency-svc-g8lkl
Dec  3 15:56:12.125: INFO: Got endpoints: latency-svc-tzvk7 [401.637591ms]
Dec  3 15:56:12.136: INFO: Got endpoints: latency-svc-g8lkl [368.174528ms]
Dec  3 15:56:12.145: INFO: Created: latency-svc-6r8cj
Dec  3 15:56:12.212: INFO: Created: latency-svc-9cwkw
Dec  3 15:56:12.212: INFO: Got endpoints: latency-svc-6r8cj [392.016341ms]
Dec  3 15:56:12.228: INFO: Got endpoints: latency-svc-9cwkw [405.103203ms]
Dec  3 15:56:12.228: INFO: Created: latency-svc-wjkw8
Dec  3 15:56:12.240: INFO: Created: latency-svc-5p9s5
Dec  3 15:56:12.241: INFO: Got endpoints: latency-svc-wjkw8 [385.064084ms]
Dec  3 15:56:12.259: INFO: Created: latency-svc-jfsbp
Dec  3 15:56:12.271: INFO: Created: latency-svc-jdkss
Dec  3 15:56:12.282: INFO: Created: latency-svc-xfbgd
Dec  3 15:56:12.308: INFO: Got endpoints: latency-svc-5p9s5 [399.348437ms]
Dec  3 15:56:12.308: INFO: Created: latency-svc-62xh2
Dec  3 15:56:12.358: INFO: Got endpoints: latency-svc-jfsbp [444.415441ms]
Dec  3 15:56:12.359: INFO: Created: latency-svc-qk5b4
Dec  3 15:56:12.374: INFO: Created: latency-svc-8v4dj
Dec  3 15:56:12.385: INFO: Created: latency-svc-rhsfx
Dec  3 15:56:12.409: INFO: Got endpoints: latency-svc-jdkss [470.879765ms]
Dec  3 15:56:12.409: INFO: Created: latency-svc-7x4pm
Dec  3 15:56:12.421: INFO: Created: latency-svc-gpmrd
Dec  3 15:56:12.433: INFO: Created: latency-svc-jpjmj
Dec  3 15:56:12.473: INFO: Created: latency-svc-bgfc7
Dec  3 15:56:12.473: INFO: Got endpoints: latency-svc-xfbgd [515.213654ms]
Dec  3 15:56:12.511: INFO: Created: latency-svc-8ww2f
Dec  3 15:56:12.511: INFO: Got endpoints: latency-svc-62xh2 [541.045991ms]
Dec  3 15:56:12.523: INFO: Created: latency-svc-mpdw5
Dec  3 15:56:12.536: INFO: Created: latency-svc-kplz2
Dec  3 15:56:12.539: INFO: Got endpoints: latency-svc-qk5b4 [555.570047ms]
Dec  3 15:56:12.562: INFO: Created: latency-svc-wfjxw
Dec  3 15:56:12.611: INFO: Created: latency-svc-4nscz
Dec  3 15:56:12.611: INFO: Got endpoints: latency-svc-8v4dj [553.058279ms]
Dec  3 15:56:12.623: INFO: Created: latency-svc-bpnkf
Dec  3 15:56:12.636: INFO: Created: latency-svc-d4hvf
Dec  3 15:56:12.641: INFO: Got endpoints: latency-svc-rhsfx [556.494256ms]
Dec  3 15:56:12.661: INFO: Created: latency-svc-xmnpw
Dec  3 15:56:12.674: INFO: Created: latency-svc-qhr4m
Dec  3 15:56:12.687: INFO: Created: latency-svc-9nzjx
Dec  3 15:56:12.688: INFO: Got endpoints: latency-svc-7x4pm [603.669294ms]
Dec  3 15:56:12.711: INFO: Created: latency-svc-gxhvg
Dec  3 15:56:12.763: INFO: Got endpoints: latency-svc-gpmrd [674.488815ms]
Dec  3 15:56:12.764: INFO: Created: latency-svc-zcvdp
Dec  3 15:56:12.797: INFO: Created: latency-svc-h7xnz
Dec  3 15:56:12.797: INFO: Got endpoints: latency-svc-jpjmj [672.148673ms]
Dec  3 15:56:12.831: INFO: Created: latency-svc-dqnv8
Dec  3 15:56:12.839: INFO: Got endpoints: latency-svc-bgfc7 [702.856754ms]
Dec  3 15:56:12.878: INFO: Created: latency-svc-tqqsn
Dec  3 15:56:12.888: INFO: Got endpoints: latency-svc-8ww2f [675.654009ms]
Dec  3 15:56:12.922: INFO: Created: latency-svc-2svjf
Dec  3 15:56:12.938: INFO: Got endpoints: latency-svc-mpdw5 [709.811582ms]
Dec  3 15:56:12.972: INFO: Created: latency-svc-h9rn4
Dec  3 15:56:13.006: INFO: Got endpoints: latency-svc-kplz2 [764.995888ms]
Dec  3 15:56:13.041: INFO: Created: latency-svc-29wfn
Dec  3 15:56:13.041: INFO: Got endpoints: latency-svc-wfjxw [732.742213ms]
Dec  3 15:56:13.076: INFO: Created: latency-svc-ldx65
Dec  3 15:56:13.087: INFO: Got endpoints: latency-svc-4nscz [729.639737ms]
Dec  3 15:56:13.132: INFO: Created: latency-svc-ljlpd
Dec  3 15:56:13.138: INFO: Got endpoints: latency-svc-bpnkf [728.987507ms]
Dec  3 15:56:13.173: INFO: Created: latency-svc-5vtwz
Dec  3 15:56:13.187: INFO: Got endpoints: latency-svc-d4hvf [714.454732ms]
Dec  3 15:56:13.236: INFO: Created: latency-svc-zwblh
Dec  3 15:56:13.237: INFO: Got endpoints: latency-svc-xmnpw [726.548109ms]
Dec  3 15:56:13.273: INFO: Created: latency-svc-gnctv
Dec  3 15:56:13.288: INFO: Got endpoints: latency-svc-qhr4m [749.607209ms]
Dec  3 15:56:13.324: INFO: Created: latency-svc-bvkl6
Dec  3 15:56:13.355: INFO: Got endpoints: latency-svc-9nzjx [743.48941ms]
Dec  3 15:56:13.390: INFO: Got endpoints: latency-svc-gxhvg [748.482746ms]
Dec  3 15:56:13.390: INFO: Created: latency-svc-x8fbl
Dec  3 15:56:13.424: INFO: Created: latency-svc-97628
Dec  3 15:56:13.438: INFO: Got endpoints: latency-svc-zcvdp [749.314141ms]
Dec  3 15:56:13.481: INFO: Created: latency-svc-85mc5
Dec  3 15:56:13.513: INFO: Got endpoints: latency-svc-h7xnz [749.705119ms]
Dec  3 15:56:13.546: INFO: Got endpoints: latency-svc-dqnv8 [749.225125ms]
Dec  3 15:56:13.546: INFO: Created: latency-svc-cjhv4
Dec  3 15:56:13.584: INFO: Created: latency-svc-whl2c
Dec  3 15:56:13.588: INFO: Got endpoints: latency-svc-tqqsn [748.158891ms]
Dec  3 15:56:13.621: INFO: Created: latency-svc-mw6cn
Dec  3 15:56:13.637: INFO: Got endpoints: latency-svc-2svjf [749.775584ms]
Dec  3 15:56:13.671: INFO: Created: latency-svc-jl75s
Dec  3 15:56:13.711: INFO: Got endpoints: latency-svc-h9rn4 [773.500858ms]
Dec  3 15:56:13.747: INFO: Created: latency-svc-x2sth
Dec  3 15:56:13.747: INFO: Got endpoints: latency-svc-29wfn [741.067066ms]
Dec  3 15:56:13.780: INFO: Created: latency-svc-pbbrf
Dec  3 15:56:13.788: INFO: Got endpoints: latency-svc-ldx65 [746.640379ms]
Dec  3 15:56:13.827: INFO: Created: latency-svc-66fmf
Dec  3 15:56:13.838: INFO: Got endpoints: latency-svc-ljlpd [750.65543ms]
Dec  3 15:56:13.873: INFO: Created: latency-svc-ncss4
Dec  3 15:56:13.888: INFO: Got endpoints: latency-svc-5vtwz [750.461457ms]
Dec  3 15:56:13.922: INFO: Created: latency-svc-gjtc6
Dec  3 15:56:13.955: INFO: Got endpoints: latency-svc-zwblh [767.887513ms]
Dec  3 15:56:13.990: INFO: Created: latency-svc-4t4xl
Dec  3 15:56:13.992: INFO: Got endpoints: latency-svc-gnctv [754.759379ms]
Dec  3 15:56:14.026: INFO: Created: latency-svc-gdtwn
Dec  3 15:56:14.038: INFO: Got endpoints: latency-svc-bvkl6 [749.993665ms]
Dec  3 15:56:14.082: INFO: Created: latency-svc-w26sr
Dec  3 15:56:14.088: INFO: Got endpoints: latency-svc-x8fbl [733.013853ms]
Dec  3 15:56:14.125: INFO: Created: latency-svc-xdk7s
Dec  3 15:56:14.138: INFO: Got endpoints: latency-svc-97628 [747.735023ms]
Dec  3 15:56:14.189: INFO: Created: latency-svc-g2785
Dec  3 15:56:14.189: INFO: Got endpoints: latency-svc-85mc5 [750.965565ms]
Dec  3 15:56:14.222: INFO: Created: latency-svc-rqpd2
Dec  3 15:56:14.238: INFO: Got endpoints: latency-svc-cjhv4 [725.450788ms]
Dec  3 15:56:14.272: INFO: Created: latency-svc-52kf4
Dec  3 15:56:14.288: INFO: Got endpoints: latency-svc-whl2c [741.425391ms]
Dec  3 15:56:14.322: INFO: Created: latency-svc-qwgmg
Dec  3 15:56:14.338: INFO: Got endpoints: latency-svc-mw6cn [750.053335ms]
Dec  3 15:56:14.372: INFO: Created: latency-svc-n8fc9
Dec  3 15:56:14.388: INFO: Got endpoints: latency-svc-jl75s [750.130162ms]
Dec  3 15:56:14.430: INFO: Created: latency-svc-btnt4
Dec  3 15:56:14.438: INFO: Got endpoints: latency-svc-x2sth [726.425507ms]
Dec  3 15:56:14.471: INFO: Created: latency-svc-zbnvf
Dec  3 15:56:14.488: INFO: Got endpoints: latency-svc-pbbrf [741.029188ms]
Dec  3 15:56:14.536: INFO: Created: latency-svc-7xn4b
Dec  3 15:56:14.539: INFO: Got endpoints: latency-svc-66fmf [751.775199ms]
Dec  3 15:56:14.576: INFO: Created: latency-svc-6tcxv
Dec  3 15:56:14.588: INFO: Got endpoints: latency-svc-ncss4 [749.437655ms]
Dec  3 15:56:14.622: INFO: Created: latency-svc-ptlwk
Dec  3 15:56:14.655: INFO: Got endpoints: latency-svc-gjtc6 [766.883786ms]
Dec  3 15:56:14.689: INFO: Got endpoints: latency-svc-4t4xl [733.031308ms]
Dec  3 15:56:14.689: INFO: Created: latency-svc-2lk87
Dec  3 15:56:14.730: INFO: Created: latency-svc-vxjvs
Dec  3 15:56:14.738: INFO: Got endpoints: latency-svc-gdtwn [745.88716ms]
Dec  3 15:56:14.787: INFO: Created: latency-svc-kwrhz
Dec  3 15:56:14.789: INFO: Got endpoints: latency-svc-w26sr [751.019743ms]
Dec  3 15:56:14.823: INFO: Created: latency-svc-h4wn9
Dec  3 15:56:14.838: INFO: Got endpoints: latency-svc-xdk7s [750.091532ms]
Dec  3 15:56:14.873: INFO: Created: latency-svc-mprvp
Dec  3 15:56:14.905: INFO: Got endpoints: latency-svc-g2785 [767.394058ms]
Dec  3 15:56:14.939: INFO: Created: latency-svc-crk6k
Dec  3 15:56:14.939: INFO: Got endpoints: latency-svc-rqpd2 [750.535703ms]
Dec  3 15:56:14.973: INFO: Created: latency-svc-b8xgv
Dec  3 15:56:14.989: INFO: Got endpoints: latency-svc-52kf4 [750.197267ms]
Dec  3 15:56:15.034: INFO: Created: latency-svc-tgl8w
Dec  3 15:56:15.038: INFO: Got endpoints: latency-svc-qwgmg [749.846792ms]
Dec  3 15:56:15.072: INFO: Created: latency-svc-gg5zn
Dec  3 15:56:15.088: INFO: Got endpoints: latency-svc-n8fc9 [749.694289ms]
Dec  3 15:56:15.121: INFO: Created: latency-svc-tp7ql
Dec  3 15:56:15.155: INFO: Got endpoints: latency-svc-btnt4 [767.354152ms]
Dec  3 15:56:15.189: INFO: Got endpoints: latency-svc-zbnvf [750.780369ms]
Dec  3 15:56:15.189: INFO: Created: latency-svc-kjhcd
Dec  3 15:56:15.223: INFO: Created: latency-svc-gdql4
Dec  3 15:56:15.238: INFO: Got endpoints: latency-svc-7xn4b [749.991479ms]
Dec  3 15:56:15.283: INFO: Created: latency-svc-48lmp
Dec  3 15:56:15.288: INFO: Got endpoints: latency-svc-6tcxv [748.000851ms]
Dec  3 15:56:15.323: INFO: Created: latency-svc-sngv5
Dec  3 15:56:15.338: INFO: Got endpoints: latency-svc-ptlwk [749.901031ms]
Dec  3 15:56:15.371: INFO: Created: latency-svc-hccnq
Dec  3 15:56:15.388: INFO: Got endpoints: latency-svc-2lk87 [732.867789ms]
Dec  3 15:56:15.426: INFO: Created: latency-svc-ltcqd
Dec  3 15:56:15.439: INFO: Got endpoints: latency-svc-vxjvs [750.335206ms]
Dec  3 15:56:15.474: INFO: Created: latency-svc-kprhf
Dec  3 15:56:15.504: INFO: Got endpoints: latency-svc-kwrhz [765.282282ms]
Dec  3 15:56:15.538: INFO: Created: latency-svc-rgxvq
Dec  3 15:56:15.540: INFO: Got endpoints: latency-svc-h4wn9 [750.243224ms]
Dec  3 15:56:15.573: INFO: Created: latency-svc-4t44b
Dec  3 15:56:15.588: INFO: Got endpoints: latency-svc-mprvp [749.593043ms]
Dec  3 15:56:15.631: INFO: Created: latency-svc-ssnhj
Dec  3 15:56:15.638: INFO: Got endpoints: latency-svc-crk6k [732.512454ms]
Dec  3 15:56:15.671: INFO: Created: latency-svc-lk5xn
Dec  3 15:56:15.688: INFO: Got endpoints: latency-svc-b8xgv [748.344416ms]
Dec  3 15:56:15.734: INFO: Created: latency-svc-xbftp
Dec  3 15:56:15.740: INFO: Got endpoints: latency-svc-tgl8w [751.709273ms]
Dec  3 15:56:15.773: INFO: Created: latency-svc-2nqz8
Dec  3 15:56:15.788: INFO: Got endpoints: latency-svc-gg5zn [750.272647ms]
Dec  3 15:56:15.823: INFO: Created: latency-svc-fzm92
Dec  3 15:56:15.864: INFO: Got endpoints: latency-svc-tp7ql [776.21001ms]
Dec  3 15:56:15.897: INFO: Created: latency-svc-v7n5s
Dec  3 15:56:15.897: INFO: Got endpoints: latency-svc-kjhcd [741.558082ms]
Dec  3 15:56:15.932: INFO: Created: latency-svc-xfkl5
Dec  3 15:56:15.938: INFO: Got endpoints: latency-svc-gdql4 [749.497545ms]
Dec  3 15:56:15.976: INFO: Created: latency-svc-bdrts
Dec  3 15:56:15.988: INFO: Got endpoints: latency-svc-48lmp [749.741155ms]
Dec  3 15:56:16.022: INFO: Created: latency-svc-r6xwd
Dec  3 15:56:16.038: INFO: Got endpoints: latency-svc-sngv5 [750.202075ms]
Dec  3 15:56:16.071: INFO: Created: latency-svc-pldm2
Dec  3 15:56:16.091: INFO: Got endpoints: latency-svc-hccnq [752.864389ms]
Dec  3 15:56:16.124: INFO: Created: latency-svc-96h8r
Dec  3 15:56:16.138: INFO: Got endpoints: latency-svc-ltcqd [749.272344ms]
Dec  3 15:56:16.176: INFO: Created: latency-svc-bc7sw
Dec  3 15:56:16.188: INFO: Got endpoints: latency-svc-kprhf [748.591764ms]
Dec  3 15:56:16.221: INFO: Created: latency-svc-qqxf4
Dec  3 15:56:16.238: INFO: Got endpoints: latency-svc-rgxvq [733.842888ms]
Dec  3 15:56:16.273: INFO: Created: latency-svc-lhvzd
Dec  3 15:56:16.288: INFO: Got endpoints: latency-svc-4t44b [748.468284ms]
Dec  3 15:56:16.337: INFO: Created: latency-svc-7bmf5
Dec  3 15:56:16.340: INFO: Got endpoints: latency-svc-ssnhj [752.180663ms]
Dec  3 15:56:16.375: INFO: Created: latency-svc-h76dl
Dec  3 15:56:16.388: INFO: Got endpoints: latency-svc-lk5xn [749.93665ms]
Dec  3 15:56:16.422: INFO: Created: latency-svc-zb75k
Dec  3 15:56:16.442: INFO: Got endpoints: latency-svc-xbftp [753.761423ms]
Dec  3 15:56:16.478: INFO: Created: latency-svc-26b6g
Dec  3 15:56:16.488: INFO: Got endpoints: latency-svc-2nqz8 [747.146921ms]
Dec  3 15:56:16.521: INFO: Created: latency-svc-7vvfd
Dec  3 15:56:16.538: INFO: Got endpoints: latency-svc-fzm92 [749.868251ms]
Dec  3 15:56:16.574: INFO: Created: latency-svc-7tkpd
Dec  3 15:56:16.588: INFO: Got endpoints: latency-svc-v7n5s [724.219375ms]
Dec  3 15:56:16.622: INFO: Created: latency-svc-kmqzf
Dec  3 15:56:16.638: INFO: Got endpoints: latency-svc-xfkl5 [740.931069ms]
Dec  3 15:56:16.676: INFO: Created: latency-svc-fgqz6
Dec  3 15:56:16.688: INFO: Got endpoints: latency-svc-bdrts [750.197906ms]
Dec  3 15:56:16.722: INFO: Created: latency-svc-fjn9k
Dec  3 15:56:16.738: INFO: Got endpoints: latency-svc-r6xwd [750.050518ms]
Dec  3 15:56:16.790: INFO: Created: latency-svc-hpn2v
Dec  3 15:56:16.790: INFO: Got endpoints: latency-svc-pldm2 [751.836636ms]
Dec  3 15:56:16.823: INFO: Created: latency-svc-rrg67
Dec  3 15:56:16.838: INFO: Got endpoints: latency-svc-96h8r [747.17007ms]
Dec  3 15:56:16.872: INFO: Created: latency-svc-9ms98
Dec  3 15:56:16.889: INFO: Got endpoints: latency-svc-bc7sw [750.896985ms]
Dec  3 15:56:16.922: INFO: Created: latency-svc-w5h4k
Dec  3 15:56:16.938: INFO: Got endpoints: latency-svc-qqxf4 [750.153251ms]
Dec  3 15:56:16.974: INFO: Created: latency-svc-24gwt
Dec  3 15:56:16.991: INFO: Got endpoints: latency-svc-lhvzd [753.263989ms]
Dec  3 15:56:17.038: INFO: Created: latency-svc-xmhn6
Dec  3 15:56:17.040: INFO: Got endpoints: latency-svc-7bmf5 [752.123263ms]
Dec  3 15:56:17.074: INFO: Created: latency-svc-x8bfb
Dec  3 15:56:17.088: INFO: Got endpoints: latency-svc-h76dl [748.077531ms]
Dec  3 15:56:17.124: INFO: Created: latency-svc-4g6hh
Dec  3 15:56:17.139: INFO: Got endpoints: latency-svc-zb75k [750.957694ms]
Dec  3 15:56:17.172: INFO: Created: latency-svc-cn59k
Dec  3 15:56:17.188: INFO: Got endpoints: latency-svc-26b6g [746.081336ms]
Dec  3 15:56:17.222: INFO: Created: latency-svc-s2bhl
Dec  3 15:56:17.238: INFO: Got endpoints: latency-svc-7vvfd [750.136339ms]
Dec  3 15:56:17.277: INFO: Created: latency-svc-wmrh8
Dec  3 15:56:17.288: INFO: Got endpoints: latency-svc-7tkpd [749.509941ms]
Dec  3 15:56:17.327: INFO: Created: latency-svc-trcws
Dec  3 15:56:17.338: INFO: Got endpoints: latency-svc-kmqzf [749.397259ms]
Dec  3 15:56:17.384: INFO: Created: latency-svc-t65cb
Dec  3 15:56:17.388: INFO: Got endpoints: latency-svc-fgqz6 [749.912278ms]
Dec  3 15:56:17.428: INFO: Created: latency-svc-ztgfn
Dec  3 15:56:17.438: INFO: Got endpoints: latency-svc-fjn9k [749.13709ms]
Dec  3 15:56:17.472: INFO: Created: latency-svc-pnrp7
Dec  3 15:56:17.498: INFO: Got endpoints: latency-svc-hpn2v [759.733834ms]
Dec  3 15:56:17.532: INFO: Created: latency-svc-lmbhv
Dec  3 15:56:17.538: INFO: Got endpoints: latency-svc-rrg67 [747.764776ms]
Dec  3 15:56:17.572: INFO: Created: latency-svc-g9mfs
Dec  3 15:56:17.588: INFO: Got endpoints: latency-svc-9ms98 [749.879954ms]
Dec  3 15:56:17.629: INFO: Created: latency-svc-mddgx
Dec  3 15:56:17.638: INFO: Got endpoints: latency-svc-w5h4k [749.307065ms]
Dec  3 15:56:17.672: INFO: Created: latency-svc-ch7vp
Dec  3 15:56:17.688: INFO: Got endpoints: latency-svc-24gwt [749.923188ms]
Dec  3 15:56:17.734: INFO: Created: latency-svc-ktmk5
Dec  3 15:56:17.738: INFO: Got endpoints: latency-svc-xmhn6 [746.985131ms]
Dec  3 15:56:17.772: INFO: Created: latency-svc-bf9hb
Dec  3 15:56:17.788: INFO: Got endpoints: latency-svc-x8bfb [747.289312ms]
Dec  3 15:56:17.822: INFO: Created: latency-svc-gbfvw
Dec  3 15:56:17.860: INFO: Got endpoints: latency-svc-4g6hh [772.148515ms]
Dec  3 15:56:17.895: INFO: Created: latency-svc-vp74c
Dec  3 15:56:17.895: INFO: Got endpoints: latency-svc-cn59k [756.206461ms]
Dec  3 15:56:17.929: INFO: Created: latency-svc-svkp2
Dec  3 15:56:17.938: INFO: Got endpoints: latency-svc-s2bhl [750.065842ms]
Dec  3 15:56:17.987: INFO: Created: latency-svc-l9pq9
Dec  3 15:56:17.990: INFO: Got endpoints: latency-svc-wmrh8 [752.452713ms]
Dec  3 15:56:18.026: INFO: Created: latency-svc-qc4qj
Dec  3 15:56:18.038: INFO: Got endpoints: latency-svc-trcws [750.297161ms]
Dec  3 15:56:18.073: INFO: Created: latency-svc-pzbwb
Dec  3 15:56:18.090: INFO: Got endpoints: latency-svc-t65cb [752.683045ms]
Dec  3 15:56:18.124: INFO: Created: latency-svc-8jhzh
Dec  3 15:56:18.138: INFO: Got endpoints: latency-svc-ztgfn [749.7547ms]
Dec  3 15:56:18.171: INFO: Created: latency-svc-fs2rn
Dec  3 15:56:18.188: INFO: Got endpoints: latency-svc-pnrp7 [750.432512ms]
Dec  3 15:56:18.222: INFO: Created: latency-svc-x6q58
Dec  3 15:56:18.238: INFO: Got endpoints: latency-svc-lmbhv [739.896457ms]
Dec  3 15:56:18.271: INFO: Created: latency-svc-gt7hn
Dec  3 15:56:18.288: INFO: Got endpoints: latency-svc-g9mfs [749.838234ms]
Dec  3 15:56:18.334: INFO: Created: latency-svc-nwpzs
Dec  3 15:56:18.338: INFO: Got endpoints: latency-svc-mddgx [749.72069ms]
Dec  3 15:56:18.372: INFO: Created: latency-svc-tbv6p
Dec  3 15:56:18.388: INFO: Got endpoints: latency-svc-ch7vp [750.119612ms]
Dec  3 15:56:18.439: INFO: Got endpoints: latency-svc-ktmk5 [750.654443ms]
Dec  3 15:56:18.439: INFO: Created: latency-svc-j5b55
Dec  3 15:56:18.488: INFO: Got endpoints: latency-svc-bf9hb [749.590452ms]
Dec  3 15:56:18.538: INFO: Got endpoints: latency-svc-gbfvw [750.616795ms]
Dec  3 15:56:18.588: INFO: Got endpoints: latency-svc-vp74c [727.465034ms]
Dec  3 15:56:18.638: INFO: Got endpoints: latency-svc-svkp2 [743.003268ms]
Dec  3 15:56:18.688: INFO: Got endpoints: latency-svc-l9pq9 [749.851272ms]
Dec  3 15:56:18.738: INFO: Got endpoints: latency-svc-qc4qj [747.815526ms]
Dec  3 15:56:18.788: INFO: Got endpoints: latency-svc-pzbwb [749.590598ms]
Dec  3 15:56:18.838: INFO: Got endpoints: latency-svc-8jhzh [747.305384ms]
Dec  3 15:56:18.894: INFO: Got endpoints: latency-svc-fs2rn [755.892535ms]
Dec  3 15:56:18.938: INFO: Got endpoints: latency-svc-x6q58 [749.985122ms]
Dec  3 15:56:18.988: INFO: Got endpoints: latency-svc-gt7hn [750.176638ms]
Dec  3 15:56:19.038: INFO: Got endpoints: latency-svc-nwpzs [750.277898ms]
Dec  3 15:56:19.088: INFO: Got endpoints: latency-svc-tbv6p [750.527614ms]
Dec  3 15:56:19.141: INFO: Got endpoints: latency-svc-j5b55 [752.933049ms]
Dec  3 15:56:19.141: INFO: Latencies: [36.250365ms 50.29488ms 64.776586ms 84.970609ms 123.014277ms 163.936397ms 175.607563ms 190.547205ms 216.936365ms 266.158807ms 286.669341ms 321.013595ms 324.817759ms 343.968078ms 358.421439ms 368.174528ms 376.295762ms 376.90669ms 378.133319ms 379.726323ms 385.064084ms 385.616526ms 386.690331ms 387.146925ms 392.016341ms 394.051083ms 395.388992ms 395.608253ms 396.531623ms 396.874053ms 398.109509ms 398.183078ms 398.247905ms 399.348437ms 399.422068ms 400.44835ms 401.253163ms 401.637591ms 402.638413ms 402.882594ms 402.942334ms 403.079191ms 404.52098ms 404.538442ms 405.103203ms 406.061645ms 406.28944ms 406.434218ms 407.007658ms 408.075849ms 408.544229ms 411.765482ms 413.765329ms 414.574852ms 423.80617ms 427.038213ms 430.037324ms 433.105407ms 438.259525ms 444.415441ms 444.426183ms 446.464252ms 446.492191ms 451.980667ms 470.879765ms 515.213654ms 541.045991ms 553.058279ms 555.570047ms 556.494256ms 603.669294ms 672.148673ms 674.488815ms 675.654009ms 702.856754ms 709.811582ms 714.454732ms 724.219375ms 725.450788ms 726.425507ms 726.548109ms 727.465034ms 728.987507ms 729.639737ms 732.512454ms 732.742213ms 732.867789ms 733.013853ms 733.031308ms 733.842888ms 739.896457ms 740.931069ms 741.029188ms 741.067066ms 741.425391ms 741.558082ms 743.003268ms 743.48941ms 745.88716ms 746.081336ms 746.640379ms 746.985131ms 747.146921ms 747.17007ms 747.289312ms 747.305384ms 747.735023ms 747.764776ms 747.815526ms 748.000851ms 748.077531ms 748.158891ms 748.344416ms 748.468284ms 748.482746ms 748.591764ms 749.13709ms 749.225125ms 749.272344ms 749.307065ms 749.314141ms 749.397259ms 749.437655ms 749.497545ms 749.509941ms 749.590452ms 749.590598ms 749.593043ms 749.607209ms 749.694289ms 749.705119ms 749.72069ms 749.741155ms 749.7547ms 749.775584ms 749.838234ms 749.846792ms 749.851272ms 749.868251ms 749.879954ms 749.901031ms 749.912278ms 749.923188ms 749.93665ms 749.985122ms 749.991479ms 749.993665ms 750.050518ms 750.053335ms 750.065842ms 750.091532ms 750.119612ms 750.130162ms 750.136339ms 750.153251ms 750.176638ms 750.197267ms 750.197906ms 750.202075ms 750.243224ms 750.272647ms 750.277898ms 750.297161ms 750.335206ms 750.432512ms 750.461457ms 750.527614ms 750.535703ms 750.616795ms 750.654443ms 750.65543ms 750.780369ms 750.896985ms 750.957694ms 750.965565ms 751.019743ms 751.709273ms 751.775199ms 751.836636ms 752.123263ms 752.180663ms 752.452713ms 752.683045ms 752.864389ms 752.933049ms 753.263989ms 753.761423ms 754.759379ms 755.892535ms 756.206461ms 759.733834ms 764.995888ms 765.282282ms 766.883786ms 767.354152ms 767.394058ms 767.887513ms 772.148515ms 773.500858ms 776.21001ms]
Dec  3 15:56:19.141: INFO: 50 %ile: 746.640379ms
Dec  3 15:56:19.141: INFO: 90 %ile: 752.180663ms
Dec  3 15:56:19.141: INFO: 99 %ile: 773.500858ms
Dec  3 15:56:19.141: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:56:19.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1799" for this suite.
Dec  3 15:56:31.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:32.073: INFO: namespace svc-latency-1799 deletion completed in 12.909582342s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:56:32.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 15:56:32.346: INFO: Waiting up to 5m0s for pod "pod-61523074-55cd-44a7-8c88-506ae82ea841" in namespace "emptydir-435" to be "success or failure"
Dec  3 15:56:32.367: INFO: Pod "pod-61523074-55cd-44a7-8c88-506ae82ea841": Phase="Pending", Reason="", readiness=false. Elapsed: 21.317154ms
Dec  3 15:56:34.389: INFO: Pod "pod-61523074-55cd-44a7-8c88-506ae82ea841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043061259s
Dec  3 15:56:36.411: INFO: Pod "pod-61523074-55cd-44a7-8c88-506ae82ea841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064953225s
STEP: Saw pod success
Dec  3 15:56:36.411: INFO: Pod "pod-61523074-55cd-44a7-8c88-506ae82ea841" satisfied condition "success or failure"
Dec  3 15:56:36.432: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-61523074-55cd-44a7-8c88-506ae82ea841 container test-container: <nil>
STEP: delete the pod
Dec  3 15:56:36.495: INFO: Waiting for pod pod-61523074-55cd-44a7-8c88-506ae82ea841 to disappear
Dec  3 15:56:36.516: INFO: Pod pod-61523074-55cd-44a7-8c88-506ae82ea841 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:56:36.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-435" for this suite.
Dec  3 15:56:42.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:43.450: INFO: namespace emptydir-435 deletion completed in 6.892708355s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:56:43.451: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec  3 15:56:43.725: INFO: Waiting up to 5m0s for pod "client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb" in namespace "containers-1437" to be "success or failure"
Dec  3 15:56:43.777: INFO: Pod "client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb": Phase="Pending", Reason="", readiness=false. Elapsed: 51.85397ms
Dec  3 15:56:45.799: INFO: Pod "client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073768557s
Dec  3 15:56:47.821: INFO: Pod "client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095632349s
STEP: Saw pod success
Dec  3 15:56:47.821: INFO: Pod "client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb" satisfied condition "success or failure"
Dec  3 15:56:47.843: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb container test-container: <nil>
STEP: delete the pod
Dec  3 15:56:47.900: INFO: Waiting for pod client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb to disappear
Dec  3 15:56:47.921: INFO: Pod client-containers-4067286a-42c9-4cd3-ab47-123b74e114fb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:56:47.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1437" for this suite.
Dec  3 15:56:54.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:54.886: INFO: namespace containers-1437 deletion completed in 6.924173339s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:56:54.887: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:56:55.158: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb" in namespace "projected-6541" to be "success or failure"
Dec  3 15:56:55.179: INFO: Pod "downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb": Phase="Pending", Reason="", readiness=false. Elapsed: 21.107754ms
Dec  3 15:56:57.202: INFO: Pod "downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044008576s
Dec  3 15:56:59.224: INFO: Pod "downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066209669s
STEP: Saw pod success
Dec  3 15:56:59.224: INFO: Pod "downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb" satisfied condition "success or failure"
Dec  3 15:56:59.245: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb container client-container: <nil>
STEP: delete the pod
Dec  3 15:56:59.311: INFO: Waiting for pod downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb to disappear
Dec  3 15:56:59.332: INFO: Pod downwardapi-volume-e96b1327-2342-4f75-b645-d0fc6fbe21cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:56:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6541" for this suite.
Dec  3 15:57:05.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:06.347: INFO: namespace projected-6541 deletion completed in 6.972872415s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:57:06.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:57:24.681: INFO: Container started at 2019-12-03 15:57:08 +0000 UTC, pod became ready at 2019-12-03 15:57:24 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:57:24.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1400" for this suite.
Dec  3 15:57:46.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:47.644: INFO: namespace container-probe-1400 deletion completed in 22.921202663s
•S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:57:47.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7771
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-a341edd1-ca27-462e-998e-032c3c3fba0d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-a341edd1-ca27-462e-998e-032c3c3fba0d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:05.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7771" for this suite.
Dec  3 15:59:27.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:28.320: INFO: namespace projected-7771 deletion completed in 23.02961023s
•SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:28.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-5519e7d8-462b-481a-85f4-034e70614e83
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:28.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4224" for this suite.
Dec  3 15:59:34.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:35.614: INFO: namespace configmap-4224 deletion completed in 6.984720741s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:35.615: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:59:35.922: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 15:59:41.969: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 15:59:46.175: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8377,SelfLink:/apis/apps/v1/namespaces/deployment-8377/deployments/test-cleanup-deployment,UID:8b4ab7c5-6404-4510-a0de-50d387bc199e,ResourceVersion:18548,Generation:1,CreationTimestamp:2019-12-03 15:59:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 15:59:42 +0000 UTC 2019-12-03 15:59:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 15:59:44 +0000 UTC 2019-12-03 15:59:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 15:59:46.200: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8377,SelfLink:/apis/apps/v1/namespaces/deployment-8377/replicasets/test-cleanup-deployment-55bbcbc84c,UID:0bcec7b0-cadf-4e4a-9c28-99c3a0b253e3,ResourceVersion:18541,Generation:1,CreationTimestamp:2019-12-03 15:59:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 8b4ab7c5-6404-4510-a0de-50d387bc199e 0xc001cada17 0xc001cada18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 15:59:46.225: INFO: Pod "test-cleanup-deployment-55bbcbc84c-sq95q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-sq95q,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8377,SelfLink:/api/v1/namespaces/deployment-8377/pods/test-cleanup-deployment-55bbcbc84c-sq95q,UID:27781875-7e43-41f8-8d4d-17a1bcb55f2b,ResourceVersion:18540,Generation:0,CreationTimestamp:2019-12-03 15:59:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.146/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 0bcec7b0-cadf-4e4a-9c28-99c3a0b253e3 0xc0028b2017 0xc0028b2018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-q98cj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-q98cj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-q98cj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028b2080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028b20a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:59:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:59:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:59:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:59:42 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.146,StartTime:2019-12-03 15:59:42 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 15:59:43 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://f0512c89079fe8a22641e80134f7761dd9f066b6942dd83241843e767dde7404}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:46.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8377" for this suite.
Dec  3 15:59:52.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:53.257: INFO: namespace deployment-8377 deletion completed in 6.983662482s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:53.257: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b8e45e33-68c2-4bb7-8dba-8c82546883ad
STEP: Creating a pod to test consume secrets
Dec  3 15:59:53.561: INFO: Waiting up to 5m0s for pod "pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d" in namespace "secrets-5956" to be "success or failure"
Dec  3 15:59:53.585: INFO: Pod "pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.875824ms
Dec  3 15:59:55.610: INFO: Pod "pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048727465s
STEP: Saw pod success
Dec  3 15:59:55.610: INFO: Pod "pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d" satisfied condition "success or failure"
Dec  3 15:59:55.640: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:59:55.704: INFO: Waiting for pod pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d to disappear
Dec  3 15:59:55.727: INFO: Pod pod-secrets-cb6bfb95-220b-402c-9e37-6b256004d79d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:55.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5956" for this suite.
Dec  3 16:00:01.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:02.778: INFO: namespace secrets-5956 deletion completed in 7.004786519s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:02.778: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 16:00:03.051: INFO: Waiting up to 5m0s for pod "pod-e433a0a5-20d2-4b22-bd3b-392474c61351" in namespace "emptydir-8650" to be "success or failure"
Dec  3 16:00:03.074: INFO: Pod "pod-e433a0a5-20d2-4b22-bd3b-392474c61351": Phase="Pending", Reason="", readiness=false. Elapsed: 23.233969ms
Dec  3 16:00:05.099: INFO: Pod "pod-e433a0a5-20d2-4b22-bd3b-392474c61351": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047685416s
Dec  3 16:00:07.124: INFO: Pod "pod-e433a0a5-20d2-4b22-bd3b-392474c61351": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07259626s
STEP: Saw pod success
Dec  3 16:00:07.124: INFO: Pod "pod-e433a0a5-20d2-4b22-bd3b-392474c61351" satisfied condition "success or failure"
Dec  3 16:00:07.148: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-e433a0a5-20d2-4b22-bd3b-392474c61351 container test-container: <nil>
STEP: delete the pod
Dec  3 16:00:07.207: INFO: Waiting for pod pod-e433a0a5-20d2-4b22-bd3b-392474c61351 to disappear
Dec  3 16:00:07.232: INFO: Pod pod-e433a0a5-20d2-4b22-bd3b-392474c61351 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:00:07.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8650" for this suite.
Dec  3 16:00:13.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:14.258: INFO: namespace emptydir-8650 deletion completed in 6.980981664s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:14.259: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3017
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-9630a333-337d-4773-8490-80f6f6cdd287
STEP: Creating configMap with name cm-test-opt-upd-a15a3759-95ae-4c40-9c59-30bc2f5a57c5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9630a333-337d-4773-8490-80f6f6cdd287
STEP: Updating configmap cm-test-opt-upd-a15a3759-95ae-4c40-9c59-30bc2f5a57c5
STEP: Creating configMap with name cm-test-opt-create-ce277c7c-5c5f-4466-91c5-16aaa31af53b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:01:32.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3017" for this suite.
Dec  3 16:01:54.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:01:55.276: INFO: namespace projected-3017 deletion completed in 23.017466281s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:01:55.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-03af9178-d411-4df0-adf6-17886a05c8d1
STEP: Creating a pod to test consume configMaps
Dec  3 16:01:55.592: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b" in namespace "configmap-2759" to be "success or failure"
Dec  3 16:01:55.615: INFO: Pod "pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.495059ms
Dec  3 16:01:57.640: INFO: Pod "pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048236604s
Dec  3 16:01:59.665: INFO: Pod "pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072709065s
Dec  3 16:02:01.691: INFO: Pod "pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098784054s
STEP: Saw pod success
Dec  3 16:02:01.691: INFO: Pod "pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b" satisfied condition "success or failure"
Dec  3 16:02:01.714: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:02:01.777: INFO: Waiting for pod pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b to disappear
Dec  3 16:02:01.800: INFO: Pod pod-configmaps-f4c5595c-59d0-4eb2-869c-41fed646bf6b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:02:01.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2759" for this suite.
Dec  3 16:02:07.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:08.834: INFO: namespace configmap-2759 deletion completed in 6.988061188s
•SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:02:08.834: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5574
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:02:09.088: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:02:41.496: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.49:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5574 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:02:41.496: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:02:42.099: INFO: Found all expected endpoints: [netserver-0]
Dec  3 16:02:42.123: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.151:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5574 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:02:42.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:02:42.609: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:02:42.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5574" for this suite.
Dec  3 16:03:04.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:05.641: INFO: namespace pod-network-test-5574 deletion completed in 22.985001873s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:03:05.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6695
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-41c3e6e3-0d10-4895-a991-e9942b851313
STEP: Creating a pod to test consume configMaps
Dec  3 16:03:05.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f" in namespace "projected-6695" to be "success or failure"
Dec  3 16:03:06.016: INFO: Pod "pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f": Phase="Pending", Reason="", readiness=false. Elapsed: 73.416841ms
Dec  3 16:03:08.041: INFO: Pod "pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097601942s
Dec  3 16:03:10.065: INFO: Pod "pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.12212371s
STEP: Saw pod success
Dec  3 16:03:10.065: INFO: Pod "pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f" satisfied condition "success or failure"
Dec  3 16:03:10.089: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:03:10.150: INFO: Waiting for pod pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f to disappear
Dec  3 16:03:10.173: INFO: Pod pod-projected-configmaps-fe1d2776-ee73-4ef2-99eb-66468a36344f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:03:10.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6695" for this suite.
Dec  3 16:03:16.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:17.208: INFO: namespace projected-6695 deletion completed in 6.988979303s
•
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:03:17.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5332.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5332.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5332.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5332.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5332.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5332.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:03:24.141: INFO: DNS probes using dns-5332/dns-test-9e142443-fd50-443c-83df-e31bde38daa0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:03:24.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5332" for this suite.
Dec  3 16:03:30.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:31.236: INFO: namespace dns-5332 deletion completed in 7.005631925s
•SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:03:31.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9335/configmap-test-aaf6659e-8d38-4b5a-925f-ebbb3efe6aef
STEP: Creating a pod to test consume configMaps
Dec  3 16:03:31.543: INFO: Waiting up to 5m0s for pod "pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea" in namespace "configmap-9335" to be "success or failure"
Dec  3 16:03:31.567: INFO: Pod "pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea": Phase="Pending", Reason="", readiness=false. Elapsed: 22.971832ms
Dec  3 16:03:33.591: INFO: Pod "pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047626991s
Dec  3 16:03:35.616: INFO: Pod "pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072664421s
Dec  3 16:03:37.641: INFO: Pod "pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.097197373s
STEP: Saw pod success
Dec  3 16:03:37.641: INFO: Pod "pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea" satisfied condition "success or failure"
Dec  3 16:03:37.664: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea container env-test: <nil>
STEP: delete the pod
Dec  3 16:03:37.729: INFO: Waiting for pod pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea to disappear
Dec  3 16:03:37.752: INFO: Pod pod-configmaps-516956f5-974f-4a2f-97e6-532cb18e75ea no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:03:37.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9335" for this suite.
Dec  3 16:03:43.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:44.794: INFO: namespace configmap-9335 deletion completed in 6.989084815s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:03:44.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5296
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  3 16:03:45.088: INFO: Waiting up to 5m0s for pod "pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4" in namespace "emptydir-5296" to be "success or failure"
Dec  3 16:03:45.111: INFO: Pod "pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4": Phase="Pending", Reason="", readiness=false. Elapsed: 23.242483ms
Dec  3 16:03:47.136: INFO: Pod "pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047679233s
Dec  3 16:03:49.161: INFO: Pod "pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072400939s
Dec  3 16:03:51.185: INFO: Pod "pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.096822891s
STEP: Saw pod success
Dec  3 16:03:51.185: INFO: Pod "pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4" satisfied condition "success or failure"
Dec  3 16:03:51.209: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4 container test-container: <nil>
STEP: delete the pod
Dec  3 16:03:51.267: INFO: Waiting for pod pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4 to disappear
Dec  3 16:03:51.290: INFO: Pod pod-10cdf1c1-4075-499e-bcf2-03452f55ecd4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:03:51.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5296" for this suite.
Dec  3 16:03:57.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:58.337: INFO: namespace emptydir-5296 deletion completed in 7.00044343s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:03:58.338: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec  3 16:03:59.219: INFO: created pod pod-service-account-defaultsa
Dec  3 16:03:59.219: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  3 16:03:59.244: INFO: created pod pod-service-account-mountsa
Dec  3 16:03:59.244: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  3 16:03:59.268: INFO: created pod pod-service-account-nomountsa
Dec  3 16:03:59.268: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  3 16:03:59.301: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  3 16:03:59.301: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  3 16:03:59.329: INFO: created pod pod-service-account-mountsa-mountspec
Dec  3 16:03:59.329: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  3 16:03:59.353: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  3 16:03:59.353: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  3 16:03:59.377: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  3 16:03:59.377: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  3 16:03:59.402: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  3 16:03:59.402: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  3 16:03:59.427: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  3 16:03:59.427: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:03:59.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3615" for this suite.
Dec  3 16:04:21.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:04:22.486: INFO: namespace svcaccounts-3615 deletion completed in 23.00949823s
•SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:04:22.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6510
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-06e7b5cd-fc43-4e95-95a3-556a8b8491e2
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-06e7b5cd-fc43-4e95-95a3-556a8b8491e2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:05:32.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6510" for this suite.
Dec  3 16:05:54.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:55.171: INFO: namespace configmap-6510 deletion completed in 23.048970184s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:05:55.172: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  3 16:06:05.601: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 16:06:05.601736    5074 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:06:05.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5874" for this suite.
Dec  3 16:06:11.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:12.626: INFO: namespace gc-5874 deletion completed in 7.000371315s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:06:12.627: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec  3 16:06:12.891: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1021'
Dec  3 16:06:13.472: INFO: stderr: ""
Dec  3 16:06:13.472: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:06:13.472: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:13.615: INFO: stderr: ""
Dec  3 16:06:13.615: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
Dec  3 16:06:13.615: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:13.762: INFO: stderr: ""
Dec  3 16:06:13.762: INFO: stdout: ""
Dec  3 16:06:13.762: INFO: update-demo-nautilus-rjphp is created but not running
Dec  3 16:06:18.763: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:18.905: INFO: stderr: ""
Dec  3 16:06:18.905: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
Dec  3 16:06:18.905: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:19.042: INFO: stderr: ""
Dec  3 16:06:19.042: INFO: stdout: "true"
Dec  3 16:06:19.042: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:19.185: INFO: stderr: ""
Dec  3 16:06:19.185: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:06:19.185: INFO: validating pod update-demo-nautilus-rjphp
Dec  3 16:06:19.296: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:06:19.296: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:06:19.296: INFO: update-demo-nautilus-rjphp is verified up and running
Dec  3 16:06:19.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-xvs45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:19.442: INFO: stderr: ""
Dec  3 16:06:19.443: INFO: stdout: ""
Dec  3 16:06:19.443: INFO: update-demo-nautilus-xvs45 is created but not running
Dec  3 16:06:24.443: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:24.608: INFO: stderr: ""
Dec  3 16:06:24.608: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
Dec  3 16:06:24.608: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:24.750: INFO: stderr: ""
Dec  3 16:06:24.750: INFO: stdout: "true"
Dec  3 16:06:24.750: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:24.888: INFO: stderr: ""
Dec  3 16:06:24.888: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:06:24.888: INFO: validating pod update-demo-nautilus-rjphp
Dec  3 16:06:24.916: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:06:24.916: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:06:24.916: INFO: update-demo-nautilus-rjphp is verified up and running
Dec  3 16:06:24.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-xvs45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:25.054: INFO: stderr: ""
Dec  3 16:06:25.054: INFO: stdout: ""
Dec  3 16:06:25.054: INFO: update-demo-nautilus-xvs45 is created but not running
Dec  3 16:06:30.054: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:30.211: INFO: stderr: ""
Dec  3 16:06:30.211: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
Dec  3 16:06:30.211: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:30.378: INFO: stderr: ""
Dec  3 16:06:30.378: INFO: stdout: "true"
Dec  3 16:06:30.378: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:30.516: INFO: stderr: ""
Dec  3 16:06:30.517: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:06:30.517: INFO: validating pod update-demo-nautilus-rjphp
Dec  3 16:06:30.543: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:06:30.543: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:06:30.543: INFO: update-demo-nautilus-rjphp is verified up and running
Dec  3 16:06:30.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-xvs45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:30.679: INFO: stderr: ""
Dec  3 16:06:30.679: INFO: stdout: "true"
Dec  3 16:06:30.679: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-xvs45 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:30.830: INFO: stderr: ""
Dec  3 16:06:30.830: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:06:30.830: INFO: validating pod update-demo-nautilus-xvs45
Dec  3 16:06:30.937: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:06:30.937: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:06:30.937: INFO: update-demo-nautilus-xvs45 is verified up and running
STEP: scaling down the replication controller
Dec  3 16:06:30.940: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:06:30.940: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1021'
Dec  3 16:06:31.169: INFO: stderr: ""
Dec  3 16:06:31.169: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:06:31.169: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:31.323: INFO: stderr: ""
Dec  3 16:06:31.323: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 16:06:36.323: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:36.462: INFO: stderr: ""
Dec  3 16:06:36.462: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 16:06:41.462: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:41.616: INFO: stderr: ""
Dec  3 16:06:41.616: INFO: stdout: "update-demo-nautilus-rjphp update-demo-nautilus-xvs45 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 16:06:46.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:46.776: INFO: stderr: ""
Dec  3 16:06:46.776: INFO: stdout: "update-demo-nautilus-rjphp "
Dec  3 16:06:46.776: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:46.944: INFO: stderr: ""
Dec  3 16:06:46.944: INFO: stdout: "true"
Dec  3 16:06:46.944: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:47.109: INFO: stderr: ""
Dec  3 16:06:47.109: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:06:47.109: INFO: validating pod update-demo-nautilus-rjphp
Dec  3 16:06:47.137: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:06:47.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:06:47.137: INFO: update-demo-nautilus-rjphp is verified up and running
STEP: scaling up the replication controller
Dec  3 16:06:47.139: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:06:47.139: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1021'
Dec  3 16:06:48.381: INFO: stderr: ""
Dec  3 16:06:48.381: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:06:48.381: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:48.544: INFO: stderr: ""
Dec  3 16:06:48.544: INFO: stdout: "update-demo-nautilus-qjtsr update-demo-nautilus-rjphp "
Dec  3 16:06:48.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qjtsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:48.695: INFO: stderr: ""
Dec  3 16:06:48.695: INFO: stdout: ""
Dec  3 16:06:48.695: INFO: update-demo-nautilus-qjtsr is created but not running
Dec  3 16:06:53.695: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:53.852: INFO: stderr: ""
Dec  3 16:06:53.852: INFO: stdout: "update-demo-nautilus-qjtsr update-demo-nautilus-rjphp "
Dec  3 16:06:53.852: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qjtsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:54.005: INFO: stderr: ""
Dec  3 16:06:54.005: INFO: stdout: ""
Dec  3 16:06:54.005: INFO: update-demo-nautilus-qjtsr is created but not running
Dec  3 16:06:59.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:06:59.157: INFO: stderr: ""
Dec  3 16:06:59.157: INFO: stdout: "update-demo-nautilus-qjtsr update-demo-nautilus-rjphp "
Dec  3 16:06:59.157: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qjtsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:06:59.309: INFO: stderr: ""
Dec  3 16:06:59.309: INFO: stdout: ""
Dec  3 16:06:59.309: INFO: update-demo-nautilus-qjtsr is created but not running
Dec  3 16:07:04.310: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1021'
Dec  3 16:07:04.457: INFO: stderr: ""
Dec  3 16:07:04.457: INFO: stdout: "update-demo-nautilus-qjtsr update-demo-nautilus-rjphp "
Dec  3 16:07:04.457: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qjtsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:07:04.612: INFO: stderr: ""
Dec  3 16:07:04.612: INFO: stdout: "true"
Dec  3 16:07:04.612: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qjtsr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:07:04.754: INFO: stderr: ""
Dec  3 16:07:04.754: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:07:04.754: INFO: validating pod update-demo-nautilus-qjtsr
Dec  3 16:07:04.863: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:07:04.863: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:07:04.863: INFO: update-demo-nautilus-qjtsr is verified up and running
Dec  3 16:07:04.863: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:07:05.043: INFO: stderr: ""
Dec  3 16:07:05.043: INFO: stdout: "true"
Dec  3 16:07:05.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-rjphp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1021'
Dec  3 16:07:05.201: INFO: stderr: ""
Dec  3 16:07:05.201: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:07:05.201: INFO: validating pod update-demo-nautilus-rjphp
Dec  3 16:07:05.228: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:07:05.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:07:05.228: INFO: update-demo-nautilus-rjphp is verified up and running
STEP: using delete to clean up resources
Dec  3 16:07:05.228: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1021'
Dec  3 16:07:05.411: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:07:05.411: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 16:07:05.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1021'
Dec  3 16:07:05.573: INFO: stderr: "No resources found.\n"
Dec  3 16:07:05.573: INFO: stdout: ""
Dec  3 16:07:05.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1021 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:07:05.719: INFO: stderr: ""
Dec  3 16:07:05.719: INFO: stdout: "update-demo-nautilus-qjtsr\nupdate-demo-nautilus-rjphp\n"
Dec  3 16:07:06.220: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1021'
Dec  3 16:07:06.377: INFO: stderr: "No resources found.\n"
Dec  3 16:07:06.377: INFO: stdout: ""
Dec  3 16:07:06.377: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1021 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:07:06.535: INFO: stderr: ""
Dec  3 16:07:06.535: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:07:06.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1021" for this suite.
Dec  3 16:07:28.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:07:29.572: INFO: namespace kubectl-1021 deletion completed in 22.990828244s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:07:29.573: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Dec  3 16:07:29.833: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6144'
Dec  3 16:07:30.747: INFO: stderr: ""
Dec  3 16:07:30.747: INFO: stdout: "pod/pause created\n"
Dec  3 16:07:30.747: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  3 16:07:30.747: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6144" to be "running and ready"
Dec  3 16:07:30.771: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 23.748743ms
Dec  3 16:07:32.795: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048413686s
Dec  3 16:07:34.821: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.073705526s
Dec  3 16:07:34.821: INFO: Pod "pause" satisfied condition "running and ready"
Dec  3 16:07:34.821: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  3 16:07:34.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-6144'
Dec  3 16:07:34.983: INFO: stderr: ""
Dec  3 16:07:34.983: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  3 16:07:34.984: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-6144'
Dec  3 16:07:35.149: INFO: stderr: ""
Dec  3 16:07:35.149: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  3 16:07:35.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-6144'
Dec  3 16:07:35.306: INFO: stderr: ""
Dec  3 16:07:35.307: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  3 16:07:35.307: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-6144'
Dec  3 16:07:35.449: INFO: stderr: ""
Dec  3 16:07:35.449: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Dec  3 16:07:35.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6144'
Dec  3 16:07:35.606: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:07:35.606: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  3 16:07:35.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-6144'
Dec  3 16:07:35.790: INFO: stderr: "No resources found.\n"
Dec  3 16:07:35.885: INFO: stdout: ""
Dec  3 16:07:35.885: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-6144 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:07:36.021: INFO: stderr: ""
Dec  3 16:07:36.021: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:07:36.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6144" for this suite.
Dec  3 16:07:42.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:07:43.068: INFO: namespace kubectl-6144 deletion completed in 7.001203022s
•SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:07:43.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-4930/secret-test-e4ef2b4a-523b-4b15-918d-ed1c3fb5dff6
STEP: Creating a pod to test consume secrets
Dec  3 16:07:43.415: INFO: Waiting up to 5m0s for pod "pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf" in namespace "secrets-4930" to be "success or failure"
Dec  3 16:07:43.439: INFO: Pod "pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 23.617935ms
Dec  3 16:07:45.463: INFO: Pod "pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048211739s
Dec  3 16:07:47.488: INFO: Pod "pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072877567s
STEP: Saw pod success
Dec  3 16:07:47.488: INFO: Pod "pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf" satisfied condition "success or failure"
Dec  3 16:07:47.512: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf container env-test: <nil>
STEP: delete the pod
Dec  3 16:07:47.581: INFO: Waiting for pod pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf to disappear
Dec  3 16:07:47.605: INFO: Pod pod-configmaps-76455a2f-837f-40d1-87e3-70b1f3cae8bf no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:07:47.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4930" for this suite.
Dec  3 16:07:53.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:07:54.663: INFO: namespace secrets-4930 deletion completed in 7.01166881s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:07:54.665: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8527.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8527.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:08:01.644: INFO: DNS probes using dns-8527/dns-test-cd980b67-d416-49bc-a308-28f9b0119f6d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:08:01.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8527" for this suite.
Dec  3 16:08:07.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:08:08.734: INFO: namespace dns-8527 deletion completed in 7.009194124s
•S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:08:08.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:08:13.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-192" for this suite.
Dec  3 16:08:57.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:08:58.211: INFO: namespace kubelet-test-192 deletion completed in 44.986620545s
•SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:08:58.211: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-e01915b3-8e42-4b22-83c0-7a4b652d80d1
STEP: Creating a pod to test consume configMaps
Dec  3 16:08:58.521: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72" in namespace "projected-7298" to be "success or failure"
Dec  3 16:08:58.544: INFO: Pod "pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72": Phase="Pending", Reason="", readiness=false. Elapsed: 23.614273ms
Dec  3 16:09:00.568: INFO: Pod "pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047392472s
Dec  3 16:09:02.593: INFO: Pod "pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071867262s
STEP: Saw pod success
Dec  3 16:09:02.593: INFO: Pod "pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72" satisfied condition "success or failure"
Dec  3 16:09:02.617: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:09:02.684: INFO: Waiting for pod pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72 to disappear
Dec  3 16:09:02.708: INFO: Pod pod-projected-configmaps-06d7c692-6784-4e81-9fcd-9da95e3afc72 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:09:02.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7298" for this suite.
Dec  3 16:09:08.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:09:09.760: INFO: namespace projected-7298 deletion completed in 7.007004633s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:09:09.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9438
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:09:10.029: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:09:36.469: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9438 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:09:36.469: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:09:37.963: INFO: Found all expected endpoints: [netserver-0]
Dec  3 16:09:37.992: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9438 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:09:37.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:09:39.490: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:09:39.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9438" for this suite.
Dec  3 16:10:01.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:02.526: INFO: namespace pod-network-test-9438 deletion completed in 22.989684111s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:10:02.526: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7962
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-bc3843eb-6bcb-412c-97ec-d01b8e5e50d1
STEP: Creating a pod to test consume configMaps
Dec  3 16:10:02.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0" in namespace "projected-7962" to be "success or failure"
Dec  3 16:10:02.859: INFO: Pod "pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.255566ms
Dec  3 16:10:04.885: INFO: Pod "pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049249764s
Dec  3 16:10:06.910: INFO: Pod "pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073724696s
STEP: Saw pod success
Dec  3 16:10:06.910: INFO: Pod "pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0" satisfied condition "success or failure"
Dec  3 16:10:06.933: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:10:06.999: INFO: Waiting for pod pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0 to disappear
Dec  3 16:10:07.022: INFO: Pod pod-projected-configmaps-622e9870-f46e-44c5-a2d3-8325f360b0b0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:10:07.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7962" for this suite.
Dec  3 16:10:13.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:14.131: INFO: namespace projected-7962 deletion completed in 7.063112479s
•SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:10:14.131: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:10:14.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938" in namespace "downward-api-1101" to be "success or failure"
Dec  3 16:10:14.438: INFO: Pod "downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938": Phase="Pending", Reason="", readiness=false. Elapsed: 23.403538ms
Dec  3 16:10:16.463: INFO: Pod "downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04915076s
Dec  3 16:10:18.488: INFO: Pod "downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073552406s
STEP: Saw pod success
Dec  3 16:10:18.488: INFO: Pod "downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938" satisfied condition "success or failure"
Dec  3 16:10:18.511: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938 container client-container: <nil>
STEP: delete the pod
Dec  3 16:10:18.581: INFO: Waiting for pod downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938 to disappear
Dec  3 16:10:18.604: INFO: Pod downwardapi-volume-97ba8aa5-4df8-48a2-ab29-71e53bc0b938 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:10:18.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1101" for this suite.
Dec  3 16:10:24.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:25.636: INFO: namespace downward-api-1101 deletion completed in 6.986856002s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:10:25.636: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8210
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 16:10:25.915: INFO: Waiting up to 5m0s for pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d" in namespace "emptydir-8210" to be "success or failure"
Dec  3 16:10:25.938: INFO: Pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.052935ms
Dec  3 16:10:27.966: INFO: Pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050611938s
Dec  3 16:10:29.990: INFO: Pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07511259s
Dec  3 16:10:32.015: INFO: Pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099539218s
Dec  3 16:10:34.039: INFO: Pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.123824247s
STEP: Saw pod success
Dec  3 16:10:34.039: INFO: Pod "pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d" satisfied condition "success or failure"
Dec  3 16:10:34.062: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d container test-container: <nil>
STEP: delete the pod
Dec  3 16:10:34.127: INFO: Waiting for pod pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d to disappear
Dec  3 16:10:34.151: INFO: Pod pod-d5e15faf-0bd7-416a-ad81-ca1aa463ed5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:10:34.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8210" for this suite.
Dec  3 16:10:40.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:10:41.185: INFO: namespace emptydir-8210 deletion completed in 6.989178476s
•SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:10:41.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 16:10:49.671: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:10:49.695: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:10:51.695: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:10:51.719: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:10:53.695: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:10:53.719: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:10:53.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4114" for this suite.
Dec  3 16:11:15.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:16.796: INFO: namespace container-lifecycle-hook-4114 deletion completed in 23.000997306s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:11:16.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7008
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-1190
STEP: Creating secret with name secret-test-3083f7d6-793d-4781-a1b3-fd6af9c3cce3
STEP: Creating a pod to test consume secrets
Dec  3 16:11:17.358: INFO: Waiting up to 5m0s for pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213" in namespace "secrets-7008" to be "success or failure"
Dec  3 16:11:17.382: INFO: Pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213": Phase="Pending", Reason="", readiness=false. Elapsed: 23.59049ms
Dec  3 16:11:19.407: INFO: Pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048314022s
Dec  3 16:11:21.431: INFO: Pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072387612s
Dec  3 16:11:23.455: INFO: Pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097036468s
Dec  3 16:11:25.480: INFO: Pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.121535963s
STEP: Saw pod success
Dec  3 16:11:25.480: INFO: Pod "pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213" satisfied condition "success or failure"
Dec  3 16:11:25.503: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:11:25.564: INFO: Waiting for pod pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213 to disappear
Dec  3 16:11:25.587: INFO: Pod pod-secrets-e92b2b08-508b-4c27-b0e3-eaba0f8a3213 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:11:25.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7008" for this suite.
Dec  3 16:11:31.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:32.622: INFO: namespace secrets-7008 deletion completed in 6.989582658s
STEP: Destroying namespace "secret-namespace-1190" for this suite.
Dec  3 16:11:38.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:39.596: INFO: namespace secret-namespace-1190 deletion completed in 6.973384494s
•SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:11:39.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 16:11:50.070: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:11:50.094: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:11:52.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:11:52.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:11:54.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:11:54.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:11:56.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:11:56.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:11:58.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:11:58.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:00.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:00.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:02.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:02.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:04.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:04.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:06.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:06.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:08.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:08.119: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:10.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:10.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:12.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:12.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:14.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:14.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:16.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:16.118: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 16:12:18.094: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 16:12:18.118: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:12:18.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9582" for this suite.
Dec  3 16:12:40.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:41.178: INFO: namespace container-lifecycle-hook-9582 deletion completed in 22.983738136s
•SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:12:41.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:12:41.529: INFO: (0) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 28.65062ms)
Dec  3 16:12:41.572: INFO: (1) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 42.500791ms)
Dec  3 16:12:41.597: INFO: (2) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.596701ms)
Dec  3 16:12:41.622: INFO: (3) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.208813ms)
Dec  3 16:12:41.647: INFO: (4) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.888509ms)
Dec  3 16:12:41.672: INFO: (5) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.938429ms)
Dec  3 16:12:41.697: INFO: (6) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.711463ms)
Dec  3 16:12:41.721: INFO: (7) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.681117ms)
Dec  3 16:12:41.748: INFO: (8) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.949888ms)
Dec  3 16:12:41.773: INFO: (9) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.807063ms)
Dec  3 16:12:41.798: INFO: (10) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.737295ms)
Dec  3 16:12:41.823: INFO: (11) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.942894ms)
Dec  3 16:12:41.848: INFO: (12) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.636249ms)
Dec  3 16:12:41.873: INFO: (13) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.673808ms)
Dec  3 16:12:41.897: INFO: (14) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.808175ms)
Dec  3 16:12:41.923: INFO: (15) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.472326ms)
Dec  3 16:12:41.953: INFO: (16) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 29.897345ms)
Dec  3 16:12:41.978: INFO: (17) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.319929ms)
Dec  3 16:12:42.004: INFO: (18) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.204551ms)
Dec  3 16:12:42.029: INFO: (19) /api/v1/nodes/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.19153ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:12:42.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9788" for this suite.
Dec  3 16:12:48.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:49.046: INFO: namespace proxy-9788 deletion completed in 6.99287637s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:12:49.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-590, will wait for the garbage collector to delete the pods
Dec  3 16:12:53.462: INFO: Deleting Job.batch foo took: 25.614659ms
Dec  3 16:12:53.918: INFO: Terminating Job.batch foo pods took: 455.804084ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:13:36.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-590" for this suite.
Dec  3 16:13:42.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:13:43.577: INFO: namespace job-590 deletion completed in 6.989680996s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:13:43.578: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-4656d924-0258-4e28-87a6-2f6717f27380
STEP: Creating a pod to test consume secrets
Dec  3 16:13:43.885: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4" in namespace "projected-5294" to be "success or failure"
Dec  3 16:13:43.909: INFO: Pod "pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4": Phase="Pending", Reason="", readiness=false. Elapsed: 23.229627ms
Dec  3 16:13:45.933: INFO: Pod "pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047404269s
Dec  3 16:13:47.958: INFO: Pod "pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072761598s
STEP: Saw pod success
Dec  3 16:13:47.958: INFO: Pod "pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4" satisfied condition "success or failure"
Dec  3 16:13:47.982: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:13:48.051: INFO: Waiting for pod pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4 to disappear
Dec  3 16:13:48.080: INFO: Pod pod-projected-secrets-f74c9bda-10d5-4f4e-8cc1-8771a4f015c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:13:48.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5294" for this suite.
Dec  3 16:13:54.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:13:55.139: INFO: namespace projected-5294 deletion completed in 7.013842634s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:13:55.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-90264df4-7adb-4e8c-9b14-7489b1a87421
STEP: Creating a pod to test consume configMaps
Dec  3 16:13:55.436: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e" in namespace "projected-5104" to be "success or failure"
Dec  3 16:13:55.460: INFO: Pod "pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.335606ms
Dec  3 16:13:57.484: INFO: Pod "pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047909677s
Dec  3 16:13:59.509: INFO: Pod "pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07247429s
STEP: Saw pod success
Dec  3 16:13:59.509: INFO: Pod "pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e" satisfied condition "success or failure"
Dec  3 16:13:59.533: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:13:59.606: INFO: Waiting for pod pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e to disappear
Dec  3 16:13:59.629: INFO: Pod pod-projected-configmaps-a836b30b-242c-4fdc-a749-08bb4fd0fb2e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:13:59.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5104" for this suite.
Dec  3 16:14:05.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:06.729: INFO: namespace projected-5104 deletion completed in 7.05386981s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:14:06.730: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:14:06.986: INFO: Creating deployment "nginx-deployment"
Dec  3 16:14:07.010: INFO: Waiting for observed generation 1
Dec  3 16:14:07.054: INFO: Waiting for all required pods to come up
Dec  3 16:14:07.086: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  3 16:14:13.147: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  3 16:14:13.183: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  3 16:14:13.223: INFO: Updating deployment nginx-deployment
Dec  3 16:14:13.223: INFO: Waiting for observed generation 2
Dec  3 16:14:15.273: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  3 16:14:15.290: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  3 16:14:15.308: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  3 16:14:15.368: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  3 16:14:15.368: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  3 16:14:15.385: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  3 16:14:15.420: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  3 16:14:15.420: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  3 16:14:15.456: INFO: Updating deployment nginx-deployment
Dec  3 16:14:15.456: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  3 16:14:15.518: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  3 16:14:15.554: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 16:14:15.594: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-4144,SelfLink:/apis/apps/v1/namespaces/deployment-4144/deployments/nginx-deployment,UID:db353300-b6c7-49ad-924a-2db5e41acc09,ResourceVersion:21546,Generation:3,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-03 16:14:15 +0000 UTC 2019-12-03 16:14:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-03 16:14:15 +0000 UTC 2019-12-03 16:14:07 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  3 16:14:15.612: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-4144,SelfLink:/apis/apps/v1/namespaces/deployment-4144/replicasets/nginx-deployment-55fb7cb77f,UID:1795fd9d-653a-4b59-946b-a5a6ab370650,ResourceVersion:21545,Generation:3,CreationTimestamp:2019-12-03 16:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment db353300-b6c7-49ad-924a-2db5e41acc09 0xc002c6dbe7 0xc002c6dbe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 16:14:15.612: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  3 16:14:15.612: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-4144,SelfLink:/apis/apps/v1/namespaces/deployment-4144/replicasets/nginx-deployment-7b8c6f4498,UID:224b41a7-e746-47cf-89e4-66e307be6794,ResourceVersion:21541,Generation:3,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment db353300-b6c7-49ad-924a-2db5e41acc09 0xc002c6dcc7 0xc002c6dcc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  3 16:14:15.649: INFO: Pod "nginx-deployment-55fb7cb77f-4xlzb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4xlzb,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-4xlzb,UID:8bdedaa2-80af-4939-b954-e71fc06e7173,ResourceVersion:21518,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc0019a16f7 0xc0019a16f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a17c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a17e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.649: INFO: Pod "nginx-deployment-55fb7cb77f-7dr9w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7dr9w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-7dr9w,UID:f72eed1b-a7fc-4cd7-b52d-df408b69edec,ResourceVersion:21542,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc0019a1870 0xc0019a1871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a1920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a19f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 16:14:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.650: INFO: Pod "nginx-deployment-55fb7cb77f-9cznp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9cznp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-9cznp,UID:2c57527a-c067-457b-a814-79b4281e214f,ResourceVersion:21530,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc0019a1b20 0xc0019a1b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a1bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a1c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.650: INFO: Pod "nginx-deployment-55fb7cb77f-crx95" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-crx95,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-crx95,UID:a4dc8dd8-1c58-48ce-9b8b-fad31c6f75b7,ResourceVersion:21491,Generation:0,CreationTimestamp:2019-12-03 16:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.193/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc0019a1ca0 0xc0019a1ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a1d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a1d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 16:14:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.650: INFO: Pod "nginx-deployment-55fb7cb77f-k67l5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-k67l5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-k67l5,UID:58f4210d-4914-42ea-bad7-1a43c69050e0,ResourceVersion:21531,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc0019a1e00 0xc0019a1e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a1e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a1e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.650: INFO: Pod "nginx-deployment-55fb7cb77f-kvrvz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kvrvz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-kvrvz,UID:3dc890b4-af4a-4d2f-952a-ffeee0b0b1f1,ResourceVersion:21492,Generation:0,CreationTimestamp:2019-12-03 16:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.59/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc0019a1f20 0xc0019a1f21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a1f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a1fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-12-03 16:14:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.650: INFO: Pod "nginx-deployment-55fb7cb77f-mvdhr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mvdhr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-mvdhr,UID:62539fd4-6c81-454d-8bd5-6bb7cce73791,ResourceVersion:21507,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d20300 0xc002d20301}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d20940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d20960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.651: INFO: Pod "nginx-deployment-55fb7cb77f-pcdh7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pcdh7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-pcdh7,UID:7b453a6e-7062-4eab-8ad9-9cef9a67c3d1,ResourceVersion:21539,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d209e0 0xc002d209e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d20a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d20a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.651: INFO: Pod "nginx-deployment-55fb7cb77f-tx7d8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tx7d8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-tx7d8,UID:eb90f855-4f04-424d-bb23-0c20bde4c977,ResourceVersion:21487,Generation:0,CreationTimestamp:2019-12-03 16:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.58/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d20b60 0xc002d20b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d20c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d20c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-12-03 16:14:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.651: INFO: Pod "nginx-deployment-55fb7cb77f-wkmbj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wkmbj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-wkmbj,UID:d98c2ef6-b35c-4a25-b270-f1a189280961,ResourceVersion:21529,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d20fc0 0xc002d20fc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d211f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.651: INFO: Pod "nginx-deployment-55fb7cb77f-wtkpz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wtkpz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-wtkpz,UID:eb40ebdd-a675-4ad7-af34-fc9a0162cec5,ResourceVersion:21537,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d21310 0xc002d21311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d21480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.652: INFO: Pod "nginx-deployment-55fb7cb77f-zvwt7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zvwt7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-zvwt7,UID:456bf0de-85e6-47d6-bc85-641662f7b9fd,ResourceVersion:21486,Generation:0,CreationTimestamp:2019-12-03 16:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.191/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d21510 0xc002d21511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d215a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d215d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 16:14:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.652: INFO: Pod "nginx-deployment-55fb7cb77f-zx4mt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zx4mt,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-55fb7cb77f-zx4mt,UID:b8633cb7-72d4-49b5-aa48-c28ac1e7b040,ResourceVersion:21490,Generation:0,CreationTimestamp:2019-12-03 16:14:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.192/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 1795fd9d-653a-4b59-946b-a5a6ab370650 0xc002d217b0 0xc002d217b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d21850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:13 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 16:14:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.652: INFO: Pod "nginx-deployment-7b8c6f4498-4g62q" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4g62q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-4g62q,UID:a3d6e86d-80be-437d-aa00-399ed7c84895,ResourceVersion:21430,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.185/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc002d21930 0xc002d21931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d219d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d219f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.185,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:10 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c6d5b4bdcf7cdc766568a14c1c5215e6d8d678c9c8b478767a81b9f91d0c4660}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.652: INFO: Pod "nginx-deployment-7b8c6f4498-7fkqt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7fkqt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-7fkqt,UID:affb36f1-8d64-4166-9898-a8ede532c6a7,ResourceVersion:21413,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.55/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc002d21ad0 0xc002d21ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21b30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d21b50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.0.55,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://cdda22f1c1aa45dd67c4c4df4c76022cf70ac500cdf3be2658239a643b047513}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.652: INFO: Pod "nginx-deployment-7b8c6f4498-86rqv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-86rqv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-86rqv,UID:bbb55b5c-6f41-4a67-90c0-46bad2989dd1,ResourceVersion:21517,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc002d21c20 0xc002d21c21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21c80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d21ca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-bcl9s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bcl9s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-bcl9s,UID:bc26fa5e-0f6c-4e74-9550-a773168a8864,ResourceVersion:21423,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.188/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc002d21d30 0xc002d21d31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d21db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.188,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:10 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://29dce632fb3c0a6cf0ef3433664a6a723fb95d77520780973f7decc52c2b4375}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-cbg8m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cbg8m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-cbg8m,UID:bad59dfa-295d-4a8d-aa71-bef70e6d7cb4,ResourceVersion:21538,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc002d21e80 0xc002d21e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d21ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d21f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-cgdqd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cgdqd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-cgdqd,UID:6b75a5ae-fb17-49f4-a025-62122f538a74,ResourceVersion:21436,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.190/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc002d21fa0 0xc002d21fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.190,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:11 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4e2b29392c139311a4f49cc0c667d01a987097424dd58469c699b435a3b66345}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-cxskq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cxskq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-cxskq,UID:673141b8-ddf4-4f24-82bc-e32c9908833d,ResourceVersion:21433,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.189/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50120 0xc003a50121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a501a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.189,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:10 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3e4114b2af8b88651b41d0f9e55461c1b43acaa0f9fd60a55af858d6dab6a4d5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-dbfp5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dbfp5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-dbfp5,UID:ae0d670e-4830-4105-8a9b-296603287aee,ResourceVersion:21547,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50270 0xc003a50271}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a502d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a502f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 16:14:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-g5ctf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-g5ctf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-g5ctf,UID:8f58b1c4-cdf1-46c8-a165-356f362b84d6,ResourceVersion:21535,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a503c0 0xc003a503c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.653: INFO: Pod "nginx-deployment-7b8c6f4498-gdb2x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gdb2x,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-gdb2x,UID:52cb71a0-b602-4143-8251-ea60e5f013e7,ResourceVersion:21416,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.54/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a504e0 0xc003a504e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.0.54,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:10 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b1f8bec90912c6fc5e8a96aafe5ba0709221213b5239c8db7b8e657da37d0418}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-hl8dv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hl8dv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-hl8dv,UID:b7c194ff-aaaf-4128-abae-d8a6e619fc87,ResourceVersion:21549,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50640 0xc003a50641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a506c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a507c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-12-03 16:14:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-jp5w9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jp5w9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-jp5w9,UID:c51b7f7e-e876-4f20-aaaf-d8e40c49dc87,ResourceVersion:21504,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50880 0xc003a50881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a508f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-mdt76" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mdt76,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-mdt76,UID:cce07d5a-47c6-43c0-837b-64456103c600,ResourceVersion:21536,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50990 0xc003a50991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a509f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-mfcwq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mfcwq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-mfcwq,UID:795f3627-05a2-49a5-9d31-88f7d6d06979,ResourceVersion:21514,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50a90 0xc003a50a91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-mp7gh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mp7gh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-mp7gh,UID:b0742ee2-b956-4c6e-a28c-38dbe3183553,ResourceVersion:21544,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50ba0 0xc003a50ba1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-12-03 16:14:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-ntrgl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ntrgl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-ntrgl,UID:6164b8d1-05a3-4851-bf29-3da2e09a44ad,ResourceVersion:21427,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.187/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50d00 0xc003a50d01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.187,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:10 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9a30d97a5838dbb01ec142c32d1c6dd902e74bb0fc89777c33cd061ce2a4db7c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.654: INFO: Pod "nginx-deployment-7b8c6f4498-pcx8m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pcx8m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-pcx8m,UID:e819d5f3-b2eb-4b1f-885c-2e42cb66630e,ResourceVersion:21534,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50e50 0xc003a50e51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.655: INFO: Pod "nginx-deployment-7b8c6f4498-qts62" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qts62,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-qts62,UID:4e46f728-200b-48d0-aef7-1f46b78802c4,ResourceVersion:21532,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a50f50 0xc003a50f51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a50fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a50fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.655: INFO: Pod "nginx-deployment-7b8c6f4498-rq9b5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rq9b5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-rq9b5,UID:3c7d3833-9a52-47f7-8249-d91b70134b0f,ResourceVersion:21548,Generation:0,CreationTimestamp:2019-12-03 16:14:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a51050 0xc003a51051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a510b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a510d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:15 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-12-03 16:14:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 16:14:15.655: INFO: Pod "nginx-deployment-7b8c6f4498-svzt5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-svzt5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4144,SelfLink:/api/v1/namespaces/deployment-4144/pods/nginx-deployment-7b8c6f4498-svzt5,UID:6c8dad63-3f95-4b6c-ad08-9a28faaee34e,ResourceVersion:21420,Generation:0,CreationTimestamp:2019-12-03 16:14:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.186/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 224b41a7-e746-47cf-89e4-66e307be6794 0xc003a511b0 0xc003a511b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwmff {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwmff,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nwmff true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003a51210} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003a51240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:14:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.186,StartTime:2019-12-03 16:14:07 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 16:14:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ef763f585ed7cd225a7428a2230f8970d0ecb45812f4bc27f4c6c1f4581a3bc2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:14:15.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4144" for this suite.
Dec  3 16:14:23.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:24.445: INFO: namespace deployment-4144 deletion completed in 8.769465781s
•
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:14:24.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-82a75f71-b4bb-4b12-af64-8762e0da86d2 in namespace container-probe-5639
Dec  3 16:14:28.738: INFO: Started pod liveness-82a75f71-b4bb-4b12-af64-8762e0da86d2 in namespace container-probe-5639
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 16:14:28.756: INFO: Initial restart count of pod liveness-82a75f71-b4bb-4b12-af64-8762e0da86d2 is 0
Dec  3 16:14:52.995: INFO: Restart count of pod container-probe-5639/liveness-82a75f71-b4bb-4b12-af64-8762e0da86d2 is now 1 (24.239092034s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:14:53.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5639" for this suite.
Dec  3 16:14:59.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:59.803: INFO: namespace container-probe-5639 deletion completed in 6.743036089s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:14:59.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  3 16:15:06.185: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 16:15:06.184950    5074 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:15:06.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-316" for this suite.
Dec  3 16:15:12.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:12.971: INFO: namespace gc-316 deletion completed in 6.768416152s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:15:12.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:15:13.240: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Dec  3 16:15:13.508: INFO: stderr: ""
Dec  3 16:15:13.508: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:20:18Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:11:50Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:15:13.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2309" for this suite.
Dec  3 16:15:19.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:20.277: INFO: namespace kubectl-2309 deletion completed in 6.749980907s
•SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:15:20.278: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 16:15:20.657: INFO: Number of nodes with available pods: 0
Dec  3 16:15:20.657: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:21.709: INFO: Number of nodes with available pods: 0
Dec  3 16:15:21.709: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:22.710: INFO: Number of nodes with available pods: 0
Dec  3 16:15:22.710: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:23.709: INFO: Number of nodes with available pods: 1
Dec  3 16:15:23.709: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:24.711: INFO: Number of nodes with available pods: 1
Dec  3 16:15:24.711: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:25.709: INFO: Number of nodes with available pods: 1
Dec  3 16:15:25.709: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:26.710: INFO: Number of nodes with available pods: 1
Dec  3 16:15:26.710: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:27.709: INFO: Number of nodes with available pods: 1
Dec  3 16:15:27.709: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:28.709: INFO: Number of nodes with available pods: 1
Dec  3 16:15:28.709: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:29.710: INFO: Number of nodes with available pods: 1
Dec  3 16:15:29.710: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:30.710: INFO: Number of nodes with available pods: 1
Dec  3 16:15:30.710: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:31.711: INFO: Number of nodes with available pods: 1
Dec  3 16:15:31.711: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:32.709: INFO: Number of nodes with available pods: 1
Dec  3 16:15:32.710: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:33.718: INFO: Number of nodes with available pods: 2
Dec  3 16:15:33.718: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  3 16:15:33.810: INFO: Number of nodes with available pods: 1
Dec  3 16:15:33.810: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:34.862: INFO: Number of nodes with available pods: 1
Dec  3 16:15:34.863: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:35.863: INFO: Number of nodes with available pods: 1
Dec  3 16:15:35.863: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:36.864: INFO: Number of nodes with available pods: 1
Dec  3 16:15:36.864: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:37.866: INFO: Number of nodes with available pods: 1
Dec  3 16:15:37.866: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:38.863: INFO: Number of nodes with available pods: 1
Dec  3 16:15:38.863: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:39.862: INFO: Number of nodes with available pods: 1
Dec  3 16:15:39.862: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:40.862: INFO: Number of nodes with available pods: 1
Dec  3 16:15:40.862: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:41.864: INFO: Number of nodes with available pods: 1
Dec  3 16:15:41.864: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:42.863: INFO: Number of nodes with available pods: 1
Dec  3 16:15:42.864: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:43.863: INFO: Number of nodes with available pods: 1
Dec  3 16:15:43.863: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:44.862: INFO: Number of nodes with available pods: 1
Dec  3 16:15:44.862: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:45.864: INFO: Number of nodes with available pods: 1
Dec  3 16:15:45.864: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:46.866: INFO: Number of nodes with available pods: 1
Dec  3 16:15:46.866: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:15:47.869: INFO: Number of nodes with available pods: 2
Dec  3 16:15:47.869: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7409, will wait for the garbage collector to delete the pods
Dec  3 16:15:47.995: INFO: Deleting DaemonSet.extensions daemon-set took: 21.639629ms
Dec  3 16:15:48.395: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.300794ms
Dec  3 16:16:04.813: INFO: Number of nodes with available pods: 0
Dec  3 16:16:04.813: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:16:04.830: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7409/daemonsets","resourceVersion":"22103"},"items":null}

Dec  3 16:16:04.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7409/pods","resourceVersion":"22103"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:16:04.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7409" for this suite.
Dec  3 16:16:10.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:11.721: INFO: namespace daemonsets-7409 deletion completed in 6.786141219s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:16:11.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-7fcfdbc9-68e9-4411-8304-298ef4fb39ba
STEP: Creating a pod to test consume secrets
Dec  3 16:16:11.993: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c" in namespace "projected-1738" to be "success or failure"
Dec  3 16:16:12.012: INFO: Pod "pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.63825ms
Dec  3 16:16:14.036: INFO: Pod "pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041962645s
Dec  3 16:16:16.055: INFO: Pod "pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061828091s
STEP: Saw pod success
Dec  3 16:16:16.055: INFO: Pod "pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c" satisfied condition "success or failure"
Dec  3 16:16:16.075: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:16:16.123: INFO: Waiting for pod pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c to disappear
Dec  3 16:16:16.141: INFO: Pod pod-projected-secrets-076d53bb-a041-4232-afbb-b8d7093de80c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:16:16.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1738" for this suite.
Dec  3 16:16:22.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:22.917: INFO: namespace projected-1738 deletion completed in 6.740223715s
•SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:16:22.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4068
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:16:23.143: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:16:49.457: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.212:8080/dial?request=hostName&protocol=http&host=100.64.0.73&port=8080&tries=1'] Namespace:pod-network-test-4068 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:16:49.457: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:16:49.944: INFO: Waiting for endpoints: map[]
Dec  3 16:16:49.962: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.212:8080/dial?request=hostName&protocol=http&host=100.64.1.211&port=8080&tries=1'] Namespace:pod-network-test-4068 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:16:49.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:16:50.436: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:16:50.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4068" for this suite.
Dec  3 16:17:12.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:17:13.218: INFO: namespace pod-network-test-4068 deletion completed in 22.745626286s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:17:13.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 16:17:16.561: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:17:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7522" for this suite.
Dec  3 16:17:22.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:17:23.387: INFO: namespace container-runtime-7522 deletion completed in 6.749954917s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:17:23.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3436
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78
Dec  3 16:17:23.653: INFO: Pod name my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78: Found 1 pods out of 1
Dec  3 16:17:23.653: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78" are running
Dec  3 16:17:31.690: INFO: Pod "my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78-hvjsg" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:17:23 +0000 UTC Reason: Message:}])
Dec  3 16:17:31.690: INFO: Trying to dial the pod
Dec  3 16:17:36.833: INFO: Controller my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78: Got expected result from replica 1 [my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78-hvjsg]: "my-hostname-basic-93ca6378-175d-4693-9e8b-0a5c2d6e8e78-hvjsg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:17:36.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3436" for this suite.
Dec  3 16:17:42.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:17:43.634: INFO: namespace replication-controller-3436 deletion completed in 6.765359371s
•SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:17:43.634: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 16:17:43.880: INFO: Waiting up to 5m0s for pod "downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625" in namespace "downward-api-4745" to be "success or failure"
Dec  3 16:17:43.898: INFO: Pod "downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625": Phase="Pending", Reason="", readiness=false. Elapsed: 17.810254ms
Dec  3 16:17:45.917: INFO: Pod "downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036751936s
Dec  3 16:17:47.935: INFO: Pod "downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055338555s
STEP: Saw pod success
Dec  3 16:17:47.935: INFO: Pod "downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625" satisfied condition "success or failure"
Dec  3 16:17:47.953: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:17:48.001: INFO: Waiting for pod downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625 to disappear
Dec  3 16:17:48.019: INFO: Pod downward-api-908032b9-26cb-4ac4-bc6f-b99f02bfc625 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:17:48.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4745" for this suite.
Dec  3 16:17:54.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:17:54.859: INFO: namespace downward-api-4745 deletion completed in 6.799360686s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:17:54.859: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4377
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec  3 16:17:55.084: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  3 16:17:55.084: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4377'
Dec  3 16:17:55.821: INFO: stderr: ""
Dec  3 16:17:55.821: INFO: stdout: "service/redis-slave created\n"
Dec  3 16:17:55.821: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  3 16:17:55.821: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4377'
Dec  3 16:17:56.272: INFO: stderr: ""
Dec  3 16:17:56.272: INFO: stdout: "service/redis-master created\n"
Dec  3 16:17:56.272: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  3 16:17:56.272: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4377'
Dec  3 16:17:56.631: INFO: stderr: ""
Dec  3 16:17:56.631: INFO: stdout: "service/frontend created\n"
Dec  3 16:17:56.631: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  3 16:17:56.631: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4377'
Dec  3 16:17:57.016: INFO: stderr: ""
Dec  3 16:17:57.016: INFO: stdout: "deployment.apps/frontend created\n"
Dec  3 16:17:57.017: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  3 16:17:57.017: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4377'
Dec  3 16:17:57.478: INFO: stderr: ""
Dec  3 16:17:57.479: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  3 16:17:57.479: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  3 16:17:57.479: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4377'
Dec  3 16:17:57.887: INFO: stderr: ""
Dec  3 16:17:57.887: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  3 16:17:57.887: INFO: Waiting for all frontend pods to be Running.
Dec  3 16:19:07.944: INFO: Waiting for frontend to serve content.
Dec  3 16:19:08.051: INFO: Trying to add a new entry to the guestbook.
Dec  3 16:19:08.178: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  3 16:19:08.307: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4377'
Dec  3 16:19:08.543: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:19:08.543: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:19:08.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4377'
Dec  3 16:19:08.747: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:19:08.747: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:19:08.747: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4377'
Dec  3 16:19:08.960: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:19:08.960: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:19:08.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4377'
Dec  3 16:19:09.175: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:19:09.175: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:19:09.175: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4377'
Dec  3 16:19:09.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:19:09.410: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:19:09.410: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4377'
Dec  3 16:19:09.619: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:19:09.619: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:19:09.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4377" for this suite.
Dec  3 16:19:47.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:19:48.593: INFO: namespace kubectl-4377 deletion completed in 38.931888329s
•
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:19:48.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 16:19:48.854: INFO: Waiting up to 5m0s for pod "downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c" in namespace "downward-api-4212" to be "success or failure"
Dec  3 16:19:48.881: INFO: Pod "downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.501315ms
Dec  3 16:19:50.906: INFO: Pod "downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052348125s
Dec  3 16:19:52.928: INFO: Pod "downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074328774s
STEP: Saw pod success
Dec  3 16:19:52.928: INFO: Pod "downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c" satisfied condition "success or failure"
Dec  3 16:19:52.950: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:19:53.015: INFO: Waiting for pod downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c to disappear
Dec  3 16:19:53.036: INFO: Pod downward-api-7f3508b1-232c-40d8-b9a6-e1934c67a92c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:19:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4212" for this suite.
Dec  3 16:19:59.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:19:59.976: INFO: namespace downward-api-4212 deletion completed in 6.898943598s
•SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:19:59.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f9bf031c-ecbb-4a98-8f65-77afa8050c8e
STEP: Creating a pod to test consume secrets
Dec  3 16:20:00.278: INFO: Waiting up to 5m0s for pod "pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc" in namespace "secrets-8430" to be "success or failure"
Dec  3 16:20:00.298: INFO: Pod "pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.868597ms
Dec  3 16:20:02.320: INFO: Pod "pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042852966s
Dec  3 16:20:04.343: INFO: Pod "pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065428636s
STEP: Saw pod success
Dec  3 16:20:04.343: INFO: Pod "pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc" satisfied condition "success or failure"
Dec  3 16:20:04.368: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc container secret-env-test: <nil>
STEP: delete the pod
Dec  3 16:20:04.429: INFO: Waiting for pod pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc to disappear
Dec  3 16:20:04.450: INFO: Pod pod-secrets-2dc2bcec-387d-48a3-8b6b-9edf9ae2a2bc no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:20:04.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8430" for this suite.
Dec  3 16:20:10.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:11.389: INFO: namespace secrets-8430 deletion completed in 6.897257478s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:20:11.389: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5810
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-60b783f9-c31b-4ea3-9e71-2c591945c750
STEP: Creating a pod to test consume configMaps
Dec  3 16:20:11.678: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc" in namespace "projected-5810" to be "success or failure"
Dec  3 16:20:11.699: INFO: Pod "pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.047908ms
Dec  3 16:20:13.721: INFO: Pod "pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043157054s
Dec  3 16:20:15.743: INFO: Pod "pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065197751s
STEP: Saw pod success
Dec  3 16:20:15.743: INFO: Pod "pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc" satisfied condition "success or failure"
Dec  3 16:20:15.764: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:20:15.827: INFO: Waiting for pod pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc to disappear
Dec  3 16:20:15.848: INFO: Pod pod-projected-configmaps-8a9e693b-8261-496f-8b87-8c647274cffc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:20:15.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5810" for this suite.
Dec  3 16:20:21.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:22.797: INFO: namespace projected-5810 deletion completed in 6.907489816s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:20:22.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 16:20:23.076: INFO: Waiting up to 5m0s for pod "downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce" in namespace "downward-api-9760" to be "success or failure"
Dec  3 16:20:23.097: INFO: Pod "downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 21.279312ms
Dec  3 16:20:25.120: INFO: Pod "downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04348328s
Dec  3 16:20:27.142: INFO: Pod "downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065637282s
STEP: Saw pod success
Dec  3 16:20:27.142: INFO: Pod "downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce" satisfied condition "success or failure"
Dec  3 16:20:27.163: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:20:27.220: INFO: Waiting for pod downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce to disappear
Dec  3 16:20:27.245: INFO: Pod downward-api-619e1dbb-5cd0-4d88-924e-0f527011d7ce no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:20:27.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9760" for this suite.
Dec  3 16:20:33.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:34.211: INFO: namespace downward-api-9760 deletion completed in 6.923597916s
•S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:20:34.211: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:20:34.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910" in namespace "downward-api-1968" to be "success or failure"
Dec  3 16:20:34.504: INFO: Pod "downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910": Phase="Pending", Reason="", readiness=false. Elapsed: 21.519964ms
Dec  3 16:20:36.526: INFO: Pod "downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043875216s
Dec  3 16:20:38.548: INFO: Pod "downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065825461s
STEP: Saw pod success
Dec  3 16:20:38.548: INFO: Pod "downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910" satisfied condition "success or failure"
Dec  3 16:20:38.569: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910 container client-container: <nil>
STEP: delete the pod
Dec  3 16:20:38.626: INFO: Waiting for pod downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910 to disappear
Dec  3 16:20:38.647: INFO: Pod downwardapi-volume-2514f8cb-8f98-4761-b8e9-cb6d26c61910 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:20:38.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1968" for this suite.
Dec  3 16:20:44.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:45.602: INFO: namespace downward-api-1968 deletion completed in 6.913741625s
•SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:20:45.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:20:45.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6321" for this suite.
Dec  3 16:21:07.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:08.803: INFO: namespace pods-6321 deletion completed in 22.888289678s
•SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:08.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec  3 16:21:09.073: INFO: Waiting up to 5m0s for pod "var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268" in namespace "var-expansion-6856" to be "success or failure"
Dec  3 16:21:09.100: INFO: Pod "var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268": Phase="Pending", Reason="", readiness=false. Elapsed: 26.605249ms
Dec  3 16:21:11.122: INFO: Pod "var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049196124s
Dec  3 16:21:13.144: INFO: Pod "var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071146708s
STEP: Saw pod success
Dec  3 16:21:13.144: INFO: Pod "var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268" satisfied condition "success or failure"
Dec  3 16:21:13.165: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:21:13.220: INFO: Waiting for pod var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268 to disappear
Dec  3 16:21:13.241: INFO: Pod var-expansion-8e3492bd-8907-4ece-a8dd-656555bc6268 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:13.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6856" for this suite.
Dec  3 16:21:19.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:20.189: INFO: namespace var-expansion-6856 deletion completed in 6.906990127s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:20.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 16:21:27.118: INFO: Successfully updated pod "labelsupdate6b8fda6d-b1f6-4f52-b26c-9991f505c615"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:29.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4408" for this suite.
Dec  3 16:21:51.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:52.123: INFO: namespace downward-api-4408 deletion completed in 22.904258967s
•SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:52.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2271/configmap-test-0eb689d1-fa84-4866-a690-a8e5dec4d414
STEP: Creating a pod to test consume configMaps
Dec  3 16:21:52.412: INFO: Waiting up to 5m0s for pod "pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b" in namespace "configmap-2271" to be "success or failure"
Dec  3 16:21:52.434: INFO: Pod "pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.571496ms
Dec  3 16:21:54.456: INFO: Pod "pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043980249s
Dec  3 16:21:56.479: INFO: Pod "pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066280805s
STEP: Saw pod success
Dec  3 16:21:56.479: INFO: Pod "pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b" satisfied condition "success or failure"
Dec  3 16:21:56.500: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b container env-test: <nil>
STEP: delete the pod
Dec  3 16:21:56.564: INFO: Waiting for pod pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b to disappear
Dec  3 16:21:56.585: INFO: Pod pod-configmaps-7caab408-60c6-492f-9811-a7691e9f995b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:56.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2271" for this suite.
Dec  3 16:22:02.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:22:03.527: INFO: namespace configmap-2271 deletion completed in 6.900731008s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:22:03.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec  3 16:22:03.773: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-598'
Dec  3 16:22:04.295: INFO: stderr: ""
Dec  3 16:22:04.295: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:22:04.295: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:22:04.486: INFO: stderr: ""
Dec  3 16:22:04.486: INFO: stdout: "update-demo-nautilus-4mxnt update-demo-nautilus-bnmjx "
Dec  3 16:22:04.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4mxnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:04.652: INFO: stderr: ""
Dec  3 16:22:04.652: INFO: stdout: ""
Dec  3 16:22:04.652: INFO: update-demo-nautilus-4mxnt is created but not running
Dec  3 16:22:09.652: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:22:09.828: INFO: stderr: ""
Dec  3 16:22:09.828: INFO: stdout: "update-demo-nautilus-4mxnt update-demo-nautilus-bnmjx "
Dec  3 16:22:09.828: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4mxnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:09.963: INFO: stderr: ""
Dec  3 16:22:09.963: INFO: stdout: ""
Dec  3 16:22:09.963: INFO: update-demo-nautilus-4mxnt is created but not running
Dec  3 16:22:14.963: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:22:15.131: INFO: stderr: ""
Dec  3 16:22:15.131: INFO: stdout: "update-demo-nautilus-4mxnt update-demo-nautilus-bnmjx "
Dec  3 16:22:15.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4mxnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:15.275: INFO: stderr: ""
Dec  3 16:22:15.275: INFO: stdout: ""
Dec  3 16:22:15.275: INFO: update-demo-nautilus-4mxnt is created but not running
Dec  3 16:22:20.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:22:20.425: INFO: stderr: ""
Dec  3 16:22:20.425: INFO: stdout: "update-demo-nautilus-4mxnt update-demo-nautilus-bnmjx "
Dec  3 16:22:20.425: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4mxnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:20.562: INFO: stderr: ""
Dec  3 16:22:20.562: INFO: stdout: ""
Dec  3 16:22:20.562: INFO: update-demo-nautilus-4mxnt is created but not running
Dec  3 16:22:25.563: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:22:25.731: INFO: stderr: ""
Dec  3 16:22:25.731: INFO: stdout: "update-demo-nautilus-4mxnt update-demo-nautilus-bnmjx "
Dec  3 16:22:25.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4mxnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:25.886: INFO: stderr: ""
Dec  3 16:22:25.886: INFO: stdout: "true"
Dec  3 16:22:25.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4mxnt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:26.062: INFO: stderr: ""
Dec  3 16:22:26.062: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:22:26.062: INFO: validating pod update-demo-nautilus-4mxnt
Dec  3 16:22:26.168: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:22:26.168: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:22:26.169: INFO: update-demo-nautilus-4mxnt is verified up and running
Dec  3 16:22:26.169: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-bnmjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:26.330: INFO: stderr: ""
Dec  3 16:22:26.330: INFO: stdout: "true"
Dec  3 16:22:26.330: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-bnmjx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:22:26.483: INFO: stderr: ""
Dec  3 16:22:26.483: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:22:26.483: INFO: validating pod update-demo-nautilus-bnmjx
Dec  3 16:22:26.590: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:22:26.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:22:26.590: INFO: update-demo-nautilus-bnmjx is verified up and running
STEP: rolling-update to new replication controller
Dec  3 16:22:26.592: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:22:26.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-598'
Dec  3 16:23:12.951: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 16:23:12.951: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:23:12.951: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:23:13.129: INFO: stderr: ""
Dec  3 16:23:13.129: INFO: stdout: "update-demo-kitten-lzr94 update-demo-kitten-zrs2s update-demo-nautilus-bnmjx "
STEP: Replicas for name=update-demo: expected=2 actual=3
Dec  3 16:23:18.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-598'
Dec  3 16:23:18.277: INFO: stderr: ""
Dec  3 16:23:18.277: INFO: stdout: "update-demo-kitten-lzr94 update-demo-kitten-zrs2s "
Dec  3 16:23:18.278: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-lzr94 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:23:18.436: INFO: stderr: ""
Dec  3 16:23:18.436: INFO: stdout: "true"
Dec  3 16:23:18.436: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-lzr94 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:23:18.582: INFO: stderr: ""
Dec  3 16:23:18.582: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 16:23:18.582: INFO: validating pod update-demo-kitten-lzr94
Dec  3 16:23:18.701: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 16:23:18.701: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 16:23:18.701: INFO: update-demo-kitten-lzr94 is verified up and running
Dec  3 16:23:18.701: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-zrs2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:23:18.852: INFO: stderr: ""
Dec  3 16:23:18.852: INFO: stdout: "true"
Dec  3 16:23:18.852: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-zrs2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-598'
Dec  3 16:23:19.039: INFO: stderr: ""
Dec  3 16:23:19.039: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 16:23:19.039: INFO: validating pod update-demo-kitten-zrs2s
Dec  3 16:23:19.145: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 16:23:19.145: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 16:23:19.145: INFO: update-demo-kitten-zrs2s is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:23:19.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-598" for this suite.
Dec  3 16:23:57.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:58.074: INFO: namespace kubectl-598 deletion completed in 38.887737542s
•SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:23:58.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:23:58.348: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee" in namespace "projected-7876" to be "success or failure"
Dec  3 16:23:58.375: INFO: Pod "downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee": Phase="Pending", Reason="", readiness=false. Elapsed: 26.392741ms
Dec  3 16:24:00.397: INFO: Pod "downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048639187s
Dec  3 16:24:02.419: INFO: Pod "downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070686048s
STEP: Saw pod success
Dec  3 16:24:02.419: INFO: Pod "downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee" satisfied condition "success or failure"
Dec  3 16:24:02.441: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee container client-container: <nil>
STEP: delete the pod
Dec  3 16:24:02.536: INFO: Waiting for pod downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee to disappear
Dec  3 16:24:02.557: INFO: Pod downwardapi-volume-66cd5474-c17f-4570-87bd-b1866810a8ee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:24:02.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7876" for this suite.
Dec  3 16:24:08.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:24:09.498: INFO: namespace projected-7876 deletion completed in 6.900214354s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:24:09.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7248
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:24:09.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631" in namespace "downward-api-7248" to be "success or failure"
Dec  3 16:24:09.796: INFO: Pod "downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631": Phase="Pending", Reason="", readiness=false. Elapsed: 21.225164ms
Dec  3 16:24:11.818: INFO: Pod "downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043559113s
Dec  3 16:24:13.841: INFO: Pod "downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066108267s
Dec  3 16:24:15.866: INFO: Pod "downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091173477s
STEP: Saw pod success
Dec  3 16:24:15.866: INFO: Pod "downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631" satisfied condition "success or failure"
Dec  3 16:24:15.888: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631 container client-container: <nil>
STEP: delete the pod
Dec  3 16:24:15.944: INFO: Waiting for pod downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631 to disappear
Dec  3 16:24:15.973: INFO: Pod downwardapi-volume-2c2af37e-e90b-47e0-8857-cee140537631 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:24:15.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7248" for this suite.
Dec  3 16:24:22.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:24:22.908: INFO: namespace downward-api-7248 deletion completed in 6.8929893s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:24:22.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-704
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:24:23.283: INFO: Create a RollingUpdate DaemonSet
Dec  3 16:24:23.305: INFO: Check that daemon pods launch on every node of the cluster
Dec  3 16:24:23.355: INFO: Number of nodes with available pods: 0
Dec  3 16:24:23.355: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:24:24.419: INFO: Number of nodes with available pods: 0
Dec  3 16:24:24.419: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:24:25.419: INFO: Number of nodes with available pods: 0
Dec  3 16:24:25.419: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:24:26.420: INFO: Number of nodes with available pods: 1
Dec  3 16:24:26.420: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:24:27.420: INFO: Number of nodes with available pods: 1
Dec  3 16:24:27.420: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:24:28.419: INFO: Number of nodes with available pods: 1
Dec  3 16:24:28.419: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:24:29.420: INFO: Number of nodes with available pods: 2
Dec  3 16:24:29.420: INFO: Number of running nodes: 2, number of available pods: 2
Dec  3 16:24:29.420: INFO: Update the DaemonSet to trigger a rollout
Dec  3 16:24:29.463: INFO: Updating DaemonSet daemon-set
Dec  3 16:24:36.572: INFO: Roll back the DaemonSet before rollout is complete
Dec  3 16:24:36.615: INFO: Updating DaemonSet daemon-set
Dec  3 16:24:36.615: INFO: Make sure DaemonSet rollback is complete
Dec  3 16:24:36.637: INFO: Wrong image for pod: daemon-set-4mnjx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 16:24:36.637: INFO: Pod daemon-set-4mnjx is not available
Dec  3 16:24:37.680: INFO: Wrong image for pod: daemon-set-4mnjx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 16:24:37.680: INFO: Pod daemon-set-4mnjx is not available
Dec  3 16:24:38.681: INFO: Wrong image for pod: daemon-set-4mnjx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 16:24:38.681: INFO: Pod daemon-set-4mnjx is not available
Dec  3 16:24:39.680: INFO: Wrong image for pod: daemon-set-4mnjx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 16:24:39.681: INFO: Pod daemon-set-4mnjx is not available
Dec  3 16:24:40.680: INFO: Wrong image for pod: daemon-set-4mnjx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 16:24:40.680: INFO: Pod daemon-set-4mnjx is not available
Dec  3 16:24:41.680: INFO: Wrong image for pod: daemon-set-4mnjx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 16:24:41.680: INFO: Pod daemon-set-4mnjx is not available
Dec  3 16:24:42.681: INFO: Pod daemon-set-j6jl9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-704, will wait for the garbage collector to delete the pods
Dec  3 16:24:42.861: INFO: Deleting DaemonSet.extensions daemon-set took: 23.623202ms
Dec  3 16:24:42.961: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.320799ms
Dec  3 16:24:54.783: INFO: Number of nodes with available pods: 0
Dec  3 16:24:54.783: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:24:54.804: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-704/daemonsets","resourceVersion":"23951"},"items":null}

Dec  3 16:24:54.825: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-704/pods","resourceVersion":"23951"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:24:54.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-704" for this suite.
Dec  3 16:25:00.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:01.866: INFO: namespace daemonsets-704 deletion completed in 6.932878756s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:25:01.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec  3 16:25:02.113: INFO: namespace kubectl-6173
Dec  3 16:25:02.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6173'
Dec  3 16:25:02.508: INFO: stderr: ""
Dec  3 16:25:02.508: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:25:03.531: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:03.531: INFO: Found 0 / 1
Dec  3 16:25:04.531: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:04.531: INFO: Found 0 / 1
Dec  3 16:25:05.531: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:05.531: INFO: Found 0 / 1
Dec  3 16:25:06.531: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:06.531: INFO: Found 0 / 1
Dec  3 16:25:07.530: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:07.531: INFO: Found 1 / 1
Dec  3 16:25:07.531: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 16:25:07.552: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:07.552: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:25:07.552: INFO: wait on redis-master startup in kubectl-6173 
Dec  3 16:25:07.552: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-fjd42 redis-master --namespace=kubectl-6173'
Dec  3 16:25:07.782: INFO: stderr: ""
Dec  3 16:25:07.782: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 16:25:06.096 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 16:25:06.096 # Server started, Redis version 3.2.12\n1:M 03 Dec 16:25:06.096 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 16:25:06.096 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  3 16:25:07.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6173'
Dec  3 16:25:08.073: INFO: stderr: ""
Dec  3 16:25:08.073: INFO: stdout: "service/rm2 exposed\n"
Dec  3 16:25:08.094: INFO: Service rm2 in namespace kubectl-6173 found.
STEP: exposing service
Dec  3 16:25:10.137: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6173'
Dec  3 16:25:10.392: INFO: stderr: ""
Dec  3 16:25:10.392: INFO: stdout: "service/rm3 exposed\n"
Dec  3 16:25:10.414: INFO: Service rm3 in namespace kubectl-6173 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:25:12.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6173" for this suite.
Dec  3 16:25:34.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:35.409: INFO: namespace kubectl-6173 deletion completed in 22.910860016s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:25:35.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:25:39.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-182" for this suite.
Dec  3 16:25:45.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:46.746: INFO: namespace kubelet-test-182 deletion completed in 6.913835055s
•SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:25:46.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  3 16:25:47.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24146,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:25:47.082: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24146,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  3 16:25:57.126: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24170,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 16:25:57.126: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24170,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  3 16:26:07.170: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24194,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:26:07.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24194,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  3 16:26:17.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24220,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:26:17.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-a,UID:e9784760-2976-4adb-811c-c86f14df4c94,ResourceVersion:24220,Generation:0,CreationTimestamp:2019-12-03 16:25:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  3 16:26:27.222: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-b,UID:dad82f0c-1a59-4b97-bc18-8017eb0b699c,ResourceVersion:24244,Generation:0,CreationTimestamp:2019-12-03 16:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:26:27.222: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-b,UID:dad82f0c-1a59-4b97-bc18-8017eb0b699c,ResourceVersion:24244,Generation:0,CreationTimestamp:2019-12-03 16:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  3 16:26:37.246: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-b,UID:dad82f0c-1a59-4b97-bc18-8017eb0b699c,ResourceVersion:24269,Generation:0,CreationTimestamp:2019-12-03 16:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:26:37.246: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-115,SelfLink:/api/v1/namespaces/watch-115/configmaps/e2e-watch-test-configmap-b,UID:dad82f0c-1a59-4b97-bc18-8017eb0b699c,ResourceVersion:24269,Generation:0,CreationTimestamp:2019-12-03 16:26:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:47.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-115" for this suite.
Dec  3 16:26:53.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:26:54.185: INFO: namespace watch-115 deletion completed in 6.89583285s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:26:54.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:26:54.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95" in namespace "downward-api-1821" to be "success or failure"
Dec  3 16:26:54.482: INFO: Pod "downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95": Phase="Pending", Reason="", readiness=false. Elapsed: 22.259276ms
Dec  3 16:26:56.504: INFO: Pod "downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044344785s
Dec  3 16:26:58.526: INFO: Pod "downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066772014s
STEP: Saw pod success
Dec  3 16:26:58.526: INFO: Pod "downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95" satisfied condition "success or failure"
Dec  3 16:26:58.548: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95 container client-container: <nil>
STEP: delete the pod
Dec  3 16:26:58.609: INFO: Waiting for pod downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95 to disappear
Dec  3 16:26:58.630: INFO: Pod downwardapi-volume-0e64e315-cd61-4480-966b-6de758642c95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:58.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1821" for this suite.
Dec  3 16:27:04.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:27:05.595: INFO: namespace downward-api-1821 deletion completed in 6.923181765s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:27:05.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  3 16:27:05.883: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:27:07.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3083" for this suite.
Dec  3 16:27:13.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:27:13.944: INFO: namespace replication-controller-3083 deletion completed in 6.900380519s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:27:13.944: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6256
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  3 16:27:14.210: INFO: Waiting up to 5m0s for pod "pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6" in namespace "emptydir-6256" to be "success or failure"
Dec  3 16:27:14.232: INFO: Pod "pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.867644ms
Dec  3 16:27:16.254: INFO: Pod "pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043983116s
Dec  3 16:27:18.276: INFO: Pod "pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065924752s
STEP: Saw pod success
Dec  3 16:27:18.276: INFO: Pod "pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6" satisfied condition "success or failure"
Dec  3 16:27:18.297: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6 container test-container: <nil>
STEP: delete the pod
Dec  3 16:27:18.359: INFO: Waiting for pod pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6 to disappear
Dec  3 16:27:18.380: INFO: Pod pod-d3ea7361-abdb-4dfa-a7dd-ffeca70ca5e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:27:18.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6256" for this suite.
Dec  3 16:27:24.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:27:25.327: INFO: namespace emptydir-6256 deletion completed in 6.905415843s
•SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:27:25.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-b0c8943c-22fd-4339-9764-3b6797ae6fb4
STEP: Creating secret with name secret-projected-all-test-volume-bf4a16c4-3d1e-4793-8b4a-145055061143
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  3 16:27:25.650: INFO: Waiting up to 5m0s for pod "projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc" in namespace "projected-7670" to be "success or failure"
Dec  3 16:27:25.672: INFO: Pod "projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.105266ms
Dec  3 16:27:27.693: INFO: Pod "projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042949115s
Dec  3 16:27:29.717: INFO: Pod "projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06649489s
STEP: Saw pod success
Dec  3 16:27:29.717: INFO: Pod "projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc" satisfied condition "success or failure"
Dec  3 16:27:29.739: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  3 16:27:29.794: INFO: Waiting for pod projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc to disappear
Dec  3 16:27:29.815: INFO: Pod projected-volume-d353a64f-2158-44ca-beba-fab5a82902dc no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:27:29.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7670" for this suite.
Dec  3 16:27:35.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:27:36.765: INFO: namespace projected-7670 deletion completed in 6.901540531s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:27:36.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  3 16:27:45.186: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:45.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:45.763: INFO: Exec stderr: ""
Dec  3 16:27:45.763: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:45.763: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:46.265: INFO: Exec stderr: ""
Dec  3 16:27:46.265: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:46.265: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:46.744: INFO: Exec stderr: ""
Dec  3 16:27:46.744: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:46.744: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:47.226: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  3 16:27:47.226: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:47.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:47.715: INFO: Exec stderr: ""
Dec  3 16:27:47.715: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:47.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:48.190: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  3 16:27:48.190: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:48.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:48.705: INFO: Exec stderr: ""
Dec  3 16:27:48.706: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:48.706: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:49.215: INFO: Exec stderr: ""
Dec  3 16:27:49.215: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:49.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:49.726: INFO: Exec stderr: ""
Dec  3 16:27:49.726: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4505 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:49.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:50.200: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:27:50.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4505" for this suite.
Dec  3 16:28:40.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:28:41.140: INFO: namespace e2e-kubelet-etc-hosts-4505 deletion completed in 50.897676147s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:28:41.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:29:41.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6387" for this suite.
Dec  3 16:30:03.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:30:04.209: INFO: namespace container-probe-6387 deletion completed in 22.743348998s
•SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:30:04.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 16:30:07.544: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:30:07.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4879" for this suite.
Dec  3 16:30:13.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:30:14.373: INFO: namespace container-runtime-4879 deletion completed in 6.754712909s
•SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:30:14.374: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4421
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec  3 16:30:14.665: INFO: Found 1 stateful pods, waiting for 3
Dec  3 16:30:24.684: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:30:24.684: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:30:24.684: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:30:24.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4421 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:30:25.744: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:30:25.744: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:30:25.744: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  3 16:30:35.862: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  3 16:30:35.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4421 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:30:36.611: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:30:36.611: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:30:36.611: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:30:36.684: INFO: Waiting for StatefulSet statefulset-4421/ss2 to complete update
Dec  3 16:30:36.684: INFO: Waiting for Pod statefulset-4421/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 16:30:36.684: INFO: Waiting for Pod statefulset-4421/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 16:30:36.684: INFO: Waiting for Pod statefulset-4421/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 16:30:46.721: INFO: Waiting for StatefulSet statefulset-4421/ss2 to complete update
Dec  3 16:30:46.721: INFO: Waiting for Pod statefulset-4421/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 16:30:46.721: INFO: Waiting for Pod statefulset-4421/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 16:30:56.720: INFO: Waiting for StatefulSet statefulset-4421/ss2 to complete update
Dec  3 16:30:56.720: INFO: Waiting for Pod statefulset-4421/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 16:31:06.720: INFO: Waiting for StatefulSet statefulset-4421/ss2 to complete update
STEP: Rolling back to a previous revision
Dec  3 16:31:16.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4421 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:31:17.582: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:31:17.582: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:31:17.582: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:31:17.666: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  3 16:31:17.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4421 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:31:18.380: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:31:18.380: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:31:18.380: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:31:48.492: INFO: Waiting for StatefulSet statefulset-4421/ss2 to complete update
Dec  3 16:31:48.492: INFO: Waiting for Pod statefulset-4421/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 16:31:58.529: INFO: Waiting for StatefulSet statefulset-4421/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 16:32:08.529: INFO: Deleting all statefulset in ns statefulset-4421
Dec  3 16:32:08.547: INFO: Scaling statefulset ss2 to 0
Dec  3 16:32:28.620: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:32:28.638: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:32:28.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4421" for this suite.
Dec  3 16:32:34.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:32:35.489: INFO: namespace statefulset-4421 deletion completed in 6.754413247s
•S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:32:35.489: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7736
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  3 16:32:39.805: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-20a9bf82-7c3d-4e87-b680-3ed9bce9fa89,GenerateName:,Namespace:events-7736,SelfLink:/api/v1/namespaces/events-7736/pods/send-events-20a9bf82-7c3d-4e87-b680-3ed9bce9fa89,UID:27317289-218e-463d-875d-c12c7ed460a5,ResourceVersion:25487,Generation:0,CreationTimestamp:2019-12-03 16:32:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 713154349,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.252/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zcspg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zcspg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-zcspg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038a4690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038a46b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:32:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:32:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:32:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:32:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.1.252,StartTime:2019-12-03 16:32:35 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-03 16:32:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://a3e6c76e14aa39dc9aab80963a82069c82cf4a6d57649572c8cb7c2dbc15ca81}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  3 16:32:41.824: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  3 16:32:43.843: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:32:43.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7736" for this suite.
Dec  3 16:33:21.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:22.629: INFO: namespace events-7736 deletion completed in 38.731787066s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:33:22.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec  3 16:33:22.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec  3 16:33:23.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:25.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:27.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:29.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:31.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:33.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:35.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:37.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:39.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:41.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987603, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:33:45.987: INFO: Waited 2.208354472s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:33:47.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3420" for this suite.
Dec  3 16:33:53.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:53.780: INFO: namespace aggregator-3420 deletion completed in 6.750654038s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:33:53.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  3 16:33:58.136: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:33:58.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6052" for this suite.
Dec  3 16:34:20.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:34:20.969: INFO: namespace replicaset-6052 deletion completed in 22.740230918s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:34:20.970: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 16:34:21.215: INFO: Waiting up to 5m0s for pod "pod-0d075bf1-e008-4fd5-a7db-36223c05bd96" in namespace "emptydir-2416" to be "success or failure"
Dec  3 16:34:21.232: INFO: Pod "pod-0d075bf1-e008-4fd5-a7db-36223c05bd96": Phase="Pending", Reason="", readiness=false. Elapsed: 17.188618ms
Dec  3 16:34:23.251: INFO: Pod "pod-0d075bf1-e008-4fd5-a7db-36223c05bd96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03614414s
Dec  3 16:34:25.270: INFO: Pod "pod-0d075bf1-e008-4fd5-a7db-36223c05bd96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055114097s
STEP: Saw pod success
Dec  3 16:34:25.270: INFO: Pod "pod-0d075bf1-e008-4fd5-a7db-36223c05bd96" satisfied condition "success or failure"
Dec  3 16:34:25.288: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod pod-0d075bf1-e008-4fd5-a7db-36223c05bd96 container test-container: <nil>
STEP: delete the pod
Dec  3 16:34:25.349: INFO: Waiting for pod pod-0d075bf1-e008-4fd5-a7db-36223c05bd96 to disappear
Dec  3 16:34:25.366: INFO: Pod pod-0d075bf1-e008-4fd5-a7db-36223c05bd96 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:34:25.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2416" for this suite.
Dec  3 16:34:31.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:34:32.137: INFO: namespace emptydir-2416 deletion completed in 6.737861765s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:34:32.138: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1073
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:34:32.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1073'
Dec  3 16:34:32.743: INFO: stderr: ""
Dec  3 16:34:32.743: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  3 16:34:32.743: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1073'
Dec  3 16:34:33.081: INFO: stderr: ""
Dec  3 16:34:33.081: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:34:34.099: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:34:34.099: INFO: Found 0 / 1
Dec  3 16:34:35.100: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:34:35.100: INFO: Found 0 / 1
Dec  3 16:34:36.099: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:34:36.100: INFO: Found 1 / 1
Dec  3 16:34:36.100: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 16:34:36.117: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:34:36.118: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:34:36.118: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-g9dmp --namespace=kubectl-1073'
Dec  3 16:34:36.308: INFO: stderr: ""
Dec  3 16:34:36.308: INFO: stdout: "Name:           redis-master-g9dmp\nNamespace:      kubectl-1073\nPriority:       0\nNode:           shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf/10.250.0.4\nStart Time:     Tue, 03 Dec 2019 16:34:32 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.64.1.5/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.64.1.5\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://319e63e6632be9ad507db8550db977914614a002734c39044be468c12466470a\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 03 Dec 2019 16:34:34 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-tpxl5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-tpxl5:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-tpxl5\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  4s    default-scheduler                                        Successfully assigned kubectl-1073/redis-master-g9dmp to shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf\n  Normal  Pulled     2s    kubelet, shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Created container redis-master\n  Normal  Started    2s    kubelet, shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf  Started container redis-master\n"
Dec  3 16:34:36.308: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-1073'
Dec  3 16:34:36.536: INFO: stderr: ""
Dec  3 16:34:36.536: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1073\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-g9dmp\n"
Dec  3 16:34:36.536: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-1073'
Dec  3 16:34:36.729: INFO: stderr: ""
Dec  3 16:34:36.729: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1073\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.111.159.78\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.64.1.5:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  3 16:34:36.764: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88'
Dec  3 16:34:37.034: INFO: stderr: ""
Dec  3 16:34:37.034: INFO: stdout: "Name:               shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westeurope\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 03 Dec 2019 14:33:42 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentUnregisterNetDevice   False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Tue, 03 Dec 2019 16:34:36 +0000   Tue, 03 Dec 2019 14:37:37 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  NetworkUnavailable            False   Tue, 03 Dec 2019 14:34:09 +0000   Tue, 03 Dec 2019 14:34:09 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Tue, 03 Dec 2019 16:34:35 +0000   Tue, 03 Dec 2019 14:33:42 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 03 Dec 2019 16:34:35 +0000   Tue, 03 Dec 2019 14:33:42 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 03 Dec 2019 16:34:35 +0000   Tue, 03 Dec 2019 14:33:42 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 03 Dec 2019 16:34:35 +0000   Tue, 03 Dec 2019 14:34:44 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  Hostname:    shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88\n  InternalIP:  10.250.0.5\nCapacity:\n attachable-volumes-azure-disk:  4\n cpu:                            2\n ephemeral-storage:              33136428Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         8145228Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  4\n cpu:                            1920m\n ephemeral-storage:              32235117134\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         6849935969\n pods:                           110\nSystem Info:\n Machine ID:                 30a70ac87e3b4c4c882197cd523cbf4a\n System UUID:                0df5ed49-cffd-0149-b68b-0f6b8cb22920\n Boot ID:                    9b3b70d4-e05f-4aae-a0b4-6d2b4fbb5cba\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.6\n Kube-Proxy Version:         v1.15.6\nPodCIDR:                     100.64.0.0/24\nProviderID:                  azure:///subscriptions//resourceGroups/shoot--it--tmv8o-1fn/providers/Microsoft.Compute/virtualMachines/shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88\nNon-terminated Pods:         (15 in total)\n  Namespace                  Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kubernetes-dashboard-5c8d9945bc-q67s2                      50m (2%)      100m (5%)   50Mi (0%)        256Mi (3%)     124m\n  kube-system                addons-nginx-ingress-controller-8468678b64-k2xb6                  100m (5%)     2 (104%)    100Mi (1%)       1Gi (15%)      124m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-t2scl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                blackbox-exporter-c87bdd467-6qxtm                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      124m\n  kube-system                calico-kube-controllers-5d785bc598-vwwh6                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                calico-node-8578h                                                 100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)    120m\n  kube-system                calico-typha-horizontal-autoscaler-554dfbfdd7-82b4r               10m (0%)      10m (0%)    0 (0%)           0 (0%)         124m\n  kube-system                calico-typha-vertical-autoscaler-656557779f-9ms99                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         124m\n  kube-system                coredns-858b686868-8npzw                                          50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     124m\n  kube-system                coredns-858b686868-vmthw                                          50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)     124m\n  kube-system                kube-proxy-m56xr                                                  20m (1%)      0 (0%)      64Mi (0%)        0 (0%)         120m\n  kube-system                metrics-server-6c6b44bdf-6fqbp                                    20m (1%)      80m (4%)    100Mi (1%)       400Mi (6%)     124m\n  kube-system                node-exporter-9bgsx                                               5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     120m\n  kube-system                node-problem-detector-9qtgm                                       20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)     120m\n  kube-system                vpn-shoot-76b5996f55-sb8l6                                        100m (5%)     1 (52%)     100Mi (1%)       1000Mi (15%)   124m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            530m (27%)  4125m (214%)\n  memory                         579Mi (8%)  3815Mi (58%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Dec  3 16:34:37.035: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-1073'
Dec  3 16:34:37.237: INFO: stderr: ""
Dec  3 16:34:37.237: INFO: stdout: "Name:         kubectl-1073\nLabels:       e2e-framework=kubectl\n              e2e-run=510d427d-36ca-4b58-895f-acd5419778a2\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:34:37.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1073" for this suite.
Dec  3 16:34:59.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:00.003: INFO: namespace kubectl-1073 deletion completed in 22.732038666s
•
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:35:00.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 16:35:04.867: INFO: Successfully updated pod "labelsupdateb3eec05c-3fbb-4b67-b469-fa543b8d7542"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:35:06.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1742" for this suite.
Dec  3 16:35:29.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:29.732: INFO: namespace projected-1742 deletion completed in 22.766462453s
•SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:35:29.733: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:35:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5002" for this suite.
Dec  3 16:36:18.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:18.824: INFO: namespace kubelet-test-5002 deletion completed in 44.736613228s
•SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:18.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6035
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-94
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:25.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4980" for this suite.
Dec  3 16:36:31.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:32.353: INFO: namespace namespaces-4980 deletion completed in 6.744258171s
STEP: Destroying namespace "nsdeletetest-6035" for this suite.
Dec  3 16:36:32.371: INFO: Namespace nsdeletetest-6035 was already deleted
STEP: Destroying namespace "nsdeletetest-94" for this suite.
Dec  3 16:36:38.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:39.124: INFO: namespace nsdeletetest-94 deletion completed in 6.753196556s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:39.124: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 16:36:39.352: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6592'
Dec  3 16:36:39.516: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 16:36:39.516: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  3 16:36:39.552: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-lwt6h]
Dec  3 16:36:39.552: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-lwt6h" in namespace "kubectl-6592" to be "running and ready"
Dec  3 16:36:39.569: INFO: Pod "e2e-test-nginx-rc-lwt6h": Phase="Pending", Reason="", readiness=false. Elapsed: 17.266423ms
Dec  3 16:36:41.587: INFO: Pod "e2e-test-nginx-rc-lwt6h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035204965s
Dec  3 16:36:43.606: INFO: Pod "e2e-test-nginx-rc-lwt6h": Phase="Running", Reason="", readiness=true. Elapsed: 4.053896725s
Dec  3 16:36:43.606: INFO: Pod "e2e-test-nginx-rc-lwt6h" satisfied condition "running and ready"
Dec  3 16:36:43.606: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-lwt6h]
Dec  3 16:36:43.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-6592'
Dec  3 16:36:43.823: INFO: stderr: ""
Dec  3 16:36:43.824: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Dec  3 16:36:43.824: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-6592'
Dec  3 16:36:44.021: INFO: stderr: ""
Dec  3 16:36:44.021: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:44.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6592" for this suite.
Dec  3 16:36:50.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:50.793: INFO: namespace kubectl-6592 deletion completed in 6.73663516s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:50.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec  3 16:36:51.015: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix120637233/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:51.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1001" for this suite.
Dec  3 16:36:57.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:57.867: INFO: namespace kubectl-1001 deletion completed in 6.774978663s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:57.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 16:36:58.096: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:37:04.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6852" for this suite.
Dec  3 16:37:10.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:37:10.941: INFO: namespace init-container-6852 deletion completed in 6.744240045s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:37:10.941: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 16:37:11.186: INFO: Waiting up to 5m0s for pod "downward-api-baa04225-b9b4-4310-865b-5c0749e93f20" in namespace "downward-api-1091" to be "success or failure"
Dec  3 16:37:11.203: INFO: Pod "downward-api-baa04225-b9b4-4310-865b-5c0749e93f20": Phase="Pending", Reason="", readiness=false. Elapsed: 17.323536ms
Dec  3 16:37:13.222: INFO: Pod "downward-api-baa04225-b9b4-4310-865b-5c0749e93f20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035851635s
Dec  3 16:37:15.243: INFO: Pod "downward-api-baa04225-b9b4-4310-865b-5c0749e93f20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057003776s
STEP: Saw pod success
Dec  3 16:37:15.243: INFO: Pod "downward-api-baa04225-b9b4-4310-865b-5c0749e93f20" satisfied condition "success or failure"
Dec  3 16:37:15.261: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downward-api-baa04225-b9b4-4310-865b-5c0749e93f20 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:37:15.315: INFO: Waiting for pod downward-api-baa04225-b9b4-4310-865b-5c0749e93f20 to disappear
Dec  3 16:37:15.342: INFO: Pod downward-api-baa04225-b9b4-4310-865b-5c0749e93f20 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:37:15.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1091" for this suite.
Dec  3 16:37:21.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:37:22.129: INFO: namespace downward-api-1091 deletion completed in 6.752581244s
•
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:37:22.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  3 16:37:22.386: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:37:36.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4528" for this suite.
Dec  3 16:37:42.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:37:43.264: INFO: namespace pods-4528 deletion completed in 6.778979113s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:37:43.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5442
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5442
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5442
Dec  3 16:37:43.562: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 16:37:53.581: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  3 16:37:53.599: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:37:54.294: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:37:54.294: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:37:54.294: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:37:54.313: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 16:38:04.332: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:38:04.332: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:38:04.406: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999304s
Dec  3 16:38:05.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980657551s
Dec  3 16:38:06.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.953911432s
Dec  3 16:38:07.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.933896758s
Dec  3 16:38:08.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.9151976s
Dec  3 16:38:09.519: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.896192791s
Dec  3 16:38:10.538: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.867635131s
Dec  3 16:38:11.557: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.848100729s
Dec  3 16:38:12.576: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.828998194s
Dec  3 16:38:13.595: INFO: Verifying statefulset ss doesn't scale past 1 for another 810.175703ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5442
Dec  3 16:38:14.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:38:15.266: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:38:15.266: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:38:15.266: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:38:15.284: INFO: Found 1 stateful pods, waiting for 3
Dec  3 16:38:25.304: INFO: Found 2 stateful pods, waiting for 3
Dec  3 16:38:35.304: INFO: Found 2 stateful pods, waiting for 3
Dec  3 16:38:45.304: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:38:45.304: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:38:45.304: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  3 16:38:45.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:38:45.972: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:38:45.972: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:38:45.972: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:38:45.972: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:38:47.173: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:38:47.173: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:38:47.173: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:38:47.173: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:38:47.843: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:38:47.843: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:38:47.843: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:38:47.843: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:38:47.862: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 16:38:57.900: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:38:57.900: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:38:57.900: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:38:57.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999375s
Dec  3 16:38:58.973: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981962902s
Dec  3 16:38:59.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.963163046s
Dec  3 16:39:01.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.938956007s
Dec  3 16:39:02.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.919830167s
Dec  3 16:39:03.054: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.900559276s
Dec  3 16:39:04.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.881830803s
Dec  3 16:39:05.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.862887457s
Dec  3 16:39:06.112: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.843604211s
Dec  3 16:39:07.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 824.292665ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5442
Dec  3 16:39:08.153: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:08.799: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:39:08.799: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:39:08.799: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:39:08.799: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:09.422: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:39:09.422: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:39:09.422: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:39:09.422: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:09.744: INFO: rc: 1
Dec  3 16:39:09.744: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0038a6ab0 exit status 1 <nil> <nil> true [0xc002a3c750 0xc002a3c768 0xc002a3c780] [0xc002a3c750 0xc002a3c768 0xc002a3c780] [0xc002a3c760 0xc002a3c778] [0xba6c10 0xba6c10] 0xc003bb65a0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Dec  3 16:39:19.745: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:19.914: INFO: rc: 1
Dec  3 16:39:19.914: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364a840 exit status 1 <nil> <nil> true [0xc0006da298 0xc0006da590 0xc0006da628] [0xc0006da298 0xc0006da590 0xc0006da628] [0xc0006da488 0xc0006da608] [0xba6c10 0xba6c10] 0xc00026a8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:39:29.915: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:30.077: INFO: rc: 1
Dec  3 16:39:30.077: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c005a0 exit status 1 <nil> <nil> true [0xc000194000 0xc000194240 0xc000194318] [0xc000194000 0xc000194240 0xc000194318] [0xc000194210 0xc0001942c0] [0xba6c10 0xba6c10] 0xc00378a780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:39:40.077: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:40.254: INFO: rc: 1
Dec  3 16:39:40.254: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bc5d0 exit status 1 <nil> <nil> true [0xc000372028 0xc000373470 0xc000373608] [0xc000372028 0xc000373470 0xc000373608] [0xc000372190 0xc0003735e8] [0xba6c10 0xba6c10] 0xc0026e14a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:39:50.254: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:39:50.405: INFO: rc: 1
Dec  3 16:39:50.405: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364ae70 exit status 1 <nil> <nil> true [0xc0006da648 0xc0006da798 0xc0006da888] [0xc0006da648 0xc0006da798 0xc0006da888] [0xc0006da748 0xc0006da7e0] [0xba6c10 0xba6c10] 0xc00026b080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:40:00.405: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:40:00.556: INFO: rc: 1
Dec  3 16:40:00.556: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bcc00 exit status 1 <nil> <nil> true [0xc000373668 0xc000373768 0xc000373998] [0xc000373668 0xc000373768 0xc000373998] [0xc000373748 0xc000373860] [0xba6c10 0xba6c10] 0xc001f350e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:40:10.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:40:10.711: INFO: rc: 1
Dec  3 16:40:10.711: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364b440 exit status 1 <nil> <nil> true [0xc0006da918 0xc0006dae58 0xc0006db000] [0xc0006da918 0xc0006dae58 0xc0006db000] [0xc0006dae30 0xc0006dafa8] [0xba6c10 0xba6c10] 0xc00026b800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:40:20.711: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:40:20.877: INFO: rc: 1
Dec  3 16:40:20.877: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364ba10 exit status 1 <nil> <nil> true [0xc0006db008 0xc0006db0c8 0xc0006db1e8] [0xc0006db008 0xc0006db0c8 0xc0006db1e8] [0xc0006db080 0xc0006db180] [0xba6c10 0xba6c10] 0xc00252ad80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:40:30.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:40:31.289: INFO: rc: 1
Dec  3 16:40:31.289: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c00c00 exit status 1 <nil> <nil> true [0xc000194350 0xc000194450 0xc0001944f8] [0xc000194350 0xc000194450 0xc0001944f8] [0xc000194400 0xc0001944d0] [0xba6c10 0xba6c10] 0xc00378b1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:40:41.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:40:41.446: INFO: rc: 1
Dec  3 16:40:41.446: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00236e660 exit status 1 <nil> <nil> true [0xc0000de938 0xc0000debe8 0xc0000dee08] [0xc0000de938 0xc0000debe8 0xc0000dee08] [0xc0000deb90 0xc0000ded98] [0xba6c10 0xba6c10] 0xc0021cb5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:40:51.446: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:40:51.603: INFO: rc: 1
Dec  3 16:40:51.603: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00236ed20 exit status 1 <nil> <nil> true [0xc0000dee58 0xc0000deee0 0xc0000df0b8] [0xc0000dee58 0xc0000deee0 0xc0000df0b8] [0xc0000deec0 0xc0000def98] [0xba6c10 0xba6c10] 0xc002070480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:41:01.604: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:41:01.838: INFO: rc: 1
Dec  3 16:41:01.838: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bd290 exit status 1 <nil> <nil> true [0xc0003739f0 0xc000373ab0 0xc000373bc0] [0xc0003739f0 0xc000373ab0 0xc000373bc0] [0xc000373a98 0xc000373b50] [0xba6c10 0xba6c10] 0xc001e76720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:41:11.838: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:41:11.992: INFO: rc: 1
Dec  3 16:41:11.992: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bd890 exit status 1 <nil> <nil> true [0xc000373bf0 0xc000373ce0 0xc000373de8] [0xc000373bf0 0xc000373ce0 0xc000373de8] [0xc000373c48 0xc000373d80] [0xba6c10 0xba6c10] 0xc001e77320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:41:21.992: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:41:22.149: INFO: rc: 1
Dec  3 16:41:22.149: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364a870 exit status 1 <nil> <nil> true [0xc0006da398 0xc0006da5b8 0xc0006da648] [0xc0006da398 0xc0006da5b8 0xc0006da648] [0xc0006da590 0xc0006da628] [0xba6c10 0xba6c10] 0xc0021ca9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:41:32.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:41:32.295: INFO: rc: 1
Dec  3 16:41:32.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364aed0 exit status 1 <nil> <nil> true [0xc0006da678 0xc0006da7c0 0xc0006da918] [0xc0006da678 0xc0006da7c0 0xc0006da918] [0xc0006da798 0xc0006da888] [0xba6c10 0xba6c10] 0xc001f344e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:41:42.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:41:42.445: INFO: rc: 1
Dec  3 16:41:42.446: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bc600 exit status 1 <nil> <nil> true [0xc0000de938 0xc0000debe8 0xc0000dee08] [0xc0000de938 0xc0000debe8 0xc0000dee08] [0xc0000deb90 0xc0000ded98] [0xba6c10 0xba6c10] 0xc0026e14a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:41:52.446: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:41:52.601: INFO: rc: 1
Dec  3 16:41:52.601: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364b4d0 exit status 1 <nil> <nil> true [0xc0006dadc8 0xc0006daf28 0xc0006db008] [0xc0006dadc8 0xc0006daf28 0xc0006db008] [0xc0006dae58 0xc0006db000] [0xba6c10 0xba6c10] 0xc00026a6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:42:02.602: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:42:02.751: INFO: rc: 1
Dec  3 16:42:02.751: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364bb00 exit status 1 <nil> <nil> true [0xc0006db020 0xc0006db110 0xc0006db1f8] [0xc0006db020 0xc0006db110 0xc0006db1f8] [0xc0006db0c8 0xc0006db1e8] [0xba6c10 0xba6c10] 0xc00026aea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:42:12.751: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:42:12.905: INFO: rc: 1
Dec  3 16:42:12.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c005d0 exit status 1 <nil> <nil> true [0xc000372028 0xc000373470 0xc000373608] [0xc000372028 0xc000373470 0xc000373608] [0xc000372190 0xc0003735e8] [0xba6c10 0xba6c10] 0xc00252b380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:42:22.905: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:42:23.068: INFO: rc: 1
Dec  3 16:42:23.068: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000d18150 exit status 1 <nil> <nil> true [0xc0006db200 0xc0006db228 0xc0006db318] [0xc0006db200 0xc0006db228 0xc0006db318] [0xc0006db218 0xc0006db2d0] [0xba6c10 0xba6c10] 0xc00026b4a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:42:33.068: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:42:33.226: INFO: rc: 1
Dec  3 16:42:33.226: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00236e6f0 exit status 1 <nil> <nil> true [0xc000194000 0xc000194240 0xc000194318] [0xc000194000 0xc000194240 0xc000194318] [0xc000194210 0xc0001942c0] [0xba6c10 0xba6c10] 0xc002070a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:42:43.226: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:42:43.377: INFO: rc: 1
Dec  3 16:42:43.377: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bcc60 exit status 1 <nil> <nil> true [0xc0000dee58 0xc0000deee0 0xc0000df0b8] [0xc0000dee58 0xc0000deee0 0xc0000df0b8] [0xc0000deec0 0xc0000def98] [0xba6c10 0xba6c10] 0xc001e76600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:42:53.377: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:42:53.531: INFO: rc: 1
Dec  3 16:42:53.531: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c00c30 exit status 1 <nil> <nil> true [0xc000373668 0xc000373768 0xc000373998] [0xc000373668 0xc000373768 0xc000373998] [0xc000373748 0xc000373860] [0xba6c10 0xba6c10] 0xc00378a180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:43:03.531: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:43:03.682: INFO: rc: 1
Dec  3 16:43:03.682: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018bd260 exit status 1 <nil> <nil> true [0xc0000df198 0xc0000df308 0xc0000df6c0] [0xc0000df198 0xc0000df308 0xc0000df6c0] [0xc0000df1e0 0xc0000df478] [0xba6c10 0xba6c10] 0xc001e771a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:43:13.682: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:43:13.830: INFO: rc: 1
Dec  3 16:43:13.830: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000d18750 exit status 1 <nil> <nil> true [0xc0006db3e0 0xc0006db500 0xc0006db5c0] [0xc0006db3e0 0xc0006db500 0xc0006db5c0] [0xc0006db490 0xc0006db580] [0xba6c10 0xba6c10] 0xc001f8e0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:43:23.830: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:43:23.984: INFO: rc: 1
Dec  3 16:43:23.984: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364a840 exit status 1 <nil> <nil> true [0xc0001941e0 0xc000194260 0xc000194350] [0xc0001941e0 0xc000194260 0xc000194350] [0xc000194240 0xc000194318] [0xba6c10 0xba6c10] 0xc00252b380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:43:33.984: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:43:34.138: INFO: rc: 1
Dec  3 16:43:34.138: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00236e660 exit status 1 <nil> <nil> true [0xc0006da298 0xc0006da590 0xc0006da628] [0xc0006da298 0xc0006da590 0xc0006da628] [0xc0006da488 0xc0006da608] [0xba6c10 0xba6c10] 0xc00026a8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:43:44.139: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:43:44.298: INFO: rc: 1
Dec  3 16:43:44.298: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000d185d0 exit status 1 <nil> <nil> true [0xc000372028 0xc000373470 0xc000373608] [0xc000372028 0xc000373470 0xc000373608] [0xc000372190 0xc0003735e8] [0xba6c10 0xba6c10] 0xc0026e14a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:43:54.299: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:43:54.447: INFO: rc: 1
Dec  3 16:43:54.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000d18bd0 exit status 1 <nil> <nil> true [0xc000373668 0xc000373768 0xc000373998] [0xc000373668 0xc000373768 0xc000373998] [0xc000373748 0xc000373860] [0xba6c10 0xba6c10] 0xc001f350e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:44:04.448: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:44:04.600: INFO: rc: 1
Dec  3 16:44:04.600: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00364aea0 exit status 1 <nil> <nil> true [0xc0001943a8 0xc000194480 0xc000194558] [0xc0001943a8 0xc000194480 0xc000194558] [0xc000194450 0xc0001944f8] [0xba6c10 0xba6c10] 0xc0021ca9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:44:14.600: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5442 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:44:14.754: INFO: rc: 1
Dec  3 16:44:14.754: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Dec  3 16:44:14.754: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 16:44:14.817: INFO: Deleting all statefulset in ns statefulset-5442
Dec  3 16:44:14.835: INFO: Scaling statefulset ss to 0
Dec  3 16:44:14.888: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:44:14.906: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:44:14.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5442" for this suite.
Dec  3 16:44:21.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:44:21.748: INFO: namespace statefulset-5442 deletion completed in 6.74997901s

• [SLOW TEST:398.484 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:44:21.749: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:44:22.073: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 16:44:22.134: INFO: Number of nodes with available pods: 0
Dec  3 16:44:22.135: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:23.187: INFO: Number of nodes with available pods: 0
Dec  3 16:44:23.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:24.187: INFO: Number of nodes with available pods: 0
Dec  3 16:44:24.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:25.188: INFO: Number of nodes with available pods: 1
Dec  3 16:44:25.188: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:26.187: INFO: Number of nodes with available pods: 1
Dec  3 16:44:26.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:27.187: INFO: Number of nodes with available pods: 1
Dec  3 16:44:27.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:28.187: INFO: Number of nodes with available pods: 1
Dec  3 16:44:28.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:29.188: INFO: Number of nodes with available pods: 1
Dec  3 16:44:29.188: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:30.187: INFO: Number of nodes with available pods: 1
Dec  3 16:44:30.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:31.187: INFO: Number of nodes with available pods: 1
Dec  3 16:44:31.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:32.188: INFO: Number of nodes with available pods: 1
Dec  3 16:44:32.188: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:33.187: INFO: Number of nodes with available pods: 1
Dec  3 16:44:33.187: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-6mz88 is running more than one daemon pod
Dec  3 16:44:34.188: INFO: Number of nodes with available pods: 2
Dec  3 16:44:34.188: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  3 16:44:34.317: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:34.317: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:35.354: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:35.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:36.354: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:36.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:37.359: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:37.359: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:38.354: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:38.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:39.354: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:39.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:40.354: INFO: Wrong image for pod: daemon-set-9jqmq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:40.354: INFO: Pod daemon-set-9jqmq is not available
Dec  3 16:44:40.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:41.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:41.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:42.355: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:42.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:43.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:43.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:44.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:44.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:45.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:45.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:46.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:46.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:47.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:47.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:48.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:48.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:49.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:49.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:50.355: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:50.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:51.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:51.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:52.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:52.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:53.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:53.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:54.355: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:54.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:55.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:55.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:56.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:56.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:57.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:57.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:58.355: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:58.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:44:59.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:44:59.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:45:00.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:45:00.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:45:01.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:45:01.354: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:45:02.355: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:45:02.355: INFO: Pod daemon-set-xnsrx is not available
Dec  3 16:45:03.354: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:45:04.355: INFO: Wrong image for pod: daemon-set-vbj96. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 16:45:04.355: INFO: Pod daemon-set-vbj96 is not available
Dec  3 16:45:05.354: INFO: Pod daemon-set-7xmbs is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  3 16:45:05.429: INFO: Number of nodes with available pods: 1
Dec  3 16:45:05.429: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 16:45:06.481: INFO: Number of nodes with available pods: 1
Dec  3 16:45:06.481: INFO: Node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf is running more than one daemon pod
Dec  3 16:45:07.481: INFO: Number of nodes with available pods: 2
Dec  3 16:45:07.481: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2250, will wait for the garbage collector to delete the pods
Dec  3 16:45:07.660: INFO: Deleting DaemonSet.extensions daemon-set took: 22.28615ms
Dec  3 16:45:07.760: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.338156ms
Dec  3 16:45:34.778: INFO: Number of nodes with available pods: 0
Dec  3 16:45:34.778: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:45:34.796: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2250/daemonsets","resourceVersion":"27812"},"items":null}

Dec  3 16:45:34.813: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2250/pods","resourceVersion":"27812"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:45:34.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2250" for this suite.
Dec  3 16:45:40.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:45:41.644: INFO: namespace daemonsets-2250 deletion completed in 6.743583523s
•SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:45:41.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec  3 16:45:41.887: INFO: Waiting up to 5m0s for pod "client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed" in namespace "containers-4316" to be "success or failure"
Dec  3 16:45:41.905: INFO: Pod "client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 17.397942ms
Dec  3 16:45:43.923: INFO: Pod "client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035731312s
Dec  3 16:45:45.941: INFO: Pod "client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053903383s
STEP: Saw pod success
Dec  3 16:45:45.942: INFO: Pod "client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed" satisfied condition "success or failure"
Dec  3 16:45:45.961: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed container test-container: <nil>
STEP: delete the pod
Dec  3 16:45:46.018: INFO: Waiting for pod client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed to disappear
Dec  3 16:45:46.035: INFO: Pod client-containers-d456feb3-01f2-402b-942f-d525b08ea5ed no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:45:46.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4316" for this suite.
Dec  3 16:45:52.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:45:52.833: INFO: namespace containers-4316 deletion completed in 6.763812738s
•SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:45:52.834: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec  3 16:45:53.057: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1984'
Dec  3 16:45:53.302: INFO: stderr: ""
Dec  3 16:45:53.302: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:45:53.302: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:45:53.449: INFO: stderr: ""
Dec  3 16:45:53.449: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:45:53.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:45:53.585: INFO: stderr: ""
Dec  3 16:45:53.585: INFO: stdout: ""
Dec  3 16:45:53.585: INFO: update-demo-nautilus-j26w4 is created but not running
Dec  3 16:45:58.585: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:45:58.798: INFO: stderr: ""
Dec  3 16:45:58.798: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:45:58.798: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:45:58.933: INFO: stderr: ""
Dec  3 16:45:58.933: INFO: stdout: "true"
Dec  3 16:45:58.933: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:45:59.070: INFO: stderr: ""
Dec  3 16:45:59.070: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:45:59.070: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:45:59.150: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:45:59.150: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:45:59.150: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:45:59.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:45:59.282: INFO: stderr: ""
Dec  3 16:45:59.282: INFO: stdout: ""
Dec  3 16:45:59.282: INFO: update-demo-nautilus-wvrgr is created but not running
Dec  3 16:46:04.283: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:46:04.420: INFO: stderr: ""
Dec  3 16:46:04.420: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:46:04.421: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:04.551: INFO: stderr: ""
Dec  3 16:46:04.551: INFO: stdout: "true"
Dec  3 16:46:04.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:04.684: INFO: stderr: ""
Dec  3 16:46:04.684: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:04.684: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:46:04.707: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:04.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:04.707: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:46:04.707: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:04.847: INFO: stderr: ""
Dec  3 16:46:04.847: INFO: stdout: ""
Dec  3 16:46:04.847: INFO: update-demo-nautilus-wvrgr is created but not running
Dec  3 16:46:09.847: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:46:09.990: INFO: stderr: ""
Dec  3 16:46:09.990: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:46:09.990: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:10.123: INFO: stderr: ""
Dec  3 16:46:10.123: INFO: stdout: "true"
Dec  3 16:46:10.123: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:10.254: INFO: stderr: ""
Dec  3 16:46:10.254: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:10.254: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:46:10.278: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:10.278: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:10.278: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:46:10.278: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:10.411: INFO: stderr: ""
Dec  3 16:46:10.411: INFO: stdout: ""
Dec  3 16:46:10.411: INFO: update-demo-nautilus-wvrgr is created but not running
Dec  3 16:46:15.411: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:46:15.547: INFO: stderr: ""
Dec  3 16:46:15.547: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:46:15.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:15.694: INFO: stderr: ""
Dec  3 16:46:15.694: INFO: stdout: "true"
Dec  3 16:46:15.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:15.827: INFO: stderr: ""
Dec  3 16:46:15.827: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:15.827: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:46:15.852: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:15.852: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:15.852: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:46:15.852: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:15.986: INFO: stderr: ""
Dec  3 16:46:15.986: INFO: stdout: ""
Dec  3 16:46:15.986: INFO: update-demo-nautilus-wvrgr is created but not running
Dec  3 16:46:20.986: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:46:21.123: INFO: stderr: ""
Dec  3 16:46:21.123: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:46:21.124: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:21.256: INFO: stderr: ""
Dec  3 16:46:21.256: INFO: stdout: "true"
Dec  3 16:46:21.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:21.389: INFO: stderr: ""
Dec  3 16:46:21.389: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:21.389: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:46:21.409: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:21.409: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:21.409: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:46:21.409: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:21.542: INFO: stderr: ""
Dec  3 16:46:21.542: INFO: stdout: ""
Dec  3 16:46:21.542: INFO: update-demo-nautilus-wvrgr is created but not running
Dec  3 16:46:26.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:46:26.701: INFO: stderr: ""
Dec  3 16:46:26.701: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:46:26.702: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:26.840: INFO: stderr: ""
Dec  3 16:46:26.840: INFO: stdout: "true"
Dec  3 16:46:26.840: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:26.978: INFO: stderr: ""
Dec  3 16:46:26.978: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:26.978: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:46:26.999: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:26.999: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:26.999: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:46:26.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:27.133: INFO: stderr: ""
Dec  3 16:46:27.133: INFO: stdout: ""
Dec  3 16:46:27.133: INFO: update-demo-nautilus-wvrgr is created but not running
Dec  3 16:46:32.133: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1984'
Dec  3 16:46:32.269: INFO: stderr: ""
Dec  3 16:46:32.269: INFO: stdout: "update-demo-nautilus-j26w4 update-demo-nautilus-wvrgr "
Dec  3 16:46:32.269: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:32.402: INFO: stderr: ""
Dec  3 16:46:32.402: INFO: stdout: "true"
Dec  3 16:46:32.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-j26w4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:32.534: INFO: stderr: ""
Dec  3 16:46:32.534: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:32.534: INFO: validating pod update-demo-nautilus-j26w4
Dec  3 16:46:32.555: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:32.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:32.555: INFO: update-demo-nautilus-j26w4 is verified up and running
Dec  3 16:46:32.555: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:32.689: INFO: stderr: ""
Dec  3 16:46:32.689: INFO: stdout: "true"
Dec  3 16:46:32.689: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wvrgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1984'
Dec  3 16:46:32.830: INFO: stderr: ""
Dec  3 16:46:32.831: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:46:32.831: INFO: validating pod update-demo-nautilus-wvrgr
Dec  3 16:46:32.944: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:46:32.944: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:46:32.944: INFO: update-demo-nautilus-wvrgr is verified up and running
STEP: using delete to clean up resources
Dec  3 16:46:32.944: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1984'
Dec  3 16:46:33.108: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:46:33.108: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 16:46:33.108: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1984'
Dec  3 16:46:33.281: INFO: stderr: "No resources found.\n"
Dec  3 16:46:33.281: INFO: stdout: ""
Dec  3 16:46:33.281: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1984 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:46:33.415: INFO: stderr: ""
Dec  3 16:46:33.415: INFO: stdout: "update-demo-nautilus-j26w4\nupdate-demo-nautilus-wvrgr\n"
Dec  3 16:46:33.915: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1984'
Dec  3 16:46:34.078: INFO: stderr: "No resources found.\n"
Dec  3 16:46:34.078: INFO: stdout: ""
Dec  3 16:46:34.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1984 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:46:34.223: INFO: stderr: ""
Dec  3 16:46:34.223: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:46:34.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1984" for this suite.
Dec  3 16:46:56.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:46:56.996: INFO: namespace kubectl-1984 deletion completed in 22.738007233s
•SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:46:56.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:46:57.231: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec  3 16:46:58.360: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:46:58.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4405" for this suite.
Dec  3 16:47:04.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:47:05.180: INFO: namespace replication-controller-4405 deletion completed in 6.76712047s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:47:05.180: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:47:05.435: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29" in namespace "downward-api-7439" to be "success or failure"
Dec  3 16:47:05.452: INFO: Pod "downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29": Phase="Pending", Reason="", readiness=false. Elapsed: 17.095272ms
Dec  3 16:47:07.471: INFO: Pod "downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036214978s
Dec  3 16:47:09.490: INFO: Pod "downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054596298s
STEP: Saw pod success
Dec  3 16:47:09.490: INFO: Pod "downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29" satisfied condition "success or failure"
Dec  3 16:47:09.508: INFO: Trying to get logs from node shoot--it--tmv8o-1fn-worker-1-6b6684b5c6-pvtxf pod downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29 container client-container: <nil>
STEP: delete the pod
Dec  3 16:47:09.556: INFO: Waiting for pod downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29 to disappear
Dec  3 16:47:09.574: INFO: Pod downwardapi-volume-5ff4b82a-bc62-41f3-bfd0-608b89944a29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:47:09.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7439" for this suite.
Dec  3 16:47:15.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:47:16.360: INFO: namespace downward-api-7439 deletion completed in 6.752390997s
•SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:47:16.360: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec  3 16:47:46.742: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:47:46.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1203 16:47:46.742261    5074 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8909" for this suite.
Dec  3 16:47:52.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:47:53.515: INFO: namespace gc-8909 deletion completed in 6.753719281s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:47:53.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec  3 16:47:53.740: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Dec  3 16:47:53.886: INFO: stderr: ""
Dec  3 16:47:53.887: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tmv8o-1fn.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:47:53.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6373" for this suite.
Dec  3 16:47:59.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:48:00.659: INFO: namespace kubectl-6373 deletion completed in 6.753153959s
•SSDec  3 16:48:00.659: INFO: Running AfterSuite actions on all nodes
Dec  3 16:48:00.659: INFO: Running AfterSuite actions on node 1
Dec  3 16:48:00.659: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 7164.266 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Flaked | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h59m58.081792206s
Test Suite Passed
