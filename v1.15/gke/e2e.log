I0219 21:19:46.387887      14 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-365921390
I0219 21:19:46.388421      14 e2e.go:243] Starting e2e run "bd843226-cd44-4346-ba18-8184ae7eabe6" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1582147184 - Will randomize all specs
Will run 215 of 4412 specs

Feb 19 21:19:46.551: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:19:46.554: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 19 21:19:46.572: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 19 21:19:46.602: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 19 21:19:46.602: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Feb 19 21:19:46.602: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 19 21:19:46.614: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'fluentd-gcp-v3.1.1' (0 seconds elapsed)
Feb 19 21:19:46.614: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
Feb 19 21:19:46.614: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Feb 19 21:19:46.614: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'prometheus-to-sd' (0 seconds elapsed)
Feb 19 21:19:46.614: INFO: e2e test version: v1.15.9
Feb 19 21:19:46.616: INFO: kube-apiserver version: v1.15.9-gke.8
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:19:46.616: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
Feb 19 21:19:46.648: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 19 21:19:51.189: INFO: Successfully updated pod "pod-update-5f49044e-8cb1-4374-b272-5e5032bd95e4"
STEP: verifying the updated pod is in kubernetes
Feb 19 21:19:51.195: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:19:51.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-304" for this suite.
Feb 19 21:20:15.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:20:15.360: INFO: namespace pods-304 deletion completed in 24.162409094s

• [SLOW TEST:28.744 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:20:15.361: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:20:15.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11" in namespace "downward-api-2145" to be "success or failure"
Feb 19 21:20:15.480: INFO: Pod "downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11": Phase="Pending", Reason="", readiness=false. Elapsed: 3.491467ms
Feb 19 21:20:17.484: INFO: Pod "downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007590207s
STEP: Saw pod success
Feb 19 21:20:17.484: INFO: Pod "downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11" satisfied condition "success or failure"
Feb 19 21:20:17.486: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11 container client-container: <nil>
STEP: delete the pod
Feb 19 21:20:17.514: INFO: Waiting for pod downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11 to disappear
Feb 19 21:20:17.517: INFO: Pod downwardapi-volume-623d2f48-e74f-45cd-bd53-ccf52c4b2e11 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:20:17.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2145" for this suite.
Feb 19 21:20:23.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:20:23.801: INFO: namespace downward-api-2145 deletion completed in 6.277992941s

• [SLOW TEST:8.440 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:20:23.802: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 21:20:23.924: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 19 21:20:28.927: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 19 21:20:28.927: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 19 21:20:28.971: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5119,SelfLink:/apis/apps/v1/namespaces/deployment-5119/deployments/test-cleanup-deployment,UID:1e6753d2-97d8-4e6f-9a7c-84f9dd4659c8,ResourceVersion:95593,Generation:1,CreationTimestamp:2020-02-19 21:20:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Feb 19 21:20:28.981: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:20:28.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5119" for this suite.
Feb 19 21:20:35.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:20:35.226: INFO: namespace deployment-5119 deletion completed in 6.219993792s

• [SLOW TEST:11.425 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:20:35.228: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 19 21:20:37.336: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:20:37.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8374" for this suite.
Feb 19 21:20:43.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:20:43.524: INFO: namespace container-runtime-8374 deletion completed in 6.167695204s

• [SLOW TEST:8.296 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:20:43.527: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f333208e-06a6-41d2-91a7-3fd72199f7fc
STEP: Creating a pod to test consume configMaps
Feb 19 21:20:43.634: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834" in namespace "projected-7834" to be "success or failure"
Feb 19 21:20:43.640: INFO: Pod "pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834": Phase="Pending", Reason="", readiness=false. Elapsed: 5.716155ms
Feb 19 21:20:45.643: INFO: Pod "pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008396269s
STEP: Saw pod success
Feb 19 21:20:45.643: INFO: Pod "pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834" satisfied condition "success or failure"
Feb 19 21:20:45.645: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:20:45.680: INFO: Waiting for pod pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834 to disappear
Feb 19 21:20:45.686: INFO: Pod pod-projected-configmaps-be1ecd2e-b324-477e-b65a-00dc6ecfb834 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:20:45.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7834" for this suite.
Feb 19 21:20:51.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:20:51.846: INFO: namespace projected-7834 deletion completed in 6.157017391s

• [SLOW TEST:8.320 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:20:51.846: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 21:20:51.895: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 19 21:20:56.957: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 19 21:20:56.957: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 19 21:20:58.960: INFO: Creating deployment "test-rollover-deployment"
Feb 19 21:20:58.970: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 19 21:21:00.979: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 19 21:21:00.986: INFO: Ensure that both replica sets have 1 created replica
Feb 19 21:21:00.991: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 19 21:21:00.999: INFO: Updating deployment test-rollover-deployment
Feb 19 21:21:00.999: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 19 21:21:03.020: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 19 21:21:03.025: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 19 21:21:03.030: INFO: all replica sets need to contain the pod-template-hash label
Feb 19 21:21:03.030: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744061, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744058, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:21:05.039: INFO: all replica sets need to contain the pod-template-hash label
Feb 19 21:21:05.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744058, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:21:07.036: INFO: all replica sets need to contain the pod-template-hash label
Feb 19 21:21:07.036: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744058, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:21:09.036: INFO: all replica sets need to contain the pod-template-hash label
Feb 19 21:21:09.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744058, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:21:11.036: INFO: all replica sets need to contain the pod-template-hash label
Feb 19 21:21:11.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744058, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:21:13.037: INFO: all replica sets need to contain the pod-template-hash label
Feb 19 21:21:13.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744059, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744064, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717744058, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:21:15.036: INFO: 
Feb 19 21:21:15.036: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 19 21:21:15.045: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4679,SelfLink:/apis/apps/v1/namespaces/deployment-4679/deployments/test-rollover-deployment,UID:85187e51-4c19-460e-bc2a-97dcd0f4a8db,ResourceVersion:95892,Generation:2,CreationTimestamp:2020-02-19 21:20:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-19 21:20:59 +0000 UTC 2020-02-19 21:20:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-19 21:21:14 +0000 UTC 2020-02-19 21:20:58 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 19 21:21:15.048: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-4679,SelfLink:/apis/apps/v1/namespaces/deployment-4679/replicasets/test-rollover-deployment-854595fc44,UID:9479b5e5-7a8d-4ffb-b2fa-018b06c77929,ResourceVersion:95883,Generation:2,CreationTimestamp:2020-02-19 21:21:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 85187e51-4c19-460e-bc2a-97dcd0f4a8db 0xc0032967a7 0xc0032967a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 19 21:21:15.048: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 19 21:21:15.048: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4679,SelfLink:/apis/apps/v1/namespaces/deployment-4679/replicasets/test-rollover-controller,UID:438312f0-37d3-45d1-9810-5288cdc0cf57,ResourceVersion:95891,Generation:2,CreationTimestamp:2020-02-19 21:20:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 85187e51-4c19-460e-bc2a-97dcd0f4a8db 0xc0032966d7 0xc0032966d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 19 21:21:15.048: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-4679,SelfLink:/apis/apps/v1/namespaces/deployment-4679/replicasets/test-rollover-deployment-9b8b997cf,UID:41167738-352f-495a-81f0-69bf0563a26d,ResourceVersion:95820,Generation:2,CreationTimestamp:2020-02-19 21:20:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 85187e51-4c19-460e-bc2a-97dcd0f4a8db 0xc003296870 0xc003296871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 19 21:21:15.051: INFO: Pod "test-rollover-deployment-854595fc44-x8l7s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-x8l7s,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-4679,SelfLink:/api/v1/namespaces/deployment-4679/pods/test-rollover-deployment-854595fc44-x8l7s,UID:ca7e7f13-d434-4ef5-bfbb-b550dc23fe88,ResourceVersion:95845,Generation:0,CreationTimestamp:2020-02-19 21:21:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 9479b5e5-7a8d-4ffb-b2fa-018b06c77929 0xc003297457 0xc003297458}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5x5w9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5x5w9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5x5w9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032974c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032974e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:21:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:21:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:21:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:21:01 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:10.56.1.6,StartTime:2020-02-19 21:21:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-19 21:21:03 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1f6f8913090b3053858c54e50310af5ee40c98f19c6807663d6d06bd0ce3a3d6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:21:15.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4679" for this suite.
Feb 19 21:21:21.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:21:21.195: INFO: namespace deployment-4679 deletion completed in 6.141036725s

• [SLOW TEST:29.349 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:21:21.197: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8602
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8602
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8602
Feb 19 21:21:21.360: INFO: Found 0 stateful pods, waiting for 1
Feb 19 21:21:31.363: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 19 21:21:31.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 21:21:31.937: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 21:21:31.937: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 21:21:31.937: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 21:21:31.940: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 19 21:21:42.044: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 21:21:42.044: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 21:21:42.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999798s
Feb 19 21:21:43.136: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990216731s
Feb 19 21:21:44.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987050675s
Feb 19 21:21:45.142: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983455043s
Feb 19 21:21:46.145: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.98023279s
Feb 19 21:21:47.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977324168s
Feb 19 21:21:48.152: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.974036804s
Feb 19 21:21:49.155: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970451317s
Feb 19 21:21:50.158: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967407266s
Feb 19 21:21:51.162: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.28192ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8602
Feb 19 21:21:52.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 21:21:52.417: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 21:21:52.417: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 21:21:52.417: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 21:21:52.421: INFO: Found 1 stateful pods, waiting for 3
Feb 19 21:22:02.424: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 21:22:02.424: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 21:22:02.424: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 19 21:22:02.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 21:22:02.693: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 21:22:02.693: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 21:22:02.693: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 21:22:02.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 21:22:03.045: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 21:22:03.045: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 21:22:03.045: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 21:22:03.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 21:22:03.327: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 21:22:03.327: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 21:22:03.327: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 21:22:03.327: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 21:22:03.330: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 19 21:22:13.336: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 21:22:13.336: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 21:22:13.336: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 21:22:13.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999779s
Feb 19 21:22:14.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993377192s
Feb 19 21:22:15.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989567394s
Feb 19 21:22:16.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985891893s
Feb 19 21:22:17.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977115884s
Feb 19 21:22:18.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973494364s
Feb 19 21:22:19.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.968936248s
Feb 19 21:22:20.387: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964960381s
Feb 19 21:22:21.391: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95777872s
Feb 19 21:22:22.395: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.058644ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8602
Feb 19 21:22:23.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 21:22:23.668: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 21:22:23.668: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 21:22:23.668: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 21:22:23.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 21:22:23.981: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 21:22:23.981: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 21:22:23.981: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 21:22:23.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-8602 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 21:22:24.251: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 21:22:24.251: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 21:22:24.251: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 21:22:24.251: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 19 21:23:04.301: INFO: Deleting all statefulset in ns statefulset-8602
Feb 19 21:23:04.305: INFO: Scaling statefulset ss to 0
Feb 19 21:23:04.316: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 21:23:04.319: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:23:04.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8602" for this suite.
Feb 19 21:23:10.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:23:10.592: INFO: namespace statefulset-8602 deletion completed in 6.23554295s

• [SLOW TEST:109.395 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:23:10.594: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-e6273cfa-efbb-4e02-9c32-e698805bb33d
STEP: Creating a pod to test consume secrets
Feb 19 21:23:10.649: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f" in namespace "projected-5157" to be "success or failure"
Feb 19 21:23:10.654: INFO: Pod "pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80981ms
Feb 19 21:23:12.661: INFO: Pod "pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01196782s
Feb 19 21:23:14.665: INFO: Pod "pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015862019s
STEP: Saw pod success
Feb 19 21:23:14.665: INFO: Pod "pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f" satisfied condition "success or failure"
Feb 19 21:23:14.668: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 19 21:23:14.690: INFO: Waiting for pod pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f to disappear
Feb 19 21:23:14.694: INFO: Pod pod-projected-secrets-ccbf2c2d-e756-4925-ab42-648c94bd729f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:23:14.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5157" for this suite.
Feb 19 21:23:20.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:23:20.838: INFO: namespace projected-5157 deletion completed in 6.14044575s

• [SLOW TEST:10.245 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:23:20.840: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-4937
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4937 to expose endpoints map[]
Feb 19 21:23:20.930: INFO: successfully validated that service endpoint-test2 in namespace services-4937 exposes endpoints map[] (5.390233ms elapsed)
STEP: Creating pod pod1 in namespace services-4937
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4937 to expose endpoints map[pod1:[80]]
Feb 19 21:23:22.980: INFO: successfully validated that service endpoint-test2 in namespace services-4937 exposes endpoints map[pod1:[80]] (2.032271939s elapsed)
STEP: Creating pod pod2 in namespace services-4937
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4937 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 19 21:23:25.026: INFO: successfully validated that service endpoint-test2 in namespace services-4937 exposes endpoints map[pod1:[80] pod2:[80]] (2.032176575s elapsed)
STEP: Deleting pod pod1 in namespace services-4937
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4937 to expose endpoints map[pod2:[80]]
Feb 19 21:23:25.068: INFO: successfully validated that service endpoint-test2 in namespace services-4937 exposes endpoints map[pod2:[80]] (28.165093ms elapsed)
STEP: Deleting pod pod2 in namespace services-4937
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4937 to expose endpoints map[]
Feb 19 21:23:26.157: INFO: successfully validated that service endpoint-test2 in namespace services-4937 exposes endpoints map[] (1.076558358s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:23:26.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4937" for this suite.
Feb 19 21:23:48.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:23:48.395: INFO: namespace services-4937 deletion completed in 22.146254965s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.556 seconds]
[sig-network] Services
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:23:48.399: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-6b843c7c-9e8e-433f-bbe1-05e11a969337
STEP: Creating secret with name s-test-opt-upd-e57582f6-2177-4bae-bb46-c98f8e7db019
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6b843c7c-9e8e-433f-bbe1-05e11a969337
STEP: Updating secret s-test-opt-upd-e57582f6-2177-4bae-bb46-c98f8e7db019
STEP: Creating secret with name s-test-opt-create-052a9702-c74c-4ca0-b6e2-988cfa0d5e63
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:25:21.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8467" for this suite.
Feb 19 21:25:43.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:25:43.445: INFO: namespace projected-8467 deletion completed in 22.176017371s

• [SLOW TEST:115.046 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:25:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Feb 19 21:25:43.533: INFO: Waiting up to 5m0s for pod "client-containers-5bac3825-99c9-4eb9-a191-b248d863a415" in namespace "containers-3021" to be "success or failure"
Feb 19 21:25:43.562: INFO: Pod "client-containers-5bac3825-99c9-4eb9-a191-b248d863a415": Phase="Pending", Reason="", readiness=false. Elapsed: 29.418648ms
Feb 19 21:25:45.565: INFO: Pod "client-containers-5bac3825-99c9-4eb9-a191-b248d863a415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032486479s
STEP: Saw pod success
Feb 19 21:25:45.565: INFO: Pod "client-containers-5bac3825-99c9-4eb9-a191-b248d863a415" satisfied condition "success or failure"
Feb 19 21:25:45.568: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod client-containers-5bac3825-99c9-4eb9-a191-b248d863a415 container test-container: <nil>
STEP: delete the pod
Feb 19 21:25:45.593: INFO: Waiting for pod client-containers-5bac3825-99c9-4eb9-a191-b248d863a415 to disappear
Feb 19 21:25:45.598: INFO: Pod client-containers-5bac3825-99c9-4eb9-a191-b248d863a415 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:25:45.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3021" for this suite.
Feb 19 21:25:51.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:25:51.736: INFO: namespace containers-3021 deletion completed in 6.135335804s

• [SLOW TEST:8.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:25:51.738: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7663/configmap-test-f3020e0d-c4c0-4192-87a1-b6b0c7f30e58
STEP: Creating a pod to test consume configMaps
Feb 19 21:25:51.874: INFO: Waiting up to 5m0s for pod "pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9" in namespace "configmap-7663" to be "success or failure"
Feb 19 21:25:51.881: INFO: Pod "pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.37328ms
Feb 19 21:25:53.884: INFO: Pod "pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009604786s
Feb 19 21:25:55.887: INFO: Pod "pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012743469s
STEP: Saw pod success
Feb 19 21:25:55.887: INFO: Pod "pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9" satisfied condition "success or failure"
Feb 19 21:25:55.890: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9 container env-test: <nil>
STEP: delete the pod
Feb 19 21:25:55.912: INFO: Waiting for pod pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9 to disappear
Feb 19 21:25:55.915: INFO: Pod pod-configmaps-3af20d7f-ecdc-49a1-93f4-6388281edbb9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:25:55.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7663" for this suite.
Feb 19 21:26:01.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:26:02.060: INFO: namespace configmap-7663 deletion completed in 6.142083454s

• [SLOW TEST:10.323 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:26:02.062: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 19 21:26:02.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-5853'
Feb 19 21:26:02.585: INFO: stderr: ""
Feb 19 21:26:02.585: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 19 21:26:02.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5853'
Feb 19 21:26:02.670: INFO: stderr: ""
Feb 19 21:26:02.670: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Feb 19 21:26:07.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5853'
Feb 19 21:26:07.759: INFO: stderr: ""
Feb 19 21:26:07.759: INFO: stdout: "update-demo-nautilus-26dxv update-demo-nautilus-kgpwr "
Feb 19 21:26:07.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-26dxv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:07.844: INFO: stderr: ""
Feb 19 21:26:07.844: INFO: stdout: "true"
Feb 19 21:26:07.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-26dxv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:07.925: INFO: stderr: ""
Feb 19 21:26:07.925: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:26:07.925: INFO: validating pod update-demo-nautilus-26dxv
Feb 19 21:26:07.933: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:26:07.933: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:26:07.933: INFO: update-demo-nautilus-26dxv is verified up and running
Feb 19 21:26:07.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-kgpwr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:08.013: INFO: stderr: ""
Feb 19 21:26:08.013: INFO: stdout: "true"
Feb 19 21:26:08.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-kgpwr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:08.094: INFO: stderr: ""
Feb 19 21:26:08.094: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:26:08.094: INFO: validating pod update-demo-nautilus-kgpwr
Feb 19 21:26:08.102: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:26:08.102: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:26:08.102: INFO: update-demo-nautilus-kgpwr is verified up and running
STEP: scaling down the replication controller
Feb 19 21:26:08.104: INFO: scanned /root for discovery docs: <nil>
Feb 19 21:26:08.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5853'
Feb 19 21:26:09.238: INFO: stderr: ""
Feb 19 21:26:09.238: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 19 21:26:09.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5853'
Feb 19 21:26:09.347: INFO: stderr: ""
Feb 19 21:26:09.347: INFO: stdout: "update-demo-nautilus-26dxv update-demo-nautilus-kgpwr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 19 21:26:14.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5853'
Feb 19 21:26:14.481: INFO: stderr: ""
Feb 19 21:26:14.481: INFO: stdout: "update-demo-nautilus-26dxv "
Feb 19 21:26:14.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-26dxv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:14.585: INFO: stderr: ""
Feb 19 21:26:14.585: INFO: stdout: "true"
Feb 19 21:26:14.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-26dxv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:14.671: INFO: stderr: ""
Feb 19 21:26:14.671: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:26:14.671: INFO: validating pod update-demo-nautilus-26dxv
Feb 19 21:26:14.679: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:26:14.679: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:26:14.679: INFO: update-demo-nautilus-26dxv is verified up and running
STEP: scaling up the replication controller
Feb 19 21:26:14.680: INFO: scanned /root for discovery docs: <nil>
Feb 19 21:26:14.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5853'
Feb 19 21:26:15.796: INFO: stderr: ""
Feb 19 21:26:15.796: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 19 21:26:15.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5853'
Feb 19 21:26:15.904: INFO: stderr: ""
Feb 19 21:26:15.904: INFO: stdout: "update-demo-nautilus-26dxv update-demo-nautilus-2xbnc "
Feb 19 21:26:15.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-26dxv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:15.992: INFO: stderr: ""
Feb 19 21:26:15.992: INFO: stdout: "true"
Feb 19 21:26:15.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-26dxv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:16.080: INFO: stderr: ""
Feb 19 21:26:16.080: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:26:16.080: INFO: validating pod update-demo-nautilus-26dxv
Feb 19 21:26:16.085: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:26:16.085: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:26:16.085: INFO: update-demo-nautilus-26dxv is verified up and running
Feb 19 21:26:16.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-2xbnc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:16.173: INFO: stderr: ""
Feb 19 21:26:16.173: INFO: stdout: "true"
Feb 19 21:26:16.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-2xbnc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5853'
Feb 19 21:26:16.259: INFO: stderr: ""
Feb 19 21:26:16.259: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:26:16.259: INFO: validating pod update-demo-nautilus-2xbnc
Feb 19 21:26:16.267: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:26:16.267: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:26:16.267: INFO: update-demo-nautilus-2xbnc is verified up and running
STEP: using delete to clean up resources
Feb 19 21:26:16.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-5853'
Feb 19 21:26:16.373: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:26:16.373: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 19 21:26:16.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5853'
Feb 19 21:26:16.476: INFO: stderr: "No resources found.\n"
Feb 19 21:26:16.476: INFO: stdout: ""
Feb 19 21:26:16.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -l name=update-demo --namespace=kubectl-5853 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 19 21:26:16.584: INFO: stderr: ""
Feb 19 21:26:16.584: INFO: stdout: "update-demo-nautilus-26dxv\nupdate-demo-nautilus-2xbnc\n"
Feb 19 21:26:17.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5853'
Feb 19 21:26:17.289: INFO: stderr: "No resources found.\n"
Feb 19 21:26:17.289: INFO: stdout: ""
Feb 19 21:26:17.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -l name=update-demo --namespace=kubectl-5853 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 19 21:26:17.432: INFO: stderr: ""
Feb 19 21:26:17.432: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:26:17.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5853" for this suite.
Feb 19 21:26:39.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:26:39.593: INFO: namespace kubectl-5853 deletion completed in 22.157889281s

• [SLOW TEST:37.531 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:26:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f020095e-b50f-4851-989f-fb4f57e9bdb4
STEP: Creating a pod to test consume secrets
Feb 19 21:26:39.643: INFO: Waiting up to 5m0s for pod "pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb" in namespace "secrets-8465" to be "success or failure"
Feb 19 21:26:39.654: INFO: Pod "pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.349911ms
Feb 19 21:26:41.659: INFO: Pod "pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015578165s
STEP: Saw pod success
Feb 19 21:26:41.659: INFO: Pod "pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb" satisfied condition "success or failure"
Feb 19 21:26:41.662: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb container secret-env-test: <nil>
STEP: delete the pod
Feb 19 21:26:41.686: INFO: Waiting for pod pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb to disappear
Feb 19 21:26:41.692: INFO: Pod pod-secrets-22f8afc5-867c-4426-ad72-e685d1b232fb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:26:41.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8465" for this suite.
Feb 19 21:26:47.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:26:47.860: INFO: namespace secrets-8465 deletion completed in 6.16414471s

• [SLOW TEST:8.267 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:26:47.861: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7838
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 19 21:26:47.894: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 19 21:27:14.139: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.56.2.22:8080/dial?request=hostName&protocol=udp&host=10.56.2.21&port=8081&tries=1'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 21:27:14.139: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:27:14.312: INFO: Waiting for endpoints: map[]
Feb 19 21:27:14.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.56.2.22:8080/dial?request=hostName&protocol=udp&host=10.56.0.13&port=8081&tries=1'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 21:27:14.316: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:27:14.605: INFO: Waiting for endpoints: map[]
Feb 19 21:27:14.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.56.2.22:8080/dial?request=hostName&protocol=udp&host=10.56.1.9&port=8081&tries=1'] Namespace:pod-network-test-7838 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 21:27:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:27:14.853: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:27:14.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7838" for this suite.
Feb 19 21:27:36.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:27:37.065: INFO: namespace pod-network-test-7838 deletion completed in 22.207897613s

• [SLOW TEST:49.204 seconds]
[sig-network] Networking
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:27:37.067: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:28:01.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2239" for this suite.
Feb 19 21:28:07.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:28:07.390: INFO: namespace namespaces-2239 deletion completed in 6.143206672s
STEP: Destroying namespace "nsdeletetest-4710" for this suite.
Feb 19 21:28:07.394: INFO: Namespace nsdeletetest-4710 was already deleted
STEP: Destroying namespace "nsdeletetest-3661" for this suite.
Feb 19 21:28:13.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:28:13.546: INFO: namespace nsdeletetest-3661 deletion completed in 6.150851264s

• [SLOW TEST:36.479 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:28:13.556: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 21:28:13.723: INFO: (0) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 127.033644ms)
Feb 19 21:28:13.728: INFO: (1) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.60695ms)
Feb 19 21:28:13.734: INFO: (2) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.099421ms)
Feb 19 21:28:13.742: INFO: (3) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 8.009406ms)
Feb 19 21:28:13.755: INFO: (4) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 12.408558ms)
Feb 19 21:28:13.766: INFO: (5) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 11.509753ms)
Feb 19 21:28:13.773: INFO: (6) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.726448ms)
Feb 19 21:28:13.779: INFO: (7) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.013284ms)
Feb 19 21:28:13.789: INFO: (8) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 9.205241ms)
Feb 19 21:28:13.794: INFO: (9) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.666572ms)
Feb 19 21:28:13.801: INFO: (10) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.639322ms)
Feb 19 21:28:13.807: INFO: (11) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.188811ms)
Feb 19 21:28:13.812: INFO: (12) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.980653ms)
Feb 19 21:28:13.818: INFO: (13) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.340329ms)
Feb 19 21:28:13.822: INFO: (14) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.534837ms)
Feb 19 21:28:13.828: INFO: (15) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.183457ms)
Feb 19 21:28:13.833: INFO: (16) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.37302ms)
Feb 19 21:28:13.839: INFO: (17) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.826395ms)
Feb 19 21:28:13.844: INFO: (18) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.144415ms)
Feb 19 21:28:13.851: INFO: (19) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.482296ms)
[AfterEach] version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:28:13.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3798" for this suite.
Feb 19 21:28:19.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:28:20.018: INFO: namespace proxy-3798 deletion completed in 6.160992969s

• [SLOW TEST:6.462 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:28:20.019: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e26b6888-70e4-40c1-a46f-8cd1f86a6823
STEP: Creating a pod to test consume secrets
Feb 19 21:28:20.087: INFO: Waiting up to 5m0s for pod "pod-secrets-80b340cf-031f-411c-80df-08501a11495a" in namespace "secrets-9911" to be "success or failure"
Feb 19 21:28:20.091: INFO: Pod "pod-secrets-80b340cf-031f-411c-80df-08501a11495a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.857146ms
Feb 19 21:28:22.095: INFO: Pod "pod-secrets-80b340cf-031f-411c-80df-08501a11495a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007826852s
STEP: Saw pod success
Feb 19 21:28:22.095: INFO: Pod "pod-secrets-80b340cf-031f-411c-80df-08501a11495a" satisfied condition "success or failure"
Feb 19 21:28:22.098: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-80b340cf-031f-411c-80df-08501a11495a container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 21:28:22.118: INFO: Waiting for pod pod-secrets-80b340cf-031f-411c-80df-08501a11495a to disappear
Feb 19 21:28:22.122: INFO: Pod pod-secrets-80b340cf-031f-411c-80df-08501a11495a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:28:22.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9911" for this suite.
Feb 19 21:28:28.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:28:28.276: INFO: namespace secrets-9911 deletion completed in 6.151478041s

• [SLOW TEST:8.257 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:28:28.278: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-66e60df9-756f-45d4-9905-99f1dd02bf67
STEP: Creating a pod to test consume configMaps
Feb 19 21:28:28.397: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43" in namespace "projected-3139" to be "success or failure"
Feb 19 21:28:28.415: INFO: Pod "pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43": Phase="Pending", Reason="", readiness=false. Elapsed: 18.354658ms
Feb 19 21:28:30.418: INFO: Pod "pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021528315s
STEP: Saw pod success
Feb 19 21:28:30.418: INFO: Pod "pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43" satisfied condition "success or failure"
Feb 19 21:28:30.421: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:28:30.450: INFO: Waiting for pod pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43 to disappear
Feb 19 21:28:30.454: INFO: Pod pod-projected-configmaps-a70e2f15-7132-4fd8-98f8-ffd5f3e8cf43 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:28:30.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3139" for this suite.
Feb 19 21:28:36.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:28:36.619: INFO: namespace projected-3139 deletion completed in 6.16177364s

• [SLOW TEST:8.341 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:28:36.623: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8047.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8047.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8047.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8047.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8047.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 108.3.60.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.60.3.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.3.60.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.60.3.108_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8047.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8047.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8047.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8047.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8047.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8047.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8047.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 108.3.60.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.60.3.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.3.60.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.60.3.108_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 19 21:28:50.862: INFO: DNS probes using dns-8047/dns-test-8af0a026-89c8-421c-9fe3-42c0f4af5b48 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:28:50.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8047" for this suite.
Feb 19 21:28:57.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:28:57.319: INFO: namespace dns-8047 deletion completed in 6.332334692s

• [SLOW TEST:20.697 seconds]
[sig-network] DNS
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:28:57.322: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:28:57.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b" in namespace "projected-379" to be "success or failure"
Feb 19 21:28:57.507: INFO: Pod "downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.737515ms
Feb 19 21:28:59.510: INFO: Pod "downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017872072s
STEP: Saw pod success
Feb 19 21:28:59.510: INFO: Pod "downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b" satisfied condition "success or failure"
Feb 19 21:28:59.513: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b container client-container: <nil>
STEP: delete the pod
Feb 19 21:28:59.531: INFO: Waiting for pod downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b to disappear
Feb 19 21:28:59.535: INFO: Pod downwardapi-volume-70a9f87f-999a-4ef6-a162-7799c605835b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:28:59.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-379" for this suite.
Feb 19 21:29:05.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:29:05.694: INFO: namespace projected-379 deletion completed in 6.156136132s

• [SLOW TEST:8.373 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:29:05.699: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-j24t
STEP: Creating a pod to test atomic-volume-subpath
Feb 19 21:29:05.756: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-j24t" in namespace "subpath-2038" to be "success or failure"
Feb 19 21:29:05.760: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.499927ms
Feb 19 21:29:07.766: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 2.009920613s
Feb 19 21:29:09.853: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 4.097248726s
Feb 19 21:29:11.856: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 6.100303405s
Feb 19 21:29:13.859: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 8.103179475s
Feb 19 21:29:15.862: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 10.106239168s
Feb 19 21:29:17.866: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 12.109620419s
Feb 19 21:29:19.870: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 14.113637752s
Feb 19 21:29:21.873: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 16.116646844s
Feb 19 21:29:23.876: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 18.119727134s
Feb 19 21:29:25.880: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Running", Reason="", readiness=true. Elapsed: 20.124177017s
Feb 19 21:29:27.883: INFO: Pod "pod-subpath-test-secret-j24t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.127541748s
STEP: Saw pod success
Feb 19 21:29:27.884: INFO: Pod "pod-subpath-test-secret-j24t" satisfied condition "success or failure"
Feb 19 21:29:27.886: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-subpath-test-secret-j24t container test-container-subpath-secret-j24t: <nil>
STEP: delete the pod
Feb 19 21:29:27.908: INFO: Waiting for pod pod-subpath-test-secret-j24t to disappear
Feb 19 21:29:27.913: INFO: Pod pod-subpath-test-secret-j24t no longer exists
STEP: Deleting pod pod-subpath-test-secret-j24t
Feb 19 21:29:27.913: INFO: Deleting pod "pod-subpath-test-secret-j24t" in namespace "subpath-2038"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:29:27.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2038" for this suite.
Feb 19 21:29:33.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:29:34.065: INFO: namespace subpath-2038 deletion completed in 6.145355306s

• [SLOW TEST:28.367 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:29:34.071: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Feb 19 21:29:34.116: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2510" to be "success or failure"
Feb 19 21:29:34.119: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.38746ms
Feb 19 21:29:36.123: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006500584s
Feb 19 21:29:38.126: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0095174s
STEP: Saw pod success
Feb 19 21:29:38.126: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 19 21:29:38.128: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-vhvp pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 19 21:29:38.155: INFO: Waiting for pod pod-host-path-test to disappear
Feb 19 21:29:38.158: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:29:38.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2510" for this suite.
Feb 19 21:29:44.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:29:44.330: INFO: namespace hostpath-2510 deletion completed in 6.168634782s

• [SLOW TEST:10.260 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:29:44.333: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8
Feb 19 21:29:44.410: INFO: Pod name my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8: Found 0 pods out of 1
Feb 19 21:29:49.417: INFO: Pod name my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8: Found 1 pods out of 1
Feb 19 21:29:49.417: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8" are running
Feb 19 21:29:49.420: INFO: Pod "my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8-c9l8m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 21:29:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 21:29:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 21:29:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 21:29:44 +0000 UTC Reason: Message:}])
Feb 19 21:29:49.420: INFO: Trying to dial the pod
Feb 19 21:29:54.438: INFO: Controller my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8: Got expected result from replica 1 [my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8-c9l8m]: "my-hostname-basic-a5d88c96-f5b6-4c0f-9afc-8bdc06a276b8-c9l8m", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:29:54.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5178" for this suite.
Feb 19 21:30:00.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:30:00.625: INFO: namespace replication-controller-5178 deletion completed in 6.18321799s

• [SLOW TEST:16.293 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:30:00.628: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9976
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9976 to expose endpoints map[]
Feb 19 21:30:00.711: INFO: successfully validated that service multi-endpoint-test in namespace services-9976 exposes endpoints map[] (8.774173ms elapsed)
STEP: Creating pod pod1 in namespace services-9976
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9976 to expose endpoints map[pod1:[100]]
Feb 19 21:30:02.760: INFO: successfully validated that service multi-endpoint-test in namespace services-9976 exposes endpoints map[pod1:[100]] (2.02639423s elapsed)
STEP: Creating pod pod2 in namespace services-9976
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9976 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 19 21:30:05.818: INFO: successfully validated that service multi-endpoint-test in namespace services-9976 exposes endpoints map[pod1:[100] pod2:[101]] (3.039043924s elapsed)
STEP: Deleting pod pod1 in namespace services-9976
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9976 to expose endpoints map[pod2:[101]]
Feb 19 21:30:05.844: INFO: successfully validated that service multi-endpoint-test in namespace services-9976 exposes endpoints map[pod2:[101]] (17.774558ms elapsed)
STEP: Deleting pod pod2 in namespace services-9976
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9976 to expose endpoints map[]
Feb 19 21:30:06.856: INFO: successfully validated that service multi-endpoint-test in namespace services-9976 exposes endpoints map[] (1.006044108s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:30:06.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9976" for this suite.
Feb 19 21:30:30.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:30:31.069: INFO: namespace services-9976 deletion completed in 24.183845754s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:30.442 seconds]
[sig-network] Services
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:30:31.072: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Feb 19 21:30:31.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-4852'
Feb 19 21:30:31.416: INFO: stderr: ""
Feb 19 21:30:31.416: INFO: stdout: "pod/pause created\n"
Feb 19 21:30:31.416: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 19 21:30:31.416: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4852" to be "running and ready"
Feb 19 21:30:31.424: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.277352ms
Feb 19 21:30:33.428: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012040891s
Feb 19 21:30:35.432: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015746149s
Feb 19 21:30:35.432: INFO: Pod "pause" satisfied condition "running and ready"
Feb 19 21:30:35.432: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 19 21:30:35.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 label pods pause testing-label=testing-label-value --namespace=kubectl-4852'
Feb 19 21:30:35.536: INFO: stderr: ""
Feb 19 21:30:35.536: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 19 21:30:35.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pod pause -L testing-label --namespace=kubectl-4852'
Feb 19 21:30:35.639: INFO: stderr: ""
Feb 19 21:30:35.639: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 19 21:30:35.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 label pods pause testing-label- --namespace=kubectl-4852'
Feb 19 21:30:35.728: INFO: stderr: ""
Feb 19 21:30:35.729: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 19 21:30:35.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pod pause -L testing-label --namespace=kubectl-4852'
Feb 19 21:30:35.810: INFO: stderr: ""
Feb 19 21:30:35.810: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Feb 19 21:30:35.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-4852'
Feb 19 21:30:35.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:30:35.903: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 19 21:30:35.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get rc,svc -l name=pause --no-headers --namespace=kubectl-4852'
Feb 19 21:30:35.999: INFO: stderr: "No resources found.\n"
Feb 19 21:30:35.999: INFO: stdout: ""
Feb 19 21:30:35.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -l name=pause --namespace=kubectl-4852 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 19 21:30:36.125: INFO: stderr: ""
Feb 19 21:30:36.125: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:30:36.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4852" for this suite.
Feb 19 21:30:42.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:30:42.298: INFO: namespace kubectl-4852 deletion completed in 6.169140385s

• [SLOW TEST:11.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:30:42.300: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 19 21:30:42.465: INFO: Waiting up to 5m0s for pod "pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2" in namespace "emptydir-3996" to be "success or failure"
Feb 19 21:30:42.483: INFO: Pod "pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.456022ms
Feb 19 21:30:44.489: INFO: Pod "pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023758237s
Feb 19 21:30:46.492: INFO: Pod "pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02672183s
STEP: Saw pod success
Feb 19 21:30:46.492: INFO: Pod "pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2" satisfied condition "success or failure"
Feb 19 21:30:46.495: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2 container test-container: <nil>
STEP: delete the pod
Feb 19 21:30:46.516: INFO: Waiting for pod pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2 to disappear
Feb 19 21:30:46.519: INFO: Pod pod-0f5ef439-7e97-42d4-b15e-dadb6fefacb2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:30:46.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3996" for this suite.
Feb 19 21:30:52.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:30:52.694: INFO: namespace emptydir-3996 deletion completed in 6.171145369s

• [SLOW TEST:10.394 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:30:52.696: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 19 21:30:52.753: INFO: Waiting up to 5m0s for pod "downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344" in namespace "downward-api-8109" to be "success or failure"
Feb 19 21:30:52.757: INFO: Pod "downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294272ms
Feb 19 21:30:54.845: INFO: Pod "downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.091389997s
STEP: Saw pod success
Feb 19 21:30:54.845: INFO: Pod "downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344" satisfied condition "success or failure"
Feb 19 21:30:54.952: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344 container dapi-container: <nil>
STEP: delete the pod
Feb 19 21:30:55.058: INFO: Waiting for pod downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344 to disappear
Feb 19 21:30:55.069: INFO: Pod downward-api-f7ee9e06-a6f1-4cda-83f7-d0cfae8b2344 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:30:55.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8109" for this suite.
Feb 19 21:31:01.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:31:01.251: INFO: namespace downward-api-8109 deletion completed in 6.171107783s

• [SLOW TEST:8.555 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:31:01.257: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 19 21:31:01.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98859,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 19 21:31:01.313: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98859,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 19 21:31:11.320: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98902,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 19 21:31:11.320: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98902,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 19 21:31:21.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98939,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 19 21:31:21.327: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98939,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 19 21:31:31.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98978,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 19 21:31:31.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-a,UID:3e23ceea-7ea5-4566-9344-bbfd3b88de39,ResourceVersion:98978,Generation:0,CreationTimestamp:2020-02-19 21:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 19 21:31:41.336: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-b,UID:1525ea13-7a4a-4c16-8f8a-bb0f7403180c,ResourceVersion:99018,Generation:0,CreationTimestamp:2020-02-19 21:31:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 19 21:31:41.336: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-b,UID:1525ea13-7a4a-4c16-8f8a-bb0f7403180c,ResourceVersion:99018,Generation:0,CreationTimestamp:2020-02-19 21:31:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 19 21:31:51.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-b,UID:1525ea13-7a4a-4c16-8f8a-bb0f7403180c,ResourceVersion:99057,Generation:0,CreationTimestamp:2020-02-19 21:31:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 19 21:31:51.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8838,SelfLink:/api/v1/namespaces/watch-8838/configmaps/e2e-watch-test-configmap-b,UID:1525ea13-7a4a-4c16-8f8a-bb0f7403180c,ResourceVersion:99057,Generation:0,CreationTimestamp:2020-02-19 21:31:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:32:01.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8838" for this suite.
Feb 19 21:32:07.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:32:07.486: INFO: namespace watch-8838 deletion completed in 6.140931034s

• [SLOW TEST:66.229 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:32:07.492: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Feb 19 21:32:08.049: INFO: created pod pod-service-account-defaultsa
Feb 19 21:32:08.049: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 19 21:32:08.076: INFO: created pod pod-service-account-mountsa
Feb 19 21:32:08.076: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 19 21:32:08.118: INFO: created pod pod-service-account-nomountsa
Feb 19 21:32:08.118: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 19 21:32:08.154: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 19 21:32:08.154: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 19 21:32:08.204: INFO: created pod pod-service-account-mountsa-mountspec
Feb 19 21:32:08.204: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 19 21:32:08.226: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 19 21:32:08.226: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 19 21:32:08.260: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 19 21:32:08.260: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 19 21:32:08.285: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 19 21:32:08.285: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 19 21:32:08.306: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 19 21:32:08.306: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:32:08.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1831" for this suite.
Feb 19 21:32:14.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:32:14.483: INFO: namespace svcaccounts-1831 deletion completed in 6.170350326s

• [SLOW TEST:6.992 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:32:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-c8057698-36b3-4a70-bb07-d662340ab8de in namespace container-probe-9864
Feb 19 21:32:16.574: INFO: Started pod test-webserver-c8057698-36b3-4a70-bb07-d662340ab8de in namespace container-probe-9864
STEP: checking the pod's current state and verifying that restartCount is present
Feb 19 21:32:16.578: INFO: Initial restart count of pod test-webserver-c8057698-36b3-4a70-bb07-d662340ab8de is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:36:17.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9864" for this suite.
Feb 19 21:36:23.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:36:23.653: INFO: namespace container-probe-9864 deletion completed in 6.154678435s

• [SLOW TEST:249.168 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:36:23.657: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-46
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-46
STEP: Deleting pre-stop pod
Feb 19 21:36:34.829: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:36:34.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-46" for this suite.
Feb 19 21:37:14.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:37:14.995: INFO: namespace prestop-46 deletion completed in 40.151073162s

• [SLOW TEST:51.338 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:37:15.001: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 19 21:37:15.049: INFO: Waiting up to 5m0s for pod "downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad" in namespace "downward-api-9257" to be "success or failure"
Feb 19 21:37:15.052: INFO: Pod "downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536728ms
Feb 19 21:37:17.057: INFO: Pod "downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008168783s
STEP: Saw pod success
Feb 19 21:37:17.058: INFO: Pod "downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad" satisfied condition "success or failure"
Feb 19 21:37:17.060: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad container dapi-container: <nil>
STEP: delete the pod
Feb 19 21:37:17.088: INFO: Waiting for pod downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad to disappear
Feb 19 21:37:17.092: INFO: Pod downward-api-abf6b304-c50b-4bec-9ad3-8d17e08199ad no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:37:17.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9257" for this suite.
Feb 19 21:37:23.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:37:23.269: INFO: namespace downward-api-9257 deletion completed in 6.173585906s

• [SLOW TEST:8.268 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:37:23.273: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Feb 19 21:37:23.319: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-365921390 proxy --unix-socket=/tmp/kubectl-proxy-unix032013557/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:37:23.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6238" for this suite.
Feb 19 21:37:29.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:37:29.535: INFO: namespace kubectl-6238 deletion completed in 6.144735857s

• [SLOW TEST:6.262 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:37:29.537: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-596ffbda-6c40-4d32-bd0e-43f7bba7f94e
STEP: Creating a pod to test consume configMaps
Feb 19 21:37:29.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174" in namespace "configmap-9146" to be "success or failure"
Feb 19 21:37:29.616: INFO: Pod "pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174": Phase="Pending", Reason="", readiness=false. Elapsed: 3.831936ms
Feb 19 21:37:31.619: INFO: Pod "pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007194535s
STEP: Saw pod success
Feb 19 21:37:31.619: INFO: Pod "pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174" satisfied condition "success or failure"
Feb 19 21:37:31.622: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:37:31.645: INFO: Waiting for pod pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174 to disappear
Feb 19 21:37:31.650: INFO: Pod pod-configmaps-28d04926-e81a-4cf9-9d99-5bb90fccd174 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:37:31.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9146" for this suite.
Feb 19 21:37:37.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:37:37.796: INFO: namespace configmap-9146 deletion completed in 6.142019251s

• [SLOW TEST:8.259 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:37:37.801: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0219 21:37:47.994094      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 19 21:37:47.994: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:37:47.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7185" for this suite.
Feb 19 21:37:54.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:37:54.153: INFO: namespace gc-7185 deletion completed in 6.156201636s

• [SLOW TEST:16.353 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:37:54.157: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 19 21:37:58.269: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 19 21:37:58.273: INFO: Pod pod-with-prestop-http-hook still exists
Feb 19 21:38:00.273: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 19 21:38:00.290: INFO: Pod pod-with-prestop-http-hook still exists
Feb 19 21:38:02.273: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 19 21:38:02.277: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:38:02.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3180" for this suite.
Feb 19 21:38:24.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:38:24.455: INFO: namespace container-lifecycle-hook-3180 deletion completed in 22.167846233s

• [SLOW TEST:30.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:38:24.457: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Feb 19 21:38:24.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1955'
Feb 19 21:38:24.999: INFO: stderr: ""
Feb 19 21:38:24.999: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Feb 19 21:38:26.002: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:38:26.002: INFO: Found 0 / 1
Feb 19 21:38:27.003: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:38:27.003: INFO: Found 0 / 1
Feb 19 21:38:28.002: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:38:28.002: INFO: Found 1 / 1
Feb 19 21:38:28.002: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 19 21:38:28.008: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:38:28.008: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 19 21:38:28.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-gb9z6 redis-master --namespace=kubectl-1955'
Feb 19 21:38:28.113: INFO: stderr: ""
Feb 19 21:38:28.113: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Feb 21:38:27.043 # Server started, Redis version 3.2.12\n1:M 19 Feb 21:38:27.043 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Feb 21:38:27.043 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 19 21:38:28.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-gb9z6 redis-master --namespace=kubectl-1955 --tail=1'
Feb 19 21:38:28.205: INFO: stderr: ""
Feb 19 21:38:28.205: INFO: stdout: "1:M 19 Feb 21:38:27.043 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 19 21:38:28.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-gb9z6 redis-master --namespace=kubectl-1955 --limit-bytes=1'
Feb 19 21:38:28.298: INFO: stderr: ""
Feb 19 21:38:28.298: INFO: stdout: " "
STEP: exposing timestamps
Feb 19 21:38:28.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-gb9z6 redis-master --namespace=kubectl-1955 --tail=1 --timestamps'
Feb 19 21:38:28.392: INFO: stderr: ""
Feb 19 21:38:28.392: INFO: stdout: "2020-02-19T21:38:27.044106039Z 1:M 19 Feb 21:38:27.043 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 19 21:38:30.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-gb9z6 redis-master --namespace=kubectl-1955 --since=1s'
Feb 19 21:38:31.015: INFO: stderr: ""
Feb 19 21:38:31.015: INFO: stdout: ""
Feb 19 21:38:31.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-gb9z6 redis-master --namespace=kubectl-1955 --since=24h'
Feb 19 21:38:31.114: INFO: stderr: ""
Feb 19 21:38:31.114: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Feb 21:38:27.043 # Server started, Redis version 3.2.12\n1:M 19 Feb 21:38:27.043 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Feb 21:38:27.043 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Feb 19 21:38:31.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1955'
Feb 19 21:38:31.206: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:38:31.206: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 19 21:38:31.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get rc,svc -l name=nginx --no-headers --namespace=kubectl-1955'
Feb 19 21:38:31.341: INFO: stderr: "No resources found.\n"
Feb 19 21:38:31.341: INFO: stdout: ""
Feb 19 21:38:31.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -l name=nginx --namespace=kubectl-1955 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 19 21:38:31.429: INFO: stderr: ""
Feb 19 21:38:31.429: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:38:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1955" for this suite.
Feb 19 21:38:37.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:38:37.567: INFO: namespace kubectl-1955 deletion completed in 6.135062442s

• [SLOW TEST:13.110 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:38:37.570: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:38:37.612: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea" in namespace "downward-api-4534" to be "success or failure"
Feb 19 21:38:37.622: INFO: Pod "downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea": Phase="Pending", Reason="", readiness=false. Elapsed: 9.5835ms
Feb 19 21:38:39.625: INFO: Pod "downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012726767s
STEP: Saw pod success
Feb 19 21:38:39.625: INFO: Pod "downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea" satisfied condition "success or failure"
Feb 19 21:38:39.628: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea container client-container: <nil>
STEP: delete the pod
Feb 19 21:38:39.652: INFO: Waiting for pod downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea to disappear
Feb 19 21:38:39.656: INFO: Pod downwardapi-volume-d732cc1a-d8bf-4ff9-a8bb-fd3cd20638ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:38:39.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4534" for this suite.
Feb 19 21:38:45.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:38:45.803: INFO: namespace downward-api-4534 deletion completed in 6.142615751s

• [SLOW TEST:8.234 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:38:45.808: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ce3f9b76-42f7-4a77-b7aa-b7165f16ef6b in namespace container-probe-1893
Feb 19 21:38:47.910: INFO: Started pod busybox-ce3f9b76-42f7-4a77-b7aa-b7165f16ef6b in namespace container-probe-1893
STEP: checking the pod's current state and verifying that restartCount is present
Feb 19 21:38:47.914: INFO: Initial restart count of pod busybox-ce3f9b76-42f7-4a77-b7aa-b7165f16ef6b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:42:48.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1893" for this suite.
Feb 19 21:42:54.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:42:55.402: INFO: namespace container-probe-1893 deletion completed in 6.69719891s

• [SLOW TEST:249.594 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:42:55.404: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Feb 19 21:42:55.612: INFO: Waiting up to 5m0s for pod "client-containers-b017f722-84c4-4b90-a23a-9952a204840d" in namespace "containers-7326" to be "success or failure"
Feb 19 21:42:55.621: INFO: Pod "client-containers-b017f722-84c4-4b90-a23a-9952a204840d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.366655ms
Feb 19 21:42:57.628: INFO: Pod "client-containers-b017f722-84c4-4b90-a23a-9952a204840d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015088097s
STEP: Saw pod success
Feb 19 21:42:57.628: INFO: Pod "client-containers-b017f722-84c4-4b90-a23a-9952a204840d" satisfied condition "success or failure"
Feb 19 21:42:57.630: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod client-containers-b017f722-84c4-4b90-a23a-9952a204840d container test-container: <nil>
STEP: delete the pod
Feb 19 21:42:57.670: INFO: Waiting for pod client-containers-b017f722-84c4-4b90-a23a-9952a204840d to disappear
Feb 19 21:42:57.680: INFO: Pod client-containers-b017f722-84c4-4b90-a23a-9952a204840d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:42:57.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7326" for this suite.
Feb 19 21:43:03.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:03.837: INFO: namespace containers-7326 deletion completed in 6.139317132s

• [SLOW TEST:8.433 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:03.840: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-724/configmap-test-16d3872d-4c75-4054-b6c9-6141a41b38f2
STEP: Creating a pod to test consume configMaps
Feb 19 21:43:03.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548" in namespace "configmap-724" to be "success or failure"
Feb 19 21:43:03.891: INFO: Pod "pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340154ms
Feb 19 21:43:05.895: INFO: Pod "pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005896671s
STEP: Saw pod success
Feb 19 21:43:05.895: INFO: Pod "pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548" satisfied condition "success or failure"
Feb 19 21:43:05.897: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548 container env-test: <nil>
STEP: delete the pod
Feb 19 21:43:05.917: INFO: Waiting for pod pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548 to disappear
Feb 19 21:43:05.920: INFO: Pod pod-configmaps-0db6b9ee-ac71-486a-9fa3-ba83ff230548 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:43:05.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-724" for this suite.
Feb 19 21:43:12.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:12.637: INFO: namespace configmap-724 deletion completed in 6.713429903s

• [SLOW TEST:8.797 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:12.638: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 19 21:43:12.727: INFO: Waiting up to 5m0s for pod "pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9" in namespace "emptydir-6869" to be "success or failure"
Feb 19 21:43:12.747: INFO: Pod "pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.388088ms
Feb 19 21:43:14.757: INFO: Pod "pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030550327s
STEP: Saw pod success
Feb 19 21:43:14.758: INFO: Pod "pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9" satisfied condition "success or failure"
Feb 19 21:43:14.761: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9 container test-container: <nil>
STEP: delete the pod
Feb 19 21:43:14.817: INFO: Waiting for pod pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9 to disappear
Feb 19 21:43:14.821: INFO: Pod pod-6a4f0cd5-70f9-4c5d-8c23-431f05d75fd9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:43:14.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6869" for this suite.
Feb 19 21:43:20.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:20.961: INFO: namespace emptydir-6869 deletion completed in 6.136538712s

• [SLOW TEST:8.324 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:20.964: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 19 21:43:20.993: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:43:23.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2008" for this suite.
Feb 19 21:43:29.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:29.613: INFO: namespace init-container-2008 deletion completed in 6.279682087s

• [SLOW TEST:8.649 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:29.618: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:43:29.666: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c" in namespace "downward-api-198" to be "success or failure"
Feb 19 21:43:29.676: INFO: Pod "downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.799569ms
Feb 19 21:43:31.680: INFO: Pod "downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013133602s
Feb 19 21:43:33.683: INFO: Pod "downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016109641s
STEP: Saw pod success
Feb 19 21:43:33.683: INFO: Pod "downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c" satisfied condition "success or failure"
Feb 19 21:43:33.685: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c container client-container: <nil>
STEP: delete the pod
Feb 19 21:43:33.715: INFO: Waiting for pod downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c to disappear
Feb 19 21:43:33.718: INFO: Pod downwardapi-volume-4ceb0929-5bd5-4337-b11b-fba28fcd9e2c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:43:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-198" for this suite.
Feb 19 21:43:39.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:40.421: INFO: namespace downward-api-198 deletion completed in 6.698466659s

• [SLOW TEST:10.803 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-3a6d4c5c-8199-4dca-8e5a-64b063014504
STEP: Creating a pod to test consume configMaps
Feb 19 21:43:40.564: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058" in namespace "configmap-5564" to be "success or failure"
Feb 19 21:43:40.578: INFO: Pod "pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058": Phase="Pending", Reason="", readiness=false. Elapsed: 13.35886ms
Feb 19 21:43:42.583: INFO: Pod "pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018961572s
STEP: Saw pod success
Feb 19 21:43:42.584: INFO: Pod "pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058" satisfied condition "success or failure"
Feb 19 21:43:42.588: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:43:42.638: INFO: Waiting for pod pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058 to disappear
Feb 19 21:43:42.649: INFO: Pod pod-configmaps-a4b44709-9acf-46c3-9a7c-353999a83058 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:43:42.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5564" for this suite.
Feb 19 21:43:48.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:48.816: INFO: namespace configmap-5564 deletion completed in 6.159240068s

• [SLOW TEST:8.394 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 19 21:43:48.911: INFO: Waiting up to 5m0s for pod "pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454" in namespace "emptydir-9990" to be "success or failure"
Feb 19 21:43:48.959: INFO: Pod "pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454": Phase="Pending", Reason="", readiness=false. Elapsed: 48.492193ms
Feb 19 21:43:50.966: INFO: Pod "pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054952454s
STEP: Saw pod success
Feb 19 21:43:50.966: INFO: Pod "pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454" satisfied condition "success or failure"
Feb 19 21:43:50.969: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454 container test-container: <nil>
STEP: delete the pod
Feb 19 21:43:50.987: INFO: Waiting for pod pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454 to disappear
Feb 19 21:43:50.991: INFO: Pod pod-1c147f46-e4f3-4260-8ccd-e8b0dee5d454 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:43:50.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9990" for this suite.
Feb 19 21:43:57.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:43:57.346: INFO: namespace emptydir-9990 deletion completed in 6.351673087s

• [SLOW TEST:8.529 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:43:57.346: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 19 21:44:05.536: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:05.542: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:07.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:07.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:09.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:09.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:11.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:11.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:13.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:13.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:15.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:15.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:17.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:17.553: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:19.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:19.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:21.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:21.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:23.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:23.545: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 19 21:44:25.542: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 19 21:44:25.547: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:44:25.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3215" for this suite.
Feb 19 21:44:41.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:44:41.718: INFO: namespace container-lifecycle-hook-3215 deletion completed in 16.165410679s

• [SLOW TEST:44.373 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:44:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3546/secret-test-a423d454-7762-4ec6-9d9e-abd2bc4433ec
STEP: Creating a pod to test consume secrets
Feb 19 21:44:41.782: INFO: Waiting up to 5m0s for pod "pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347" in namespace "secrets-3546" to be "success or failure"
Feb 19 21:44:41.788: INFO: Pod "pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347": Phase="Pending", Reason="", readiness=false. Elapsed: 5.655521ms
Feb 19 21:44:43.790: INFO: Pod "pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008452769s
Feb 19 21:44:45.793: INFO: Pod "pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011412389s
STEP: Saw pod success
Feb 19 21:44:45.793: INFO: Pod "pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347" satisfied condition "success or failure"
Feb 19 21:44:45.796: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347 container env-test: <nil>
STEP: delete the pod
Feb 19 21:44:45.814: INFO: Waiting for pod pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347 to disappear
Feb 19 21:44:45.817: INFO: Pod pod-configmaps-91b42bd2-9eb4-4577-b885-60d51c456347 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:44:45.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3546" for this suite.
Feb 19 21:44:51.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:44:51.958: INFO: namespace secrets-3546 deletion completed in 6.137946383s

• [SLOW TEST:10.237 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:44:51.961: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 19 21:44:58.066: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:44:58.068: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:00.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:00.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:02.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:02.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:04.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:04.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:06.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:06.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:08.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:08.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:10.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:10.149: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:12.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:12.135: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:14.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:14.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:16.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:16.071: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:18.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:18.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:20.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:20.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:22.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:22.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:24.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:24.072: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 19 21:45:26.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 19 21:45:26.072: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:45:26.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4270" for this suite.
Feb 19 21:45:48.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:45:48.237: INFO: namespace container-lifecycle-hook-4270 deletion completed in 22.153422636s

• [SLOW TEST:56.277 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:45:48.240: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-defa4f12-8d42-4039-9928-f8625ba3961b
STEP: Creating a pod to test consume configMaps
Feb 19 21:45:48.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7" in namespace "projected-2803" to be "success or failure"
Feb 19 21:45:48.311: INFO: Pod "pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7": Phase="Pending", Reason="", readiness=false. Elapsed: 15.357889ms
Feb 19 21:45:50.314: INFO: Pod "pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018332996s
Feb 19 21:45:52.317: INFO: Pod "pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021206827s
STEP: Saw pod success
Feb 19 21:45:52.317: INFO: Pod "pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7" satisfied condition "success or failure"
Feb 19 21:45:52.319: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:45:52.338: INFO: Waiting for pod pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7 to disappear
Feb 19 21:45:52.342: INFO: Pod pod-projected-configmaps-c084efe2-2078-4653-8f05-f8433d6383b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:45:52.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2803" for this suite.
Feb 19 21:45:58.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:45:58.501: INFO: namespace projected-2803 deletion completed in 6.153701064s

• [SLOW TEST:10.262 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:45:58.504: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:46:03.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8051" for this suite.
Feb 19 21:46:25.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:46:25.777: INFO: namespace replication-controller-8051 deletion completed in 22.131126913s

• [SLOW TEST:27.274 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:46:25.780: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 21:46:25.901: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"03791f6f-f11d-4f77-9842-f67bc56e89d1", Controller:(*bool)(0xc002080a12), BlockOwnerDeletion:(*bool)(0xc002080a13)}}
Feb 19 21:46:25.915: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"974388ad-eaef-4c3c-b599-f3322634479c", Controller:(*bool)(0xc002080bda), BlockOwnerDeletion:(*bool)(0xc002080bdb)}}
Feb 19 21:46:25.926: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4f246728-863c-419b-873e-2724a2803f50", Controller:(*bool)(0xc002080de2), BlockOwnerDeletion:(*bool)(0xc002080de3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:46:30.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3369" for this suite.
Feb 19 21:46:36.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:46:37.092: INFO: namespace gc-3369 deletion completed in 6.146015415s

• [SLOW TEST:11.312 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:46:37.095: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 19 21:46:39.865: INFO: Successfully updated pod "annotationupdate3f52feb8-2f1b-4bd5-a268-bca026804c06"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:46:42.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5079" for this suite.
Feb 19 21:47:04.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:47:04.438: INFO: namespace downward-api-5079 deletion completed in 22.17127906s

• [SLOW TEST:27.343 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:47:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 19 21:47:04.742: INFO: Pod name wrapped-volume-race-1cf6e224-2f6c-424a-b8d2-52e52ecd4141: Found 0 pods out of 5
Feb 19 21:47:09.748: INFO: Pod name wrapped-volume-race-1cf6e224-2f6c-424a-b8d2-52e52ecd4141: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1cf6e224-2f6c-424a-b8d2-52e52ecd4141 in namespace emptydir-wrapper-182, will wait for the garbage collector to delete the pods
Feb 19 21:47:19.834: INFO: Deleting ReplicationController wrapped-volume-race-1cf6e224-2f6c-424a-b8d2-52e52ecd4141 took: 6.87352ms
Feb 19 21:47:20.434: INFO: Terminating ReplicationController wrapped-volume-race-1cf6e224-2f6c-424a-b8d2-52e52ecd4141 pods took: 600.417577ms
STEP: Creating RC which spawns configmap-volume pods
Feb 19 21:47:56.771: INFO: Pod name wrapped-volume-race-c869e429-7f78-4d3a-acf2-3d679715736b: Found 0 pods out of 5
Feb 19 21:48:01.776: INFO: Pod name wrapped-volume-race-c869e429-7f78-4d3a-acf2-3d679715736b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c869e429-7f78-4d3a-acf2-3d679715736b in namespace emptydir-wrapper-182, will wait for the garbage collector to delete the pods
Feb 19 21:48:13.899: INFO: Deleting ReplicationController wrapped-volume-race-c869e429-7f78-4d3a-acf2-3d679715736b took: 42.115766ms
Feb 19 21:48:14.500: INFO: Terminating ReplicationController wrapped-volume-race-c869e429-7f78-4d3a-acf2-3d679715736b pods took: 600.451548ms
STEP: Creating RC which spawns configmap-volume pods
Feb 19 21:48:56.424: INFO: Pod name wrapped-volume-race-b89b52fb-4882-4bbb-90c9-da809d7ee8f9: Found 0 pods out of 5
Feb 19 21:49:01.429: INFO: Pod name wrapped-volume-race-b89b52fb-4882-4bbb-90c9-da809d7ee8f9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b89b52fb-4882-4bbb-90c9-da809d7ee8f9 in namespace emptydir-wrapper-182, will wait for the garbage collector to delete the pods
Feb 19 21:49:13.542: INFO: Deleting ReplicationController wrapped-volume-race-b89b52fb-4882-4bbb-90c9-da809d7ee8f9 took: 19.356333ms
Feb 19 21:49:14.143: INFO: Terminating ReplicationController wrapped-volume-race-b89b52fb-4882-4bbb-90c9-da809d7ee8f9 pods took: 600.273269ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:49:56.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-182" for this suite.
Feb 19 21:50:04.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:50:04.581: INFO: namespace emptydir-wrapper-182 deletion completed in 8.191653243s

• [SLOW TEST:180.131 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:50:04.582: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-39b8e962-255f-4360-8037-682e20df1a9c
STEP: Creating a pod to test consume configMaps
Feb 19 21:50:04.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17" in namespace "configmap-8315" to be "success or failure"
Feb 19 21:50:04.636: INFO: Pod "pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17": Phase="Pending", Reason="", readiness=false. Elapsed: 6.998674ms
Feb 19 21:50:06.639: INFO: Pod "pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009919522s
STEP: Saw pod success
Feb 19 21:50:06.639: INFO: Pod "pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17" satisfied condition "success or failure"
Feb 19 21:50:06.642: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:50:06.671: INFO: Waiting for pod pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17 to disappear
Feb 19 21:50:06.674: INFO: Pod pod-configmaps-c60d5476-9cf3-44c1-8044-ac28f442de17 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:50:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8315" for this suite.
Feb 19 21:50:12.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:50:12.822: INFO: namespace configmap-8315 deletion completed in 6.144163345s

• [SLOW TEST:8.241 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:50:12.824: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Feb 19 21:50:12.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1904'
Feb 19 21:50:13.480: INFO: stderr: ""
Feb 19 21:50:13.480: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 19 21:50:13.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Feb 19 21:50:13.650: INFO: stderr: ""
Feb 19 21:50:13.650: INFO: stdout: "update-demo-nautilus-8jl9s update-demo-nautilus-wqsd8 "
Feb 19 21:50:13.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-8jl9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Feb 19 21:50:13.824: INFO: stderr: ""
Feb 19 21:50:13.824: INFO: stdout: ""
Feb 19 21:50:13.824: INFO: update-demo-nautilus-8jl9s is created but not running
Feb 19 21:50:18.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1904'
Feb 19 21:50:18.919: INFO: stderr: ""
Feb 19 21:50:18.919: INFO: stdout: "update-demo-nautilus-8jl9s update-demo-nautilus-wqsd8 "
Feb 19 21:50:18.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-8jl9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Feb 19 21:50:19.004: INFO: stderr: ""
Feb 19 21:50:19.004: INFO: stdout: "true"
Feb 19 21:50:19.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-8jl9s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Feb 19 21:50:19.089: INFO: stderr: ""
Feb 19 21:50:19.089: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:50:19.089: INFO: validating pod update-demo-nautilus-8jl9s
Feb 19 21:50:19.098: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:50:19.098: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:50:19.098: INFO: update-demo-nautilus-8jl9s is verified up and running
Feb 19 21:50:19.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-wqsd8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Feb 19 21:50:19.183: INFO: stderr: ""
Feb 19 21:50:19.183: INFO: stdout: "true"
Feb 19 21:50:19.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-wqsd8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1904'
Feb 19 21:50:19.269: INFO: stderr: ""
Feb 19 21:50:19.269: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 21:50:19.269: INFO: validating pod update-demo-nautilus-wqsd8
Feb 19 21:50:19.275: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 21:50:19.276: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 21:50:19.276: INFO: update-demo-nautilus-wqsd8 is verified up and running
STEP: using delete to clean up resources
Feb 19 21:50:19.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1904'
Feb 19 21:50:19.401: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:50:19.401: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 19 21:50:19.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1904'
Feb 19 21:50:19.536: INFO: stderr: "No resources found.\n"
Feb 19 21:50:19.536: INFO: stdout: ""
Feb 19 21:50:19.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -l name=update-demo --namespace=kubectl-1904 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 19 21:50:19.664: INFO: stderr: ""
Feb 19 21:50:19.664: INFO: stdout: "update-demo-nautilus-8jl9s\nupdate-demo-nautilus-wqsd8\n"
Feb 19 21:50:20.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1904'
Feb 19 21:50:20.259: INFO: stderr: "No resources found.\n"
Feb 19 21:50:20.259: INFO: stdout: ""
Feb 19 21:50:20.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -l name=update-demo --namespace=kubectl-1904 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 19 21:50:20.346: INFO: stderr: ""
Feb 19 21:50:20.346: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:50:20.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1904" for this suite.
Feb 19 21:50:26.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:50:26.506: INFO: namespace kubectl-1904 deletion completed in 6.151536218s

• [SLOW TEST:13.682 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:50:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 19 21:50:28.789: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:50:28.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-300" for this suite.
Feb 19 21:50:34.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:50:34.993: INFO: namespace container-runtime-300 deletion completed in 6.168495037s

• [SLOW TEST:8.483 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:50:34.993: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 19 21:50:35.032: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 19 21:50:35.040: INFO: Waiting for terminating namespaces to be deleted...
Feb 19 21:50:35.046: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-nfnp before test
Feb 19 21:50:35.058: INFO: l7-default-backend-84c9fcfbb-7vdk9 from kube-system started at 2020-02-19 14:29:47 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.058: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 19 21:50:35.058: INFO: kube-dns-autoscaler-6b7f784798-8zlzg from kube-system started at 2020-02-19 14:29:48 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.058: INFO: 	Container autoscaler ready: true, restart count 0
Feb 19 21:50:35.058: INFO: kube-dns-5dbbd9cc58-g9z7k from kube-system started at 2020-02-19 14:29:58 +0000 UTC (4 container statuses recorded)
Feb 19 21:50:35.059: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 19 21:50:35.059: INFO: 	Container kubedns ready: true, restart count 0
Feb 19 21:50:35.059: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 21:50:35.059: INFO: 	Container sidecar ready: true, restart count 0
Feb 19 21:50:35.059: INFO: fluentd-gcp-v3.1.1-kgrjm from kube-system started at 2020-02-19 14:30:01 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.059: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 21:50:35.059: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 21:50:35.059: INFO: fluentd-gcp-scaler-dd489f778-qvlt4 from kube-system started at 2020-02-19 14:29:49 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.059: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Feb 19 21:50:35.059: INFO: kube-proxy-gke-c115-default-pool-249bf33f-nfnp from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.059: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 21:50:35.059: INFO: prometheus-to-sd-8dvjw from kube-system started at 2020-02-19 14:29:48 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.059: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 21:50:35.060: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 21:50:35.060: INFO: metrics-server-v0.3.3-6d96fcc55-kwrms from kube-system started at 2020-02-19 14:29:53 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.060: INFO: 	Container metrics-server ready: true, restart count 0
Feb 19 21:50:35.060: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 19 21:50:35.060: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-nvt2l from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 19 21:50:35.060: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 21:50:35.060: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-qclh before test
Feb 19 21:50:35.107: INFO: kube-dns-5dbbd9cc58-wksp8 from kube-system started at 2020-02-19 14:29:47 +0000 UTC (4 container statuses recorded)
Feb 19 21:50:35.107: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 19 21:50:35.107: INFO: 	Container kubedns ready: true, restart count 0
Feb 19 21:50:35.107: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 21:50:35.107: INFO: 	Container sidecar ready: true, restart count 0
Feb 19 21:50:35.107: INFO: fluentd-gcp-v3.1.1-sgx4k from kube-system started at 2020-02-19 14:30:06 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.107: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 21:50:35.107: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 21:50:35.108: INFO: stackdriver-metadata-agent-cluster-level-5d4656b6d8-ldr2s from kube-system started at 2020-02-19 14:30:22 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.108: INFO: 	Container metadata-agent ready: true, restart count 0
Feb 19 21:50:35.108: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Feb 19 21:50:35.108: INFO: heapster-gke-76c9bd686-5tq5s from kube-system started at 2020-02-19 14:30:25 +0000 UTC (3 container statuses recorded)
Feb 19 21:50:35.108: INFO: 	Container heapster ready: true, restart count 0
Feb 19 21:50:35.108: INFO: 	Container heapster-nanny ready: true, restart count 0
Feb 19 21:50:35.108: INFO: 	Container prom-to-sd ready: true, restart count 0
Feb 19 21:50:35.108: INFO: kube-proxy-gke-c115-default-pool-249bf33f-qclh from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.108: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 21:50:35.108: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-sn5gm from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.109: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 19 21:50:35.109: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 21:50:35.109: INFO: prometheus-to-sd-hk44q from kube-system started at 2020-02-19 14:29:48 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.109: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 21:50:35.109: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 21:50:35.109: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-vhvp before test
Feb 19 21:50:35.123: INFO: event-exporter-v0.3.0-74bf544f8b-ckv4s from kube-system started at 2020-02-19 14:29:45 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.123: INFO: 	Container event-exporter ready: true, restart count 0
Feb 19 21:50:35.123: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 21:50:35.123: INFO: fluentd-gcp-v3.1.1-d8brx from kube-system started at 2020-02-19 14:30:06 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.123: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 21:50:35.124: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 21:50:35.124: INFO: sonobuoy-e2e-job-bec128f92cf64f0f from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.124: INFO: 	Container e2e ready: true, restart count 0
Feb 19 21:50:35.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 19 21:50:35.124: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-9xkwf from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 19 21:50:35.124: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 21:50:35.124: INFO: prometheus-to-sd-b68nv from kube-system started at 2020-02-19 14:29:47 +0000 UTC (2 container statuses recorded)
Feb 19 21:50:35.124: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 21:50:35.124: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 21:50:35.124: INFO: kube-proxy-gke-c115-default-pool-249bf33f-vhvp from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.124: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 21:50:35.125: INFO: sonobuoy from sonobuoy started at 2020-02-19 21:19:23 +0000 UTC (1 container statuses recorded)
Feb 19 21:50:35.125: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8e7e5c10-4f1a-4e86-b142-e0ada5a5ab36 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8e7e5c10-4f1a-4e86-b142-e0ada5a5ab36 off the node gke-c115-default-pool-249bf33f-nfnp
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8e7e5c10-4f1a-4e86-b142-e0ada5a5ab36
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:50:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4655" for this suite.
Feb 19 21:50:47.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:50:47.377: INFO: namespace sched-pred-4655 deletion completed in 8.158780031s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.384 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:50:47.379: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-4b20a727-5f8a-4c29-89c3-d72688973649
STEP: Creating a pod to test consume secrets
Feb 19 21:50:47.437: INFO: Waiting up to 5m0s for pod "pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5" in namespace "secrets-5333" to be "success or failure"
Feb 19 21:50:47.440: INFO: Pod "pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078757ms
Feb 19 21:50:49.443: INFO: Pod "pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006326712s
STEP: Saw pod success
Feb 19 21:50:49.443: INFO: Pod "pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5" satisfied condition "success or failure"
Feb 19 21:50:49.447: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 21:50:49.473: INFO: Waiting for pod pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5 to disappear
Feb 19 21:50:49.477: INFO: Pod pod-secrets-6d614df3-b6a9-4762-adf0-6313bea61ed5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:50:49.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5333" for this suite.
Feb 19 21:50:55.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:50:56.620: INFO: namespace secrets-5333 deletion completed in 7.140302896s

• [SLOW TEST:9.242 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:50:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-10d41bf5-9c08-464e-af7d-7eae517d268e
STEP: Creating a pod to test consume secrets
Feb 19 21:50:56.673: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67" in namespace "projected-3650" to be "success or failure"
Feb 19 21:50:56.676: INFO: Pod "pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67": Phase="Pending", Reason="", readiness=false. Elapsed: 3.248693ms
Feb 19 21:50:58.679: INFO: Pod "pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006395131s
STEP: Saw pod success
Feb 19 21:50:58.679: INFO: Pod "pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67" satisfied condition "success or failure"
Feb 19 21:50:58.681: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 19 21:50:58.702: INFO: Waiting for pod pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67 to disappear
Feb 19 21:50:58.706: INFO: Pod pod-projected-secrets-62fae7e1-fa79-4bd8-afa7-d5cf9b7edf67 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:50:58.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3650" for this suite.
Feb 19 21:51:04.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:51:04.847: INFO: namespace projected-3650 deletion completed in 6.137577708s

• [SLOW TEST:8.224 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:51:04.849: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 21:51:04.885: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 19 21:51:05.918: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:51:06.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8414" for this suite.
Feb 19 21:51:14.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:51:15.073: INFO: namespace replication-controller-8414 deletion completed in 8.141905214s

• [SLOW TEST:10.224 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:51:15.073: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:51:17.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3475" for this suite.
Feb 19 21:52:11.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:52:11.357: INFO: namespace kubelet-test-3475 deletion completed in 54.18530677s

• [SLOW TEST:56.284 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:52:11.360: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 19 21:52:11.406: INFO: Waiting up to 5m0s for pod "downward-api-24b829f7-a24d-42ee-989b-57250d62b06b" in namespace "downward-api-6821" to be "success or failure"
Feb 19 21:52:11.409: INFO: Pod "downward-api-24b829f7-a24d-42ee-989b-57250d62b06b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.633467ms
Feb 19 21:52:13.412: INFO: Pod "downward-api-24b829f7-a24d-42ee-989b-57250d62b06b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005627225s
STEP: Saw pod success
Feb 19 21:52:13.412: INFO: Pod "downward-api-24b829f7-a24d-42ee-989b-57250d62b06b" satisfied condition "success or failure"
Feb 19 21:52:13.415: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downward-api-24b829f7-a24d-42ee-989b-57250d62b06b container dapi-container: <nil>
STEP: delete the pod
Feb 19 21:52:13.435: INFO: Waiting for pod downward-api-24b829f7-a24d-42ee-989b-57250d62b06b to disappear
Feb 19 21:52:13.438: INFO: Pod downward-api-24b829f7-a24d-42ee-989b-57250d62b06b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:52:13.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6821" for this suite.
Feb 19 21:52:19.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:52:19.583: INFO: namespace downward-api-6821 deletion completed in 6.141210792s

• [SLOW TEST:8.223 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:52:19.589: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 19 21:52:19.623: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:52:23.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4749" for this suite.
Feb 19 21:52:29.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:52:29.936: INFO: namespace init-container-4749 deletion completed in 6.147863132s

• [SLOW TEST:10.348 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:52:29.939: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7177
I0219 21:52:29.981031      14 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7177, replica count: 1
I0219 21:52:31.031736      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0219 21:52:32.031959      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0219 21:52:33.032153      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 19 21:52:33.172: INFO: Created: latency-svc-w9pbs
Feb 19 21:52:33.253: INFO: Got endpoints: latency-svc-w9pbs [121.281418ms]
Feb 19 21:52:33.281: INFO: Created: latency-svc-f5hhs
Feb 19 21:52:33.291: INFO: Created: latency-svc-f2xfp
Feb 19 21:52:33.302: INFO: Got endpoints: latency-svc-f5hhs [48.474303ms]
Feb 19 21:52:33.315: INFO: Created: latency-svc-wqjtt
Feb 19 21:52:33.335: INFO: Got endpoints: latency-svc-f2xfp [80.987752ms]
Feb 19 21:52:33.336: INFO: Created: latency-svc-mtpsc
Feb 19 21:52:33.358: INFO: Created: latency-svc-kh75n
Feb 19 21:52:33.364: INFO: Got endpoints: latency-svc-wqjtt [110.533283ms]
Feb 19 21:52:33.384: INFO: Created: latency-svc-6mt49
Feb 19 21:52:33.385: INFO: Got endpoints: latency-svc-mtpsc [130.278779ms]
Feb 19 21:52:33.389: INFO: Created: latency-svc-64kfs
Feb 19 21:52:33.391: INFO: Got endpoints: latency-svc-kh75n [135.601291ms]
Feb 19 21:52:33.392: INFO: Created: latency-svc-kdldw
Feb 19 21:52:33.405: INFO: Got endpoints: latency-svc-6mt49 [149.184542ms]
Feb 19 21:52:33.424: INFO: Created: latency-svc-m8skr
Feb 19 21:52:33.428: INFO: Got endpoints: latency-svc-kdldw [173.961615ms]
Feb 19 21:52:33.429: INFO: Got endpoints: latency-svc-64kfs [174.44374ms]
Feb 19 21:52:33.443: INFO: Created: latency-svc-dc5g8
Feb 19 21:52:33.461: INFO: Got endpoints: latency-svc-m8skr [205.913814ms]
Feb 19 21:52:33.464: INFO: Created: latency-svc-rlxpz
Feb 19 21:52:33.480: INFO: Got endpoints: latency-svc-dc5g8 [224.977329ms]
Feb 19 21:52:33.485: INFO: Created: latency-svc-nkkvx
Feb 19 21:52:33.498: INFO: Got endpoints: latency-svc-rlxpz [242.910468ms]
Feb 19 21:52:33.498: INFO: Created: latency-svc-2vpft
Feb 19 21:52:33.502: INFO: Got endpoints: latency-svc-nkkvx [247.083772ms]
Feb 19 21:52:33.510: INFO: Created: latency-svc-6gwk6
Feb 19 21:52:33.527: INFO: Created: latency-svc-dbqqc
Feb 19 21:52:33.565: INFO: Created: latency-svc-lkqd2
Feb 19 21:52:33.578: INFO: Got endpoints: latency-svc-2vpft [322.906878ms]
Feb 19 21:52:33.579: INFO: Got endpoints: latency-svc-6gwk6 [323.636822ms]
Feb 19 21:52:33.594: INFO: Got endpoints: latency-svc-dbqqc [339.061382ms]
Feb 19 21:52:33.607: INFO: Created: latency-svc-4ddzl
Feb 19 21:52:33.620: INFO: Got endpoints: latency-svc-lkqd2 [318.235137ms]
Feb 19 21:52:33.623: INFO: Created: latency-svc-ck4jr
Feb 19 21:52:33.638: INFO: Got endpoints: latency-svc-4ddzl [303.054116ms]
Feb 19 21:52:33.642: INFO: Created: latency-svc-7xfh5
Feb 19 21:52:33.642: INFO: Created: latency-svc-xl6p9
Feb 19 21:52:33.663: INFO: Created: latency-svc-2xkqg
Feb 19 21:52:33.665: INFO: Got endpoints: latency-svc-ck4jr [300.552738ms]
Feb 19 21:52:33.679: INFO: Got endpoints: latency-svc-7xfh5 [287.639285ms]
Feb 19 21:52:33.680: INFO: Created: latency-svc-4dx2w
Feb 19 21:52:33.689: INFO: Created: latency-svc-gl7h4
Feb 19 21:52:33.705: INFO: Got endpoints: latency-svc-2xkqg [300.034461ms]
Feb 19 21:52:33.706: INFO: Created: latency-svc-scm4j
Feb 19 21:52:33.706: INFO: Got endpoints: latency-svc-xl6p9 [321.199669ms]
Feb 19 21:52:33.709: INFO: Created: latency-svc-5gjgt
Feb 19 21:52:33.726: INFO: Created: latency-svc-chjtz
Feb 19 21:52:33.734: INFO: Got endpoints: latency-svc-4dx2w [304.659727ms]
Feb 19 21:52:33.734: INFO: Got endpoints: latency-svc-gl7h4 [305.437234ms]
Feb 19 21:52:33.741: INFO: Created: latency-svc-9x7s5
Feb 19 21:52:33.748: INFO: Got endpoints: latency-svc-scm4j [268.397599ms]
Feb 19 21:52:33.762: INFO: Got endpoints: latency-svc-5gjgt [301.477854ms]
Feb 19 21:52:33.776: INFO: Created: latency-svc-78bs8
Feb 19 21:52:33.786: INFO: Created: latency-svc-ppjnd
Feb 19 21:52:33.795: INFO: Got endpoints: latency-svc-chjtz [297.379068ms]
Feb 19 21:52:33.795: INFO: Got endpoints: latency-svc-9x7s5 [293.222456ms]
Feb 19 21:52:33.808: INFO: Got endpoints: latency-svc-78bs8 [229.167864ms]
Feb 19 21:52:33.823: INFO: Created: latency-svc-mz8gk
Feb 19 21:52:33.831: INFO: Created: latency-svc-gfvlr
Feb 19 21:52:33.839: INFO: Got endpoints: latency-svc-mz8gk [244.190693ms]
Feb 19 21:52:33.839: INFO: Got endpoints: latency-svc-ppjnd [260.153309ms]
Feb 19 21:52:33.847: INFO: Got endpoints: latency-svc-gfvlr [227.013308ms]
Feb 19 21:52:33.857: INFO: Created: latency-svc-4hszl
Feb 19 21:52:33.867: INFO: Created: latency-svc-5kdt5
Feb 19 21:52:33.881: INFO: Created: latency-svc-6hgf8
Feb 19 21:52:33.898: INFO: Created: latency-svc-qq7x7
Feb 19 21:52:33.900: INFO: Got endpoints: latency-svc-5kdt5 [234.741462ms]
Feb 19 21:52:33.900: INFO: Got endpoints: latency-svc-4hszl [261.773038ms]
Feb 19 21:52:33.917: INFO: Created: latency-svc-m8k6x
Feb 19 21:52:33.925: INFO: Got endpoints: latency-svc-6hgf8 [246.307277ms]
Feb 19 21:52:33.925: INFO: Got endpoints: latency-svc-qq7x7 [220.494325ms]
Feb 19 21:52:33.930: INFO: Created: latency-svc-rh4m7
Feb 19 21:52:33.941: INFO: Got endpoints: latency-svc-m8k6x [235.525368ms]
Feb 19 21:52:33.952: INFO: Got endpoints: latency-svc-rh4m7 [217.659014ms]
Feb 19 21:52:33.954: INFO: Created: latency-svc-ndpqg
Feb 19 21:52:33.971: INFO: Created: latency-svc-lhg8w
Feb 19 21:52:33.983: INFO: Created: latency-svc-jsnjg
Feb 19 21:52:33.991: INFO: Got endpoints: latency-svc-lhg8w [242.308197ms]
Feb 19 21:52:33.991: INFO: Got endpoints: latency-svc-ndpqg [257.212279ms]
Feb 19 21:52:34.009: INFO: Got endpoints: latency-svc-jsnjg [246.237957ms]
Feb 19 21:52:34.009: INFO: Created: latency-svc-ft52q
Feb 19 21:52:34.032: INFO: Got endpoints: latency-svc-ft52q [236.717546ms]
Feb 19 21:52:34.032: INFO: Created: latency-svc-s544b
Feb 19 21:52:34.051: INFO: Got endpoints: latency-svc-s544b [256.218864ms]
Feb 19 21:52:34.053: INFO: Created: latency-svc-85pt2
Feb 19 21:52:34.070: INFO: Got endpoints: latency-svc-85pt2 [262.605817ms]
Feb 19 21:52:34.073: INFO: Created: latency-svc-xbfvq
Feb 19 21:52:34.073: INFO: Created: latency-svc-mmrl7
Feb 19 21:52:34.092: INFO: Got endpoints: latency-svc-mmrl7 [252.298622ms]
Feb 19 21:52:34.092: INFO: Got endpoints: latency-svc-xbfvq [253.181009ms]
Feb 19 21:52:34.100: INFO: Created: latency-svc-l68lj
Feb 19 21:52:34.119: INFO: Created: latency-svc-xtc4w
Feb 19 21:52:34.134: INFO: Got endpoints: latency-svc-l68lj [286.302545ms]
Feb 19 21:52:34.135: INFO: Created: latency-svc-7x7l8
Feb 19 21:52:34.147: INFO: Created: latency-svc-mn7gr
Feb 19 21:52:34.160: INFO: Created: latency-svc-tvd52
Feb 19 21:52:34.160: INFO: Got endpoints: latency-svc-xtc4w [260.307801ms]
Feb 19 21:52:34.177: INFO: Created: latency-svc-mgf4x
Feb 19 21:52:34.177: INFO: Created: latency-svc-68bmk
Feb 19 21:52:34.177: INFO: Created: latency-svc-h9n9w
Feb 19 21:52:34.177: INFO: Created: latency-svc-98l6s
Feb 19 21:52:34.219: INFO: Created: latency-svc-8jvsx
Feb 19 21:52:34.234: INFO: Got endpoints: latency-svc-7x7l8 [141.597935ms]
Feb 19 21:52:34.236: INFO: Created: latency-svc-pscl6
Feb 19 21:52:34.253: INFO: Created: latency-svc-rhrvb
Feb 19 21:52:34.262: INFO: Created: latency-svc-lhprt
Feb 19 21:52:34.269: INFO: Got endpoints: latency-svc-mn7gr [368.282005ms]
Feb 19 21:52:34.284: INFO: Created: latency-svc-5k5kd
Feb 19 21:52:34.327: INFO: Created: latency-svc-6hqwh
Feb 19 21:52:34.336: INFO: Created: latency-svc-d5kh7
Feb 19 21:52:34.350: INFO: Got endpoints: latency-svc-tvd52 [425.209452ms]
Feb 19 21:52:34.384: INFO: Created: latency-svc-d6czc
Feb 19 21:52:34.419: INFO: Created: latency-svc-n66pf
Feb 19 21:52:34.451: INFO: Created: latency-svc-mvcbq
Feb 19 21:52:34.469: INFO: Got endpoints: latency-svc-98l6s [517.769584ms]
Feb 19 21:52:34.470: INFO: Got endpoints: latency-svc-h9n9w [544.177443ms]
Feb 19 21:52:34.478: INFO: Got endpoints: latency-svc-68bmk [536.124076ms]
Feb 19 21:52:34.503: INFO: Created: latency-svc-ntngx
Feb 19 21:52:34.517: INFO: Got endpoints: latency-svc-mgf4x [525.950128ms]
Feb 19 21:52:34.597: INFO: Created: latency-svc-qdf2q
Feb 19 21:52:34.599: INFO: Got endpoints: latency-svc-8jvsx [607.173561ms]
Feb 19 21:52:34.599: INFO: Created: latency-svc-8575w
Feb 19 21:52:34.632: INFO: Created: latency-svc-txrq5
Feb 19 21:52:34.675: INFO: Created: latency-svc-bj8kz
Feb 19 21:52:34.694: INFO: Got endpoints: latency-svc-rhrvb [662.250342ms]
Feb 19 21:52:34.695: INFO: Got endpoints: latency-svc-pscl6 [685.859696ms]
Feb 19 21:52:34.714: INFO: Got endpoints: latency-svc-lhprt [662.456382ms]
Feb 19 21:52:34.731: INFO: Created: latency-svc-v4hmp
Feb 19 21:52:34.765: INFO: Got endpoints: latency-svc-5k5kd [694.679669ms]
Feb 19 21:52:34.784: INFO: Created: latency-svc-sqp4t
Feb 19 21:52:34.799: INFO: Created: latency-svc-j2qgb
Feb 19 21:52:34.813: INFO: Got endpoints: latency-svc-6hqwh [678.513474ms]
Feb 19 21:52:34.819: INFO: Created: latency-svc-p5qh2
Feb 19 21:52:34.825: INFO: Created: latency-svc-r664g
Feb 19 21:52:34.838: INFO: Got endpoints: latency-svc-d5kh7 [745.759306ms]
Feb 19 21:52:34.852: INFO: Created: latency-svc-jjkzw
Feb 19 21:52:34.858: INFO: Created: latency-svc-r8lhq
Feb 19 21:52:34.883: INFO: Got endpoints: latency-svc-d6czc [722.52621ms]
Feb 19 21:52:34.897: INFO: Created: latency-svc-rr2rc
Feb 19 21:52:34.934: INFO: Got endpoints: latency-svc-n66pf [699.514679ms]
Feb 19 21:52:34.946: INFO: Created: latency-svc-4dxvg
Feb 19 21:52:34.986: INFO: Got endpoints: latency-svc-mvcbq [717.01084ms]
Feb 19 21:52:34.999: INFO: Created: latency-svc-6wd5f
Feb 19 21:52:35.035: INFO: Got endpoints: latency-svc-ntngx [684.448633ms]
Feb 19 21:52:35.049: INFO: Created: latency-svc-cj2wm
Feb 19 21:52:35.082: INFO: Got endpoints: latency-svc-qdf2q [612.707955ms]
Feb 19 21:52:35.095: INFO: Created: latency-svc-tqm6b
Feb 19 21:52:35.137: INFO: Got endpoints: latency-svc-8575w [659.224621ms]
Feb 19 21:52:35.147: INFO: Created: latency-svc-rcvq2
Feb 19 21:52:35.183: INFO: Got endpoints: latency-svc-txrq5 [712.873861ms]
Feb 19 21:52:35.195: INFO: Created: latency-svc-65wsl
Feb 19 21:52:35.234: INFO: Got endpoints: latency-svc-bj8kz [716.391063ms]
Feb 19 21:52:35.247: INFO: Created: latency-svc-gz627
Feb 19 21:52:35.285: INFO: Got endpoints: latency-svc-v4hmp [686.089215ms]
Feb 19 21:52:35.296: INFO: Created: latency-svc-r2s9v
Feb 19 21:52:35.334: INFO: Got endpoints: latency-svc-sqp4t [639.14043ms]
Feb 19 21:52:35.345: INFO: Created: latency-svc-bqh2j
Feb 19 21:52:35.384: INFO: Got endpoints: latency-svc-j2qgb [689.161763ms]
Feb 19 21:52:35.396: INFO: Created: latency-svc-87wbp
Feb 19 21:52:35.434: INFO: Got endpoints: latency-svc-p5qh2 [719.516303ms]
Feb 19 21:52:35.451: INFO: Created: latency-svc-slpbb
Feb 19 21:52:35.483: INFO: Got endpoints: latency-svc-r664g [718.36993ms]
Feb 19 21:52:35.496: INFO: Created: latency-svc-fx7t8
Feb 19 21:52:35.541: INFO: Got endpoints: latency-svc-jjkzw [728.516149ms]
Feb 19 21:52:35.585: INFO: Created: latency-svc-rdcgp
Feb 19 21:52:35.589: INFO: Got endpoints: latency-svc-r8lhq [751.02288ms]
Feb 19 21:52:35.605: INFO: Created: latency-svc-n57lr
Feb 19 21:52:35.635: INFO: Got endpoints: latency-svc-rr2rc [751.832429ms]
Feb 19 21:52:35.650: INFO: Created: latency-svc-p4v95
Feb 19 21:52:35.683: INFO: Got endpoints: latency-svc-4dxvg [748.758509ms]
Feb 19 21:52:35.695: INFO: Created: latency-svc-hsmhc
Feb 19 21:52:35.733: INFO: Got endpoints: latency-svc-6wd5f [746.487832ms]
Feb 19 21:52:35.749: INFO: Created: latency-svc-bb7js
Feb 19 21:52:35.784: INFO: Got endpoints: latency-svc-cj2wm [748.498548ms]
Feb 19 21:52:35.797: INFO: Created: latency-svc-pnrbk
Feb 19 21:52:35.833: INFO: Got endpoints: latency-svc-tqm6b [751.072635ms]
Feb 19 21:52:35.855: INFO: Created: latency-svc-jv5kc
Feb 19 21:52:35.928: INFO: Got endpoints: latency-svc-rcvq2 [791.141258ms]
Feb 19 21:52:35.933: INFO: Got endpoints: latency-svc-65wsl [750.183301ms]
Feb 19 21:52:35.947: INFO: Created: latency-svc-twdqz
Feb 19 21:52:35.954: INFO: Created: latency-svc-nwd22
Feb 19 21:52:35.983: INFO: Got endpoints: latency-svc-gz627 [749.528737ms]
Feb 19 21:52:36.000: INFO: Created: latency-svc-qmx2r
Feb 19 21:52:36.036: INFO: Got endpoints: latency-svc-r2s9v [750.731887ms]
Feb 19 21:52:36.050: INFO: Created: latency-svc-9njq5
Feb 19 21:52:36.083: INFO: Got endpoints: latency-svc-bqh2j [749.498452ms]
Feb 19 21:52:36.094: INFO: Created: latency-svc-6hbtn
Feb 19 21:52:36.133: INFO: Got endpoints: latency-svc-87wbp [748.614536ms]
Feb 19 21:52:36.146: INFO: Created: latency-svc-5qf92
Feb 19 21:52:36.184: INFO: Got endpoints: latency-svc-slpbb [750.23171ms]
Feb 19 21:52:36.195: INFO: Created: latency-svc-wkqtk
Feb 19 21:52:36.233: INFO: Got endpoints: latency-svc-fx7t8 [749.812067ms]
Feb 19 21:52:36.249: INFO: Created: latency-svc-9zm89
Feb 19 21:52:36.284: INFO: Got endpoints: latency-svc-rdcgp [742.68991ms]
Feb 19 21:52:36.297: INFO: Created: latency-svc-z4z2g
Feb 19 21:52:36.334: INFO: Got endpoints: latency-svc-n57lr [744.695097ms]
Feb 19 21:52:36.349: INFO: Created: latency-svc-mv7q8
Feb 19 21:52:36.384: INFO: Got endpoints: latency-svc-p4v95 [748.968413ms]
Feb 19 21:52:36.395: INFO: Created: latency-svc-fjlq6
Feb 19 21:52:36.433: INFO: Got endpoints: latency-svc-hsmhc [749.870766ms]
Feb 19 21:52:36.447: INFO: Created: latency-svc-4ftdk
Feb 19 21:52:36.484: INFO: Got endpoints: latency-svc-bb7js [750.856396ms]
Feb 19 21:52:36.499: INFO: Created: latency-svc-hwhvl
Feb 19 21:52:36.540: INFO: Got endpoints: latency-svc-pnrbk [756.217513ms]
Feb 19 21:52:36.568: INFO: Created: latency-svc-rnw7p
Feb 19 21:52:36.594: INFO: Got endpoints: latency-svc-jv5kc [760.67137ms]
Feb 19 21:52:36.610: INFO: Created: latency-svc-82w9b
Feb 19 21:52:36.633: INFO: Got endpoints: latency-svc-twdqz [704.367149ms]
Feb 19 21:52:36.645: INFO: Created: latency-svc-4xmc7
Feb 19 21:52:36.683: INFO: Got endpoints: latency-svc-nwd22 [749.609353ms]
Feb 19 21:52:36.695: INFO: Created: latency-svc-ldtpx
Feb 19 21:52:36.733: INFO: Got endpoints: latency-svc-qmx2r [750.056492ms]
Feb 19 21:52:36.750: INFO: Created: latency-svc-nj9c4
Feb 19 21:52:36.784: INFO: Got endpoints: latency-svc-9njq5 [748.440946ms]
Feb 19 21:52:36.799: INFO: Created: latency-svc-wm7dl
Feb 19 21:52:36.833: INFO: Got endpoints: latency-svc-6hbtn [749.848043ms]
Feb 19 21:52:36.846: INFO: Created: latency-svc-fkcfg
Feb 19 21:52:36.883: INFO: Got endpoints: latency-svc-5qf92 [749.2659ms]
Feb 19 21:52:36.894: INFO: Created: latency-svc-4zzjv
Feb 19 21:52:36.934: INFO: Got endpoints: latency-svc-wkqtk [750.429923ms]
Feb 19 21:52:36.951: INFO: Created: latency-svc-jlvrx
Feb 19 21:52:36.982: INFO: Got endpoints: latency-svc-9zm89 [748.656632ms]
Feb 19 21:52:36.997: INFO: Created: latency-svc-xq4jn
Feb 19 21:52:37.035: INFO: Got endpoints: latency-svc-z4z2g [750.744389ms]
Feb 19 21:52:37.046: INFO: Created: latency-svc-thh6j
Feb 19 21:52:37.084: INFO: Got endpoints: latency-svc-mv7q8 [750.514496ms]
Feb 19 21:52:37.100: INFO: Created: latency-svc-p6gmj
Feb 19 21:52:37.133: INFO: Got endpoints: latency-svc-fjlq6 [749.195713ms]
Feb 19 21:52:37.148: INFO: Created: latency-svc-tfcc9
Feb 19 21:52:37.183: INFO: Got endpoints: latency-svc-4ftdk [750.403129ms]
Feb 19 21:52:37.195: INFO: Created: latency-svc-94gkg
Feb 19 21:52:37.233: INFO: Got endpoints: latency-svc-hwhvl [749.707494ms]
Feb 19 21:52:37.246: INFO: Created: latency-svc-v469c
Feb 19 21:52:37.283: INFO: Got endpoints: latency-svc-rnw7p [742.737169ms]
Feb 19 21:52:37.293: INFO: Created: latency-svc-427ws
Feb 19 21:52:37.334: INFO: Got endpoints: latency-svc-82w9b [739.715778ms]
Feb 19 21:52:37.346: INFO: Created: latency-svc-57dqh
Feb 19 21:52:37.383: INFO: Got endpoints: latency-svc-4xmc7 [750.046646ms]
Feb 19 21:52:37.393: INFO: Created: latency-svc-7kwqf
Feb 19 21:52:37.433: INFO: Got endpoints: latency-svc-ldtpx [750.354414ms]
Feb 19 21:52:37.447: INFO: Created: latency-svc-j8wph
Feb 19 21:52:37.483: INFO: Got endpoints: latency-svc-nj9c4 [749.63515ms]
Feb 19 21:52:37.495: INFO: Created: latency-svc-pjc8h
Feb 19 21:52:37.533: INFO: Got endpoints: latency-svc-wm7dl [748.58759ms]
Feb 19 21:52:37.562: INFO: Created: latency-svc-qn5gv
Feb 19 21:52:37.583: INFO: Got endpoints: latency-svc-fkcfg [749.919734ms]
Feb 19 21:52:37.598: INFO: Created: latency-svc-sswsc
Feb 19 21:52:37.633: INFO: Got endpoints: latency-svc-4zzjv [749.897035ms]
Feb 19 21:52:37.649: INFO: Created: latency-svc-k49l8
Feb 19 21:52:37.683: INFO: Got endpoints: latency-svc-jlvrx [748.778708ms]
Feb 19 21:52:37.693: INFO: Created: latency-svc-z6xmd
Feb 19 21:52:37.733: INFO: Got endpoints: latency-svc-xq4jn [750.407247ms]
Feb 19 21:52:37.749: INFO: Created: latency-svc-j9sd6
Feb 19 21:52:37.784: INFO: Got endpoints: latency-svc-thh6j [748.175866ms]
Feb 19 21:52:37.794: INFO: Created: latency-svc-pxb5d
Feb 19 21:52:37.833: INFO: Got endpoints: latency-svc-p6gmj [747.87695ms]
Feb 19 21:52:37.847: INFO: Created: latency-svc-dnflx
Feb 19 21:52:37.883: INFO: Got endpoints: latency-svc-tfcc9 [748.973848ms]
Feb 19 21:52:37.894: INFO: Created: latency-svc-bkk5t
Feb 19 21:52:37.934: INFO: Got endpoints: latency-svc-94gkg [750.042134ms]
Feb 19 21:52:37.947: INFO: Created: latency-svc-lssrj
Feb 19 21:52:37.984: INFO: Got endpoints: latency-svc-v469c [750.675527ms]
Feb 19 21:52:38.004: INFO: Created: latency-svc-252bm
Feb 19 21:52:38.034: INFO: Got endpoints: latency-svc-427ws [750.752454ms]
Feb 19 21:52:38.050: INFO: Created: latency-svc-lxk6x
Feb 19 21:52:38.083: INFO: Got endpoints: latency-svc-57dqh [748.289744ms]
Feb 19 21:52:38.096: INFO: Created: latency-svc-jkbz8
Feb 19 21:52:38.133: INFO: Got endpoints: latency-svc-7kwqf [750.305381ms]
Feb 19 21:52:38.160: INFO: Created: latency-svc-6jj7r
Feb 19 21:52:38.183: INFO: Got endpoints: latency-svc-j8wph [749.268652ms]
Feb 19 21:52:38.195: INFO: Created: latency-svc-gcrmq
Feb 19 21:52:38.233: INFO: Got endpoints: latency-svc-pjc8h [749.824159ms]
Feb 19 21:52:38.246: INFO: Created: latency-svc-7hlrj
Feb 19 21:52:38.284: INFO: Got endpoints: latency-svc-qn5gv [750.86108ms]
Feb 19 21:52:38.295: INFO: Created: latency-svc-tmtkv
Feb 19 21:52:38.333: INFO: Got endpoints: latency-svc-sswsc [749.511972ms]
Feb 19 21:52:38.346: INFO: Created: latency-svc-4x4kz
Feb 19 21:52:38.383: INFO: Got endpoints: latency-svc-k49l8 [750.727996ms]
Feb 19 21:52:38.397: INFO: Created: latency-svc-2tll7
Feb 19 21:52:38.434: INFO: Got endpoints: latency-svc-z6xmd [749.967229ms]
Feb 19 21:52:38.450: INFO: Created: latency-svc-btcmv
Feb 19 21:52:38.483: INFO: Got endpoints: latency-svc-j9sd6 [749.560281ms]
Feb 19 21:52:38.503: INFO: Created: latency-svc-hk8qm
Feb 19 21:52:38.541: INFO: Got endpoints: latency-svc-pxb5d [756.865313ms]
Feb 19 21:52:38.571: INFO: Created: latency-svc-6c8c9
Feb 19 21:52:38.590: INFO: Got endpoints: latency-svc-dnflx [757.107008ms]
Feb 19 21:52:38.613: INFO: Created: latency-svc-fkzx5
Feb 19 21:52:38.633: INFO: Got endpoints: latency-svc-bkk5t [750.396808ms]
Feb 19 21:52:38.647: INFO: Created: latency-svc-zncj9
Feb 19 21:52:38.683: INFO: Got endpoints: latency-svc-lssrj [748.945338ms]
Feb 19 21:52:38.696: INFO: Created: latency-svc-fdj4b
Feb 19 21:52:38.734: INFO: Got endpoints: latency-svc-252bm [749.371153ms]
Feb 19 21:52:38.748: INFO: Created: latency-svc-vkv9r
Feb 19 21:52:38.783: INFO: Got endpoints: latency-svc-lxk6x [749.131989ms]
Feb 19 21:52:38.799: INFO: Created: latency-svc-7z56l
Feb 19 21:52:38.834: INFO: Got endpoints: latency-svc-jkbz8 [750.737488ms]
Feb 19 21:52:38.852: INFO: Created: latency-svc-qcqzd
Feb 19 21:52:38.883: INFO: Got endpoints: latency-svc-6jj7r [749.877665ms]
Feb 19 21:52:38.895: INFO: Created: latency-svc-mhngr
Feb 19 21:52:38.933: INFO: Got endpoints: latency-svc-gcrmq [750.882746ms]
Feb 19 21:52:38.946: INFO: Created: latency-svc-rb947
Feb 19 21:52:38.983: INFO: Got endpoints: latency-svc-7hlrj [750.11758ms]
Feb 19 21:52:39.007: INFO: Created: latency-svc-mgrt4
Feb 19 21:52:39.033: INFO: Got endpoints: latency-svc-tmtkv [749.047798ms]
Feb 19 21:52:39.052: INFO: Created: latency-svc-s6pc4
Feb 19 21:52:39.085: INFO: Got endpoints: latency-svc-4x4kz [751.347041ms]
Feb 19 21:52:39.097: INFO: Created: latency-svc-mbsg2
Feb 19 21:52:39.133: INFO: Got endpoints: latency-svc-2tll7 [749.509247ms]
Feb 19 21:52:39.148: INFO: Created: latency-svc-2n9vj
Feb 19 21:52:39.187: INFO: Got endpoints: latency-svc-btcmv [753.521589ms]
Feb 19 21:52:39.204: INFO: Created: latency-svc-79thd
Feb 19 21:52:39.233: INFO: Got endpoints: latency-svc-hk8qm [750.258177ms]
Feb 19 21:52:39.282: INFO: Created: latency-svc-cxk52
Feb 19 21:52:39.288: INFO: Got endpoints: latency-svc-6c8c9 [746.957056ms]
Feb 19 21:52:39.301: INFO: Created: latency-svc-598c6
Feb 19 21:52:39.333: INFO: Got endpoints: latency-svc-fkzx5 [743.364846ms]
Feb 19 21:52:39.347: INFO: Created: latency-svc-ls8tv
Feb 19 21:52:39.383: INFO: Got endpoints: latency-svc-zncj9 [749.798878ms]
Feb 19 21:52:39.403: INFO: Created: latency-svc-tshm6
Feb 19 21:52:39.433: INFO: Got endpoints: latency-svc-fdj4b [750.40125ms]
Feb 19 21:52:39.446: INFO: Created: latency-svc-m4r94
Feb 19 21:52:39.484: INFO: Got endpoints: latency-svc-vkv9r [750.281117ms]
Feb 19 21:52:39.500: INFO: Created: latency-svc-x4zlb
Feb 19 21:52:39.535: INFO: Got endpoints: latency-svc-7z56l [751.09635ms]
Feb 19 21:52:39.574: INFO: Created: latency-svc-zr5g8
Feb 19 21:52:39.586: INFO: Got endpoints: latency-svc-qcqzd [752.137966ms]
Feb 19 21:52:39.601: INFO: Created: latency-svc-k5tcz
Feb 19 21:52:39.633: INFO: Got endpoints: latency-svc-mhngr [749.54082ms]
Feb 19 21:52:39.647: INFO: Created: latency-svc-45q8w
Feb 19 21:52:39.683: INFO: Got endpoints: latency-svc-rb947 [749.750528ms]
Feb 19 21:52:39.695: INFO: Created: latency-svc-lc59v
Feb 19 21:52:39.792: INFO: Got endpoints: latency-svc-mgrt4 [808.6036ms]
Feb 19 21:52:39.892: INFO: Got endpoints: latency-svc-s6pc4 [858.058489ms]
Feb 19 21:52:39.928: INFO: Got endpoints: latency-svc-mbsg2 [842.53369ms]
Feb 19 21:52:40.008: INFO: Got endpoints: latency-svc-2n9vj [874.98649ms]
Feb 19 21:52:40.148: INFO: Created: latency-svc-8jf4s
Feb 19 21:52:40.353: INFO: Created: latency-svc-t9lbc
Feb 19 21:52:40.363: INFO: Got endpoints: latency-svc-cxk52 [1.12939929s]
Feb 19 21:52:40.363: INFO: Got endpoints: latency-svc-598c6 [1.07498523s]
Feb 19 21:52:40.363: INFO: Got endpoints: latency-svc-79thd [1.175249715s]
Feb 19 21:52:40.434: INFO: Created: latency-svc-mppf7
Feb 19 21:52:40.437: INFO: Got endpoints: latency-svc-ls8tv [1.103863517s]
Feb 19 21:52:40.438: INFO: Got endpoints: latency-svc-tshm6 [1.054362949s]
Feb 19 21:52:40.484: INFO: Created: latency-svc-2qk9b
Feb 19 21:52:40.484: INFO: Got endpoints: latency-svc-m4r94 [1.050936132s]
Feb 19 21:52:40.523: INFO: Got endpoints: latency-svc-45q8w [889.690661ms]
Feb 19 21:52:40.582: INFO: Created: latency-svc-zrdq8
Feb 19 21:52:40.582: INFO: Got endpoints: latency-svc-x4zlb [1.097471411s]
Feb 19 21:52:40.582: INFO: Got endpoints: latency-svc-lc59v [898.639393ms]
Feb 19 21:52:40.582: INFO: Got endpoints: latency-svc-zr5g8 [1.047503683s]
Feb 19 21:52:40.583: INFO: Got endpoints: latency-svc-k5tcz [996.68947ms]
Feb 19 21:52:40.615: INFO: Got endpoints: latency-svc-8jf4s [822.296018ms]
Feb 19 21:52:40.616: INFO: Created: latency-svc-65m9m
Feb 19 21:52:40.654: INFO: Created: latency-svc-b6cmx
Feb 19 21:52:40.689: INFO: Created: latency-svc-ccdkp
Feb 19 21:52:40.693: INFO: Got endpoints: latency-svc-t9lbc [801.561476ms]
Feb 19 21:52:40.693: INFO: Got endpoints: latency-svc-mppf7 [765.86798ms]
Feb 19 21:52:40.694: INFO: Got endpoints: latency-svc-2qk9b [685.358916ms]
Feb 19 21:52:40.737: INFO: Created: latency-svc-26vqf
Feb 19 21:52:40.749: INFO: Got endpoints: latency-svc-zrdq8 [386.617301ms]
Feb 19 21:52:40.751: INFO: Created: latency-svc-5j7hz
Feb 19 21:52:40.769: INFO: Created: latency-svc-pn7mm
Feb 19 21:52:40.771: INFO: Got endpoints: latency-svc-65m9m [407.477302ms]
Feb 19 21:52:40.784: INFO: Created: latency-svc-kjn5p
Feb 19 21:52:40.800: INFO: Created: latency-svc-wphvt
Feb 19 21:52:40.822: INFO: Got endpoints: latency-svc-b6cmx [459.274567ms]
Feb 19 21:52:40.823: INFO: Created: latency-svc-bdjns
Feb 19 21:52:40.841: INFO: Created: latency-svc-98xf8
Feb 19 21:52:40.852: INFO: Created: latency-svc-k7c5n
Feb 19 21:52:40.863: INFO: Got endpoints: latency-svc-ccdkp [425.717649ms]
Feb 19 21:52:40.866: INFO: Created: latency-svc-cmjw4
Feb 19 21:52:40.880: INFO: Created: latency-svc-cp5qs
Feb 19 21:52:40.890: INFO: Created: latency-svc-kp6dr
Feb 19 21:52:40.897: INFO: Got endpoints: latency-svc-26vqf [459.446314ms]
Feb 19 21:52:40.906: INFO: Created: latency-svc-2dbvv
Feb 19 21:52:40.929: INFO: Created: latency-svc-7jt9k
Feb 19 21:52:40.944: INFO: Created: latency-svc-592wc
Feb 19 21:52:40.950: INFO: Created: latency-svc-8gcdb
Feb 19 21:52:40.953: INFO: Got endpoints: latency-svc-5j7hz [468.616118ms]
Feb 19 21:52:40.967: INFO: Created: latency-svc-vmvpx
Feb 19 21:52:40.973: INFO: Created: latency-svc-rkxsc
Feb 19 21:52:40.983: INFO: Got endpoints: latency-svc-pn7mm [459.417616ms]
Feb 19 21:52:41.006: INFO: Created: latency-svc-br74h
Feb 19 21:52:41.033: INFO: Got endpoints: latency-svc-kjn5p [451.436382ms]
Feb 19 21:52:41.055: INFO: Created: latency-svc-grqgt
Feb 19 21:52:41.085: INFO: Got endpoints: latency-svc-wphvt [501.926911ms]
Feb 19 21:52:41.139: INFO: Got endpoints: latency-svc-bdjns [556.599445ms]
Feb 19 21:52:41.184: INFO: Got endpoints: latency-svc-98xf8 [602.224776ms]
Feb 19 21:52:41.234: INFO: Got endpoints: latency-svc-k7c5n [619.000353ms]
Feb 19 21:52:41.284: INFO: Got endpoints: latency-svc-cmjw4 [590.790333ms]
Feb 19 21:52:41.334: INFO: Got endpoints: latency-svc-cp5qs [639.874924ms]
Feb 19 21:52:41.386: INFO: Got endpoints: latency-svc-kp6dr [692.405079ms]
Feb 19 21:52:41.435: INFO: Got endpoints: latency-svc-2dbvv [685.407277ms]
Feb 19 21:52:41.485: INFO: Got endpoints: latency-svc-7jt9k [714.602881ms]
Feb 19 21:52:41.533: INFO: Got endpoints: latency-svc-592wc [710.248696ms]
Feb 19 21:52:41.585: INFO: Got endpoints: latency-svc-8gcdb [721.366667ms]
Feb 19 21:52:41.635: INFO: Got endpoints: latency-svc-vmvpx [737.247096ms]
Feb 19 21:52:41.684: INFO: Got endpoints: latency-svc-rkxsc [730.503098ms]
Feb 19 21:52:41.735: INFO: Got endpoints: latency-svc-br74h [752.679636ms]
Feb 19 21:52:41.784: INFO: Got endpoints: latency-svc-grqgt [750.064782ms]
Feb 19 21:52:41.784: INFO: Latencies: [48.474303ms 80.987752ms 110.533283ms 130.278779ms 135.601291ms 141.597935ms 149.184542ms 173.961615ms 174.44374ms 205.913814ms 217.659014ms 220.494325ms 224.977329ms 227.013308ms 229.167864ms 234.741462ms 235.525368ms 236.717546ms 242.308197ms 242.910468ms 244.190693ms 246.237957ms 246.307277ms 247.083772ms 252.298622ms 253.181009ms 256.218864ms 257.212279ms 260.153309ms 260.307801ms 261.773038ms 262.605817ms 268.397599ms 286.302545ms 287.639285ms 293.222456ms 297.379068ms 300.034461ms 300.552738ms 301.477854ms 303.054116ms 304.659727ms 305.437234ms 318.235137ms 321.199669ms 322.906878ms 323.636822ms 339.061382ms 368.282005ms 386.617301ms 407.477302ms 425.209452ms 425.717649ms 451.436382ms 459.274567ms 459.417616ms 459.446314ms 468.616118ms 501.926911ms 517.769584ms 525.950128ms 536.124076ms 544.177443ms 556.599445ms 590.790333ms 602.224776ms 607.173561ms 612.707955ms 619.000353ms 639.14043ms 639.874924ms 659.224621ms 662.250342ms 662.456382ms 678.513474ms 684.448633ms 685.358916ms 685.407277ms 685.859696ms 686.089215ms 689.161763ms 692.405079ms 694.679669ms 699.514679ms 704.367149ms 710.248696ms 712.873861ms 714.602881ms 716.391063ms 717.01084ms 718.36993ms 719.516303ms 721.366667ms 722.52621ms 728.516149ms 730.503098ms 737.247096ms 739.715778ms 742.68991ms 742.737169ms 743.364846ms 744.695097ms 745.759306ms 746.487832ms 746.957056ms 747.87695ms 748.175866ms 748.289744ms 748.440946ms 748.498548ms 748.58759ms 748.614536ms 748.656632ms 748.758509ms 748.778708ms 748.945338ms 748.968413ms 748.973848ms 749.047798ms 749.131989ms 749.195713ms 749.2659ms 749.268652ms 749.371153ms 749.498452ms 749.509247ms 749.511972ms 749.528737ms 749.54082ms 749.560281ms 749.609353ms 749.63515ms 749.707494ms 749.750528ms 749.798878ms 749.812067ms 749.824159ms 749.848043ms 749.870766ms 749.877665ms 749.897035ms 749.919734ms 749.967229ms 750.042134ms 750.046646ms 750.056492ms 750.064782ms 750.11758ms 750.183301ms 750.23171ms 750.258177ms 750.281117ms 750.305381ms 750.354414ms 750.396808ms 750.40125ms 750.403129ms 750.407247ms 750.429923ms 750.514496ms 750.675527ms 750.727996ms 750.731887ms 750.737488ms 750.744389ms 750.752454ms 750.856396ms 750.86108ms 750.882746ms 751.02288ms 751.072635ms 751.09635ms 751.347041ms 751.832429ms 752.137966ms 752.679636ms 753.521589ms 756.217513ms 756.865313ms 757.107008ms 760.67137ms 765.86798ms 791.141258ms 801.561476ms 808.6036ms 822.296018ms 842.53369ms 858.058489ms 874.98649ms 889.690661ms 898.639393ms 996.68947ms 1.047503683s 1.050936132s 1.054362949s 1.07498523s 1.097471411s 1.103863517s 1.12939929s 1.175249715s]
Feb 19 21:52:41.784: INFO: 50 %ile: 743.364846ms
Feb 19 21:52:41.784: INFO: 90 %ile: 760.67137ms
Feb 19 21:52:41.784: INFO: 99 %ile: 1.12939929s
Feb 19 21:52:41.784: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:52:41.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7177" for this suite.
Feb 19 21:52:57.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:52:58.081: INFO: namespace svc-latency-7177 deletion completed in 16.291922557s

• [SLOW TEST:28.143 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:52:58.086: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-a77f4f7f-961e-475a-a5ae-a4663545a9c1 in namespace container-probe-7931
Feb 19 21:53:02.148: INFO: Started pod liveness-a77f4f7f-961e-475a-a5ae-a4663545a9c1 in namespace container-probe-7931
STEP: checking the pod's current state and verifying that restartCount is present
Feb 19 21:53:02.155: INFO: Initial restart count of pod liveness-a77f4f7f-961e-475a-a5ae-a4663545a9c1 is 0
Feb 19 21:53:16.258: INFO: Restart count of pod container-probe-7931/liveness-a77f4f7f-961e-475a-a5ae-a4663545a9c1 is now 1 (14.102992986s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:53:16.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7931" for this suite.
Feb 19 21:53:22.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:53:22.451: INFO: namespace container-probe-7931 deletion completed in 6.149790455s

• [SLOW TEST:24.366 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:53:22.458: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-2347cf14-4e03-45d2-b371-f27ca08ad128
STEP: Creating a pod to test consume secrets
Feb 19 21:53:22.498: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05" in namespace "projected-7108" to be "success or failure"
Feb 19 21:53:22.501: INFO: Pod "pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.161043ms
Feb 19 21:53:24.504: INFO: Pod "pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005951724s
STEP: Saw pod success
Feb 19 21:53:24.504: INFO: Pod "pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05" satisfied condition "success or failure"
Feb 19 21:53:24.507: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 19 21:53:24.527: INFO: Waiting for pod pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05 to disappear
Feb 19 21:53:24.539: INFO: Pod pod-projected-secrets-47e7e138-d380-49ed-a7f6-8635410e4c05 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:53:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7108" for this suite.
Feb 19 21:53:30.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:53:30.702: INFO: namespace projected-7108 deletion completed in 6.158164943s

• [SLOW TEST:8.245 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:53:30.706: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Feb 19 21:53:30.808: INFO: Waiting up to 5m0s for pod "client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5" in namespace "containers-3565" to be "success or failure"
Feb 19 21:53:30.810: INFO: Pod "client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.472173ms
Feb 19 21:53:32.813: INFO: Pod "client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005442068s
Feb 19 21:53:34.816: INFO: Pod "client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008089335s
STEP: Saw pod success
Feb 19 21:53:34.816: INFO: Pod "client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5" satisfied condition "success or failure"
Feb 19 21:53:34.820: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5 container test-container: <nil>
STEP: delete the pod
Feb 19 21:53:34.841: INFO: Waiting for pod client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5 to disappear
Feb 19 21:53:34.847: INFO: Pod client-containers-ae19d4c1-afc0-4d5e-a969-196b3dc749f5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:53:34.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3565" for this suite.
Feb 19 21:53:40.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:53:41.152: INFO: namespace containers-3565 deletion completed in 6.299518168s

• [SLOW TEST:10.447 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:53:41.157: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 19 21:53:41.203: INFO: Waiting up to 5m0s for pod "pod-70f99562-d7e8-4f89-a070-a376bb7b801f" in namespace "emptydir-5963" to be "success or failure"
Feb 19 21:53:41.207: INFO: Pod "pod-70f99562-d7e8-4f89-a070-a376bb7b801f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618952ms
Feb 19 21:53:43.211: INFO: Pod "pod-70f99562-d7e8-4f89-a070-a376bb7b801f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007040348s
Feb 19 21:53:45.214: INFO: Pod "pod-70f99562-d7e8-4f89-a070-a376bb7b801f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01010205s
STEP: Saw pod success
Feb 19 21:53:45.214: INFO: Pod "pod-70f99562-d7e8-4f89-a070-a376bb7b801f" satisfied condition "success or failure"
Feb 19 21:53:45.217: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-70f99562-d7e8-4f89-a070-a376bb7b801f container test-container: <nil>
STEP: delete the pod
Feb 19 21:53:45.244: INFO: Waiting for pod pod-70f99562-d7e8-4f89-a070-a376bb7b801f to disappear
Feb 19 21:53:45.249: INFO: Pod pod-70f99562-d7e8-4f89-a070-a376bb7b801f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:53:45.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5963" for this suite.
Feb 19 21:53:51.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:53:51.430: INFO: namespace emptydir-5963 deletion completed in 6.176374644s

• [SLOW TEST:10.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:53:51.433: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Feb 19 21:53:51.472: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 19 21:53:51.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1124'
Feb 19 21:53:51.818: INFO: stderr: ""
Feb 19 21:53:51.818: INFO: stdout: "service/redis-slave created\n"
Feb 19 21:53:51.818: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 19 21:53:51.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1124'
Feb 19 21:53:52.073: INFO: stderr: ""
Feb 19 21:53:52.073: INFO: stdout: "service/redis-master created\n"
Feb 19 21:53:52.073: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 19 21:53:52.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1124'
Feb 19 21:53:52.333: INFO: stderr: ""
Feb 19 21:53:52.333: INFO: stdout: "service/frontend created\n"
Feb 19 21:53:52.334: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 19 21:53:52.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1124'
Feb 19 21:53:52.625: INFO: stderr: ""
Feb 19 21:53:52.625: INFO: stdout: "deployment.apps/frontend created\n"
Feb 19 21:53:52.625: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 19 21:53:52.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1124'
Feb 19 21:53:52.881: INFO: stderr: ""
Feb 19 21:53:52.881: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 19 21:53:52.881: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 19 21:53:52.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-1124'
Feb 19 21:53:53.193: INFO: stderr: ""
Feb 19 21:53:53.193: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 19 21:53:53.193: INFO: Waiting for all frontend pods to be Running.
Feb 19 21:54:18.244: INFO: Waiting for frontend to serve content.
Feb 19 21:54:19.302: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Feb 19 21:54:24.318: INFO: Trying to add a new entry to the guestbook.
Feb 19 21:54:24.332: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 19 21:54:24.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1124'
Feb 19 21:54:24.442: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:54:24.442: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 19 21:54:24.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1124'
Feb 19 21:54:24.561: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:54:24.561: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 19 21:54:24.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1124'
Feb 19 21:54:24.664: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:54:24.664: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 19 21:54:24.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1124'
Feb 19 21:54:24.780: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:54:24.780: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 19 21:54:24.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1124'
Feb 19 21:54:24.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:54:24.864: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 19 21:54:24.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete --grace-period=0 --force -f - --namespace=kubectl-1124'
Feb 19 21:54:24.949: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 19 21:54:24.949: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:54:24.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1124" for this suite.
Feb 19 21:55:06.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:55:07.121: INFO: namespace kubectl-1124 deletion completed in 42.167984046s

• [SLOW TEST:75.688 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:55:07.123: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 19 21:55:07.208: INFO: namespace kubectl-9478
Feb 19 21:55:07.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-9478'
Feb 19 21:55:07.503: INFO: stderr: ""
Feb 19 21:55:07.503: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 19 21:55:08.506: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:55:08.506: INFO: Found 0 / 1
Feb 19 21:55:09.506: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:55:09.506: INFO: Found 1 / 1
Feb 19 21:55:09.506: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 19 21:55:09.510: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 21:55:09.510: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 19 21:55:09.510: INFO: wait on redis-master startup in kubectl-9478 
Feb 19 21:55:09.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs redis-master-zzkwp redis-master --namespace=kubectl-9478'
Feb 19 21:55:09.609: INFO: stderr: ""
Feb 19 21:55:09.609: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Feb 21:55:08.544 # Server started, Redis version 3.2.12\n1:M 19 Feb 21:55:08.544 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Feb 21:55:08.544 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 19 21:55:09.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9478'
Feb 19 21:55:09.761: INFO: stderr: ""
Feb 19 21:55:09.761: INFO: stdout: "service/rm2 exposed\n"
Feb 19 21:55:09.844: INFO: Service rm2 in namespace kubectl-9478 found.
STEP: exposing service
Feb 19 21:55:11.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9478'
Feb 19 21:55:12.239: INFO: stderr: ""
Feb 19 21:55:12.239: INFO: stdout: "service/rm3 exposed\n"
Feb 19 21:55:12.254: INFO: Service rm3 in namespace kubectl-9478 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:55:14.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9478" for this suite.
Feb 19 21:55:36.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:55:36.448: INFO: namespace kubectl-9478 deletion completed in 22.186115602s

• [SLOW TEST:29.326 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:55:36.452: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 19 21:55:36.503: INFO: Waiting up to 5m0s for pod "pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6" in namespace "emptydir-3744" to be "success or failure"
Feb 19 21:55:36.506: INFO: Pod "pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.071939ms
Feb 19 21:55:38.509: INFO: Pod "pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00593585s
STEP: Saw pod success
Feb 19 21:55:38.509: INFO: Pod "pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6" satisfied condition "success or failure"
Feb 19 21:55:38.511: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6 container test-container: <nil>
STEP: delete the pod
Feb 19 21:55:38.529: INFO: Waiting for pod pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6 to disappear
Feb 19 21:55:38.541: INFO: Pod pod-c8c6ad31-c2bf-48c2-8ca0-e61207c4d4d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:55:38.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3744" for this suite.
Feb 19 21:55:44.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:55:44.715: INFO: namespace emptydir-3744 deletion completed in 6.163802567s

• [SLOW TEST:8.263 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:55:44.717: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 19 21:55:44.771: INFO: Waiting up to 5m0s for pod "downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705" in namespace "downward-api-6166" to be "success or failure"
Feb 19 21:55:44.775: INFO: Pod "downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705": Phase="Pending", Reason="", readiness=false. Elapsed: 3.896592ms
Feb 19 21:55:46.778: INFO: Pod "downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007116437s
STEP: Saw pod success
Feb 19 21:55:46.778: INFO: Pod "downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705" satisfied condition "success or failure"
Feb 19 21:55:46.782: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705 container dapi-container: <nil>
STEP: delete the pod
Feb 19 21:55:46.811: INFO: Waiting for pod downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705 to disappear
Feb 19 21:55:46.821: INFO: Pod downward-api-0f1ac2e8-7b9d-42e9-bf16-64c3a3855705 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:55:46.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6166" for this suite.
Feb 19 21:55:52.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:55:52.980: INFO: namespace downward-api-6166 deletion completed in 6.15301924s

• [SLOW TEST:8.263 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:55:52.983: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4287
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 19 21:55:53.020: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 19 21:56:15.233: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.56.1.25:8080/dial?request=hostName&protocol=http&host=10.56.0.28&port=8080&tries=1'] Namespace:pod-network-test-4287 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 21:56:15.233: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:56:15.420: INFO: Waiting for endpoints: map[]
Feb 19 21:56:15.423: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.56.1.25:8080/dial?request=hostName&protocol=http&host=10.56.2.100&port=8080&tries=1'] Namespace:pod-network-test-4287 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 21:56:15.423: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:56:15.650: INFO: Waiting for endpoints: map[]
Feb 19 21:56:15.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.56.1.25:8080/dial?request=hostName&protocol=http&host=10.56.1.24&port=8080&tries=1'] Namespace:pod-network-test-4287 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 21:56:15.655: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 21:56:15.850: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:56:15.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4287" for this suite.
Feb 19 21:56:37.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:56:38.035: INFO: namespace pod-network-test-4287 deletion completed in 22.181229807s

• [SLOW TEST:45.053 seconds]
[sig-network] Networking
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:56:38.040: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 19 21:56:40.214: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:56:40.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-15" for this suite.
Feb 19 21:56:46.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:56:46.417: INFO: namespace container-runtime-15 deletion completed in 6.166506731s

• [SLOW TEST:8.378 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:56:46.422: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:56:46.466: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62" in namespace "downward-api-8712" to be "success or failure"
Feb 19 21:56:46.470: INFO: Pod "downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529946ms
Feb 19 21:56:48.474: INFO: Pod "downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007873426s
STEP: Saw pod success
Feb 19 21:56:48.474: INFO: Pod "downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62" satisfied condition "success or failure"
Feb 19 21:56:48.477: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62 container client-container: <nil>
STEP: delete the pod
Feb 19 21:56:48.511: INFO: Waiting for pod downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62 to disappear
Feb 19 21:56:48.515: INFO: Pod downwardapi-volume-e257eb46-0f34-4374-a989-b5dd59ecfb62 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:56:48.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8712" for this suite.
Feb 19 21:56:54.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:56:54.680: INFO: namespace downward-api-8712 deletion completed in 6.158806489s

• [SLOW TEST:8.258 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:56:54.683: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 21:56:55.301: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 19 21:56:55.336: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 19 21:57:00.341: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 19 21:57:00.341: INFO: Creating deployment "test-rolling-update-deployment"
Feb 19 21:57:00.353: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 19 21:57:00.367: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 19 21:57:02.373: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 19 21:57:02.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746220, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746220, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746220, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746220, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 21:57:04.380: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 19 21:57:04.388: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5734,SelfLink:/apis/apps/v1/namespaces/deployment-5734/deployments/test-rolling-update-deployment,UID:1dc6e6b8-408d-4d90-93ba-0eed7b86fb62,ResourceVersion:108006,Generation:1,CreationTimestamp:2020-02-19 21:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-02-19 21:57:00 +0000 UTC 2020-02-19 21:57:00 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-02-19 21:57:02 +0000 UTC 2020-02-19 21:57:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 19 21:57:04.391: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-5734,SelfLink:/apis/apps/v1/namespaces/deployment-5734/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:39f35a11-0655-4881-b173-4227d417bfa1,ResourceVersion:107998,Generation:1,CreationTimestamp:2020-02-19 21:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1dc6e6b8-408d-4d90-93ba-0eed7b86fb62 0xc002b45267 0xc002b45268}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 19 21:57:04.391: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 19 21:57:04.391: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5734,SelfLink:/apis/apps/v1/namespaces/deployment-5734/replicasets/test-rolling-update-controller,UID:2958d449-47f1-4b8d-a6bc-b020dfa06688,ResourceVersion:108005,Generation:2,CreationTimestamp:2020-02-19 21:56:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1dc6e6b8-408d-4d90-93ba-0eed7b86fb62 0xc002b45197 0xc002b45198}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 19 21:57:04.394: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-rm8f4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-rm8f4,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-5734,SelfLink:/api/v1/namespaces/deployment-5734/pods/test-rolling-update-deployment-79f6b9d75c-rm8f4,UID:1a53c42b-2a73-464c-8717-ba3fdf2bac55,ResourceVersion:107997,Generation:0,CreationTimestamp:2020-02-19 21:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 39f35a11-0655-4881-b173-4227d417bfa1 0xc002b45b67 0xc002b45b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-szttl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-szttl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-szttl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b45bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b45bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:57:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:57:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:57:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 21:57:00 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:10.56.2.103,StartTime:2020-02-19 21:57:00 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-02-19 21:57:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://8fe5a9488d10ea1bbcdb7b52bb75c586952573df68ff160bfbe674757ed2a8b7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:57:04.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5734" for this suite.
Feb 19 21:57:12.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:57:12.650: INFO: namespace deployment-5734 deletion completed in 8.252145817s

• [SLOW TEST:17.967 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:57:12.652: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 19 21:57:15.874: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:57:15.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4300" for this suite.
Feb 19 21:57:21.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:57:22.056: INFO: namespace container-runtime-4300 deletion completed in 6.157068217s

• [SLOW TEST:9.405 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:57:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 19 21:57:22.223: INFO: Number of nodes with available pods: 0
Feb 19 21:57:22.223: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 21:57:23.231: INFO: Number of nodes with available pods: 0
Feb 19 21:57:23.231: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 21:57:24.232: INFO: Number of nodes with available pods: 2
Feb 19 21:57:24.232: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 21:57:25.241: INFO: Number of nodes with available pods: 3
Feb 19 21:57:25.241: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 19 21:57:25.408: INFO: Number of nodes with available pods: 2
Feb 19 21:57:25.408: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 21:57:26.414: INFO: Number of nodes with available pods: 2
Feb 19 21:57:26.414: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 21:57:27.422: INFO: Number of nodes with available pods: 2
Feb 19 21:57:27.422: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 21:57:28.415: INFO: Number of nodes with available pods: 3
Feb 19 21:57:28.416: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5118, will wait for the garbage collector to delete the pods
Feb 19 21:57:28.502: INFO: Deleting DaemonSet.extensions daemon-set took: 7.787047ms
Feb 19 21:57:29.002: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.241203ms
Feb 19 21:57:35.506: INFO: Number of nodes with available pods: 0
Feb 19 21:57:35.506: INFO: Number of running nodes: 0, number of available pods: 0
Feb 19 21:57:35.510: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5118/daemonsets","resourceVersion":"108219"},"items":null}

Feb 19 21:57:35.512: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5118/pods","resourceVersion":"108219"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:57:35.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5118" for this suite.
Feb 19 21:57:41.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:57:41.704: INFO: namespace daemonsets-5118 deletion completed in 6.176476723s

• [SLOW TEST:19.644 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:57:41.709: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-28800b9b-6bda-402d-a12c-83f780984f13
STEP: Creating a pod to test consume secrets
Feb 19 21:57:41.795: INFO: Waiting up to 5m0s for pod "pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5" in namespace "secrets-9950" to be "success or failure"
Feb 19 21:57:41.798: INFO: Pod "pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.866299ms
Feb 19 21:57:43.802: INFO: Pod "pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006384431s
STEP: Saw pod success
Feb 19 21:57:43.802: INFO: Pod "pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5" satisfied condition "success or failure"
Feb 19 21:57:43.805: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5 container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 21:57:43.827: INFO: Waiting for pod pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5 to disappear
Feb 19 21:57:43.833: INFO: Pod pod-secrets-88a832c4-9b0b-4544-8169-946b826066d5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:57:43.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9950" for this suite.
Feb 19 21:57:49.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:57:49.989: INFO: namespace secrets-9950 deletion completed in 6.153161551s
STEP: Destroying namespace "secret-namespace-8694" for this suite.
Feb 19 21:57:58.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:57:58.248: INFO: namespace secret-namespace-8694 deletion completed in 8.258140287s

• [SLOW TEST:16.540 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:57:58.251: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:57:58.311: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f" in namespace "projected-8136" to be "success or failure"
Feb 19 21:57:58.314: INFO: Pod "downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50609ms
Feb 19 21:58:00.318: INFO: Pod "downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006612254s
STEP: Saw pod success
Feb 19 21:58:00.318: INFO: Pod "downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f" satisfied condition "success or failure"
Feb 19 21:58:00.320: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f container client-container: <nil>
STEP: delete the pod
Feb 19 21:58:00.343: INFO: Waiting for pod downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f to disappear
Feb 19 21:58:00.350: INFO: Pod downwardapi-volume-5d5ba2ca-6881-4d94-8368-a2eb4893b15f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:58:00.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8136" for this suite.
Feb 19 21:58:06.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:58:06.539: INFO: namespace projected-8136 deletion completed in 6.182794157s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:58:06.541: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 21:58:06.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32" in namespace "downward-api-7295" to be "success or failure"
Feb 19 21:58:06.635: INFO: Pod "downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32": Phase="Pending", Reason="", readiness=false. Elapsed: 3.181958ms
Feb 19 21:58:08.638: INFO: Pod "downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006588251s
STEP: Saw pod success
Feb 19 21:58:08.638: INFO: Pod "downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32" satisfied condition "success or failure"
Feb 19 21:58:08.641: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32 container client-container: <nil>
STEP: delete the pod
Feb 19 21:58:08.663: INFO: Waiting for pod downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32 to disappear
Feb 19 21:58:08.669: INFO: Pod downwardapi-volume-9a0130e1-0e3f-404a-b4c8-8120f5c8bc32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:58:08.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7295" for this suite.
Feb 19 21:58:14.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:58:14.841: INFO: namespace downward-api-7295 deletion completed in 6.168384414s

• [SLOW TEST:8.300 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:58:14.842: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:58:18.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1421" for this suite.
Feb 19 21:58:24.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:58:25.142: INFO: namespace kubelet-test-1421 deletion completed in 6.246811154s

• [SLOW TEST:10.301 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:58:25.145: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-915c8866-9d95-4af5-83fb-96884bb571ef
STEP: Creating a pod to test consume configMaps
Feb 19 21:58:25.418: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816" in namespace "projected-9311" to be "success or failure"
Feb 19 21:58:25.423: INFO: Pod "pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816": Phase="Pending", Reason="", readiness=false. Elapsed: 5.511439ms
Feb 19 21:58:27.469: INFO: Pod "pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051272214s
STEP: Saw pod success
Feb 19 21:58:27.469: INFO: Pod "pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816" satisfied condition "success or failure"
Feb 19 21:58:27.481: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 21:58:27.497: INFO: Waiting for pod pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816 to disappear
Feb 19 21:58:27.500: INFO: Pod pod-projected-configmaps-64238247-5e6b-4be3-b369-63561438e816 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:58:27.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9311" for this suite.
Feb 19 21:58:33.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:58:33.646: INFO: namespace projected-9311 deletion completed in 6.14282121s

• [SLOW TEST:8.502 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:58:33.650: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-0f126faa-25f5-4661-9159-86a164c303fd
STEP: Creating secret with name s-test-opt-upd-42316bf7-7e27-451a-95cb-a5b63d586d64
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0f126faa-25f5-4661-9159-86a164c303fd
STEP: Updating secret s-test-opt-upd-42316bf7-7e27-451a-95cb-a5b63d586d64
STEP: Creating secret with name s-test-opt-create-74466dda-b1ac-4df5-a782-ae55f69ba227
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:58:37.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2557" for this suite.
Feb 19 21:59:01.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 21:59:02.067: INFO: namespace secrets-2557 deletion completed in 24.278071876s

• [SLOW TEST:28.417 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 21:59:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-5360404a-c046-4885-bc19-5d8f37a001b3 in namespace container-probe-4247
Feb 19 21:59:04.139: INFO: Started pod busybox-5360404a-c046-4885-bc19-5d8f37a001b3 in namespace container-probe-4247
STEP: checking the pod's current state and verifying that restartCount is present
Feb 19 21:59:04.141: INFO: Initial restart count of pod busybox-5360404a-c046-4885-bc19-5d8f37a001b3 is 0
Feb 19 21:59:58.430: INFO: Restart count of pod container-probe-4247/busybox-5360404a-c046-4885-bc19-5d8f37a001b3 is now 1 (54.288983871s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 21:59:58.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4247" for this suite.
Feb 19 22:00:04.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:00:04.629: INFO: namespace container-probe-4247 deletion completed in 6.174452913s

• [SLOW TEST:62.560 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:00:04.630: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:00:04.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35" in namespace "projected-7586" to be "success or failure"
Feb 19 22:00:04.759: INFO: Pod "downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35": Phase="Pending", Reason="", readiness=false. Elapsed: 9.023686ms
Feb 19 22:00:06.765: INFO: Pod "downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014795343s
STEP: Saw pod success
Feb 19 22:00:06.765: INFO: Pod "downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35" satisfied condition "success or failure"
Feb 19 22:00:06.768: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35 container client-container: <nil>
STEP: delete the pod
Feb 19 22:00:06.793: INFO: Waiting for pod downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35 to disappear
Feb 19 22:00:06.796: INFO: Pod downwardapi-volume-497aedbd-689d-460b-89ed-555139307a35 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:00:06.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7586" for this suite.
Feb 19 22:00:12.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:00:13.080: INFO: namespace projected-7586 deletion completed in 6.281318121s

• [SLOW TEST:8.450 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:00:13.083: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:00:13.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3670" for this suite.
Feb 19 22:00:35.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:00:35.400: INFO: namespace pods-3670 deletion completed in 22.165939848s

• [SLOW TEST:22.318 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:00:35.404: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-nlmg
STEP: Creating a pod to test atomic-volume-subpath
Feb 19 22:00:35.524: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nlmg" in namespace "subpath-584" to be "success or failure"
Feb 19 22:00:35.527: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.293651ms
Feb 19 22:00:37.530: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006497506s
Feb 19 22:00:39.534: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 4.009960008s
Feb 19 22:00:41.537: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 6.013169338s
Feb 19 22:00:43.544: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 8.020618865s
Feb 19 22:00:45.547: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 10.023415947s
Feb 19 22:00:47.550: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 12.026445681s
Feb 19 22:00:49.554: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 14.029847091s
Feb 19 22:00:51.556: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 16.032539776s
Feb 19 22:00:53.560: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 18.035810757s
Feb 19 22:00:55.568: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 20.044757231s
Feb 19 22:00:57.573: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Running", Reason="", readiness=true. Elapsed: 22.049314508s
Feb 19 22:00:59.576: INFO: Pod "pod-subpath-test-downwardapi-nlmg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05265799s
STEP: Saw pod success
Feb 19 22:00:59.576: INFO: Pod "pod-subpath-test-downwardapi-nlmg" satisfied condition "success or failure"
Feb 19 22:00:59.579: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-subpath-test-downwardapi-nlmg container test-container-subpath-downwardapi-nlmg: <nil>
STEP: delete the pod
Feb 19 22:00:59.606: INFO: Waiting for pod pod-subpath-test-downwardapi-nlmg to disappear
Feb 19 22:00:59.612: INFO: Pod pod-subpath-test-downwardapi-nlmg no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nlmg
Feb 19 22:00:59.612: INFO: Deleting pod "pod-subpath-test-downwardapi-nlmg" in namespace "subpath-584"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:00:59.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-584" for this suite.
Feb 19 22:01:05.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:01:05.755: INFO: namespace subpath-584 deletion completed in 6.136578908s

• [SLOW TEST:30.351 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:01:05.759: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:01:05.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4396" for this suite.
Feb 19 22:01:11.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:01:11.966: INFO: namespace services-4396 deletion completed in 6.164679236s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.208 seconds]
[sig-network] Services
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:01:11.969: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 19 22:01:12.845: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5211,SelfLink:/api/v1/namespaces/watch-5211/configmaps/e2e-watch-test-resource-version,UID:3b4b67f2-91c6-48af-bacf-7e95216332ff,ResourceVersion:109347,Generation:0,CreationTimestamp:2020-02-19 22:01:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 19 22:01:12.846: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5211,SelfLink:/api/v1/namespaces/watch-5211/configmaps/e2e-watch-test-resource-version,UID:3b4b67f2-91c6-48af-bacf-7e95216332ff,ResourceVersion:109348,Generation:0,CreationTimestamp:2020-02-19 22:01:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:01:12.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5211" for this suite.
Feb 19 22:01:18.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:01:19.027: INFO: namespace watch-5211 deletion completed in 6.166417912s

• [SLOW TEST:7.058 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:01:19.029: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 19 22:01:19.077: INFO: Waiting up to 5m0s for pod "pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063" in namespace "emptydir-2046" to be "success or failure"
Feb 19 22:01:19.088: INFO: Pod "pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063": Phase="Pending", Reason="", readiness=false. Elapsed: 11.04875ms
Feb 19 22:01:21.093: INFO: Pod "pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016060372s
Feb 19 22:01:23.096: INFO: Pod "pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019665518s
STEP: Saw pod success
Feb 19 22:01:23.097: INFO: Pod "pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063" satisfied condition "success or failure"
Feb 19 22:01:23.101: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063 container test-container: <nil>
STEP: delete the pod
Feb 19 22:01:23.154: INFO: Waiting for pod pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063 to disappear
Feb 19 22:01:23.166: INFO: Pod pod-6aa1ee89-d7ce-4e79-b3c6-e504d9f02063 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:01:23.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2046" for this suite.
Feb 19 22:01:29.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:01:29.365: INFO: namespace emptydir-2046 deletion completed in 6.18612739s

• [SLOW TEST:10.337 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:01:29.367: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Feb 19 22:01:31.438: INFO: Pod pod-hostip-d9d52072-5e2d-4433-9850-1d677b66ea59 has hostIP: 10.142.0.4
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:01:31.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6939" for this suite.
Feb 19 22:01:53.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:01:53.622: INFO: namespace pods-6939 deletion completed in 22.180139933s

• [SLOW TEST:24.255 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:01:53.626: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:01:53.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d" in namespace "downward-api-6010" to be "success or failure"
Feb 19 22:01:53.699: INFO: Pod "downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.290183ms
Feb 19 22:01:55.776: INFO: Pod "downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078819889s
STEP: Saw pod success
Feb 19 22:01:55.776: INFO: Pod "downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d" satisfied condition "success or failure"
Feb 19 22:01:55.886: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d container client-container: <nil>
STEP: delete the pod
Feb 19 22:01:56.466: INFO: Waiting for pod downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d to disappear
Feb 19 22:01:56.588: INFO: Pod downwardapi-volume-e3df1587-60f8-4b03-a02e-a00b2c3c864d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:01:56.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6010" for this suite.
Feb 19 22:02:02.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:02:02.836: INFO: namespace downward-api-6010 deletion completed in 6.165643623s

• [SLOW TEST:9.210 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:02:02.837: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:02:02.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f" in namespace "downward-api-4606" to be "success or failure"
Feb 19 22:02:02.892: INFO: Pod "downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021394ms
Feb 19 22:02:04.895: INFO: Pod "downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008962126s
STEP: Saw pod success
Feb 19 22:02:04.895: INFO: Pod "downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f" satisfied condition "success or failure"
Feb 19 22:02:04.897: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f container client-container: <nil>
STEP: delete the pod
Feb 19 22:02:04.917: INFO: Waiting for pod downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f to disappear
Feb 19 22:02:04.922: INFO: Pod downwardapi-volume-9f00cea6-6692-4ad6-8b28-eacd4a19330f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:02:04.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4606" for this suite.
Feb 19 22:02:10.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:02:11.110: INFO: namespace downward-api-4606 deletion completed in 6.18186043s

• [SLOW TEST:8.273 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:02:11.112: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5471
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 19 22:02:11.178: INFO: Found 0 stateful pods, waiting for 3
Feb 19 22:02:21.182: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:02:21.182: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:02:21.182: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 19 22:02:21.216: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 19 22:02:31.255: INFO: Updating stateful set ss2
Feb 19 22:02:31.348: INFO: Waiting for Pod statefulset-5471/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Feb 19 22:02:41.468: INFO: Found 1 stateful pods, waiting for 3
Feb 19 22:02:51.471: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:02:51.472: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:02:51.472: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 19 22:02:51.496: INFO: Updating stateful set ss2
Feb 19 22:02:51.542: INFO: Waiting for Pod statefulset-5471/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Feb 19 22:03:01.568: INFO: Updating stateful set ss2
Feb 19 22:03:01.589: INFO: Waiting for StatefulSet statefulset-5471/ss2 to complete update
Feb 19 22:03:01.590: INFO: Waiting for Pod statefulset-5471/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 19 22:03:11.596: INFO: Deleting all statefulset in ns statefulset-5471
Feb 19 22:03:11.605: INFO: Scaling statefulset ss2 to 0
Feb 19 22:03:21.636: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 22:03:21.640: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:03:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5471" for this suite.
Feb 19 22:03:27.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:03:27.809: INFO: namespace statefulset-5471 deletion completed in 6.149241681s

• [SLOW TEST:76.697 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:03:27.812: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-nqnq
STEP: Creating a pod to test atomic-volume-subpath
Feb 19 22:03:27.860: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nqnq" in namespace "subpath-7149" to be "success or failure"
Feb 19 22:03:27.869: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.545492ms
Feb 19 22:03:29.874: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 2.01373883s
Feb 19 22:03:31.877: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 4.016582731s
Feb 19 22:03:33.880: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 6.019717655s
Feb 19 22:03:35.885: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 8.024137582s
Feb 19 22:03:37.888: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027358304s
Feb 19 22:03:39.991: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 12.130256205s
Feb 19 22:03:42.095: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 14.234965983s
Feb 19 22:03:44.098: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 16.237757633s
Feb 19 22:03:46.101: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 18.240500473s
Feb 19 22:03:48.104: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Running", Reason="", readiness=true. Elapsed: 20.243491127s
Feb 19 22:03:50.107: INFO: Pod "pod-subpath-test-configmap-nqnq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.246219626s
STEP: Saw pod success
Feb 19 22:03:50.107: INFO: Pod "pod-subpath-test-configmap-nqnq" satisfied condition "success or failure"
Feb 19 22:03:50.109: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-subpath-test-configmap-nqnq container test-container-subpath-configmap-nqnq: <nil>
STEP: delete the pod
Feb 19 22:03:50.148: INFO: Waiting for pod pod-subpath-test-configmap-nqnq to disappear
Feb 19 22:03:50.154: INFO: Pod pod-subpath-test-configmap-nqnq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nqnq
Feb 19 22:03:50.155: INFO: Deleting pod "pod-subpath-test-configmap-nqnq" in namespace "subpath-7149"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:03:50.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7149" for this suite.
Feb 19 22:03:58.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:03:58.424: INFO: namespace subpath-7149 deletion completed in 8.264648045s

• [SLOW TEST:30.613 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:03:58.427: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 19 22:03:58.537: INFO: Waiting up to 5m0s for pod "pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef" in namespace "emptydir-475" to be "success or failure"
Feb 19 22:03:58.546: INFO: Pod "pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.673091ms
Feb 19 22:04:00.550: INFO: Pod "pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012497058s
STEP: Saw pod success
Feb 19 22:04:00.550: INFO: Pod "pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef" satisfied condition "success or failure"
Feb 19 22:04:00.552: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef container test-container: <nil>
STEP: delete the pod
Feb 19 22:04:00.622: INFO: Waiting for pod pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef to disappear
Feb 19 22:04:00.627: INFO: Pod pod-2442a8c3-f5b7-4c91-add9-f0fa7686aaef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:04:00.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-475" for this suite.
Feb 19 22:04:06.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:04:06.801: INFO: namespace emptydir-475 deletion completed in 6.171173478s

• [SLOW TEST:8.374 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:04:06.802: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 19 22:04:06.845: INFO: PodSpec: initContainers in spec.initContainers
Feb 19 22:04:50.864: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a6c75eac-5fa4-45a2-9b02-1602bad07c00", GenerateName:"", Namespace:"init-container-2159", SelfLink:"/api/v1/namespaces/init-container-2159/pods/pod-init-a6c75eac-5fa4-45a2-9b02-1602bad07c00", UID:"d1e37372-e433-4644-a82d-f4b03f8f38db", ResourceVersion:"110481", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717746646, loc:(*time.Location)(0x7ed0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"845050564"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-b86s6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00337c040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b86s6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b86s6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b86s6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002057af8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gke-c115-default-pool-249bf33f-qclh", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001996840), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002057d10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002057d30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002057d38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002057d3c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746646, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746646, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746646, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746646, loc:(*time.Location)(0x7ed0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.142.0.2", PodIP:"10.56.1.33", StartTime:(*v1.Time)(0xc00226aa60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d60e70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d60ee0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://623f47151380874bd0702ec41fc1acafebd6049a7180834c87dac84dd35922b1"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00226aac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00226aaa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:04:50.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2159" for this suite.
Feb 19 22:05:14.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:05:15.012: INFO: namespace init-container-2159 deletion completed in 24.142839287s

• [SLOW TEST:68.211 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:05:15.015: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:05:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:05:17.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8200" for this suite.
Feb 19 22:06:11.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:06:11.307: INFO: namespace pods-8200 deletion completed in 54.184202176s

• [SLOW TEST:56.292 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:06:11.308: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:06:11.363: INFO: Creating deployment "test-recreate-deployment"
Feb 19 22:06:11.377: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 19 22:06:11.409: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 19 22:06:13.417: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 19 22:06:13.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746771, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746771, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746771, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717746771, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:06:15.427: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 19 22:06:15.435: INFO: Updating deployment test-recreate-deployment
Feb 19 22:06:15.435: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 19 22:06:15.667: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4806,SelfLink:/apis/apps/v1/namespaces/deployment-4806/deployments/test-recreate-deployment,UID:9bbca9b9-c4bb-46a2-bebf-4f50d41a83e2,ResourceVersion:110865,Generation:2,CreationTimestamp:2020-02-19 22:06:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-02-19 22:06:15 +0000 UTC 2020-02-19 22:06:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-02-19 22:06:15 +0000 UTC 2020-02-19 22:06:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 19 22:06:15.671: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-4806,SelfLink:/apis/apps/v1/namespaces/deployment-4806/replicasets/test-recreate-deployment-5c8c9cc69d,UID:a3346a30-ee27-4c7a-bc4f-8345b57d7d55,ResourceVersion:110863,Generation:1,CreationTimestamp:2020-02-19 22:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9bbca9b9-c4bb-46a2-bebf-4f50d41a83e2 0xc000314017 0xc000314018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 19 22:06:15.671: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 19 22:06:15.672: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-4806,SelfLink:/apis/apps/v1/namespaces/deployment-4806/replicasets/test-recreate-deployment-6df85df6b9,UID:df614ff7-72d2-4737-9783-4c8777dc1486,ResourceVersion:110855,Generation:2,CreationTimestamp:2020-02-19 22:06:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9bbca9b9-c4bb-46a2-bebf-4f50d41a83e2 0xc0003142c7 0xc0003142c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 19 22:06:15.674: INFO: Pod "test-recreate-deployment-5c8c9cc69d-zs2ms" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-zs2ms,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-4806,SelfLink:/api/v1/namespaces/deployment-4806/pods/test-recreate-deployment-5c8c9cc69d-zs2ms,UID:d9fcf013-5d06-4f5f-adc1-d737c09d45bf,ResourceVersion:110864,Generation:0,CreationTimestamp:2020-02-19 22:06:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d a3346a30-ee27-4c7a-bc4f-8345b57d7d55 0xc0020564c7 0xc0020564c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qn6qr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qn6qr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qn6qr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002056530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002056550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:06:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:06:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:06:15 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:06:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:06:15.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4806" for this suite.
Feb 19 22:06:21.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:06:21.823: INFO: namespace deployment-4806 deletion completed in 6.141657444s

• [SLOW TEST:10.516 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:06:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Feb 19 22:06:21.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-6385'
Feb 19 22:06:22.364: INFO: stderr: ""
Feb 19 22:06:22.364: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 19 22:06:23.369: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:06:23.369: INFO: Found 0 / 1
Feb 19 22:06:24.367: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:06:24.367: INFO: Found 1 / 1
Feb 19 22:06:24.367: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 19 22:06:24.372: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:06:24.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 19 22:06:24.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 patch pod redis-master-ntmb7 --namespace=kubectl-6385 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 19 22:06:24.464: INFO: stderr: ""
Feb 19 22:06:24.464: INFO: stdout: "pod/redis-master-ntmb7 patched\n"
STEP: checking annotations
Feb 19 22:06:24.467: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:06:24.467: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:06:24.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6385" for this suite.
Feb 19 22:06:46.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:06:46.632: INFO: namespace kubectl-6385 deletion completed in 22.162632687s

• [SLOW TEST:24.805 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:06:46.636: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6115
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 19 22:06:46.714: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 19 22:07:08.956: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.56.2.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6115 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:07:08.956: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:07:10.107: INFO: Found all expected endpoints: [netserver-0]
Feb 19 22:07:10.209: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.56.0.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6115 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:07:10.209: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:07:11.458: INFO: Found all expected endpoints: [netserver-1]
Feb 19 22:07:11.462: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.56.1.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6115 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:07:11.462: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:07:12.667: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:07:12.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6115" for this suite.
Feb 19 22:07:34.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:07:34.912: INFO: namespace pod-network-test-6115 deletion completed in 22.239119808s

• [SLOW TEST:48.277 seconds]
[sig-network] Networking
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:07:34.913: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-e4660490-2ae0-4484-8468-994edc38aa86
STEP: Creating a pod to test consume configMaps
Feb 19 22:07:34.970: INFO: Waiting up to 5m0s for pod "pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de" in namespace "configmap-6396" to be "success or failure"
Feb 19 22:07:34.974: INFO: Pod "pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862116ms
Feb 19 22:07:36.978: INFO: Pod "pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de": Phase="Running", Reason="", readiness=true. Elapsed: 2.008609168s
Feb 19 22:07:38.987: INFO: Pod "pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017697975s
STEP: Saw pod success
Feb 19 22:07:38.987: INFO: Pod "pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de" satisfied condition "success or failure"
Feb 19 22:07:38.990: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:07:39.018: INFO: Waiting for pod pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de to disappear
Feb 19 22:07:39.025: INFO: Pod pod-configmaps-1de2cb1f-e548-4fe6-9f2b-55ba9fe2f7de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:07:39.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6396" for this suite.
Feb 19 22:07:45.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:07:45.180: INFO: namespace configmap-6396 deletion completed in 6.151390719s

• [SLOW TEST:10.268 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:07:45.183: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Feb 19 22:07:45.223: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-365921390 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:07:45.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-227" for this suite.
Feb 19 22:07:51.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:07:51.457: INFO: namespace kubectl-227 deletion completed in 6.151100155s

• [SLOW TEST:6.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:07:51.462: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:07:51.502: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:07:53.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1908" for this suite.
Feb 19 22:08:37.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:08:37.883: INFO: namespace pods-1908 deletion completed in 44.165062646s

• [SLOW TEST:46.421 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:08:37.886: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-0b5df3a4-bd63-4ab8-82fa-0e0c74dd858f
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-0b5df3a4-bd63-4ab8-82fa-0e0c74dd858f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:10:05.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7301" for this suite.
Feb 19 22:10:29.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:10:29.985: INFO: namespace configmap-7301 deletion completed in 24.489988669s

• [SLOW TEST:112.100 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:10:29.987: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:10:30.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5542'
Feb 19 22:10:30.182: INFO: stderr: ""
Feb 19 22:10:30.182: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Feb 19 22:10:30.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete pods e2e-test-nginx-pod --namespace=kubectl-5542'
Feb 19 22:10:35.331: INFO: stderr: ""
Feb 19 22:10:35.331: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:10:35.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5542" for this suite.
Feb 19 22:10:41.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:10:41.576: INFO: namespace kubectl-5542 deletion completed in 6.235787455s

• [SLOW TEST:11.589 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:10:41.577: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e45ec266-400d-4349-8fbc-c086a9124157
STEP: Creating configMap with name cm-test-opt-upd-d0f19ff8-b36e-4f32-96e5-40c08eddeead
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e45ec266-400d-4349-8fbc-c086a9124157
STEP: Updating configmap cm-test-opt-upd-d0f19ff8-b36e-4f32-96e5-40c08eddeead
STEP: Creating configMap with name cm-test-opt-create-02132bb7-f4b9-4f7d-b13c-4581e34efef2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:10:45.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1211" for this suite.
Feb 19 22:11:07.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:11:07.974: INFO: namespace configmap-1211 deletion completed in 22.170138707s

• [SLOW TEST:26.398 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:11:07.976: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-38afa87a-5187-4091-b9d6-763adeecdea5
STEP: Creating a pod to test consume secrets
Feb 19 22:11:08.027: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8" in namespace "projected-5367" to be "success or failure"
Feb 19 22:11:08.038: INFO: Pod "pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.764886ms
Feb 19 22:11:10.118: INFO: Pod "pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.087232929s
STEP: Saw pod success
Feb 19 22:11:10.118: INFO: Pod "pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8" satisfied condition "success or failure"
Feb 19 22:11:10.224: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8 container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:11:10.418: INFO: Waiting for pod pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8 to disappear
Feb 19 22:11:10.427: INFO: Pod pod-projected-secrets-06a2ee5d-27c8-460c-9146-ed176c9f7ac8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:11:10.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5367" for this suite.
Feb 19 22:11:16.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:11:16.628: INFO: namespace projected-5367 deletion completed in 6.194980773s

• [SLOW TEST:8.653 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:11:16.631: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5229.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5229.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5229.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5229.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5229.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5229.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 19 22:11:30.754: INFO: DNS probes using dns-5229/dns-test-f1a1467b-1eef-4b21-86cd-f32cdb6a931e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:11:30.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5229" for this suite.
Feb 19 22:11:36.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:11:36.937: INFO: namespace dns-5229 deletion completed in 6.14996612s

• [SLOW TEST:20.307 seconds]
[sig-network] DNS
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:11:36.943: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:12:36.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6347" for this suite.
Feb 19 22:13:01.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:13:01.159: INFO: namespace container-probe-6347 deletion completed in 24.155971768s

• [SLOW TEST:84.216 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:13:01.161: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:13:01.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9905" for this suite.
Feb 19 22:13:07.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:13:07.390: INFO: namespace kubelet-test-9905 deletion completed in 6.150216556s

• [SLOW TEST:6.229 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:13:07.394: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:13:07.421: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:13:09.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3295" for this suite.
Feb 19 22:13:16.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:13:16.535: INFO: namespace custom-resource-definition-3295 deletion completed in 6.451097928s

• [SLOW TEST:9.142 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:13:16.539: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:13:16.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7" in namespace "projected-6601" to be "success or failure"
Feb 19 22:13:16.638: INFO: Pod "downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669833ms
Feb 19 22:13:18.642: INFO: Pod "downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006828256s
STEP: Saw pod success
Feb 19 22:13:18.642: INFO: Pod "downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7" satisfied condition "success or failure"
Feb 19 22:13:18.644: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7 container client-container: <nil>
STEP: delete the pod
Feb 19 22:13:18.672: INFO: Waiting for pod downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7 to disappear
Feb 19 22:13:18.676: INFO: Pod downwardapi-volume-5b22039a-40b1-4355-bbb1-ba52b2db49d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:13:18.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6601" for this suite.
Feb 19 22:13:24.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:13:24.859: INFO: namespace projected-6601 deletion completed in 6.179699797s

• [SLOW TEST:8.320 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:13:24.861: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6085, will wait for the garbage collector to delete the pods
Feb 19 22:13:29.286: INFO: Deleting Job.batch foo took: 6.402031ms
Feb 19 22:13:29.786: INFO: Terminating Job.batch foo pods took: 500.276115ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:14:05.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6085" for this suite.
Feb 19 22:14:11.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:14:11.544: INFO: namespace job-6085 deletion completed in 6.151164202s

• [SLOW TEST:46.683 seconds]
[sig-apps] Job
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:14:11.544: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Feb 19 22:14:11.602: INFO: Waiting up to 5m0s for pod "downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7" in namespace "downward-api-6640" to be "success or failure"
Feb 19 22:14:11.605: INFO: Pod "downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.308105ms
Feb 19 22:14:13.608: INFO: Pod "downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006482058s
Feb 19 22:14:15.612: INFO: Pod "downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009884779s
STEP: Saw pod success
Feb 19 22:14:15.612: INFO: Pod "downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7" satisfied condition "success or failure"
Feb 19 22:14:15.615: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7 container dapi-container: <nil>
STEP: delete the pod
Feb 19 22:14:15.667: INFO: Waiting for pod downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7 to disappear
Feb 19 22:14:15.671: INFO: Pod downward-api-cf577b74-f561-4bc0-a8e7-4adfab2340a7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:14:15.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6640" for this suite.
Feb 19 22:14:21.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:14:21.835: INFO: namespace downward-api-6640 deletion completed in 6.161156037s

• [SLOW TEST:10.291 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:14:21.838: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Feb 19 22:14:21.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 cluster-info'
Feb 19 22:14:21.961: INFO: stderr: ""
Feb 19 22:14:21.961: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.60.0.1:443\x1b[0m\n\x1b[0;32mGLBCDefaultBackend\x1b[0m is running at \x1b[0;33mhttps://10.60.0.1:443/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\x1b[0m\n\x1b[0;32mHeapster\x1b[0m is running at \x1b[0;33mhttps://10.60.0.1:443/api/v1/namespaces/kube-system/services/heapster/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.60.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.60.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:14:21.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7044" for this suite.
Feb 19 22:14:27.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:14:28.192: INFO: namespace kubectl-7044 deletion completed in 6.228316636s

• [SLOW TEST:6.355 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:14:28.194: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 19 22:14:32.334: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-7bdb2332-c960-4445-9b8a-c0e51f00e74d,GenerateName:,Namespace:events-8559,SelfLink:/api/v1/namespaces/events-8559/pods/send-events-7bdb2332-c960-4445-9b8a-c0e51f00e74d,UID:435fe457-677b-444d-a03d-c2b0197944ef,ResourceVersion:113189,Generation:0,CreationTimestamp:2020-02-19 22:14:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 271550221,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z5xkq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z5xkq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-z5xkq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024da300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024da320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:14:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:14:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:14:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:14:28 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:10.56.2.136,StartTime:2020-02-19 22:14:28 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-02-19 22:14:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://74b8f4a2d21513a36685c243f919fc0a52f0c8b21c918e07f74e993e2a02f268}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 19 22:14:34.339: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 19 22:14:36.343: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:14:36.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8559" for this suite.
Feb 19 22:15:16.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:15:16.503: INFO: namespace events-8559 deletion completed in 40.144895073s

• [SLOW TEST:48.309 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:15:16.505: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4193.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4193.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 19 22:15:20.693: INFO: DNS probes using dns-4193/dns-test-5723671d-a07c-4760-bd57-a6534ecc217c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:15:20.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4193" for this suite.
Feb 19 22:15:26.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:15:26.881: INFO: namespace dns-4193 deletion completed in 6.15456078s

• [SLOW TEST:10.376 seconds]
[sig-network] DNS
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:15:26.883: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:15:26.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1024'
Feb 19 22:15:27.011: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 19 22:15:27.011: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 19 22:15:27.019: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 19 22:15:27.038: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 19 22:15:27.056: INFO: scanned /root for discovery docs: <nil>
Feb 19 22:15:27.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1024'
Feb 19 22:15:43.002: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 19 22:15:43.002: INFO: stdout: "Created e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e\nScaling up e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 19 22:15:43.002: INFO: stdout: "Created e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e\nScaling up e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 19 22:15:43.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1024'
Feb 19 22:15:43.103: INFO: stderr: ""
Feb 19 22:15:43.103: INFO: stdout: "e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e-hm5p5 e2e-test-nginx-rc-sbzrm "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Feb 19 22:15:48.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1024'
Feb 19 22:15:48.193: INFO: stderr: ""
Feb 19 22:15:48.193: INFO: stdout: "e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e-hm5p5 "
Feb 19 22:15:48.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e-hm5p5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1024'
Feb 19 22:15:48.278: INFO: stderr: ""
Feb 19 22:15:48.278: INFO: stdout: "true"
Feb 19 22:15:48.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e-hm5p5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1024'
Feb 19 22:15:48.363: INFO: stderr: ""
Feb 19 22:15:48.363: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Feb 19 22:15:48.363: INFO: e2e-test-nginx-rc-3fdf55832124ff621405571e7c8ee11e-hm5p5 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Feb 19 22:15:48.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete rc e2e-test-nginx-rc --namespace=kubectl-1024'
Feb 19 22:15:48.457: INFO: stderr: ""
Feb 19 22:15:48.457: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:15:48.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1024" for this suite.
Feb 19 22:15:54.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:15:54.630: INFO: namespace kubectl-1024 deletion completed in 6.16172868s

• [SLOW TEST:27.748 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:15:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 19 22:15:54.675: INFO: Waiting up to 5m0s for pod "pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3" in namespace "emptydir-1729" to be "success or failure"
Feb 19 22:15:54.678: INFO: Pod "pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477984ms
Feb 19 22:15:56.771: INFO: Pod "pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095544414s
Feb 19 22:15:58.774: INFO: Pod "pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098254701s
STEP: Saw pod success
Feb 19 22:15:58.774: INFO: Pod "pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3" satisfied condition "success or failure"
Feb 19 22:15:58.777: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3 container test-container: <nil>
STEP: delete the pod
Feb 19 22:15:58.800: INFO: Waiting for pod pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3 to disappear
Feb 19 22:15:58.803: INFO: Pod pod-b6db9b0d-1d47-4999-9a8f-ed434b8ff2a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:15:58.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1729" for this suite.
Feb 19 22:16:04.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:16:04.955: INFO: namespace emptydir-1729 deletion completed in 6.147950859s

• [SLOW TEST:10.323 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:16:04.956: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0219 22:16:15.318956      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 19 22:16:15.319: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:16:15.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8067" for this suite.
Feb 19 22:16:21.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:16:21.499: INFO: namespace gc-8067 deletion completed in 6.177217466s

• [SLOW TEST:16.544 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:16:21.502: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:16:21.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-9864'
Feb 19 22:16:21.914: INFO: stderr: ""
Feb 19 22:16:21.914: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 19 22:16:21.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-9864'
Feb 19 22:16:22.383: INFO: stderr: ""
Feb 19 22:16:22.383: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 19 22:16:23.387: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:16:23.387: INFO: Found 0 / 1
Feb 19 22:16:24.386: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:16:24.386: INFO: Found 1 / 1
Feb 19 22:16:24.386: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 19 22:16:24.389: INFO: Selector matched 1 pods for map[app:redis]
Feb 19 22:16:24.389: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 19 22:16:24.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 describe pod redis-master-lx8hm --namespace=kubectl-9864'
Feb 19 22:16:24.724: INFO: stderr: ""
Feb 19 22:16:24.724: INFO: stdout: "Name:           redis-master-lx8hm\nNamespace:      kubectl-9864\nPriority:       0\nNode:           gke-c115-default-pool-249bf33f-nfnp/10.142.0.4\nStart Time:     Wed, 19 Feb 2020 22:16:21 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.56.2.141\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://297162de921ebdbb3fea53c33c28ddc7aba7942a79930ff2a206325b03ca38c4\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 19 Feb 2020 22:16:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-cdm2j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-cdm2j:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-cdm2j\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                          Message\n  ----    ------     ----  ----                                          -------\n  Normal  Scheduled  3s    default-scheduler                             Successfully assigned kubectl-9864/redis-master-lx8hm to gke-c115-default-pool-249bf33f-nfnp\n  Normal  Pulled     2s    kubelet, gke-c115-default-pool-249bf33f-nfnp  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, gke-c115-default-pool-249bf33f-nfnp  Created container redis-master\n  Normal  Started    1s    kubelet, gke-c115-default-pool-249bf33f-nfnp  Started container redis-master\n"
Feb 19 22:16:24.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 describe rc redis-master --namespace=kubectl-9864'
Feb 19 22:16:24.834: INFO: stderr: ""
Feb 19 22:16:24.834: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9864\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-lx8hm\n"
Feb 19 22:16:24.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 describe service redis-master --namespace=kubectl-9864'
Feb 19 22:16:24.940: INFO: stderr: ""
Feb 19 22:16:24.940: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9864\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.60.12.144\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.56.2.141:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 19 22:16:24.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 describe node gke-c115-default-pool-249bf33f-nfnp'
Feb 19 22:16:25.084: INFO: stderr: ""
Feb 19 22:16:25.084: INFO: stdout: "Name:               gke-c115-default-pool-249bf33f-nfnp\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/fluentd-ds-ready=true\n                    beta.kubernetes.io/instance-type=n1-standard-1\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/gke-nodepool=default-pool\n                    cloud.google.com/gke-os-distribution=cos\n                    failure-domain.beta.kubernetes.io/region=us-east1\n                    failure-domain.beta.kubernetes.io/zone=us-east1-c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gke-c115-default-pool-249bf33f-nfnp\n                    kubernetes.io/os=linux\nAnnotations:        container.googleapis.com/instance_id: 2758912346857470807\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 19 Feb 2020 14:29:45 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  KernelDeadlock                False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Wed, 19 Feb 2020 22:16:16 +0000   Wed, 19 Feb 2020 14:29:49 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  NetworkUnavailable            False   Wed, 19 Feb 2020 14:29:46 +0000   Wed, 19 Feb 2020 14:29:46 +0000   RouteCreated                    NodeController create implicit route\n  MemoryPressure                False   Wed, 19 Feb 2020 22:15:56 +0000   Wed, 19 Feb 2020 14:29:45 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Wed, 19 Feb 2020 22:15:56 +0000   Wed, 19 Feb 2020 14:29:45 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Wed, 19 Feb 2020 22:15:56 +0000   Wed, 19 Feb 2020 14:29:45 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Wed, 19 Feb 2020 22:15:56 +0000   Wed, 19 Feb 2020 14:29:46 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.142.0.4\n  ExternalIP:   34.74.125.18\n  InternalDNS:  gke-c115-default-pool-249bf33f-nfnp.c.pkm-scratch.internal\n  Hostname:     gke-c115-default-pool-249bf33f-nfnp.c.pkm-scratch.internal\nCapacity:\n attachable-volumes-gce-pd:  127\n cpu:                        1\n ephemeral-storage:          98868448Ki\n hugepages-2Mi:              0\n memory:                     3785956Ki\n pods:                       110\nAllocatable:\n attachable-volumes-gce-pd:  127\n cpu:                        940m\n ephemeral-storage:          47093746742\n hugepages-2Mi:              0\n memory:                     2700516Ki\n pods:                       110\nSystem Info:\n Machine ID:                 ad836139a872e9f3b155346158de6625\n System UUID:                ad836139-a872-e9f3-b155-346158de6625\n Boot ID:                    22f02607-e94e-4576-8690-89f72f12a2cc\n Kernel Version:             4.19.76+\n OS Image:                   Container-Optimized OS from Google\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://19.3.1\n Kubelet Version:            v1.15.9-gke.8\n Kube-Proxy Version:         v1.15.9-gke.8\nPodCIDR:                     10.56.2.0/24\nProviderID:                  gce://pkm-scratch/us-east1-c/gke-c115-default-pool-249bf33f-nfnp\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                fluentd-gcp-scaler-dd489f778-qvlt4                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         7h46m\n  kube-system                fluentd-gcp-v3.1.1-kgrjm                                   100m (10%)    1 (106%)    200Mi (7%)       500Mi (18%)    7h46m\n  kube-system                kube-dns-5dbbd9cc58-g9z7k                                  260m (27%)    0 (0%)      110Mi (4%)       170Mi (6%)     7h46m\n  kube-system                kube-dns-autoscaler-6b7f784798-8zlzg                       20m (2%)      0 (0%)      10Mi (0%)        0 (0%)         7h46m\n  kube-system                kube-proxy-gke-c115-default-pool-249bf33f-nfnp             100m (10%)    0 (0%)      0 (0%)           0 (0%)         7h46m\n  kube-system                l7-default-backend-84c9fcfbb-7vdk9                         10m (1%)      10m (1%)    20Mi (0%)        20Mi (0%)      7h46m\n  kube-system                metrics-server-v0.3.3-6d96fcc55-kwrms                      48m (5%)      143m (15%)  105Mi (3%)       355Mi (13%)    7h46m\n  kube-system                prometheus-to-sd-8dvjw                                     1m (0%)       3m (0%)     20Mi (0%)        20Mi (0%)      7h46m\n  kubectl-9864               redis-master-lx8hm                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-nvt2l    0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests     Limits\n  --------                   --------     ------\n  cpu                        539m (57%)   1156m (122%)\n  memory                     465Mi (17%)  1065Mi (40%)\n  ephemeral-storage          0 (0%)       0 (0%)\n  attachable-volumes-gce-pd  0            0\nEvents:                      <none>\n"
Feb 19 22:16:25.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 describe namespace kubectl-9864'
Feb 19 22:16:25.194: INFO: stderr: ""
Feb 19 22:16:25.194: INFO: stdout: "Name:         kubectl-9864\nLabels:       e2e-framework=kubectl\n              e2e-run=bd843226-cd44-4346-ba18-8184ae7eabe6\nAnnotations:  <none>\nStatus:       Active\n\nResource Quotas\n Name:                       gke-resource-quotas\n Resource                    Used  Hard\n --------                    ---   ---\n count/ingresses.extensions  0     100\n count/jobs.batch            0     5k\n pods                        1     1500\n services                    1     500\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:16:25.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9864" for this suite.
Feb 19 22:16:47.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:16:47.374: INFO: namespace kubectl-9864 deletion completed in 22.172512678s

• [SLOW TEST:25.872 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:16:47.380: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 19 22:16:47.410: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Feb 19 22:16:48.231: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Feb 19 22:16:50.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:16:52.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:16:54.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:16:56.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:16:58.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:17:00.305: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717747408, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 19 22:17:03.479: INFO: Waited 1.169329385s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:17:05.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6515" for this suite.
Feb 19 22:17:11.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:17:11.644: INFO: namespace aggregator-6515 deletion completed in 6.221101668s

• [SLOW TEST:24.264 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:17:11.647: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 19 22:17:11.702: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 19 22:17:16.705: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:17:17.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1997" for this suite.
Feb 19 22:17:23.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:17:23.885: INFO: namespace replication-controller-1997 deletion completed in 6.154458548s

• [SLOW TEST:12.238 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:17:23.886: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0219 22:17:30.023255      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 19 22:17:30.023: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:17:30.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2971" for this suite.
Feb 19 22:17:36.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:17:36.208: INFO: namespace gc-2971 deletion completed in 6.180171505s

• [SLOW TEST:12.322 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:17:36.210: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5512a949-5de3-4dd3-af9d-36dc760b7c06
STEP: Creating a pod to test consume secrets
Feb 19 22:17:36.264: INFO: Waiting up to 5m0s for pod "pod-secrets-e416263b-effe-498b-b830-0d48141b3897" in namespace "secrets-1556" to be "success or failure"
Feb 19 22:17:36.267: INFO: Pod "pod-secrets-e416263b-effe-498b-b830-0d48141b3897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.496985ms
Feb 19 22:17:38.270: INFO: Pod "pod-secrets-e416263b-effe-498b-b830-0d48141b3897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005854702s
Feb 19 22:17:40.350: INFO: Pod "pod-secrets-e416263b-effe-498b-b830-0d48141b3897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086050412s
STEP: Saw pod success
Feb 19 22:17:40.350: INFO: Pod "pod-secrets-e416263b-effe-498b-b830-0d48141b3897" satisfied condition "success or failure"
Feb 19 22:17:40.412: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-e416263b-effe-498b-b830-0d48141b3897 container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:17:40.478: INFO: Waiting for pod pod-secrets-e416263b-effe-498b-b830-0d48141b3897 to disappear
Feb 19 22:17:40.487: INFO: Pod pod-secrets-e416263b-effe-498b-b830-0d48141b3897 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:17:40.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1556" for this suite.
Feb 19 22:17:46.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:17:46.672: INFO: namespace secrets-1556 deletion completed in 6.177573324s

• [SLOW TEST:10.462 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:17:46.674: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 19 22:17:46.773: INFO: Waiting up to 5m0s for pod "pod-cfbf4380-34ea-4215-a472-c5cda6ddac33" in namespace "emptydir-3599" to be "success or failure"
Feb 19 22:17:46.779: INFO: Pod "pod-cfbf4380-34ea-4215-a472-c5cda6ddac33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81202ms
Feb 19 22:17:48.783: INFO: Pod "pod-cfbf4380-34ea-4215-a472-c5cda6ddac33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009502737s
STEP: Saw pod success
Feb 19 22:17:48.783: INFO: Pod "pod-cfbf4380-34ea-4215-a472-c5cda6ddac33" satisfied condition "success or failure"
Feb 19 22:17:48.785: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-cfbf4380-34ea-4215-a472-c5cda6ddac33 container test-container: <nil>
STEP: delete the pod
Feb 19 22:17:48.816: INFO: Waiting for pod pod-cfbf4380-34ea-4215-a472-c5cda6ddac33 to disappear
Feb 19 22:17:48.820: INFO: Pod pod-cfbf4380-34ea-4215-a472-c5cda6ddac33 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:17:48.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3599" for this suite.
Feb 19 22:17:54.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:17:57.952: INFO: namespace emptydir-3599 deletion completed in 9.128110072s

• [SLOW TEST:11.278 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:17:57.954: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-fbd38ae8-52e6-4891-9358-93be1a312f0e
STEP: Creating a pod to test consume secrets
Feb 19 22:17:58.192: INFO: Waiting up to 5m0s for pod "pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc" in namespace "secrets-1963" to be "success or failure"
Feb 19 22:17:58.200: INFO: Pod "pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.319286ms
Feb 19 22:18:00.203: INFO: Pod "pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011591844s
Feb 19 22:18:02.207: INFO: Pod "pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01498515s
STEP: Saw pod success
Feb 19 22:18:02.207: INFO: Pod "pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc" satisfied condition "success or failure"
Feb 19 22:18:02.209: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:18:02.229: INFO: Waiting for pod pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc to disappear
Feb 19 22:18:02.231: INFO: Pod pod-secrets-c761088f-0514-46e6-a3de-82b4f79175cc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:18:02.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1963" for this suite.
Feb 19 22:18:08.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:18:08.384: INFO: namespace secrets-1963 deletion completed in 6.149217173s

• [SLOW TEST:10.430 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:18:08.386: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 19 22:18:12.586: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-365921390 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 19 22:18:27.698: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:18:27.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8844" for this suite.
Feb 19 22:18:33.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:18:33.883: INFO: namespace pods-8844 deletion completed in 6.170864829s

• [SLOW TEST:25.498 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:18:33.885: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0219 22:19:13.963747      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 19 22:19:13.963: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:19:13.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1715" for this suite.
Feb 19 22:19:19.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:19:20.194: INFO: namespace gc-1715 deletion completed in 6.227887342s

• [SLOW TEST:46.309 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:19:20.195: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-25ccf5d1-69ab-4716-a5fb-f1c28ab542af
STEP: Creating a pod to test consume secrets
Feb 19 22:19:20.264: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914" in namespace "projected-8512" to be "success or failure"
Feb 19 22:19:20.269: INFO: Pod "pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113577ms
Feb 19 22:19:22.272: INFO: Pod "pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007686365s
STEP: Saw pod success
Feb 19 22:19:22.273: INFO: Pod "pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914" satisfied condition "success or failure"
Feb 19 22:19:22.277: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:19:22.301: INFO: Waiting for pod pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914 to disappear
Feb 19 22:19:22.303: INFO: Pod pod-projected-secrets-3f2fbd29-b5aa-4df6-9be2-bb9bc399f914 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:19:22.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8512" for this suite.
Feb 19 22:19:28.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:19:28.543: INFO: namespace projected-8512 deletion completed in 6.236647187s

• [SLOW TEST:8.349 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:19:28.544: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:19:28.674: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 19 22:19:28.717: INFO: Number of nodes with available pods: 0
Feb 19 22:19:28.718: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:19:29.724: INFO: Number of nodes with available pods: 0
Feb 19 22:19:29.724: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:19:30.754: INFO: Number of nodes with available pods: 1
Feb 19 22:19:30.754: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:19:31.724: INFO: Number of nodes with available pods: 3
Feb 19 22:19:31.724: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 19 22:19:31.774: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:31.774: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:31.774: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:32.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:32.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:32.782: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:33.784: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:33.785: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:33.785: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:33.785: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:34.783: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:34.783: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:34.783: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:34.783: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:35.784: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:35.784: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:35.784: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:35.784: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:36.787: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:36.787: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:36.787: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:36.787: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:37.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:37.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:37.782: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:37.782: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:38.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:38.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:38.782: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:38.782: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:39.877: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:39.877: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:39.877: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:39.877: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:40.784: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:40.784: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:40.784: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:40.784: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:41.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:41.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:41.782: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:41.782: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:42.787: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:42.787: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:42.787: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:42.787: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:43.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:43.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:43.782: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:43.782: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:44.783: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:44.783: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:44.783: INFO: Wrong image for pod: daemon-set-g27x9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:44.783: INFO: Pod daemon-set-g27x9 is not available
Feb 19 22:19:45.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:45.782: INFO: Pod daemon-set-5t2sc is not available
Feb 19 22:19:45.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:46.791: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:46.791: INFO: Pod daemon-set-5t2sc is not available
Feb 19 22:19:46.791: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:47.785: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:47.785: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:48.785: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:48.785: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:48.785: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:49.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:49.782: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:49.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:50.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:50.782: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:50.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:51.783: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:51.784: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:51.784: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:52.782: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:52.783: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:52.783: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:53.783: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:53.783: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:53.783: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:54.877: INFO: Wrong image for pod: daemon-set-24b8b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:54.877: INFO: Pod daemon-set-24b8b is not available
Feb 19 22:19:54.878: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:55.785: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:55.786: INFO: Pod daemon-set-lqstg is not available
Feb 19 22:19:56.814: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:56.814: INFO: Pod daemon-set-lqstg is not available
Feb 19 22:19:57.817: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:58.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:59.782: INFO: Wrong image for pod: daemon-set-5wqnh. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Feb 19 22:19:59.782: INFO: Pod daemon-set-5wqnh is not available
Feb 19 22:20:00.782: INFO: Pod daemon-set-tvssr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 19 22:20:00.791: INFO: Number of nodes with available pods: 2
Feb 19 22:20:00.791: INFO: Node gke-c115-default-pool-249bf33f-qclh is running more than one daemon pod
Feb 19 22:20:01.801: INFO: Number of nodes with available pods: 2
Feb 19 22:20:01.801: INFO: Node gke-c115-default-pool-249bf33f-qclh is running more than one daemon pod
Feb 19 22:20:02.798: INFO: Number of nodes with available pods: 3
Feb 19 22:20:02.798: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2224, will wait for the garbage collector to delete the pods
Feb 19 22:20:02.875: INFO: Deleting DaemonSet.extensions daemon-set took: 9.00718ms
Feb 19 22:20:03.375: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.222855ms
Feb 19 22:20:15.478: INFO: Number of nodes with available pods: 0
Feb 19 22:20:15.478: INFO: Number of running nodes: 0, number of available pods: 0
Feb 19 22:20:15.495: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2224/daemonsets","resourceVersion":"115219"},"items":null}

Feb 19 22:20:15.498: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2224/pods","resourceVersion":"115219"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:20:15.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2224" for this suite.
Feb 19 22:20:21.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:20:21.669: INFO: namespace daemonsets-2224 deletion completed in 6.151925508s

• [SLOW TEST:53.125 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:20:21.672: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-5c92cdd2-a28e-49fb-a136-57764614f71b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:20:23.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7819" for this suite.
Feb 19 22:20:45.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:20:45.899: INFO: namespace configmap-7819 deletion completed in 22.145225s

• [SLOW TEST:24.228 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:20:45.903: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:20:45.952: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499" in namespace "projected-7296" to be "success or failure"
Feb 19 22:20:45.954: INFO: Pod "downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064054ms
Feb 19 22:20:47.957: INFO: Pod "downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005144525s
Feb 19 22:20:49.960: INFO: Pod "downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008329014s
STEP: Saw pod success
Feb 19 22:20:49.960: INFO: Pod "downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499" satisfied condition "success or failure"
Feb 19 22:20:49.963: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499 container client-container: <nil>
STEP: delete the pod
Feb 19 22:20:49.992: INFO: Waiting for pod downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499 to disappear
Feb 19 22:20:49.998: INFO: Pod downwardapi-volume-bdb26900-4079-49f9-a23b-ba3600acf499 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:20:49.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7296" for this suite.
Feb 19 22:20:56.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:20:58.229: INFO: namespace projected-7296 deletion completed in 8.227752574s

• [SLOW TEST:12.327 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:20:58.231: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 19 22:20:58.289: INFO: Waiting up to 5m0s for pod "pod-96dd8644-d439-4289-8cee-37c978f05afc" in namespace "emptydir-5341" to be "success or failure"
Feb 19 22:20:58.299: INFO: Pod "pod-96dd8644-d439-4289-8cee-37c978f05afc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.135905ms
Feb 19 22:21:00.302: INFO: Pod "pod-96dd8644-d439-4289-8cee-37c978f05afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013287339s
Feb 19 22:21:02.307: INFO: Pod "pod-96dd8644-d439-4289-8cee-37c978f05afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018067757s
STEP: Saw pod success
Feb 19 22:21:02.307: INFO: Pod "pod-96dd8644-d439-4289-8cee-37c978f05afc" satisfied condition "success or failure"
Feb 19 22:21:02.312: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-96dd8644-d439-4289-8cee-37c978f05afc container test-container: <nil>
STEP: delete the pod
Feb 19 22:21:02.401: INFO: Waiting for pod pod-96dd8644-d439-4289-8cee-37c978f05afc to disappear
Feb 19 22:21:02.411: INFO: Pod pod-96dd8644-d439-4289-8cee-37c978f05afc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:21:02.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5341" for this suite.
Feb 19 22:21:08.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:21:08.578: INFO: namespace emptydir-5341 deletion completed in 6.160052621s

• [SLOW TEST:10.347 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:21:08.580: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Feb 19 22:21:08.612: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:21:12.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6328" for this suite.
Feb 19 22:21:34.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:21:34.941: INFO: namespace init-container-6328 deletion completed in 22.146196462s

• [SLOW TEST:26.362 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:21:34.947: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-30055421-d01a-499f-bd8f-17b28b29bcbb in namespace container-probe-2680
Feb 19 22:21:39.023: INFO: Started pod liveness-30055421-d01a-499f-bd8f-17b28b29bcbb in namespace container-probe-2680
STEP: checking the pod's current state and verifying that restartCount is present
Feb 19 22:21:39.026: INFO: Initial restart count of pod liveness-30055421-d01a-499f-bd8f-17b28b29bcbb is 0
Feb 19 22:21:51.052: INFO: Restart count of pod container-probe-2680/liveness-30055421-d01a-499f-bd8f-17b28b29bcbb is now 1 (12.025233547s elapsed)
Feb 19 22:22:11.243: INFO: Restart count of pod container-probe-2680/liveness-30055421-d01a-499f-bd8f-17b28b29bcbb is now 2 (32.216191481s elapsed)
Feb 19 22:22:31.281: INFO: Restart count of pod container-probe-2680/liveness-30055421-d01a-499f-bd8f-17b28b29bcbb is now 3 (52.254165116s elapsed)
Feb 19 22:22:51.310: INFO: Restart count of pod container-probe-2680/liveness-30055421-d01a-499f-bd8f-17b28b29bcbb is now 4 (1m12.283793185s elapsed)
Feb 19 22:23:53.679: INFO: Restart count of pod container-probe-2680/liveness-30055421-d01a-499f-bd8f-17b28b29bcbb is now 5 (2m14.652634302s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:23:53.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2680" for this suite.
Feb 19 22:23:59.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:23:59.840: INFO: namespace container-probe-2680 deletion completed in 6.139415953s

• [SLOW TEST:144.894 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:23:59.845: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 19 22:23:59.895: INFO: Waiting up to 5m0s for pod "pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604" in namespace "emptydir-5253" to be "success or failure"
Feb 19 22:23:59.900: INFO: Pod "pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604": Phase="Pending", Reason="", readiness=false. Elapsed: 4.567171ms
Feb 19 22:24:01.903: INFO: Pod "pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007528273s
STEP: Saw pod success
Feb 19 22:24:01.903: INFO: Pod "pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604" satisfied condition "success or failure"
Feb 19 22:24:01.906: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604 container test-container: <nil>
STEP: delete the pod
Feb 19 22:24:01.926: INFO: Waiting for pod pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604 to disappear
Feb 19 22:24:01.930: INFO: Pod pod-bc88c553-fcc4-4eec-a0e3-50826ad0e604 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:24:01.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5253" for this suite.
Feb 19 22:24:07.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:24:08.074: INFO: namespace emptydir-5253 deletion completed in 6.140139417s

• [SLOW TEST:8.230 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:24:08.078: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 19 22:24:11.092: INFO: Successfully updated pod "labelsupdate0ea380b7-3a83-43db-8949-4637b1b87fec"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:24:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8097" for this suite.
Feb 19 22:24:37.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:24:37.265: INFO: namespace projected-8097 deletion completed in 22.144007062s

• [SLOW TEST:29.187 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:24:37.267: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:24:37.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6889'
Feb 19 22:24:37.413: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 19 22:24:37.413: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Feb 19 22:24:37.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete jobs e2e-test-nginx-job --namespace=kubectl-6889'
Feb 19 22:24:37.540: INFO: stderr: ""
Feb 19 22:24:37.540: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:24:37.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6889" for this suite.
Feb 19 22:24:43.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:24:43.700: INFO: namespace kubectl-6889 deletion completed in 6.154252717s

• [SLOW TEST:6.433 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:24:43.703: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 19 22:24:43.732: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 19 22:24:43.738: INFO: Waiting for terminating namespaces to be deleted...
Feb 19 22:24:43.742: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-nfnp before test
Feb 19 22:24:43.753: INFO: kube-proxy-gke-c115-default-pool-249bf33f-nfnp from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.753: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 22:24:43.753: INFO: prometheus-to-sd-8dvjw from kube-system started at 2020-02-19 14:29:48 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.753: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:24:43.753: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 22:24:43.753: INFO: metrics-server-v0.3.3-6d96fcc55-kwrms from kube-system started at 2020-02-19 14:29:53 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.754: INFO: 	Container metrics-server ready: true, restart count 0
Feb 19 22:24:43.754: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 19 22:24:43.754: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-nvt2l from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.754: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 19 22:24:43.754: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 22:24:43.754: INFO: l7-default-backend-84c9fcfbb-7vdk9 from kube-system started at 2020-02-19 14:29:47 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.754: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 19 22:24:43.754: INFO: kube-dns-autoscaler-6b7f784798-8zlzg from kube-system started at 2020-02-19 14:29:48 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.754: INFO: 	Container autoscaler ready: true, restart count 0
Feb 19 22:24:43.754: INFO: kube-dns-5dbbd9cc58-g9z7k from kube-system started at 2020-02-19 14:29:58 +0000 UTC (4 container statuses recorded)
Feb 19 22:24:43.754: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 19 22:24:43.755: INFO: 	Container kubedns ready: true, restart count 0
Feb 19 22:24:43.755: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:24:43.755: INFO: 	Container sidecar ready: true, restart count 0
Feb 19 22:24:43.755: INFO: fluentd-gcp-v3.1.1-kgrjm from kube-system started at 2020-02-19 14:30:01 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.755: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 22:24:43.755: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 22:24:43.755: INFO: fluentd-gcp-scaler-dd489f778-qvlt4 from kube-system started at 2020-02-19 14:29:49 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.755: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Feb 19 22:24:43.755: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-qclh before test
Feb 19 22:24:43.770: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-sn5gm from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.770: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 19 22:24:43.770: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 22:24:43.771: INFO: prometheus-to-sd-hk44q from kube-system started at 2020-02-19 14:29:48 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.771: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:24:43.771: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 22:24:43.771: INFO: kube-dns-5dbbd9cc58-wksp8 from kube-system started at 2020-02-19 14:29:47 +0000 UTC (4 container statuses recorded)
Feb 19 22:24:43.771: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 19 22:24:43.771: INFO: 	Container kubedns ready: true, restart count 0
Feb 19 22:24:43.771: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:24:43.771: INFO: 	Container sidecar ready: true, restart count 0
Feb 19 22:24:43.771: INFO: fluentd-gcp-v3.1.1-sgx4k from kube-system started at 2020-02-19 14:30:06 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.772: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 22:24:43.772: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 22:24:43.772: INFO: stackdriver-metadata-agent-cluster-level-5d4656b6d8-ldr2s from kube-system started at 2020-02-19 14:30:22 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.772: INFO: 	Container metadata-agent ready: true, restart count 0
Feb 19 22:24:43.772: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Feb 19 22:24:43.772: INFO: heapster-gke-76c9bd686-5tq5s from kube-system started at 2020-02-19 14:30:25 +0000 UTC (3 container statuses recorded)
Feb 19 22:24:43.772: INFO: 	Container heapster ready: true, restart count 0
Feb 19 22:24:43.772: INFO: 	Container heapster-nanny ready: true, restart count 0
Feb 19 22:24:43.772: INFO: 	Container prom-to-sd ready: true, restart count 0
Feb 19 22:24:43.772: INFO: kube-proxy-gke-c115-default-pool-249bf33f-qclh from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.773: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 22:24:43.773: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-vhvp before test
Feb 19 22:24:43.782: INFO: sonobuoy-e2e-job-bec128f92cf64f0f from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.782: INFO: 	Container e2e ready: true, restart count 0
Feb 19 22:24:43.782: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 19 22:24:43.782: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-9xkwf from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.783: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 19 22:24:43.783: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 22:24:43.783: INFO: prometheus-to-sd-b68nv from kube-system started at 2020-02-19 14:29:47 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.783: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:24:43.783: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 22:24:43.783: INFO: kube-proxy-gke-c115-default-pool-249bf33f-vhvp from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.783: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 22:24:43.783: INFO: sonobuoy from sonobuoy started at 2020-02-19 21:19:23 +0000 UTC (1 container statuses recorded)
Feb 19 22:24:43.783: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 19 22:24:43.783: INFO: event-exporter-v0.3.0-74bf544f8b-ckv4s from kube-system started at 2020-02-19 14:29:45 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.783: INFO: 	Container event-exporter ready: true, restart count 0
Feb 19 22:24:43.783: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 22:24:43.783: INFO: fluentd-gcp-v3.1.1-d8brx from kube-system started at 2020-02-19 14:30:06 +0000 UTC (2 container statuses recorded)
Feb 19 22:24:43.783: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 22:24:43.783: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f4ee07335b440d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:24:44.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7708" for this suite.
Feb 19 22:24:50.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:24:50.955: INFO: namespace sched-pred-7708 deletion completed in 6.140293855s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.253 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:24:50.959: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-94bb25d8-18f9-49bd-8abd-4d34a452f3da
STEP: Creating a pod to test consume secrets
Feb 19 22:24:51.062: INFO: Waiting up to 5m0s for pod "pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4" in namespace "secrets-8724" to be "success or failure"
Feb 19 22:24:51.065: INFO: Pod "pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.922668ms
Feb 19 22:24:53.068: INFO: Pod "pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005731015s
STEP: Saw pod success
Feb 19 22:24:53.068: INFO: Pod "pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4" satisfied condition "success or failure"
Feb 19 22:24:53.071: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4 container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:24:53.092: INFO: Waiting for pod pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4 to disappear
Feb 19 22:24:53.094: INFO: Pod pod-secrets-eec31493-ec53-4c63-9939-9f870381dcb4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:24:53.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8724" for this suite.
Feb 19 22:24:59.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:24:59.239: INFO: namespace secrets-8724 deletion completed in 6.141657847s

• [SLOW TEST:8.281 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:24:59.240: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 19 22:25:01.819: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7e2a39ee-7b39-4a43-8442-589f0cdc3f38"
Feb 19 22:25:01.820: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7e2a39ee-7b39-4a43-8442-589f0cdc3f38" in namespace "pods-8056" to be "terminated due to deadline exceeded"
Feb 19 22:25:01.828: INFO: Pod "pod-update-activedeadlineseconds-7e2a39ee-7b39-4a43-8442-589f0cdc3f38": Phase="Running", Reason="", readiness=true. Elapsed: 8.870558ms
Feb 19 22:25:03.834: INFO: Pod "pod-update-activedeadlineseconds-7e2a39ee-7b39-4a43-8442-589f0cdc3f38": Phase="Running", Reason="", readiness=true. Elapsed: 2.014236378s
Feb 19 22:25:05.837: INFO: Pod "pod-update-activedeadlineseconds-7e2a39ee-7b39-4a43-8442-589f0cdc3f38": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.017285068s
Feb 19 22:25:05.837: INFO: Pod "pod-update-activedeadlineseconds-7e2a39ee-7b39-4a43-8442-589f0cdc3f38" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:25:05.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8056" for this suite.
Feb 19 22:25:11.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:25:12.941: INFO: namespace pods-8056 deletion completed in 7.101381696s

• [SLOW TEST:13.702 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:25:12.947: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6813
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6813
STEP: Creating statefulset with conflicting port in namespace statefulset-6813
STEP: Waiting until pod test-pod will start running in namespace statefulset-6813
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6813
Feb 19 22:25:17.147: INFO: Observed stateful pod in namespace: statefulset-6813, name: ss-0, uid: da3aed53-626f-4746-b7cb-5c49a5415c16, status phase: Pending. Waiting for statefulset controller to delete.
Feb 19 22:25:17.720: INFO: Observed stateful pod in namespace: statefulset-6813, name: ss-0, uid: da3aed53-626f-4746-b7cb-5c49a5415c16, status phase: Failed. Waiting for statefulset controller to delete.
Feb 19 22:25:17.734: INFO: Observed stateful pod in namespace: statefulset-6813, name: ss-0, uid: da3aed53-626f-4746-b7cb-5c49a5415c16, status phase: Failed. Waiting for statefulset controller to delete.
Feb 19 22:25:17.737: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6813
STEP: Removing pod with conflicting port in namespace statefulset-6813
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6813 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 19 22:25:21.771: INFO: Deleting all statefulset in ns statefulset-6813
Feb 19 22:25:21.794: INFO: Scaling statefulset ss to 0
Feb 19 22:25:41.814: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 22:25:41.822: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:25:41.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6813" for this suite.
Feb 19 22:25:47.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:25:48.064: INFO: namespace statefulset-6813 deletion completed in 6.193934763s

• [SLOW TEST:35.118 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:25:48.067: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e5005a4f-f5a5-464c-8fab-c9b539c52f0a
STEP: Creating configMap with name cm-test-opt-upd-8a24551c-0e0f-4bb3-8ad9-bc4dda033b67
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e5005a4f-f5a5-464c-8fab-c9b539c52f0a
STEP: Updating configmap cm-test-opt-upd-8a24551c-0e0f-4bb3-8ad9-bc4dda033b67
STEP: Creating configMap with name cm-test-opt-create-a0184150-30d4-4e27-8326-8be0e12cc5f9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:25:52.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4299" for this suite.
Feb 19 22:26:14.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:26:14.413: INFO: namespace projected-4299 deletion completed in 22.159575284s

• [SLOW TEST:26.347 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:26:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Feb 19 22:26:14.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 create -f - --namespace=kubectl-4707'
Feb 19 22:26:14.824: INFO: stderr: ""
Feb 19 22:26:14.824: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 19 22:26:14.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4707'
Feb 19 22:26:14.936: INFO: stderr: ""
Feb 19 22:26:14.936: INFO: stdout: "update-demo-nautilus-6kt6w update-demo-nautilus-qwzvm "
Feb 19 22:26:14.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-6kt6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:15.026: INFO: stderr: ""
Feb 19 22:26:15.026: INFO: stdout: ""
Feb 19 22:26:15.026: INFO: update-demo-nautilus-6kt6w is created but not running
Feb 19 22:26:20.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4707'
Feb 19 22:26:20.113: INFO: stderr: ""
Feb 19 22:26:20.113: INFO: stdout: "update-demo-nautilus-6kt6w update-demo-nautilus-qwzvm "
Feb 19 22:26:20.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-6kt6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:20.197: INFO: stderr: ""
Feb 19 22:26:20.197: INFO: stdout: "true"
Feb 19 22:26:20.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-6kt6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:20.302: INFO: stderr: ""
Feb 19 22:26:20.302: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 22:26:20.302: INFO: validating pod update-demo-nautilus-6kt6w
Feb 19 22:26:20.311: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 22:26:20.311: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 22:26:20.311: INFO: update-demo-nautilus-6kt6w is verified up and running
Feb 19 22:26:20.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-qwzvm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:20.393: INFO: stderr: ""
Feb 19 22:26:20.393: INFO: stdout: "true"
Feb 19 22:26:20.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-nautilus-qwzvm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:20.476: INFO: stderr: ""
Feb 19 22:26:20.476: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 19 22:26:20.476: INFO: validating pod update-demo-nautilus-qwzvm
Feb 19 22:26:20.487: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 19 22:26:20.487: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 19 22:26:20.487: INFO: update-demo-nautilus-qwzvm is verified up and running
STEP: rolling-update to new replication controller
Feb 19 22:26:20.489: INFO: scanned /root for discovery docs: <nil>
Feb 19 22:26:20.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4707'
Feb 19 22:26:43.343: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 19 22:26:43.343: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 19 22:26:43.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4707'
Feb 19 22:26:43.439: INFO: stderr: ""
Feb 19 22:26:43.439: INFO: stdout: "update-demo-kitten-rz6qc update-demo-kitten-svm2c update-demo-nautilus-6kt6w "
STEP: Replicas for name=update-demo: expected=2 actual=3
Feb 19 22:26:48.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4707'
Feb 19 22:26:48.531: INFO: stderr: ""
Feb 19 22:26:48.531: INFO: stdout: "update-demo-kitten-rz6qc update-demo-kitten-svm2c "
Feb 19 22:26:48.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-kitten-rz6qc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:48.622: INFO: stderr: ""
Feb 19 22:26:48.622: INFO: stdout: "true"
Feb 19 22:26:48.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-kitten-rz6qc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:48.707: INFO: stderr: ""
Feb 19 22:26:48.707: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 19 22:26:48.707: INFO: validating pod update-demo-kitten-rz6qc
Feb 19 22:26:48.715: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 19 22:26:48.715: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 19 22:26:48.715: INFO: update-demo-kitten-rz6qc is verified up and running
Feb 19 22:26:48.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-kitten-svm2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:48.800: INFO: stderr: ""
Feb 19 22:26:48.800: INFO: stdout: "true"
Feb 19 22:26:48.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pods update-demo-kitten-svm2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4707'
Feb 19 22:26:48.900: INFO: stderr: ""
Feb 19 22:26:48.900: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 19 22:26:48.900: INFO: validating pod update-demo-kitten-svm2c
Feb 19 22:26:48.908: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 19 22:26:48.908: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 19 22:26:48.908: INFO: update-demo-kitten-svm2c is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:26:48.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4707" for this suite.
Feb 19 22:27:12.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:27:13.260: INFO: namespace kubectl-4707 deletion completed in 24.345442608s

• [SLOW TEST:58.845 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:27:13.262: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:27:13.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9207'
Feb 19 22:27:13.399: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 19 22:27:13.399: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 19 22:27:15.413: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-65f4m]
Feb 19 22:27:15.413: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-65f4m" in namespace "kubectl-9207" to be "running and ready"
Feb 19 22:27:15.416: INFO: Pod "e2e-test-nginx-rc-65f4m": Phase="Running", Reason="", readiness=true. Elapsed: 2.950175ms
Feb 19 22:27:15.416: INFO: Pod "e2e-test-nginx-rc-65f4m" satisfied condition "running and ready"
Feb 19 22:27:15.416: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-65f4m]
Feb 19 22:27:15.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 logs rc/e2e-test-nginx-rc --namespace=kubectl-9207'
Feb 19 22:27:15.530: INFO: stderr: ""
Feb 19 22:27:15.530: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Feb 19 22:27:15.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete rc e2e-test-nginx-rc --namespace=kubectl-9207'
Feb 19 22:27:15.654: INFO: stderr: ""
Feb 19 22:27:15.654: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:27:15.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9207" for this suite.
Feb 19 22:27:21.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:27:21.829: INFO: namespace kubectl-9207 deletion completed in 6.170334056s

• [SLOW TEST:8.567 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:27:21.834: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 19 22:27:21.880: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 19 22:27:29.039: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:27:29.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8798" for this suite.
Feb 19 22:27:35.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:27:35.422: INFO: namespace pods-8798 deletion completed in 6.224393063s

• [SLOW TEST:13.589 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:27:35.424: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-1c0be54d-35bc-43e2-8074-47c4dfd36bda
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:27:35.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-373" for this suite.
Feb 19 22:27:41.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:27:41.684: INFO: namespace configmap-373 deletion completed in 6.171792713s

• [SLOW TEST:6.261 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:27:41.686: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-146c671f-aaf4-4583-b583-81a47f9da026
STEP: Creating a pod to test consume configMaps
Feb 19 22:27:41.740: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031" in namespace "configmap-37" to be "success or failure"
Feb 19 22:27:41.746: INFO: Pod "pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565198ms
Feb 19 22:27:43.751: INFO: Pod "pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010063186s
Feb 19 22:27:45.754: INFO: Pod "pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013232143s
STEP: Saw pod success
Feb 19 22:27:45.754: INFO: Pod "pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031" satisfied condition "success or failure"
Feb 19 22:27:45.756: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:27:45.775: INFO: Waiting for pod pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031 to disappear
Feb 19 22:27:45.780: INFO: Pod pod-configmaps-ce327fe9-90a6-4c52-a87c-35f66b218031 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:27:45.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-37" for this suite.
Feb 19 22:27:51.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:27:51.962: INFO: namespace configmap-37 deletion completed in 6.179325794s

• [SLOW TEST:10.277 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:27:51.969: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:27:52.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18" in namespace "downward-api-8463" to be "success or failure"
Feb 19 22:27:52.014: INFO: Pod "downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.670998ms
Feb 19 22:27:54.017: INFO: Pod "downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005830454s
Feb 19 22:27:56.112: INFO: Pod "downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.101119054s
STEP: Saw pod success
Feb 19 22:27:56.112: INFO: Pod "downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18" satisfied condition "success or failure"
Feb 19 22:27:56.213: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18 container client-container: <nil>
STEP: delete the pod
Feb 19 22:27:56.499: INFO: Waiting for pod downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18 to disappear
Feb 19 22:27:56.502: INFO: Pod downwardapi-volume-6070697c-aa20-4356-ad2d-7595eb775d18 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:27:56.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8463" for this suite.
Feb 19 22:28:02.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:28:02.755: INFO: namespace downward-api-8463 deletion completed in 6.246559066s

• [SLOW TEST:10.787 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:28:02.758: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-f85ce14d-8624-458d-8938-83d2573412a2
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:28:02.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4957" for this suite.
Feb 19 22:28:08.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:28:08.944: INFO: namespace secrets-4957 deletion completed in 6.144468944s

• [SLOW TEST:6.187 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:28:08.946: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 19 22:28:09.039: INFO: Number of nodes with available pods: 0
Feb 19 22:28:09.039: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:28:10.252: INFO: Number of nodes with available pods: 0
Feb 19 22:28:10.252: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:28:11.047: INFO: Number of nodes with available pods: 1
Feb 19 22:28:11.047: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:28:12.226: INFO: Number of nodes with available pods: 3
Feb 19 22:28:12.226: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 19 22:28:12.781: INFO: Number of nodes with available pods: 2
Feb 19 22:28:12.781: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:13.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:13.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:14.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:14.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:15.790: INFO: Number of nodes with available pods: 2
Feb 19 22:28:15.790: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:16.787: INFO: Number of nodes with available pods: 2
Feb 19 22:28:16.787: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:17.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:17.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:18.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:18.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:19.790: INFO: Number of nodes with available pods: 2
Feb 19 22:28:19.790: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:20.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:20.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:21.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:21.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:22.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:22.789: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:23.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:23.789: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:24.788: INFO: Number of nodes with available pods: 2
Feb 19 22:28:24.788: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:25.789: INFO: Number of nodes with available pods: 2
Feb 19 22:28:25.789: INFO: Node gke-c115-default-pool-249bf33f-vhvp is running more than one daemon pod
Feb 19 22:28:26.790: INFO: Number of nodes with available pods: 3
Feb 19 22:28:26.790: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-207, will wait for the garbage collector to delete the pods
Feb 19 22:28:26.852: INFO: Deleting DaemonSet.extensions daemon-set took: 7.365757ms
Feb 19 22:28:27.452: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.199599ms
Feb 19 22:28:35.357: INFO: Number of nodes with available pods: 0
Feb 19 22:28:35.357: INFO: Number of running nodes: 0, number of available pods: 0
Feb 19 22:28:35.362: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-207/daemonsets","resourceVersion":"117718"},"items":null}

Feb 19 22:28:35.364: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-207/pods","resourceVersion":"117718"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:28:35.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-207" for this suite.
Feb 19 22:28:41.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:28:41.527: INFO: namespace daemonsets-207 deletion completed in 6.1436541s

• [SLOW TEST:32.581 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:28:41.530: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 19 22:28:45.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec pod-sharedvolume-ade06cb4-6bca-4a69-a1b6-415adec66897 -c busybox-main-container --namespace=emptydir-6680 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 19 22:28:45.867: INFO: stderr: ""
Feb 19 22:28:45.867: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:28:45.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6680" for this suite.
Feb 19 22:28:51.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:28:52.003: INFO: namespace emptydir-6680 deletion completed in 6.130514756s

• [SLOW TEST:10.474 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:28:52.005: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:28:52.033: INFO: Creating ReplicaSet my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7
Feb 19 22:28:52.043: INFO: Pod name my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7: Found 0 pods out of 1
Feb 19 22:28:57.144: INFO: Pod name my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7: Found 1 pods out of 1
Feb 19 22:28:57.144: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7" is running
Feb 19 22:28:57.420: INFO: Pod "my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7-j6nrn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 22:28:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 22:28:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 22:28:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-19 22:28:52 +0000 UTC Reason: Message:}])
Feb 19 22:28:57.421: INFO: Trying to dial the pod
Feb 19 22:29:02.431: INFO: Controller my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7: Got expected result from replica 1 [my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7-j6nrn]: "my-hostname-basic-942d5579-7767-47c2-bce4-d07a16f4f5d7-j6nrn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:29:02.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5980" for this suite.
Feb 19 22:29:08.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:29:08.605: INFO: namespace replicaset-5980 deletion completed in 6.170123516s

• [SLOW TEST:16.600 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:29:08.606: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:29:08.663: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23" in namespace "projected-4796" to be "success or failure"
Feb 19 22:29:08.666: INFO: Pod "downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.855247ms
Feb 19 22:29:10.737: INFO: Pod "downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073162987s
STEP: Saw pod success
Feb 19 22:29:10.737: INFO: Pod "downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23" satisfied condition "success or failure"
Feb 19 22:29:10.789: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23 container client-container: <nil>
STEP: delete the pod
Feb 19 22:29:10.813: INFO: Waiting for pod downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23 to disappear
Feb 19 22:29:10.818: INFO: Pod downwardapi-volume-96552a14-8a40-4db4-a977-4fdab1073e23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:29:10.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4796" for this suite.
Feb 19 22:29:16.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:29:16.956: INFO: namespace projected-4796 deletion completed in 6.13298965s

• [SLOW TEST:8.350 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:29:16.958: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d8765f9d-aecb-44f6-801f-5d08b046e9a8
STEP: Creating a pod to test consume secrets
Feb 19 22:29:17.007: INFO: Waiting up to 5m0s for pod "pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28" in namespace "secrets-926" to be "success or failure"
Feb 19 22:29:17.015: INFO: Pod "pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28": Phase="Pending", Reason="", readiness=false. Elapsed: 8.543347ms
Feb 19 22:29:19.018: INFO: Pod "pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011334255s
Feb 19 22:29:21.021: INFO: Pod "pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013999501s
STEP: Saw pod success
Feb 19 22:29:21.021: INFO: Pod "pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28" satisfied condition "success or failure"
Feb 19 22:29:21.023: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28 container secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:29:21.042: INFO: Waiting for pod pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28 to disappear
Feb 19 22:29:21.045: INFO: Pod pod-secrets-1aabe047-00f4-4cf1-8626-1064da91bd28 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:29:21.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-926" for this suite.
Feb 19 22:29:27.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:29:27.183: INFO: namespace secrets-926 deletion completed in 6.133254998s

• [SLOW TEST:10.225 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:29:27.183: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-eeaf74e6-1ce7-4611-8441-b74ac0a01f99
STEP: Creating a pod to test consume configMaps
Feb 19 22:29:27.240: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12" in namespace "configmap-280" to be "success or failure"
Feb 19 22:29:27.251: INFO: Pod "pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12": Phase="Pending", Reason="", readiness=false. Elapsed: 11.573706ms
Feb 19 22:29:29.255: INFO: Pod "pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01533702s
STEP: Saw pod success
Feb 19 22:29:29.255: INFO: Pod "pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12" satisfied condition "success or failure"
Feb 19 22:29:29.258: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:29:29.278: INFO: Waiting for pod pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12 to disappear
Feb 19 22:29:29.290: INFO: Pod pod-configmaps-bc5f948a-7482-4436-94b1-5c7a934eea12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:29:29.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-280" for this suite.
Feb 19 22:29:35.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:29:35.471: INFO: namespace configmap-280 deletion completed in 6.17644443s

• [SLOW TEST:8.289 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:29:35.474: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:29:41.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8052" for this suite.
Feb 19 22:29:47.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:29:47.865: INFO: namespace namespaces-8052 deletion completed in 6.150166582s
STEP: Destroying namespace "nsdeletetest-9750" for this suite.
Feb 19 22:29:47.868: INFO: Namespace nsdeletetest-9750 was already deleted
STEP: Destroying namespace "nsdeletetest-1416" for this suite.
Feb 19 22:29:53.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:29:54.029: INFO: namespace nsdeletetest-1416 deletion completed in 6.161213879s

• [SLOW TEST:18.556 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:29:54.034: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 19 22:30:00.689: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:00.689: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:00.851: INFO: Exec stderr: ""
Feb 19 22:30:00.851: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:00.851: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:01.081: INFO: Exec stderr: ""
Feb 19 22:30:01.081: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:01.081: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:01.289: INFO: Exec stderr: ""
Feb 19 22:30:01.289: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:01.289: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:01.504: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 19 22:30:01.504: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:01.504: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:01.722: INFO: Exec stderr: ""
Feb 19 22:30:01.723: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:01.723: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:01.925: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 19 22:30:01.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:01.926: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:02.075: INFO: Exec stderr: ""
Feb 19 22:30:02.075: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:02.075: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:02.268: INFO: Exec stderr: ""
Feb 19 22:30:02.268: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:02.268: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:02.466: INFO: Exec stderr: ""
Feb 19 22:30:02.466: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2818 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:30:02.466: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:30:02.673: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:30:02.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2818" for this suite.
Feb 19 22:30:42.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:30:42.959: INFO: namespace e2e-kubelet-etc-hosts-2818 deletion completed in 40.281490302s

• [SLOW TEST:48.925 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:30:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:30:47.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7878" for this suite.
Feb 19 22:31:25.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:31:25.296: INFO: namespace kubelet-test-7878 deletion completed in 38.1593305s

• [SLOW TEST:42.337 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:31:25.301: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0219 22:31:56.126285      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 19 22:31:56.127: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:31:56.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7446" for this suite.
Feb 19 22:32:02.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:32:02.303: INFO: namespace gc-7446 deletion completed in 6.169918312s

• [SLOW TEST:37.002 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:32:02.304: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9496.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9496.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9496.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9496.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 19 22:32:06.394: INFO: DNS probes using dns-test-87913856-bc0d-4a20-b231-33f21317baf1 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9496.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9496.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9496.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9496.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 19 22:32:10.508: INFO: DNS probes using dns-test-cd73b94d-a765-4310-8acf-88131e088100 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9496.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9496.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9496.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9496.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 19 22:32:14.739: INFO: DNS probes using dns-test-5993633f-99f2-49a8-8fef-919a510fb48e succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:32:14.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9496" for this suite.
Feb 19 22:32:20.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:32:20.963: INFO: namespace dns-9496 deletion completed in 6.159226593s

• [SLOW TEST:18.660 seconds]
[sig-network] DNS
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:32:20.966: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:32:21.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-9831'
Feb 19 22:32:21.154: INFO: stderr: ""
Feb 19 22:32:21.154: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 19 22:32:26.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 get pod e2e-test-nginx-pod --namespace=kubectl-9831 -o json'
Feb 19 22:32:26.287: INFO: stderr: ""
Feb 19 22:32:26.287: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-02-19T22:32:21Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-9831\",\n        \"resourceVersion\": \"118920\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9831/pods/e2e-test-nginx-pod\",\n        \"uid\": \"591fc2f0-f6b4-489f-97b2-36bac2a5d9db\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pnklq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gke-c115-default-pool-249bf33f-nfnp\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pnklq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pnklq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-19T22:32:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-19T22:32:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-19T22:32:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-19T22:32:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://888ce071e0e1cc4a8bb5bd05dc918c07529372e847f34769f697a41031bf4798\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-19T22:32:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.142.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.56.2.183\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-19T22:32:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 19 22:32:26.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 replace -f - --namespace=kubectl-9831'
Feb 19 22:32:26.532: INFO: stderr: ""
Feb 19 22:32:26.532: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Feb 19 22:32:26.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete pods e2e-test-nginx-pod --namespace=kubectl-9831'
Feb 19 22:32:29.383: INFO: stderr: ""
Feb 19 22:32:29.383: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:32:29.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9831" for this suite.
Feb 19 22:32:35.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:32:35.528: INFO: namespace kubectl-9831 deletion completed in 6.140552456s

• [SLOW TEST:14.562 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:32:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:32:37.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2120" for this suite.
Feb 19 22:33:29.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:33:29.773: INFO: namespace kubelet-test-2120 deletion completed in 52.15387995s

• [SLOW TEST:54.242 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:33:29.780: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5453
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5453
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5453
Feb 19 22:33:29.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 19 22:33:39.982: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 19 22:33:40.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 22:33:40.726: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 22:33:40.726: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 22:33:40.726: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 22:33:40.732: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 19 22:33:50.735: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 22:33:50.735: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 22:33:50.761: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:33:50.761: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:33:50.761: INFO: 
Feb 19 22:33:50.761: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 19 22:33:51.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987152927s
Feb 19 22:33:52.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983358883s
Feb 19 22:33:53.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980058989s
Feb 19 22:33:54.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976233988s
Feb 19 22:33:55.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.911425675s
Feb 19 22:33:57.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.821384571s
Feb 19 22:33:58.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.737388647s
Feb 19 22:33:59.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.730569287s
Feb 19 22:34:00.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 726.757221ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5453
Feb 19 22:34:01.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:01.303: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 22:34:01.303: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 22:34:01.303: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 22:34:01.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:01.594: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 19 22:34:01.594: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 22:34:01.594: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 22:34:01.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:01.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 19 22:34:01.846: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 22:34:01.846: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 22:34:01.849: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:34:01.849: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:34:01.849: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 19 22:34:01.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 22:34:02.165: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 22:34:02.165: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 22:34:02.165: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 22:34:02.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 22:34:02.513: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 22:34:02.513: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 22:34:02.513: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 22:34:02.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 22:34:02.774: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 22:34:02.774: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 22:34:02.774: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 22:34:02.774: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 22:34:02.778: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 19 22:34:12.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 22:34:12.790: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 22:34:12.790: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 19 22:34:12.822: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:12.822: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:12.822: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:12.822: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:12.822: INFO: 
Feb 19 22:34:12.822: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:13.825: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:13.825: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:13.825: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:13.825: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:13.825: INFO: 
Feb 19 22:34:13.825: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:14.829: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:14.829: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:14.829: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:14.829: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:14.829: INFO: 
Feb 19 22:34:14.829: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:15.832: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:15.832: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:15.832: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:15.832: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:15.832: INFO: 
Feb 19 22:34:15.832: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:16.836: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:16.836: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:16.836: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:16.836: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:16.836: INFO: 
Feb 19 22:34:16.836: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:17.839: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:17.839: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:17.839: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:17.839: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:17.839: INFO: 
Feb 19 22:34:17.839: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:18.843: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:18.843: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:18.843: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:18.843: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:18.843: INFO: 
Feb 19 22:34:18.843: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:19.847: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:19.847: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:19.847: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:19.847: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:19.847: INFO: 
Feb 19 22:34:19.847: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:20.850: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:20.850: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:20.850: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:20.850: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:20.850: INFO: 
Feb 19 22:34:20.850: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 19 22:34:21.853: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Feb 19 22:34:21.853: INFO: ss-0  gke-c115-default-pool-249bf33f-nfnp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:29 +0000 UTC  }]
Feb 19 22:34:21.854: INFO: ss-1  gke-c115-default-pool-249bf33f-qclh  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:21.854: INFO: ss-2  gke-c115-default-pool-249bf33f-vhvp  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:34:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:33:50 +0000 UTC  }]
Feb 19 22:34:21.854: INFO: 
Feb 19 22:34:21.854: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5453
Feb 19 22:34:22.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:22.970: INFO: rc: 1
Feb 19 22:34:22.971: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0019ca720 exit status 1 <nil> <nil> true [0xc002756c90 0xc002756ca8 0xc002756cc8] [0xc002756c90 0xc002756ca8 0xc002756cc8] [0xc002756ca0 0xc002756cc0] [0xba6c10 0xba6c10] 0xc0016eb0e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Feb 19 22:34:32.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:33.060: INFO: rc: 1
Feb 19 22:34:33.061: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cab10 exit status 1 <nil> <nil> true [0xc002756cd0 0xc002756ce8 0xc002756d00] [0xc002756cd0 0xc002756ce8 0xc002756d00] [0xc002756ce0 0xc002756cf8] [0xba6c10 0xba6c10] 0xc001e2c900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:34:43.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:43.152: INFO: rc: 1
Feb 19 22:34:43.152: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019caf30 exit status 1 <nil> <nil> true [0xc002756d08 0xc002756d20 0xc002756d38] [0xc002756d08 0xc002756d20 0xc002756d38] [0xc002756d18 0xc002756d30] [0xba6c10 0xba6c10] 0xc001df6360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:34:53.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:34:53.241: INFO: rc: 1
Feb 19 22:34:53.241: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cb410 exit status 1 <nil> <nil> true [0xc002756d40 0xc002756d58 0xc002756d70] [0xc002756d40 0xc002756d58 0xc002756d70] [0xc002756d50 0xc002756d68] [0xba6c10 0xba6c10] 0xc001df7f20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:35:03.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:35:03.351: INFO: rc: 1
Feb 19 22:35:03.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cb8c0 exit status 1 <nil> <nil> true [0xc002756d78 0xc002756d90 0xc002756da8] [0xc002756d78 0xc002756d90 0xc002756da8] [0xc002756d88 0xc002756da0] [0xba6c10 0xba6c10] 0xc002772d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:35:13.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:35:13.434: INFO: rc: 1
Feb 19 22:35:13.434: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cbcb0 exit status 1 <nil> <nil> true [0xc002756db0 0xc002756dc8 0xc002756de8] [0xc002756db0 0xc002756dc8 0xc002756de8] [0xc002756dc0 0xc002756de0] [0xba6c10 0xba6c10] 0xc002773980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:35:23.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:35:23.531: INFO: rc: 1
Feb 19 22:35:23.531: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002074150 exit status 1 <nil> <nil> true [0xc002756df0 0xc002756e08 0xc002756e20] [0xc002756df0 0xc002756e08 0xc002756e20] [0xc002756e00 0xc002756e18] [0xba6c10 0xba6c10] 0xc0019c6de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:35:33.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:35:33.622: INFO: rc: 1
Feb 19 22:35:33.622: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020744e0 exit status 1 <nil> <nil> true [0xc002756e28 0xc002756e40 0xc002756e58] [0xc002756e28 0xc002756e40 0xc002756e58] [0xc002756e38 0xc002756e50] [0xba6c10 0xba6c10] 0xc0019c79e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:35:43.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:35:43.707: INFO: rc: 1
Feb 19 22:35:43.707: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002074870 exit status 1 <nil> <nil> true [0xc002756e60 0xc002756e78 0xc002756e90] [0xc002756e60 0xc002756e78 0xc002756e90] [0xc002756e70 0xc002756e88] [0xba6c10 0xba6c10] 0xc0019966c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:35:53.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:35:53.796: INFO: rc: 1
Feb 19 22:35:53.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019ca750 exit status 1 <nil> <nil> true [0xc002756010 0xc002756028 0xc002756080] [0xc002756010 0xc002756028 0xc002756080] [0xc002756020 0xc002756068] [0xba6c10 0xba6c10] 0xc0019c6f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:36:03.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:36:03.888: INFO: rc: 1
Feb 19 22:36:03.888: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cab70 exit status 1 <nil> <nil> true [0xc002756088 0xc0027560b8 0xc0027560e0] [0xc002756088 0xc0027560b8 0xc0027560e0] [0xc002756098 0xc0027560d8] [0xba6c10 0xba6c10] 0xc0019c7b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:36:13.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:36:13.971: INFO: rc: 1
Feb 19 22:36:13.971: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019caff0 exit status 1 <nil> <nil> true [0xc0027560e8 0xc002756130 0xc0027561c0] [0xc0027560e8 0xc002756130 0xc0027561c0] [0xc002756120 0xc002756190] [0xba6c10 0xba6c10] 0xc002772ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:36:23.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:36:24.129: INFO: rc: 1
Feb 19 22:36:24.129: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cb5c0 exit status 1 <nil> <nil> true [0xc0027561e8 0xc002756230 0xc002756268] [0xc0027561e8 0xc002756230 0xc002756268] [0xc0027561f8 0xc002756258] [0xba6c10 0xba6c10] 0xc0027736e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:36:34.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:36:34.413: INFO: rc: 1
Feb 19 22:36:34.413: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cb9b0 exit status 1 <nil> <nil> true [0xc002756278 0xc0027562f0 0xc002756360] [0xc002756278 0xc0027562f0 0xc002756360] [0xc0027562d0 0xc002756340] [0xba6c10 0xba6c10] 0xc001df6fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:36:44.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:36:44.504: INFO: rc: 1
Feb 19 22:36:44.504: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cbe00 exit status 1 <nil> <nil> true [0xc002756388 0xc0027563b0 0xc0027563d8] [0xc002756388 0xc0027563b0 0xc0027563d8] [0xc0027563a8 0xc0027563c8] [0xba6c10 0xba6c10] 0xc001e2c900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:36:54.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:36:54.592: INFO: rc: 1
Feb 19 22:36:54.592: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001848330 exit status 1 <nil> <nil> true [0xc0027563e8 0xc002756438 0xc002756460] [0xc0027563e8 0xc002756438 0xc002756460] [0xc002756418 0xc002756458] [0xba6c10 0xba6c10] 0xc0016ea240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:37:04.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:37:04.688: INFO: rc: 1
Feb 19 22:37:04.688: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001848ae0 exit status 1 <nil> <nil> true [0xc002756468 0xc0027564b0 0xc0027564d0] [0xc002756468 0xc0027564b0 0xc0027564d0] [0xc0027564a0 0xc0027564c8] [0xba6c10 0xba6c10] 0xc0016eb920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:37:14.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:37:14.775: INFO: rc: 1
Feb 19 22:37:14.775: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001849080 exit status 1 <nil> <nil> true [0xc0027564e0 0xc002756508 0xc0027565b8] [0xc0027564e0 0xc002756508 0xc0027565b8] [0xc002756500 0xc0027565a0] [0xba6c10 0xba6c10] 0xc00120f140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:37:24.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:37:24.867: INFO: rc: 1
Feb 19 22:37:24.867: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001849530 exit status 1 <nil> <nil> true [0xc0027565c8 0xc002756600 0xc002756648] [0xc0027565c8 0xc002756600 0xc002756648] [0xc0027565e8 0xc002756630] [0xba6c10 0xba6c10] 0xc00113d020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:37:34.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:37:34.951: INFO: rc: 1
Feb 19 22:37:34.951: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0018498f0 exit status 1 <nil> <nil> true [0xc002756658 0xc002756680 0xc0027566b8] [0xc002756658 0xc002756680 0xc0027566b8] [0xc002756678 0xc002756698] [0xba6c10 0xba6c10] 0xc001645500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:37:44.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:37:45.039: INFO: rc: 1
Feb 19 22:37:45.039: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0013863f0 exit status 1 <nil> <nil> true [0xc0027566d0 0xc002756708 0xc002756738] [0xc0027566d0 0xc002756708 0xc002756738] [0xc0027566f8 0xc002756730] [0xba6c10 0xba6c10] 0xc00177c300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:37:55.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:37:55.334: INFO: rc: 1
Feb 19 22:37:55.334: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001848720 exit status 1 <nil> <nil> true [0xc002756010 0xc002756028 0xc002756080] [0xc002756010 0xc002756028 0xc002756080] [0xc002756020 0xc002756068] [0xba6c10 0xba6c10] 0xc001645a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:38:05.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:38:05.425: INFO: rc: 1
Feb 19 22:38:05.425: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001848cc0 exit status 1 <nil> <nil> true [0xc002756088 0xc0027560b8 0xc0027560e0] [0xc002756088 0xc0027560b8 0xc0027560e0] [0xc002756098 0xc0027560d8] [0xba6c10 0xba6c10] 0xc00113d320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:38:15.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:38:15.524: INFO: rc: 1
Feb 19 22:38:15.524: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0018492f0 exit status 1 <nil> <nil> true [0xc0027560e8 0xc002756130 0xc0027561c0] [0xc0027560e8 0xc002756130 0xc0027561c0] [0xc002756120 0xc002756190] [0xba6c10 0xba6c10] 0xc00120e9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:38:25.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:38:25.623: INFO: rc: 1
Feb 19 22:38:25.623: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0018496e0 exit status 1 <nil> <nil> true [0xc0027561e8 0xc002756230 0xc002756268] [0xc0027561e8 0xc002756230 0xc002756268] [0xc0027561f8 0xc002756258] [0xba6c10 0xba6c10] 0xc0016ea7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:38:35.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:38:35.718: INFO: rc: 1
Feb 19 22:38:35.719: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019ca270 exit status 1 <nil> <nil> true [0xc002756278 0xc0027562f0 0xc002756360] [0xc002756278 0xc0027562f0 0xc002756360] [0xc0027562d0 0xc002756340] [0xba6c10 0xba6c10] 0xc0016ebe00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:38:45.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:38:45.807: INFO: rc: 1
Feb 19 22:38:45.807: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019ca810 exit status 1 <nil> <nil> true [0xc002756388 0xc0027563b0 0xc0027563d8] [0xc002756388 0xc0027563b0 0xc0027563d8] [0xc0027563a8 0xc0027563c8] [0xba6c10 0xba6c10] 0xc001e2d500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:38:55.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:38:56.164: INFO: rc: 1
Feb 19 22:38:56.164: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cac00 exit status 1 <nil> <nil> true [0xc0027563e8 0xc002756438 0xc002756460] [0xc0027563e8 0xc002756438 0xc002756460] [0xc002756418 0xc002756458] [0xba6c10 0xba6c10] 0xc001df72c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:39:06.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:39:06.252: INFO: rc: 1
Feb 19 22:39:06.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cb0b0 exit status 1 <nil> <nil> true [0xc002756468 0xc0027564b0 0xc0027564d0] [0xc002756468 0xc0027564b0 0xc0027564d0] [0xc0027564a0 0xc0027564c8] [0xba6c10 0xba6c10] 0xc002772840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:39:16.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:39:16.345: INFO: rc: 1
Feb 19 22:39:16.346: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0019cb680 exit status 1 <nil> <nil> true [0xc0027564e0 0xc002756508 0xc0027565b8] [0xc0027564e0 0xc002756508 0xc0027565b8] [0xc002756500 0xc0027565a0] [0xba6c10 0xba6c10] 0xc0027734a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 19 22:39:26.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-5453 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:39:26.431: INFO: rc: 1
Feb 19 22:39:26.431: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 19 22:39:26.431: INFO: Scaling statefulset ss to 0
Feb 19 22:39:26.466: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 19 22:39:26.468: INFO: Deleting all statefulset in ns statefulset-5453
Feb 19 22:39:26.471: INFO: Scaling statefulset ss to 0
Feb 19 22:39:26.481: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 22:39:26.484: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:39:26.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5453" for this suite.
Feb 19 22:39:32.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:39:32.652: INFO: namespace statefulset-5453 deletion completed in 6.150741327s

• [SLOW TEST:362.873 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:39:32.655: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 19 22:39:32.700: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2113,SelfLink:/api/v1/namespaces/watch-2113/configmaps/e2e-watch-test-watch-closed,UID:03288309-6e58-4f2e-a965-52687347492c,ResourceVersion:120632,Generation:0,CreationTimestamp:2020-02-19 22:39:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 19 22:39:32.700: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2113,SelfLink:/api/v1/namespaces/watch-2113/configmaps/e2e-watch-test-watch-closed,UID:03288309-6e58-4f2e-a965-52687347492c,ResourceVersion:120633,Generation:0,CreationTimestamp:2020-02-19 22:39:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 19 22:39:32.714: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2113,SelfLink:/api/v1/namespaces/watch-2113/configmaps/e2e-watch-test-watch-closed,UID:03288309-6e58-4f2e-a965-52687347492c,ResourceVersion:120634,Generation:0,CreationTimestamp:2020-02-19 22:39:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 19 22:39:32.715: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2113,SelfLink:/api/v1/namespaces/watch-2113/configmaps/e2e-watch-test-watch-closed,UID:03288309-6e58-4f2e-a965-52687347492c,ResourceVersion:120635,Generation:0,CreationTimestamp:2020-02-19 22:39:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:39:32.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2113" for this suite.
Feb 19 22:39:38.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:39:38.983: INFO: namespace watch-2113 deletion completed in 6.264028415s

• [SLOW TEST:6.329 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:39:38.986: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Feb 19 22:39:39.098: INFO: Waiting up to 5m0s for pod "var-expansion-66596f20-995d-42e3-8983-3336d3549e11" in namespace "var-expansion-5511" to be "success or failure"
Feb 19 22:39:39.110: INFO: Pod "var-expansion-66596f20-995d-42e3-8983-3336d3549e11": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005566ms
Feb 19 22:39:41.113: INFO: Pod "var-expansion-66596f20-995d-42e3-8983-3336d3549e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01541944s
STEP: Saw pod success
Feb 19 22:39:41.114: INFO: Pod "var-expansion-66596f20-995d-42e3-8983-3336d3549e11" satisfied condition "success or failure"
Feb 19 22:39:41.117: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod var-expansion-66596f20-995d-42e3-8983-3336d3549e11 container dapi-container: <nil>
STEP: delete the pod
Feb 19 22:39:41.143: INFO: Waiting for pod var-expansion-66596f20-995d-42e3-8983-3336d3549e11 to disappear
Feb 19 22:39:41.147: INFO: Pod var-expansion-66596f20-995d-42e3-8983-3336d3549e11 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:39:41.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5511" for this suite.
Feb 19 22:39:47.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:39:47.333: INFO: namespace var-expansion-5511 deletion completed in 6.182931649s

• [SLOW TEST:8.348 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:39:47.334: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 19 22:39:49.935: INFO: Successfully updated pod "labelsupdatec6c86f69-ff3a-43c4-bf91-2d906a4b5f9c"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:39:51.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3302" for this suite.
Feb 19 22:40:15.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:40:16.129: INFO: namespace downward-api-3302 deletion completed in 24.166989651s

• [SLOW TEST:28.795 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:40:16.130: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:40:16.324: INFO: (0) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 151.706224ms)
Feb 19 22:40:16.330: INFO: (1) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.712897ms)
Feb 19 22:40:16.337: INFO: (2) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.721946ms)
Feb 19 22:40:16.352: INFO: (3) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 14.055062ms)
Feb 19 22:40:16.360: INFO: (4) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 7.972586ms)
Feb 19 22:40:16.364: INFO: (5) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.309742ms)
Feb 19 22:40:16.370: INFO: (6) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.592068ms)
Feb 19 22:40:16.378: INFO: (7) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 8.481485ms)
Feb 19 22:40:16.383: INFO: (8) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.004103ms)
Feb 19 22:40:16.388: INFO: (9) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.743736ms)
Feb 19 22:40:16.393: INFO: (10) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.213427ms)
Feb 19 22:40:16.400: INFO: (11) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.08154ms)
Feb 19 22:40:16.405: INFO: (12) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.54556ms)
Feb 19 22:40:16.410: INFO: (13) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.204433ms)
Feb 19 22:40:16.416: INFO: (14) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.50011ms)
Feb 19 22:40:16.421: INFO: (15) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.137873ms)
Feb 19 22:40:16.426: INFO: (16) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 5.143314ms)
Feb 19 22:40:16.431: INFO: (17) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.808704ms)
Feb 19 22:40:16.438: INFO: (18) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 6.935041ms)
Feb 19 22:40:16.443: INFO: (19) /api/v1/nodes/gke-c115-default-pool-249bf33f-nfnp:10250/proxy/logs/: <pre>
<a href="cloud-init.log">cloud-init.log</a>
<a href="containers/">containers/</a>
<a href="... (200; 4.974292ms)
[AfterEach] version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:40:16.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-696" for this suite.
Feb 19 22:40:22.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:40:22.605: INFO: namespace proxy-696 deletion completed in 6.15857466s

• [SLOW TEST:6.475 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:40:22.606: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Feb 19 22:40:25.207: INFO: Successfully updated pod "annotationupdate26fbb627-ea4a-4968-9ee9-f09f78eec0ed"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:40:27.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9401" for this suite.
Feb 19 22:40:49.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:40:49.490: INFO: namespace projected-9401 deletion completed in 22.249615367s

• [SLOW TEST:26.885 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:40:49.492: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:41:14.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5672" for this suite.
Feb 19 22:41:20.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:41:21.124: INFO: namespace container-runtime-5672 deletion completed in 6.145105035s

• [SLOW TEST:31.633 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:41:21.127: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 19 22:41:24.262: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:41:25.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-458" for this suite.
Feb 19 22:41:47.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:41:47.449: INFO: namespace replicaset-458 deletion completed in 22.157917427s

• [SLOW TEST:26.322 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:41:47.451: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-nzwh
STEP: Creating a pod to test atomic-volume-subpath
Feb 19 22:41:47.517: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nzwh" in namespace "subpath-1208" to be "success or failure"
Feb 19 22:41:47.522: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085601ms
Feb 19 22:41:49.525: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007251408s
Feb 19 22:41:51.528: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 4.010373713s
Feb 19 22:41:53.531: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 6.013610966s
Feb 19 22:41:55.628: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 8.110970804s
Feb 19 22:41:57.766: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 10.248627183s
Feb 19 22:41:59.769: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 12.251574286s
Feb 19 22:42:01.772: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 14.254095596s
Feb 19 22:42:03.777: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 16.259273475s
Feb 19 22:42:05.780: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 18.262365339s
Feb 19 22:42:07.783: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Running", Reason="", readiness=true. Elapsed: 20.265397828s
Feb 19 22:42:09.865: INFO: Pod "pod-subpath-test-configmap-nzwh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.347029586s
STEP: Saw pod success
Feb 19 22:42:09.865: INFO: Pod "pod-subpath-test-configmap-nzwh" satisfied condition "success or failure"
Feb 19 22:42:09.967: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-subpath-test-configmap-nzwh container test-container-subpath-configmap-nzwh: <nil>
STEP: delete the pod
Feb 19 22:42:10.452: INFO: Waiting for pod pod-subpath-test-configmap-nzwh to disappear
Feb 19 22:42:10.527: INFO: Pod pod-subpath-test-configmap-nzwh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nzwh
Feb 19 22:42:10.527: INFO: Deleting pod "pod-subpath-test-configmap-nzwh" in namespace "subpath-1208"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:42:10.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1208" for this suite.
Feb 19 22:42:16.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:42:16.784: INFO: namespace subpath-1208 deletion completed in 6.149285343s

• [SLOW TEST:29.333 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:42:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:42:22.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2561" for this suite.
Feb 19 22:42:28.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:42:28.707: INFO: namespace watch-2561 deletion completed in 6.31466253s

• [SLOW TEST:11.920 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:42:28.708: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Feb 19 22:42:28.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 --namespace=kubectl-8379 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 19 22:42:31.032: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 19 22:42:31.032: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:42:33.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8379" for this suite.
Feb 19 22:42:39.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:42:39.184: INFO: namespace kubectl-8379 deletion completed in 6.142959002s

• [SLOW TEST:10.476 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:42:39.186: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-0b9f85e9-729d-4e07-80fa-d947d70d45cd
STEP: Creating secret with name secret-projected-all-test-volume-058e71be-6d3e-4b5f-be64-dbca00104475
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 19 22:42:39.301: INFO: Waiting up to 5m0s for pod "projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6" in namespace "projected-3269" to be "success or failure"
Feb 19 22:42:39.310: INFO: Pod "projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.264169ms
Feb 19 22:42:41.314: INFO: Pod "projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013416665s
STEP: Saw pod success
Feb 19 22:42:41.314: INFO: Pod "projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6" satisfied condition "success or failure"
Feb 19 22:42:41.323: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 19 22:42:41.384: INFO: Waiting for pod projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6 to disappear
Feb 19 22:42:41.395: INFO: Pod projected-volume-ffce8599-baf1-49f6-863c-5cb6aa1281e6 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:42:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3269" for this suite.
Feb 19 22:42:47.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:42:47.574: INFO: namespace projected-3269 deletion completed in 6.16704102s

• [SLOW TEST:8.388 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:42:47.575: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:42:47.642: INFO: Create a RollingUpdate DaemonSet
Feb 19 22:42:47.650: INFO: Check that daemon pods launch on every node of the cluster
Feb 19 22:42:47.663: INFO: Number of nodes with available pods: 0
Feb 19 22:42:47.663: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:42:48.669: INFO: Number of nodes with available pods: 0
Feb 19 22:42:48.669: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:42:49.670: INFO: Number of nodes with available pods: 2
Feb 19 22:42:49.670: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:42:50.670: INFO: Number of nodes with available pods: 3
Feb 19 22:42:50.670: INFO: Number of running nodes: 3, number of available pods: 3
Feb 19 22:42:50.670: INFO: Update the DaemonSet to trigger a rollout
Feb 19 22:42:50.683: INFO: Updating DaemonSet daemon-set
Feb 19 22:43:05.696: INFO: Roll back the DaemonSet before rollout is complete
Feb 19 22:43:05.713: INFO: Updating DaemonSet daemon-set
Feb 19 22:43:05.713: INFO: Make sure DaemonSet rollback is complete
Feb 19 22:43:05.759: INFO: Wrong image for pod: daemon-set-mv8h2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 19 22:43:05.759: INFO: Pod daemon-set-mv8h2 is not available
Feb 19 22:43:06.776: INFO: Wrong image for pod: daemon-set-mv8h2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 19 22:43:06.776: INFO: Pod daemon-set-mv8h2 is not available
Feb 19 22:43:07.776: INFO: Wrong image for pod: daemon-set-mv8h2. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Feb 19 22:43:07.776: INFO: Pod daemon-set-mv8h2 is not available
Feb 19 22:43:08.776: INFO: Pod daemon-set-ddj2s is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2093, will wait for the garbage collector to delete the pods
Feb 19 22:43:08.850: INFO: Deleting DaemonSet.extensions daemon-set took: 7.193138ms
Feb 19 22:43:09.450: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.219496ms
Feb 19 22:43:12.859: INFO: Number of nodes with available pods: 0
Feb 19 22:43:12.859: INFO: Number of running nodes: 0, number of available pods: 0
Feb 19 22:43:12.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2093/daemonsets","resourceVersion":"121937"},"items":null}

Feb 19 22:43:12.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2093/pods","resourceVersion":"121937"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:43:12.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2093" for this suite.
Feb 19 22:43:18.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:43:19.124: INFO: namespace daemonsets-2093 deletion completed in 6.168636571s

• [SLOW TEST:31.549 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:43:19.127: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:43:21.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-70" for this suite.
Feb 19 22:43:27.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:43:27.491: INFO: namespace emptydir-wrapper-70 deletion completed in 6.270711307s

• [SLOW TEST:8.364 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:43:27.493: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Feb 19 22:43:27.589: INFO: Waiting up to 5m0s for pod "var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1" in namespace "var-expansion-5891" to be "success or failure"
Feb 19 22:43:27.597: INFO: Pod "var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.352556ms
Feb 19 22:43:29.600: INFO: Pod "var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010302009s
STEP: Saw pod success
Feb 19 22:43:29.600: INFO: Pod "var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1" satisfied condition "success or failure"
Feb 19 22:43:29.602: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1 container dapi-container: <nil>
STEP: delete the pod
Feb 19 22:43:29.626: INFO: Waiting for pod var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1 to disappear
Feb 19 22:43:29.632: INFO: Pod var-expansion-0db295a0-5358-43c5-b05e-a124fe714ec1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:43:29.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5891" for this suite.
Feb 19 22:43:35.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:43:35.806: INFO: namespace var-expansion-5891 deletion completed in 6.170251953s

• [SLOW TEST:8.313 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:43:35.806: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:43:35.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044" in namespace "projected-3623" to be "success or failure"
Feb 19 22:43:35.872: INFO: Pod "downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044": Phase="Pending", Reason="", readiness=false. Elapsed: 4.673077ms
Feb 19 22:43:37.876: INFO: Pod "downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008193808s
STEP: Saw pod success
Feb 19 22:43:37.876: INFO: Pod "downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044" satisfied condition "success or failure"
Feb 19 22:43:37.878: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044 container client-container: <nil>
STEP: delete the pod
Feb 19 22:43:37.900: INFO: Waiting for pod downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044 to disappear
Feb 19 22:43:37.902: INFO: Pod downwardapi-volume-c521dfa4-83b3-4ecd-b735-7e7b1dd8d044 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:43:37.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3623" for this suite.
Feb 19 22:43:43.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:43:44.060: INFO: namespace projected-3623 deletion completed in 6.153430137s

• [SLOW TEST:8.254 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:43:44.061: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Feb 19 22:43:44.105: INFO: Waiting up to 5m0s for pod "var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2" in namespace "var-expansion-5802" to be "success or failure"
Feb 19 22:43:44.109: INFO: Pod "var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5453ms
Feb 19 22:43:46.112: INFO: Pod "var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006595648s
STEP: Saw pod success
Feb 19 22:43:46.112: INFO: Pod "var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2" satisfied condition "success or failure"
Feb 19 22:43:46.114: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2 container dapi-container: <nil>
STEP: delete the pod
Feb 19 22:43:46.134: INFO: Waiting for pod var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2 to disappear
Feb 19 22:43:46.138: INFO: Pod var-expansion-53fb2714-0ca6-474b-83d9-0addbcd054d2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:43:46.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5802" for this suite.
Feb 19 22:43:52.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:43:52.298: INFO: namespace var-expansion-5802 deletion completed in 6.156318692s

• [SLOW TEST:8.237 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:43:52.303: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:43:52.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2" in namespace "downward-api-7194" to be "success or failure"
Feb 19 22:43:52.354: INFO: Pod "downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.160436ms
Feb 19 22:43:54.359: INFO: Pod "downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01136841s
STEP: Saw pod success
Feb 19 22:43:54.359: INFO: Pod "downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2" satisfied condition "success or failure"
Feb 19 22:43:54.363: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2 container client-container: <nil>
STEP: delete the pod
Feb 19 22:43:54.389: INFO: Waiting for pod downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2 to disappear
Feb 19 22:43:54.392: INFO: Pod downwardapi-volume-a5924ca3-8bb5-471e-b88f-bde09581eed2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:43:54.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7194" for this suite.
Feb 19 22:44:00.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:44:00.567: INFO: namespace downward-api-7194 deletion completed in 6.16935152s

• [SLOW TEST:8.264 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:44:00.570: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:44:02.809: INFO: Waiting up to 5m0s for pod "client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a" in namespace "pods-7825" to be "success or failure"
Feb 19 22:44:02.813: INFO: Pod "client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062605ms
Feb 19 22:44:04.816: INFO: Pod "client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007398784s
STEP: Saw pod success
Feb 19 22:44:04.816: INFO: Pod "client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a" satisfied condition "success or failure"
Feb 19 22:44:04.819: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-qclh pod client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a container env3cont: <nil>
STEP: delete the pod
Feb 19 22:44:04.843: INFO: Waiting for pod client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a to disappear
Feb 19 22:44:04.848: INFO: Pod client-envvars-7d99cb40-e085-4f65-8ec5-f8689b96689a no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:44:04.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7825" for this suite.
Feb 19 22:44:46.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:44:47.004: INFO: namespace pods-7825 deletion completed in 42.152567716s

• [SLOW TEST:46.435 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:44:47.007: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-0e6db6fe-5ff6-44b4-8d6d-d34d3d96a5fb
STEP: Creating a pod to test consume configMaps
Feb 19 22:44:47.136: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9" in namespace "projected-9653" to be "success or failure"
Feb 19 22:44:47.141: INFO: Pod "pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37876ms
Feb 19 22:44:49.144: INFO: Pod "pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008645738s
STEP: Saw pod success
Feb 19 22:44:49.144: INFO: Pod "pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9" satisfied condition "success or failure"
Feb 19 22:44:49.147: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:44:49.175: INFO: Waiting for pod pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9 to disappear
Feb 19 22:44:49.181: INFO: Pod pod-projected-configmaps-5039c572-9413-40dc-9497-75ef048e8ca9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:44:49.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9653" for this suite.
Feb 19 22:44:55.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:44:58.287: INFO: namespace projected-9653 deletion completed in 9.102606967s

• [SLOW TEST:11.281 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:44:58.289: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-9ntg4 in namespace proxy-618
I0219 22:44:58.405953      14 runners.go:180] Created replication controller with name: proxy-service-9ntg4, namespace: proxy-618, replica count: 1
I0219 22:44:59.456850      14 runners.go:180] proxy-service-9ntg4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0219 22:45:00.457184      14 runners.go:180] proxy-service-9ntg4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0219 22:45:01.457636      14 runners.go:180] proxy-service-9ntg4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0219 22:45:02.457978      14 runners.go:180] proxy-service-9ntg4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0219 22:45:03.458467      14 runners.go:180] proxy-service-9ntg4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0219 22:45:04.458740      14 runners.go:180] proxy-service-9ntg4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 19 22:45:04.462: INFO: setup took 6.133085663s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 19 22:45:04.481: INFO: (0) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 18.375204ms)
Feb 19 22:45:04.482: INFO: (0) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 20.108188ms)
Feb 19 22:45:04.484: INFO: (0) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 21.068462ms)
Feb 19 22:45:04.484: INFO: (0) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 20.876614ms)
Feb 19 22:45:04.484: INFO: (0) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 21.475047ms)
Feb 19 22:45:04.484: INFO: (0) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 22.024593ms)
Feb 19 22:45:04.485: INFO: (0) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 22.568853ms)
Feb 19 22:45:04.485: INFO: (0) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 22.449799ms)
Feb 19 22:45:04.485: INFO: (0) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 22.524716ms)
Feb 19 22:45:04.502: INFO: (0) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 39.416931ms)
Feb 19 22:45:04.511: INFO: (0) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 49.040421ms)
Feb 19 22:45:04.516: INFO: (0) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 53.50585ms)
Feb 19 22:45:04.516: INFO: (0) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 53.482206ms)
Feb 19 22:45:04.516: INFO: (0) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 53.722084ms)
Feb 19 22:45:04.526: INFO: (0) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 63.178094ms)
Feb 19 22:45:04.559: INFO: (0) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 96.006424ms)
Feb 19 22:45:04.580: INFO: (1) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 20.041724ms)
Feb 19 22:45:04.581: INFO: (1) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 21.809773ms)
Feb 19 22:45:04.581: INFO: (1) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 22.096079ms)
Feb 19 22:45:04.581: INFO: (1) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 21.567277ms)
Feb 19 22:45:04.581: INFO: (1) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 22.224881ms)
Feb 19 22:45:04.582: INFO: (1) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 22.756376ms)
Feb 19 22:45:04.582: INFO: (1) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 22.104244ms)
Feb 19 22:45:04.584: INFO: (1) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 24.388427ms)
Feb 19 22:45:04.584: INFO: (1) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 24.84757ms)
Feb 19 22:45:04.584: INFO: (1) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 24.803851ms)
Feb 19 22:45:04.584: INFO: (1) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 25.3509ms)
Feb 19 22:45:04.585: INFO: (1) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 25.605511ms)
Feb 19 22:45:04.585: INFO: (1) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 25.832614ms)
Feb 19 22:45:04.586: INFO: (1) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 26.449542ms)
Feb 19 22:45:04.586: INFO: (1) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 26.210494ms)
Feb 19 22:45:04.586: INFO: (1) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 26.460306ms)
Feb 19 22:45:04.595: INFO: (2) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 8.75554ms)
Feb 19 22:45:04.596: INFO: (2) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 9.970046ms)
Feb 19 22:45:04.598: INFO: (2) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 11.812466ms)
Feb 19 22:45:04.606: INFO: (2) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 19.831777ms)
Feb 19 22:45:04.609: INFO: (2) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 22.47234ms)
Feb 19 22:45:04.609: INFO: (2) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 22.997971ms)
Feb 19 22:45:04.610: INFO: (2) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 22.993756ms)
Feb 19 22:45:04.610: INFO: (2) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 23.018445ms)
Feb 19 22:45:04.610: INFO: (2) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 23.503495ms)
Feb 19 22:45:04.610: INFO: (2) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 23.37663ms)
Feb 19 22:45:04.610: INFO: (2) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 24.035803ms)
Feb 19 22:45:04.611: INFO: (2) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 23.841636ms)
Feb 19 22:45:04.611: INFO: (2) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 23.830859ms)
Feb 19 22:45:04.611: INFO: (2) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 24.055623ms)
Feb 19 22:45:04.611: INFO: (2) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 24.465667ms)
Feb 19 22:45:04.611: INFO: (2) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 24.838672ms)
Feb 19 22:45:04.626: INFO: (3) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 13.401364ms)
Feb 19 22:45:04.630: INFO: (3) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 17.835335ms)
Feb 19 22:45:04.630: INFO: (3) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 17.641893ms)
Feb 19 22:45:04.633: INFO: (3) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 21.269216ms)
Feb 19 22:45:04.633: INFO: (3) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 21.100458ms)
Feb 19 22:45:04.633: INFO: (3) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 21.745587ms)
Feb 19 22:45:04.634: INFO: (3) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 21.76055ms)
Feb 19 22:45:04.634: INFO: (3) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 21.735995ms)
Feb 19 22:45:04.634: INFO: (3) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 21.713327ms)
Feb 19 22:45:04.634: INFO: (3) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 22.255574ms)
Feb 19 22:45:04.634: INFO: (3) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 22.663577ms)
Feb 19 22:45:04.635: INFO: (3) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 22.742436ms)
Feb 19 22:45:04.637: INFO: (3) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 25.538499ms)
Feb 19 22:45:04.637: INFO: (3) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 24.885473ms)
Feb 19 22:45:04.637: INFO: (3) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 25.408747ms)
Feb 19 22:45:04.642: INFO: (3) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 29.95691ms)
Feb 19 22:45:04.657: INFO: (4) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 14.16325ms)
Feb 19 22:45:04.658: INFO: (4) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 15.358692ms)
Feb 19 22:45:04.659: INFO: (4) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 16.198242ms)
Feb 19 22:45:04.663: INFO: (4) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 20.454768ms)
Feb 19 22:45:04.664: INFO: (4) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 21.416195ms)
Feb 19 22:45:04.664: INFO: (4) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 22.150529ms)
Feb 19 22:45:04.664: INFO: (4) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 21.953731ms)
Feb 19 22:45:04.664: INFO: (4) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 21.52245ms)
Feb 19 22:45:04.665: INFO: (4) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 22.871517ms)
Feb 19 22:45:04.665: INFO: (4) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 23.157024ms)
Feb 19 22:45:04.666: INFO: (4) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 22.761489ms)
Feb 19 22:45:04.666: INFO: (4) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 22.972705ms)
Feb 19 22:45:04.670: INFO: (4) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 24.666203ms)
Feb 19 22:45:04.670: INFO: (4) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 27.45598ms)
Feb 19 22:45:04.670: INFO: (4) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 27.466802ms)
Feb 19 22:45:04.671: INFO: (4) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 28.261563ms)
Feb 19 22:45:04.686: INFO: (5) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 14.911277ms)
Feb 19 22:45:04.686: INFO: (5) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 14.647326ms)
Feb 19 22:45:04.688: INFO: (5) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 16.356168ms)
Feb 19 22:45:04.689: INFO: (5) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 17.436497ms)
Feb 19 22:45:04.691: INFO: (5) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 19.094025ms)
Feb 19 22:45:04.691: INFO: (5) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 19.207261ms)
Feb 19 22:45:04.691: INFO: (5) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 19.723743ms)
Feb 19 22:45:04.691: INFO: (5) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 19.455791ms)
Feb 19 22:45:04.691: INFO: (5) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 19.517501ms)
Feb 19 22:45:04.694: INFO: (5) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 23.210441ms)
Feb 19 22:45:04.696: INFO: (5) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 24.603089ms)
Feb 19 22:45:04.696: INFO: (5) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 24.129276ms)
Feb 19 22:45:04.697: INFO: (5) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 24.975897ms)
Feb 19 22:45:04.697: INFO: (5) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 25.523894ms)
Feb 19 22:45:04.699: INFO: (5) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 27.240594ms)
Feb 19 22:45:04.699: INFO: (5) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 27.862152ms)
Feb 19 22:45:04.717: INFO: (6) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 17.028131ms)
Feb 19 22:45:04.717: INFO: (6) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 17.279688ms)
Feb 19 22:45:04.717: INFO: (6) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 17.36096ms)
Feb 19 22:45:04.717: INFO: (6) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 17.196207ms)
Feb 19 22:45:04.718: INFO: (6) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 19.018568ms)
Feb 19 22:45:04.719: INFO: (6) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 19.010118ms)
Feb 19 22:45:04.719: INFO: (6) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 19.2735ms)
Feb 19 22:45:04.719: INFO: (6) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 18.950221ms)
Feb 19 22:45:04.719: INFO: (6) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 19.226325ms)
Feb 19 22:45:04.721: INFO: (6) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 20.716027ms)
Feb 19 22:45:04.722: INFO: (6) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 22.26911ms)
Feb 19 22:45:04.722: INFO: (6) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 22.380358ms)
Feb 19 22:45:04.722: INFO: (6) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 22.526446ms)
Feb 19 22:45:04.722: INFO: (6) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 22.184396ms)
Feb 19 22:45:04.723: INFO: (6) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 23.030294ms)
Feb 19 22:45:04.724: INFO: (6) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 24.203132ms)
Feb 19 22:45:04.740: INFO: (7) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 15.489702ms)
Feb 19 22:45:04.740: INFO: (7) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 15.609512ms)
Feb 19 22:45:04.740: INFO: (7) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 15.884788ms)
Feb 19 22:45:04.749: INFO: (7) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 23.943882ms)
Feb 19 22:45:04.749: INFO: (7) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 24.115667ms)
Feb 19 22:45:04.751: INFO: (7) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 26.032464ms)
Feb 19 22:45:04.751: INFO: (7) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 26.542847ms)
Feb 19 22:45:04.752: INFO: (7) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 26.448581ms)
Feb 19 22:45:04.760: INFO: (7) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 34.990472ms)
Feb 19 22:45:04.760: INFO: (7) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 35.383056ms)
Feb 19 22:45:04.761: INFO: (7) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 35.076014ms)
Feb 19 22:45:04.761: INFO: (7) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 35.695802ms)
Feb 19 22:45:04.761: INFO: (7) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 35.958879ms)
Feb 19 22:45:04.761: INFO: (7) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 36.130491ms)
Feb 19 22:45:04.761: INFO: (7) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 35.927871ms)
Feb 19 22:45:04.762: INFO: (7) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 36.376649ms)
Feb 19 22:45:04.773: INFO: (8) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 11.15979ms)
Feb 19 22:45:04.778: INFO: (8) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 15.759079ms)
Feb 19 22:45:04.778: INFO: (8) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 15.872818ms)
Feb 19 22:45:04.781: INFO: (8) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 19.060314ms)
Feb 19 22:45:04.789: INFO: (8) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 26.937362ms)
Feb 19 22:45:04.790: INFO: (8) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 27.786877ms)
Feb 19 22:45:04.790: INFO: (8) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 27.894466ms)
Feb 19 22:45:04.790: INFO: (8) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 27.515769ms)
Feb 19 22:45:04.790: INFO: (8) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 28.062118ms)
Feb 19 22:45:04.790: INFO: (8) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 28.007933ms)
Feb 19 22:45:04.792: INFO: (8) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 30.381634ms)
Feb 19 22:45:04.799: INFO: (8) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 36.991532ms)
Feb 19 22:45:04.799: INFO: (8) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 37.139046ms)
Feb 19 22:45:04.799: INFO: (8) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 37.240445ms)
Feb 19 22:45:04.800: INFO: (8) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 37.68167ms)
Feb 19 22:45:04.800: INFO: (8) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 37.533489ms)
Feb 19 22:45:04.814: INFO: (9) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 13.680166ms)
Feb 19 22:45:04.814: INFO: (9) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 13.800612ms)
Feb 19 22:45:04.814: INFO: (9) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 14.317188ms)
Feb 19 22:45:04.814: INFO: (9) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 14.356543ms)
Feb 19 22:45:04.815: INFO: (9) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 14.420221ms)
Feb 19 22:45:04.816: INFO: (9) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 15.424435ms)
Feb 19 22:45:04.817: INFO: (9) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 16.305408ms)
Feb 19 22:45:04.817: INFO: (9) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 17.099361ms)
Feb 19 22:45:04.818: INFO: (9) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 17.196434ms)
Feb 19 22:45:04.819: INFO: (9) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 18.027262ms)
Feb 19 22:45:04.819: INFO: (9) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 18.01851ms)
Feb 19 22:45:04.820: INFO: (9) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 19.078344ms)
Feb 19 22:45:04.823: INFO: (9) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 22.136034ms)
Feb 19 22:45:04.823: INFO: (9) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 22.595288ms)
Feb 19 22:45:04.823: INFO: (9) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 23.169896ms)
Feb 19 22:45:04.824: INFO: (9) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 23.238723ms)
Feb 19 22:45:04.834: INFO: (10) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 10.822991ms)
Feb 19 22:45:04.946: INFO: (10) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 121.8791ms)
Feb 19 22:45:04.954: INFO: (10) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 130.132391ms)
Feb 19 22:45:04.954: INFO: (10) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 130.3546ms)
Feb 19 22:45:04.955: INFO: (10) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 130.704674ms)
Feb 19 22:45:04.955: INFO: (10) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 130.929857ms)
Feb 19 22:45:04.955: INFO: (10) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 131.100466ms)
Feb 19 22:45:04.955: INFO: (10) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 130.635536ms)
Feb 19 22:45:04.960: INFO: (10) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 136.011019ms)
Feb 19 22:45:04.962: INFO: (10) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 137.659535ms)
Feb 19 22:45:04.962: INFO: (10) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 138.159529ms)
Feb 19 22:45:04.963: INFO: (10) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 138.577093ms)
Feb 19 22:45:04.970: INFO: (10) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 145.812244ms)
Feb 19 22:45:04.976: INFO: (10) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 151.988608ms)
Feb 19 22:45:04.976: INFO: (10) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 152.476702ms)
Feb 19 22:45:04.977: INFO: (10) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 152.712768ms)
Feb 19 22:45:05.020: INFO: (11) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 43.063101ms)
Feb 19 22:45:05.020: INFO: (11) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 42.944628ms)
Feb 19 22:45:05.020: INFO: (11) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 43.038524ms)
Feb 19 22:45:05.029: INFO: (11) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 51.65846ms)
Feb 19 22:45:05.029: INFO: (11) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 51.763573ms)
Feb 19 22:45:05.029: INFO: (11) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 52.060771ms)
Feb 19 22:45:05.030: INFO: (11) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 52.439204ms)
Feb 19 22:45:05.031: INFO: (11) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 53.831091ms)
Feb 19 22:45:05.035: INFO: (11) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 57.264804ms)
Feb 19 22:45:05.035: INFO: (11) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 57.800498ms)
Feb 19 22:45:05.035: INFO: (11) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 58.214066ms)
Feb 19 22:45:05.035: INFO: (11) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 57.758279ms)
Feb 19 22:45:05.035: INFO: (11) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 57.819841ms)
Feb 19 22:45:05.036: INFO: (11) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 58.464297ms)
Feb 19 22:45:05.039: INFO: (11) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 61.153986ms)
Feb 19 22:45:05.039: INFO: (11) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 61.575215ms)
Feb 19 22:45:05.078: INFO: (12) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 38.661584ms)
Feb 19 22:45:05.079: INFO: (12) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 39.762394ms)
Feb 19 22:45:05.079: INFO: (12) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 40.083267ms)
Feb 19 22:45:05.079: INFO: (12) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 40.264722ms)
Feb 19 22:45:05.087: INFO: (12) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 47.667761ms)
Feb 19 22:45:05.087: INFO: (12) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 48.032642ms)
Feb 19 22:45:05.088: INFO: (12) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 48.034484ms)
Feb 19 22:45:05.089: INFO: (12) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 49.525261ms)
Feb 19 22:45:05.089: INFO: (12) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 49.137075ms)
Feb 19 22:45:05.091: INFO: (12) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 51.065569ms)
Feb 19 22:45:05.091: INFO: (12) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 51.66158ms)
Feb 19 22:45:05.095: INFO: (12) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 54.677703ms)
Feb 19 22:45:05.095: INFO: (12) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 55.025505ms)
Feb 19 22:45:05.095: INFO: (12) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 55.08186ms)
Feb 19 22:45:05.095: INFO: (12) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 55.70126ms)
Feb 19 22:45:05.095: INFO: (12) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 56.179598ms)
Feb 19 22:45:05.138: INFO: (13) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 42.011137ms)
Feb 19 22:45:05.138: INFO: (13) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 42.871366ms)
Feb 19 22:45:05.144: INFO: (13) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 47.656707ms)
Feb 19 22:45:05.149: INFO: (13) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 53.176876ms)
Feb 19 22:45:05.152: INFO: (13) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 56.471509ms)
Feb 19 22:45:05.153: INFO: (13) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 57.075736ms)
Feb 19 22:45:05.154: INFO: (13) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 58.434813ms)
Feb 19 22:45:05.154: INFO: (13) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 57.612472ms)
Feb 19 22:45:05.154: INFO: (13) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 58.686802ms)
Feb 19 22:45:05.154: INFO: (13) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 58.512597ms)
Feb 19 22:45:05.154: INFO: (13) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 58.700479ms)
Feb 19 22:45:05.157: INFO: (13) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 60.493226ms)
Feb 19 22:45:05.157: INFO: (13) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 60.570753ms)
Feb 19 22:45:05.157: INFO: (13) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 60.812287ms)
Feb 19 22:45:05.157: INFO: (13) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 61.725193ms)
Feb 19 22:45:05.157: INFO: (13) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 61.418685ms)
Feb 19 22:45:05.211: INFO: (14) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 53.316685ms)
Feb 19 22:45:05.214: INFO: (14) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 54.57395ms)
Feb 19 22:45:05.223: INFO: (14) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 63.694079ms)
Feb 19 22:45:05.224: INFO: (14) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 66.395626ms)
Feb 19 22:45:05.225: INFO: (14) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 67.728218ms)
Feb 19 22:45:05.226: INFO: (14) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 68.062729ms)
Feb 19 22:45:05.226: INFO: (14) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 68.428417ms)
Feb 19 22:45:05.226: INFO: (14) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 66.556163ms)
Feb 19 22:45:05.226: INFO: (14) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 68.933328ms)
Feb 19 22:45:05.226: INFO: (14) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 68.702163ms)
Feb 19 22:45:05.227: INFO: (14) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 66.81366ms)
Feb 19 22:45:05.231: INFO: (14) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 71.652386ms)
Feb 19 22:45:05.232: INFO: (14) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 71.941866ms)
Feb 19 22:45:05.232: INFO: (14) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 72.158226ms)
Feb 19 22:45:05.233: INFO: (14) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 75.523486ms)
Feb 19 22:45:05.236: INFO: (14) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 76.314484ms)
Feb 19 22:45:05.291: INFO: (15) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 53.785312ms)
Feb 19 22:45:05.291: INFO: (15) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 54.192444ms)
Feb 19 22:45:05.294: INFO: (15) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 57.424527ms)
Feb 19 22:45:05.295: INFO: (15) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 57.780787ms)
Feb 19 22:45:05.295: INFO: (15) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 57.753876ms)
Feb 19 22:45:05.295: INFO: (15) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 58.483691ms)
Feb 19 22:45:05.295: INFO: (15) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 58.819077ms)
Feb 19 22:45:05.301: INFO: (15) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 63.725039ms)
Feb 19 22:45:05.301: INFO: (15) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 63.538184ms)
Feb 19 22:45:05.301: INFO: (15) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 64.107476ms)
Feb 19 22:45:05.301: INFO: (15) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 64.092202ms)
Feb 19 22:45:05.301: INFO: (15) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 64.23758ms)
Feb 19 22:45:05.302: INFO: (15) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 65.339024ms)
Feb 19 22:45:05.303: INFO: (15) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 66.697006ms)
Feb 19 22:45:05.303: INFO: (15) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 66.857803ms)
Feb 19 22:45:05.307: INFO: (15) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 69.632951ms)
Feb 19 22:45:05.328: INFO: (16) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 20.802196ms)
Feb 19 22:45:05.344: INFO: (16) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 36.405158ms)
Feb 19 22:45:05.347: INFO: (16) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 39.808206ms)
Feb 19 22:45:05.347: INFO: (16) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 40.199813ms)
Feb 19 22:45:05.348: INFO: (16) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 40.651158ms)
Feb 19 22:45:05.348: INFO: (16) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 39.843004ms)
Feb 19 22:45:05.348: INFO: (16) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 40.628765ms)
Feb 19 22:45:05.350: INFO: (16) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 42.030261ms)
Feb 19 22:45:05.360: INFO: (16) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 52.91557ms)
Feb 19 22:45:05.360: INFO: (16) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 52.784103ms)
Feb 19 22:45:05.360: INFO: (16) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 53.127494ms)
Feb 19 22:45:05.361: INFO: (16) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 53.275577ms)
Feb 19 22:45:05.361: INFO: (16) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 53.964796ms)
Feb 19 22:45:05.361: INFO: (16) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 53.615882ms)
Feb 19 22:45:05.363: INFO: (16) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 55.82871ms)
Feb 19 22:45:05.364: INFO: (16) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 56.251111ms)
Feb 19 22:45:05.383: INFO: (17) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 18.416576ms)
Feb 19 22:45:05.384: INFO: (17) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 19.343801ms)
Feb 19 22:45:05.384: INFO: (17) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 18.930042ms)
Feb 19 22:45:05.384: INFO: (17) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 19.769425ms)
Feb 19 22:45:05.413: INFO: (17) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 48.741683ms)
Feb 19 22:45:05.414: INFO: (17) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 48.834687ms)
Feb 19 22:45:05.415: INFO: (17) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 50.797632ms)
Feb 19 22:45:05.416: INFO: (17) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 51.719217ms)
Feb 19 22:45:05.417: INFO: (17) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 51.570774ms)
Feb 19 22:45:05.417: INFO: (17) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 52.507784ms)
Feb 19 22:45:05.420: INFO: (17) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 55.978621ms)
Feb 19 22:45:05.420: INFO: (17) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 56.127462ms)
Feb 19 22:45:05.421: INFO: (17) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 55.744205ms)
Feb 19 22:45:05.422: INFO: (17) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 58.219751ms)
Feb 19 22:45:05.422: INFO: (17) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 57.831261ms)
Feb 19 22:45:05.423: INFO: (17) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 57.939989ms)
Feb 19 22:45:05.443: INFO: (18) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 19.214196ms)
Feb 19 22:45:05.445: INFO: (18) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 21.78732ms)
Feb 19 22:45:05.449: INFO: (18) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 25.717742ms)
Feb 19 22:45:05.449: INFO: (18) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 25.374624ms)
Feb 19 22:45:05.450: INFO: (18) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 26.65584ms)
Feb 19 22:45:05.450: INFO: (18) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 26.300965ms)
Feb 19 22:45:05.450: INFO: (18) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 26.503393ms)
Feb 19 22:45:05.455: INFO: (18) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 31.182695ms)
Feb 19 22:45:05.455: INFO: (18) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 31.783631ms)
Feb 19 22:45:05.455: INFO: (18) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 32.099713ms)
Feb 19 22:45:05.455: INFO: (18) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 32.137732ms)
Feb 19 22:45:05.458: INFO: (18) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 35.0023ms)
Feb 19 22:45:05.458: INFO: (18) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 34.562932ms)
Feb 19 22:45:05.458: INFO: (18) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 34.8888ms)
Feb 19 22:45:05.458: INFO: (18) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 35.13597ms)
Feb 19 22:45:05.460: INFO: (18) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 36.136483ms)
Feb 19 22:45:05.501: INFO: (19) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 41.28483ms)
Feb 19 22:45:05.501: INFO: (19) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:460/proxy/: tls baz (200; 41.443874ms)
Feb 19 22:45:05.507: INFO: (19) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:462/proxy/: tls qux (200; 46.932187ms)
Feb 19 22:45:05.507: INFO: (19) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">test</... (200; 47.374885ms)
Feb 19 22:45:05.511: INFO: (19) /api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/https:proxy-service-9ntg4-8dtqd:443/proxy/tlsrewriteme... (200; 50.333736ms)
Feb 19 22:45:05.513: INFO: (19) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:1080/proxy/rewriteme">t... (200; 53.084445ms)
Feb 19 22:45:05.515: INFO: (19) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 54.404328ms)
Feb 19 22:45:05.515: INFO: (19) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd:160/proxy/: foo (200; 54.386073ms)
Feb 19 22:45:05.515: INFO: (19) /api/v1/namespaces/proxy-618/pods/http:proxy-service-9ntg4-8dtqd:162/proxy/: bar (200; 54.96879ms)
Feb 19 22:45:05.517: INFO: (19) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname2/proxy/: bar (200; 56.905946ms)
Feb 19 22:45:05.520: INFO: (19) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname1/proxy/: foo (200; 60.249902ms)
Feb 19 22:45:05.522: INFO: (19) /api/v1/namespaces/proxy-618/services/http:proxy-service-9ntg4:portname1/proxy/: foo (200; 61.799002ms)
Feb 19 22:45:05.524: INFO: (19) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname1/proxy/: tls baz (200; 64.004421ms)
Feb 19 22:45:05.524: INFO: (19) /api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/: <a href="/api/v1/namespaces/proxy-618/pods/proxy-service-9ntg4-8dtqd/proxy/rewriteme">test</a> (200; 63.954541ms)
Feb 19 22:45:05.524: INFO: (19) /api/v1/namespaces/proxy-618/services/proxy-service-9ntg4:portname2/proxy/: bar (200; 63.95147ms)
Feb 19 22:45:05.531: INFO: (19) /api/v1/namespaces/proxy-618/services/https:proxy-service-9ntg4:tlsportname2/proxy/: tls qux (200; 70.634815ms)
STEP: deleting ReplicationController proxy-service-9ntg4 in namespace proxy-618, will wait for the garbage collector to delete the pods
Feb 19 22:45:05.667: INFO: Deleting ReplicationController proxy-service-9ntg4 took: 16.014641ms
Feb 19 22:45:06.167: INFO: Terminating ReplicationController proxy-service-9ntg4 pods took: 500.203957ms
[AfterEach] version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:45:15.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-618" for this suite.
Feb 19 22:45:21.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:45:21.531: INFO: namespace proxy-618 deletion completed in 6.152313364s

• [SLOW TEST:23.243 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:45:21.537: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-fb4d4edd-e5f8-4e63-b82f-26059993c943
STEP: Creating a pod to test consume configMaps
Feb 19 22:45:21.612: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea" in namespace "projected-613" to be "success or failure"
Feb 19 22:45:21.619: INFO: Pod "pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.693093ms
Feb 19 22:45:23.622: INFO: Pod "pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010148134s
Feb 19 22:45:25.626: INFO: Pod "pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013929913s
STEP: Saw pod success
Feb 19 22:45:25.626: INFO: Pod "pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea" satisfied condition "success or failure"
Feb 19 22:45:25.628: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:45:25.648: INFO: Waiting for pod pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea to disappear
Feb 19 22:45:25.654: INFO: Pod pod-projected-configmaps-e067e5b2-5382-488f-816f-1aaf722101ea no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:45:25.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-613" for this suite.
Feb 19 22:45:31.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:45:32.299: INFO: namespace projected-613 deletion completed in 6.641601979s

• [SLOW TEST:10.762 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:45:32.305: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:45:32.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775" in namespace "projected-4481" to be "success or failure"
Feb 19 22:45:32.370: INFO: Pod "downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.700696ms
Feb 19 22:45:34.373: INFO: Pod "downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005742161s
STEP: Saw pod success
Feb 19 22:45:34.374: INFO: Pod "downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775" satisfied condition "success or failure"
Feb 19 22:45:34.377: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775 container client-container: <nil>
STEP: delete the pod
Feb 19 22:45:34.399: INFO: Waiting for pod downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775 to disappear
Feb 19 22:45:34.403: INFO: Pod downwardapi-volume-bac06f49-1583-4beb-b416-c75f4e3bb775 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:45:34.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4481" for this suite.
Feb 19 22:45:40.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:45:41.512: INFO: namespace projected-4481 deletion completed in 7.104903134s

• [SLOW TEST:9.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:45:41.522: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9118
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Feb 19 22:45:41.634: INFO: Found 0 stateful pods, waiting for 3
Feb 19 22:45:51.638: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:45:51.639: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:45:51.639: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 19 22:45:51.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-9118 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 22:45:51.887: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 22:45:51.887: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 22:45:51.887: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 19 22:46:01.941: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 19 22:46:12.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-9118 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:46:13.051: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 22:46:13.051: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 22:46:13.052: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

STEP: Rolling back to a previous revision
Feb 19 22:46:33.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-9118 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 19 22:46:33.650: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Feb 19 22:46:33.650: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 19 22:46:33.650: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 19 22:46:43.684: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 19 22:46:53.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 exec --namespace=statefulset-9118 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 19 22:46:54.242: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Feb 19 22:46:54.242: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 19 22:46:54.242: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 19 22:47:04.258: INFO: Waiting for StatefulSet statefulset-9118/ss2 to complete update
Feb 19 22:47:04.258: INFO: Waiting for Pod statefulset-9118/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 19 22:47:04.258: INFO: Waiting for Pod statefulset-9118/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 19 22:47:04.258: INFO: Waiting for Pod statefulset-9118/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 19 22:47:14.264: INFO: Waiting for StatefulSet statefulset-9118/ss2 to complete update
Feb 19 22:47:14.265: INFO: Waiting for Pod statefulset-9118/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Feb 19 22:47:24.264: INFO: Waiting for StatefulSet statefulset-9118/ss2 to complete update
Feb 19 22:47:24.265: INFO: Waiting for Pod statefulset-9118/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Feb 19 22:47:34.264: INFO: Deleting all statefulset in ns statefulset-9118
Feb 19 22:47:34.266: INFO: Scaling statefulset ss2 to 0
Feb 19 22:47:44.286: INFO: Waiting for statefulset status.replicas updated to 0
Feb 19 22:47:44.289: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:47:44.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9118" for this suite.
Feb 19 22:47:50.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:47:50.479: INFO: namespace statefulset-9118 deletion completed in 6.170255096s

• [SLOW TEST:128.957 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:47:50.481: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:47:50.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 version'
Feb 19 22:47:50.690: INFO: stderr: ""
Feb 19 22:47:50.690: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.9\", GitCommit:\"2e808b7cb054ee242b68e62455323aa783991f03\", GitTreeState:\"clean\", BuildDate:\"2020-01-18T23:33:14Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.9-gke.8\", GitCommit:\"a9973cbb2722793e2ea08d20880633ca61d3e669\", GitTreeState:\"clean\", BuildDate:\"2020-02-07T00:50:57Z\", GoVersion:\"go1.12.12b4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:47:50.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5294" for this suite.
Feb 19 22:48:00.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:48:00.839: INFO: namespace kubectl-5294 deletion completed in 10.145897604s

• [SLOW TEST:10.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:48:00.842: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5baf803a-d362-4bde-92da-d29be8b2aa4d
STEP: Creating a pod to test consume configMaps
Feb 19 22:48:00.894: INFO: Waiting up to 5m0s for pod "pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493" in namespace "configmap-8233" to be "success or failure"
Feb 19 22:48:00.898: INFO: Pod "pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956392ms
Feb 19 22:48:02.902: INFO: Pod "pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007079612s
Feb 19 22:48:04.905: INFO: Pod "pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010331502s
STEP: Saw pod success
Feb 19 22:48:04.905: INFO: Pod "pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493" satisfied condition "success or failure"
Feb 19 22:48:04.907: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:48:04.926: INFO: Waiting for pod pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493 to disappear
Feb 19 22:48:04.931: INFO: Pod pod-configmaps-440762d4-1e9e-4162-8140-8c4d2993b493 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:48:04.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8233" for this suite.
Feb 19 22:48:13.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:48:13.136: INFO: namespace configmap-8233 deletion completed in 8.200719559s

• [SLOW TEST:12.294 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:48:13.138: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 19 22:48:19.284: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 19 22:48:19.286: INFO: Pod pod-with-poststart-http-hook still exists
Feb 19 22:48:21.287: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 19 22:48:21.290: INFO: Pod pod-with-poststart-http-hook still exists
Feb 19 22:48:23.287: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 19 22:48:23.290: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:48:23.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5202" for this suite.
Feb 19 22:48:45.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:48:45.464: INFO: namespace container-lifecycle-hook-5202 deletion completed in 22.171552566s

• [SLOW TEST:32.327 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:48:45.468: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Feb 19 22:48:45.719: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f" in namespace "projected-5097" to be "success or failure"
Feb 19 22:48:45.732: INFO: Pod "downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.804067ms
Feb 19 22:48:47.735: INFO: Pod "downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015732272s
Feb 19 22:48:49.738: INFO: Pod "downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018718291s
STEP: Saw pod success
Feb 19 22:48:49.738: INFO: Pod "downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f" satisfied condition "success or failure"
Feb 19 22:48:49.740: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f container client-container: <nil>
STEP: delete the pod
Feb 19 22:48:49.777: INFO: Waiting for pod downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f to disappear
Feb 19 22:48:49.780: INFO: Pod downwardapi-volume-efd480df-92ff-4d72-ae77-5c466a1d498f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:48:49.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5097" for this suite.
Feb 19 22:48:59.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:48:59.961: INFO: namespace projected-5097 deletion completed in 10.177652204s

• [SLOW TEST:14.493 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:48:59.965: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 19 22:49:00.014: INFO: Waiting up to 5m0s for pod "pod-016e1efe-b959-4068-bb51-33f5145f4296" in namespace "emptydir-4507" to be "success or failure"
Feb 19 22:49:00.019: INFO: Pod "pod-016e1efe-b959-4068-bb51-33f5145f4296": Phase="Pending", Reason="", readiness=false. Elapsed: 5.56488ms
Feb 19 22:49:02.023: INFO: Pod "pod-016e1efe-b959-4068-bb51-33f5145f4296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008803432s
STEP: Saw pod success
Feb 19 22:49:02.023: INFO: Pod "pod-016e1efe-b959-4068-bb51-33f5145f4296" satisfied condition "success or failure"
Feb 19 22:49:02.025: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-016e1efe-b959-4068-bb51-33f5145f4296 container test-container: <nil>
STEP: delete the pod
Feb 19 22:49:02.066: INFO: Waiting for pod pod-016e1efe-b959-4068-bb51-33f5145f4296 to disappear
Feb 19 22:49:02.083: INFO: Pod pod-016e1efe-b959-4068-bb51-33f5145f4296 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:49:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4507" for this suite.
Feb 19 22:49:08.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:49:08.270: INFO: namespace emptydir-4507 deletion completed in 6.183721102s

• [SLOW TEST:8.305 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:49:08.275: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:49:08.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-3839'
Feb 19 22:49:08.412: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 19 22:49:08.412: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Feb 19 22:49:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3839'
Feb 19 22:49:12.969: INFO: stderr: ""
Feb 19 22:49:12.969: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:49:12.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3839" for this suite.
Feb 19 22:49:19.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:49:19.172: INFO: namespace kubectl-3839 deletion completed in 6.170701993s

• [SLOW TEST:10.898 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:49:19.175: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-d1c160d2-9a4f-4c23-aec4-9b8fffd0d002
STEP: Creating a pod to test consume secrets
Feb 19 22:49:19.224: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce" in namespace "projected-7753" to be "success or failure"
Feb 19 22:49:19.228: INFO: Pod "pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce": Phase="Pending", Reason="", readiness=false. Elapsed: 3.452158ms
Feb 19 22:49:21.231: INFO: Pod "pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006674091s
Feb 19 22:49:23.234: INFO: Pod "pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010019195s
STEP: Saw pod success
Feb 19 22:49:23.235: INFO: Pod "pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce" satisfied condition "success or failure"
Feb 19 22:49:23.237: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 19 22:49:23.256: INFO: Waiting for pod pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce to disappear
Feb 19 22:49:23.260: INFO: Pod pod-projected-secrets-90de8e2b-7113-482a-8493-a0ee036116ce no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:49:23.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7753" for this suite.
Feb 19 22:49:29.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:49:29.439: INFO: namespace projected-7753 deletion completed in 6.17600586s

• [SLOW TEST:10.264 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:49:29.443: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Feb 19 22:49:29.484: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 19 22:49:29.510: INFO: Waiting for terminating namespaces to be deleted...
Feb 19 22:49:29.514: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-nfnp before test
Feb 19 22:49:29.528: INFO: prometheus-to-sd-8dvjw from kube-system started at 2020-02-19 14:29:48 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.528: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:49:29.528: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 22:49:29.528: INFO: metrics-server-v0.3.3-6d96fcc55-kwrms from kube-system started at 2020-02-19 14:29:53 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.529: INFO: 	Container metrics-server ready: true, restart count 0
Feb 19 22:49:29.529: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 19 22:49:29.529: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-nvt2l from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.529: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 19 22:49:29.529: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 22:49:29.529: INFO: l7-default-backend-84c9fcfbb-7vdk9 from kube-system started at 2020-02-19 14:29:47 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.529: INFO: 	Container default-http-backend ready: true, restart count 0
Feb 19 22:49:29.529: INFO: kube-dns-autoscaler-6b7f784798-8zlzg from kube-system started at 2020-02-19 14:29:48 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.529: INFO: 	Container autoscaler ready: true, restart count 0
Feb 19 22:49:29.529: INFO: kube-dns-5dbbd9cc58-g9z7k from kube-system started at 2020-02-19 14:29:58 +0000 UTC (4 container statuses recorded)
Feb 19 22:49:29.530: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 19 22:49:29.530: INFO: 	Container kubedns ready: true, restart count 0
Feb 19 22:49:29.530: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:49:29.530: INFO: 	Container sidecar ready: true, restart count 0
Feb 19 22:49:29.530: INFO: fluentd-gcp-v3.1.1-kgrjm from kube-system started at 2020-02-19 14:30:01 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.530: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 22:49:29.530: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 22:49:29.530: INFO: fluentd-gcp-scaler-dd489f778-qvlt4 from kube-system started at 2020-02-19 14:29:49 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.530: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Feb 19 22:49:29.530: INFO: kube-proxy-gke-c115-default-pool-249bf33f-nfnp from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.530: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 22:49:29.530: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-qclh before test
Feb 19 22:49:29.581: INFO: kube-proxy-gke-c115-default-pool-249bf33f-qclh from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.582: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 22:49:29.582: INFO: kube-dns-5dbbd9cc58-wksp8 from kube-system started at 2020-02-19 14:29:47 +0000 UTC (4 container statuses recorded)
Feb 19 22:49:29.582: INFO: 	Container dnsmasq ready: true, restart count 0
Feb 19 22:49:29.582: INFO: 	Container kubedns ready: true, restart count 0
Feb 19 22:49:29.582: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:49:29.582: INFO: 	Container sidecar ready: true, restart count 0
Feb 19 22:49:29.582: INFO: fluentd-gcp-v3.1.1-sgx4k from kube-system started at 2020-02-19 14:30:06 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.582: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 22:49:29.582: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 22:49:29.582: INFO: stackdriver-metadata-agent-cluster-level-5d4656b6d8-ldr2s from kube-system started at 2020-02-19 14:30:22 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.582: INFO: 	Container metadata-agent ready: true, restart count 0
Feb 19 22:49:29.583: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Feb 19 22:49:29.583: INFO: heapster-gke-76c9bd686-5tq5s from kube-system started at 2020-02-19 14:30:25 +0000 UTC (3 container statuses recorded)
Feb 19 22:49:29.583: INFO: 	Container heapster ready: true, restart count 0
Feb 19 22:49:29.583: INFO: 	Container heapster-nanny ready: true, restart count 0
Feb 19 22:49:29.583: INFO: 	Container prom-to-sd ready: true, restart count 0
Feb 19 22:49:29.583: INFO: prometheus-to-sd-hk44q from kube-system started at 2020-02-19 14:29:48 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.583: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:49:29.583: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 22:49:29.584: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-sn5gm from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.584: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 19 22:49:29.584: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 22:49:29.584: INFO: 
Logging pods the kubelet thinks is on node gke-c115-default-pool-249bf33f-vhvp before test
Feb 19 22:49:29.599: INFO: kube-proxy-gke-c115-default-pool-249bf33f-vhvp from kube-system started at 2020-02-19 14:29:45 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.599: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 19 22:49:29.599: INFO: sonobuoy from sonobuoy started at 2020-02-19 21:19:23 +0000 UTC (1 container statuses recorded)
Feb 19 22:49:29.599: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 19 22:49:29.599: INFO: sonobuoy-e2e-job-bec128f92cf64f0f from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.600: INFO: 	Container e2e ready: true, restart count 0
Feb 19 22:49:29.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 19 22:49:29.600: INFO: sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-9xkwf from sonobuoy started at 2020-02-19 21:19:24 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.600: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 19 22:49:29.600: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 19 22:49:29.600: INFO: prometheus-to-sd-b68nv from kube-system started at 2020-02-19 14:29:47 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.600: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Feb 19 22:49:29.600: INFO: 	Container prometheus-to-sd-new-model ready: true, restart count 0
Feb 19 22:49:29.600: INFO: event-exporter-v0.3.0-74bf544f8b-ckv4s from kube-system started at 2020-02-19 14:29:45 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.600: INFO: 	Container event-exporter ready: true, restart count 0
Feb 19 22:49:29.600: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Feb 19 22:49:29.600: INFO: fluentd-gcp-v3.1.1-d8brx from kube-system started at 2020-02-19 14:30:06 +0000 UTC (2 container statuses recorded)
Feb 19 22:49:29.600: INFO: 	Container fluentd-gcp ready: true, restart count 0
Feb 19 22:49:29.600: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node gke-c115-default-pool-249bf33f-nfnp
STEP: verifying the node has the label node gke-c115-default-pool-249bf33f-qclh
STEP: verifying the node has the label node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod event-exporter-v0.3.0-74bf544f8b-ckv4s requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod fluentd-gcp-scaler-dd489f778-qvlt4 requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod fluentd-gcp-v3.1.1-d8brx requesting resource cpu=100m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod fluentd-gcp-v3.1.1-kgrjm requesting resource cpu=100m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod fluentd-gcp-v3.1.1-sgx4k requesting resource cpu=100m on Node gke-c115-default-pool-249bf33f-qclh
Feb 19 22:49:29.667: INFO: Pod heapster-gke-76c9bd686-5tq5s requesting resource cpu=63m on Node gke-c115-default-pool-249bf33f-qclh
Feb 19 22:49:29.667: INFO: Pod kube-dns-5dbbd9cc58-g9z7k requesting resource cpu=260m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod kube-dns-5dbbd9cc58-wksp8 requesting resource cpu=260m on Node gke-c115-default-pool-249bf33f-qclh
Feb 19 22:49:29.667: INFO: Pod kube-dns-autoscaler-6b7f784798-8zlzg requesting resource cpu=20m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod kube-proxy-gke-c115-default-pool-249bf33f-nfnp requesting resource cpu=100m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod kube-proxy-gke-c115-default-pool-249bf33f-qclh requesting resource cpu=100m on Node gke-c115-default-pool-249bf33f-qclh
Feb 19 22:49:29.667: INFO: Pod kube-proxy-gke-c115-default-pool-249bf33f-vhvp requesting resource cpu=100m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod l7-default-backend-84c9fcfbb-7vdk9 requesting resource cpu=10m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod metrics-server-v0.3.3-6d96fcc55-kwrms requesting resource cpu=48m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod prometheus-to-sd-8dvjw requesting resource cpu=1m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod prometheus-to-sd-b68nv requesting resource cpu=1m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod prometheus-to-sd-hk44q requesting resource cpu=1m on Node gke-c115-default-pool-249bf33f-qclh
Feb 19 22:49:29.667: INFO: Pod stackdriver-metadata-agent-cluster-level-5d4656b6d8-ldr2s requesting resource cpu=93m on Node gke-c115-default-pool-249bf33f-qclh
Feb 19 22:49:29.667: INFO: Pod sonobuoy requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod sonobuoy-e2e-job-bec128f92cf64f0f requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-9xkwf requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-vhvp
Feb 19 22:49:29.667: INFO: Pod sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-nvt2l requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-nfnp
Feb 19 22:49:29.667: INFO: Pod sonobuoy-systemd-logs-daemon-set-33172e0d368e456c-sn5gm requesting resource cpu=0m on Node gke-c115-default-pool-249bf33f-qclh
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4.15f4ef612aafe346], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2767/filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4 to gke-c115-default-pool-249bf33f-qclh]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4.15f4ef6199da28b4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4.15f4ef619d60e4f9], Reason = [Created], Message = [Created container filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4.15f4ef61a4aa2af7], Reason = [Started], Message = [Started container filler-pod-006ccb37-9a33-46d0-b396-38a6c2831de4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae.15f4ef612f11a10c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2767/filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae to gke-c115-default-pool-249bf33f-vhvp]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae.15f4ef619cac68aa], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae.15f4ef61a187879d], Reason = [Created], Message = [Created container filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae.15f4ef61aa15419c], Reason = [Started], Message = [Started container filler-pod-97841dfb-2361-4f80-a6b5-400c5fcfe1ae]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49.15f4ef6129108322], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2767/filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49 to gke-c115-default-pool-249bf33f-nfnp]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49.15f4ef61972726ae], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49.15f4ef619b48ae79], Reason = [Created], Message = [Created container filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49.15f4ef61a41cf68b], Reason = [Started], Message = [Started container filler-pod-f272cb05-4115-4765-a236-a822d3a2ff49]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f4ef62275d3b6f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node gke-c115-default-pool-249bf33f-qclh
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node gke-c115-default-pool-249bf33f-vhvp
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node gke-c115-default-pool-249bf33f-nfnp
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:49:34.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2767" for this suite.
Feb 19 22:49:42.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:49:43.206: INFO: namespace sched-pred-2767 deletion completed in 8.297649514s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:13.764 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:49:43.211: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:50:03.335: INFO: Container started at 2020-02-19 22:49:44 +0000 UTC, pod became ready at 2020-02-19 22:50:02 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:50:03.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-131" for this suite.
Feb 19 22:50:25.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:50:25.592: INFO: namespace container-probe-131 deletion completed in 22.252824124s

• [SLOW TEST:42.381 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:50:25.596: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:50:25.767: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 19 22:50:25.803: INFO: Number of nodes with available pods: 0
Feb 19 22:50:25.803: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 19 22:50:25.866: INFO: Number of nodes with available pods: 0
Feb 19 22:50:25.866: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:26.869: INFO: Number of nodes with available pods: 0
Feb 19 22:50:26.869: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:27.874: INFO: Number of nodes with available pods: 1
Feb 19 22:50:27.874: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 19 22:50:27.956: INFO: Number of nodes with available pods: 1
Feb 19 22:50:27.957: INFO: Number of running nodes: 0, number of available pods: 1
Feb 19 22:50:28.960: INFO: Number of nodes with available pods: 0
Feb 19 22:50:28.960: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 19 22:50:28.978: INFO: Number of nodes with available pods: 0
Feb 19 22:50:28.978: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:29.986: INFO: Number of nodes with available pods: 0
Feb 19 22:50:29.986: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:30.981: INFO: Number of nodes with available pods: 0
Feb 19 22:50:30.981: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:31.985: INFO: Number of nodes with available pods: 0
Feb 19 22:50:31.985: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:33.078: INFO: Number of nodes with available pods: 0
Feb 19 22:50:33.078: INFO: Node gke-c115-default-pool-249bf33f-nfnp is running more than one daemon pod
Feb 19 22:50:33.982: INFO: Number of nodes with available pods: 1
Feb 19 22:50:33.982: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-856, will wait for the garbage collector to delete the pods
Feb 19 22:50:34.052: INFO: Deleting DaemonSet.extensions daemon-set took: 7.360032ms
Feb 19 22:50:34.552: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.189598ms
Feb 19 22:50:37.455: INFO: Number of nodes with available pods: 0
Feb 19 22:50:37.455: INFO: Number of running nodes: 0, number of available pods: 0
Feb 19 22:50:37.458: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-856/daemonsets","resourceVersion":"124367"},"items":null}

Feb 19 22:50:37.461: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-856/pods","resourceVersion":"124367"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:50:37.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-856" for this suite.
Feb 19 22:50:45.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:50:45.641: INFO: namespace daemonsets-856 deletion completed in 8.152637506s

• [SLOW TEST:20.046 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:50:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Feb 19 22:50:45.686: INFO: Waiting up to 5m0s for pod "client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95" in namespace "containers-9705" to be "success or failure"
Feb 19 22:50:45.698: INFO: Pod "client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95": Phase="Pending", Reason="", readiness=false. Elapsed: 11.821716ms
Feb 19 22:50:47.702: INFO: Pod "client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015617037s
STEP: Saw pod success
Feb 19 22:50:47.702: INFO: Pod "client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95" satisfied condition "success or failure"
Feb 19 22:50:47.704: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95 container test-container: <nil>
STEP: delete the pod
Feb 19 22:50:47.727: INFO: Waiting for pod client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95 to disappear
Feb 19 22:50:47.732: INFO: Pod client-containers-59dd0f12-47d2-447f-a101-8a4f4b7bfd95 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:50:47.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9705" for this suite.
Feb 19 22:50:53.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:50:53.902: INFO: namespace containers-9705 deletion completed in 6.165011376s

• [SLOW TEST:8.259 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:50:53.905: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-eec7f71f-96ee-4ccb-b7a4-cb3609b4bb67
STEP: Creating a pod to test consume configMaps
Feb 19 22:50:54.013: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721" in namespace "projected-2894" to be "success or failure"
Feb 19 22:50:54.017: INFO: Pod "pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304812ms
Feb 19 22:50:56.131: INFO: Pod "pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.117892867s
STEP: Saw pod success
Feb 19 22:50:56.131: INFO: Pod "pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721" satisfied condition "success or failure"
Feb 19 22:50:56.254: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 19 22:50:56.948: INFO: Waiting for pod pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721 to disappear
Feb 19 22:50:57.029: INFO: Pod pod-projected-configmaps-4f3ee75f-12a1-4db3-b283-299cac469721 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:50:57.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2894" for this suite.
Feb 19 22:51:03.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:51:04.110: INFO: namespace projected-2894 deletion completed in 6.773841604s

• [SLOW TEST:10.205 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:51:04.112: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 19 22:51:04.221: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9391,SelfLink:/api/v1/namespaces/watch-9391/configmaps/e2e-watch-test-label-changed,UID:b8e84eb5-ebf8-405a-8a70-41b0a4e0728e,ResourceVersion:124523,Generation:0,CreationTimestamp:2020-02-19 22:51:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 19 22:51:04.221: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9391,SelfLink:/api/v1/namespaces/watch-9391/configmaps/e2e-watch-test-label-changed,UID:b8e84eb5-ebf8-405a-8a70-41b0a4e0728e,ResourceVersion:124524,Generation:0,CreationTimestamp:2020-02-19 22:51:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 19 22:51:04.221: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9391,SelfLink:/api/v1/namespaces/watch-9391/configmaps/e2e-watch-test-label-changed,UID:b8e84eb5-ebf8-405a-8a70-41b0a4e0728e,ResourceVersion:124525,Generation:0,CreationTimestamp:2020-02-19 22:51:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 19 22:51:14.263: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9391,SelfLink:/api/v1/namespaces/watch-9391/configmaps/e2e-watch-test-label-changed,UID:b8e84eb5-ebf8-405a-8a70-41b0a4e0728e,ResourceVersion:124561,Generation:0,CreationTimestamp:2020-02-19 22:51:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 19 22:51:14.263: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9391,SelfLink:/api/v1/namespaces/watch-9391/configmaps/e2e-watch-test-label-changed,UID:b8e84eb5-ebf8-405a-8a70-41b0a4e0728e,ResourceVersion:124562,Generation:0,CreationTimestamp:2020-02-19 22:51:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 19 22:51:14.263: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9391,SelfLink:/api/v1/namespaces/watch-9391/configmaps/e2e-watch-test-label-changed,UID:b8e84eb5-ebf8-405a-8a70-41b0a4e0728e,ResourceVersion:124563,Generation:0,CreationTimestamp:2020-02-19 22:51:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:51:14.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9391" for this suite.
Feb 19 22:51:20.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:51:20.455: INFO: namespace watch-9391 deletion completed in 6.185672258s

• [SLOW TEST:16.343 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:51:20.455: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 19 22:51:25.029: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5293 pod-service-account-e28cb8ae-d878-413a-b6c3-75e1e15a630b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 19 22:51:25.344: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5293 pod-service-account-e28cb8ae-d878-413a-b6c3-75e1e15a630b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 19 22:51:25.621: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5293 pod-service-account-e28cb8ae-d878-413a-b6c3-75e1e15a630b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:51:26.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5293" for this suite.
Feb 19 22:51:32.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:51:32.704: INFO: namespace svcaccounts-5293 deletion completed in 6.547296056s

• [SLOW TEST:12.249 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:51:32.708: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0219 22:51:33.836366      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 19 22:51:33.836: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:51:33.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1603" for this suite.
Feb 19 22:51:39.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:51:41.072: INFO: namespace gc-1603 deletion completed in 7.231528262s

• [SLOW TEST:8.365 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:51:41.074: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 19 22:51:41.154: INFO: Waiting up to 5m0s for pod "pod-a79e0502-661f-4bd3-84fd-2af565e45892" in namespace "emptydir-3769" to be "success or failure"
Feb 19 22:51:41.196: INFO: Pod "pod-a79e0502-661f-4bd3-84fd-2af565e45892": Phase="Pending", Reason="", readiness=false. Elapsed: 41.661831ms
Feb 19 22:51:43.204: INFO: Pod "pod-a79e0502-661f-4bd3-84fd-2af565e45892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049330863s
Feb 19 22:51:45.207: INFO: Pod "pod-a79e0502-661f-4bd3-84fd-2af565e45892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05240235s
STEP: Saw pod success
Feb 19 22:51:45.207: INFO: Pod "pod-a79e0502-661f-4bd3-84fd-2af565e45892" satisfied condition "success or failure"
Feb 19 22:51:45.212: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-a79e0502-661f-4bd3-84fd-2af565e45892 container test-container: <nil>
STEP: delete the pod
Feb 19 22:51:45.243: INFO: Waiting for pod pod-a79e0502-661f-4bd3-84fd-2af565e45892 to disappear
Feb 19 22:51:45.257: INFO: Pod pod-a79e0502-661f-4bd3-84fd-2af565e45892 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:51:45.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3769" for this suite.
Feb 19 22:51:51.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:51:51.445: INFO: namespace emptydir-3769 deletion completed in 6.181991777s

• [SLOW TEST:10.372 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:51:51.449: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 19 22:51:51.513: INFO: Waiting up to 5m0s for pod "pod-6f0e661f-f6c2-4688-82a3-34b15cd20355" in namespace "emptydir-7826" to be "success or failure"
Feb 19 22:51:51.525: INFO: Pod "pod-6f0e661f-f6c2-4688-82a3-34b15cd20355": Phase="Pending", Reason="", readiness=false. Elapsed: 11.917908ms
Feb 19 22:51:53.528: INFO: Pod "pod-6f0e661f-f6c2-4688-82a3-34b15cd20355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01498609s
Feb 19 22:51:55.627: INFO: Pod "pod-6f0e661f-f6c2-4688-82a3-34b15cd20355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.113184636s
STEP: Saw pod success
Feb 19 22:51:55.627: INFO: Pod "pod-6f0e661f-f6c2-4688-82a3-34b15cd20355" satisfied condition "success or failure"
Feb 19 22:51:55.755: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-6f0e661f-f6c2-4688-82a3-34b15cd20355 container test-container: <nil>
STEP: delete the pod
Feb 19 22:51:56.639: INFO: Waiting for pod pod-6f0e661f-f6c2-4688-82a3-34b15cd20355 to disappear
Feb 19 22:51:56.740: INFO: Pod pod-6f0e661f-f6c2-4688-82a3-34b15cd20355 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:51:56.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7826" for this suite.
Feb 19 22:52:03.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:52:03.667: INFO: namespace emptydir-7826 deletion completed in 6.825607696s

• [SLOW TEST:12.218 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:52:03.669: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9885
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 19 22:52:03.705: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 19 22:52:19.921: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.56.0.63:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:52:19.921: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:52:20.075: INFO: Found all expected endpoints: [netserver-0]
Feb 19 22:52:20.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.56.1.58:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:52:20.079: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:52:20.293: INFO: Found all expected endpoints: [netserver-1]
Feb 19 22:52:20.296: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.56.2.228:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 19 22:52:20.296: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
Feb 19 22:52:20.516: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:52:20.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9885" for this suite.
Feb 19 22:52:44.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:52:44.717: INFO: namespace pod-network-test-9885 deletion completed in 24.193569729s

• [SLOW TEST:41.049 seconds]
[sig-network] Networking
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:52:44.719: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-xth7
STEP: Creating a pod to test atomic-volume-subpath
Feb 19 22:52:44.781: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xth7" in namespace "subpath-2835" to be "success or failure"
Feb 19 22:52:44.786: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.21589ms
Feb 19 22:52:46.789: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008464582s
Feb 19 22:52:48.793: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 4.011751501s
Feb 19 22:52:50.796: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 6.014632641s
Feb 19 22:52:52.807: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 8.026159105s
Feb 19 22:52:54.912: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 10.131294955s
Feb 19 22:52:57.013: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 12.231818867s
Feb 19 22:52:59.016: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 14.235134208s
Feb 19 22:53:01.022: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 16.241468959s
Feb 19 22:53:03.026: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 18.244616935s
Feb 19 22:53:05.029: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Running", Reason="", readiness=true. Elapsed: 20.2475832s
Feb 19 22:53:07.033: INFO: Pod "pod-subpath-test-projected-xth7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.252238273s
STEP: Saw pod success
Feb 19 22:53:07.034: INFO: Pod "pod-subpath-test-projected-xth7" satisfied condition "success or failure"
Feb 19 22:53:07.036: INFO: Trying to get logs from node gke-c115-default-pool-249bf33f-nfnp pod pod-subpath-test-projected-xth7 container test-container-subpath-projected-xth7: <nil>
STEP: delete the pod
Feb 19 22:53:07.057: INFO: Waiting for pod pod-subpath-test-projected-xth7 to disappear
Feb 19 22:53:07.060: INFO: Pod pod-subpath-test-projected-xth7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-xth7
Feb 19 22:53:07.060: INFO: Deleting pod "pod-subpath-test-projected-xth7" in namespace "subpath-2835"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:53:07.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2835" for this suite.
Feb 19 22:53:15.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:53:15.267: INFO: namespace subpath-2835 deletion completed in 8.19941337s

• [SLOW TEST:30.548 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:53:15.270: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Feb 19 22:53:15.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 api-versions'
Feb 19 22:53:15.568: INFO: stderr: ""
Feb 19 22:53:15.568: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncloud.google.com/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nnetworking.gke.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nnodemanagement.gke.io/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscalingpolicy.kope.io/v1alpha1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:53:15.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5836" for this suite.
Feb 19 22:53:21.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:53:21.725: INFO: namespace kubectl-5836 deletion completed in 6.154052433s

• [SLOW TEST:6.456 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:53:21.731: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Feb 19 22:53:21.759: INFO: Creating deployment "nginx-deployment"
Feb 19 22:53:21.767: INFO: Waiting for observed generation 1
Feb 19 22:53:23.778: INFO: Waiting for all required pods to come up
Feb 19 22:53:23.781: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 19 22:53:27.801: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 19 22:53:27.815: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 19 22:53:27.830: INFO: Updating deployment nginx-deployment
Feb 19 22:53:27.830: INFO: Waiting for observed generation 2
Feb 19 22:53:29.858: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 19 22:53:29.860: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 19 22:53:29.863: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 19 22:53:29.871: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 19 22:53:29.871: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 19 22:53:29.875: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 19 22:53:29.880: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 19 22:53:29.881: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 19 22:53:29.887: INFO: Updating deployment nginx-deployment
Feb 19 22:53:29.887: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 19 22:53:29.923: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 19 22:53:31.953: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Feb 19 22:53:31.958: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-2906,SelfLink:/apis/apps/v1/namespaces/deployment-2906/deployments/nginx-deployment,UID:4f75ca47-b38a-4044-b872-44da14ce4c87,ResourceVersion:125457,Generation:3,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2020-02-19 22:53:29 +0000 UTC 2020-02-19 22:53:29 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-02-19 22:53:30 +0000 UTC 2020-02-19 22:53:21 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 19 22:53:31.961: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-2906,SelfLink:/apis/apps/v1/namespaces/deployment-2906/replicasets/nginx-deployment-55fb7cb77f,UID:b135db48-0d53-4224-8474-3b91d7532810,ResourceVersion:125452,Generation:3,CreationTimestamp:2020-02-19 22:53:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4f75ca47-b38a-4044-b872-44da14ce4c87 0xc002527b37 0xc002527b38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 19 22:53:31.962: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 19 22:53:31.962: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-2906,SelfLink:/apis/apps/v1/namespaces/deployment-2906/replicasets/nginx-deployment-7b8c6f4498,UID:5994dba6-07eb-4f2a-bf69-999bdab13c27,ResourceVersion:125453,Generation:3,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4f75ca47-b38a-4044-b872-44da14ce4c87 0xc002527c07 0xc002527c08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 19 22:53:31.967: INFO: Pod "nginx-deployment-55fb7cb77f-4xlv6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4xlv6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-4xlv6,UID:c05376f8-1eda-4e2c-a16c-2836f83e3c78,ResourceVersion:125451,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9e5a7 0xc002a9e5a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9e610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9e630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.967: INFO: Pod "nginx-deployment-55fb7cb77f-5sfmn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5sfmn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-5sfmn,UID:c58a190c-d651-45e5-8ead-b7b0e63bae9d,ResourceVersion:125448,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9e700 0xc002a9e701}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9e770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9e790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.967: INFO: Pod "nginx-deployment-55fb7cb77f-9b7z4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9b7z4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-9b7z4,UID:62cf7594-be4d-4ff7-868c-aca7ef33b002,ResourceVersion:125388,Generation:0,CreationTimestamp:2020-02-19 22:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9e860 0xc002a9e861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9e8d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9e8f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:29 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.967: INFO: Pod "nginx-deployment-55fb7cb77f-bbdsg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bbdsg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-bbdsg,UID:dc9cdd89-504d-44e7-b004-040e8d2b0aac,ResourceVersion:125401,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9e9c0 0xc002a9e9c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9ea30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9ea50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.968: INFO: Pod "nginx-deployment-55fb7cb77f-cwv64" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cwv64,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-cwv64,UID:1aa3ae43-497f-41ac-9fa4-807005543222,ResourceVersion:125410,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9eb20 0xc002a9eb21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9eb90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9ebb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.968: INFO: Pod "nginx-deployment-55fb7cb77f-kphb6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kphb6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-kphb6,UID:cd4870c9-3907-4d94-bd8e-9ba2473f93f7,ResourceVersion:125396,Generation:0,CreationTimestamp:2020-02-19 22:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9ec80 0xc002a9ec81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9ecf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9ed10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.968: INFO: Pod "nginx-deployment-55fb7cb77f-m5nmk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-m5nmk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-m5nmk,UID:705d5f25-d4d3-4613-9664-5462c4cc45b0,ResourceVersion:125381,Generation:0,CreationTimestamp:2020-02-19 22:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9ede0 0xc002a9ede1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9ee50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9ee70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:10.56.2.235,StartTime:2020-02-19 22:53:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found: manifest unknown: manifest unknown,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.968: INFO: Pod "nginx-deployment-55fb7cb77f-mxtvp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-mxtvp,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-mxtvp,UID:a02592f9-1cc4-4b48-ad1b-d667ed9d5792,ResourceVersion:125363,Generation:0,CreationTimestamp:2020-02-19 22:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9ef60 0xc002a9ef61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9efd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9eff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.968: INFO: Pod "nginx-deployment-55fb7cb77f-nt6cv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-nt6cv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-nt6cv,UID:d506780b-e3eb-4e1a-b800-d91172d1b23b,ResourceVersion:125428,Generation:0,CreationTimestamp:2020-02-19 22:53:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9f0c0 0xc002a9f0c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:10.56.1.62,StartTime:2020-02-19 22:53:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found: manifest unknown: manifest unknown,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-55fb7cb77f-tmqbg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tmqbg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-tmqbg,UID:26ea5112-529a-41b6-97a3-fda994fd140f,ResourceVersion:125442,Generation:0,CreationTimestamp:2020-02-19 22:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9f240 0xc002a9f241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f2b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f2d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:10.56.1.63,StartTime:2020-02-19 22:53:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found: manifest unknown: manifest unknown,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-55fb7cb77f-vq95s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vq95s,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-vq95s,UID:ae73d2c0-59d4-4182-b3c4-bed30136c912,ResourceVersion:125419,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9f3c0 0xc002a9f3c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-55fb7cb77f-vrvhh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vrvhh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-vrvhh,UID:bbd51c21-a06e-493a-8f9b-c169bdd8c90d,ResourceVersion:125372,Generation:0,CreationTimestamp:2020-02-19 22:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9f520 0xc002a9f521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:28 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:10.56.0.67,StartTime:2020-02-19 22:53:28 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found: manifest unknown: manifest unknown,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-55fb7cb77f-wmcnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wmcnz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-55fb7cb77f-wmcnz,UID:1019911d-e9e2-4c1c-a99a-4c94f998eaf8,ResourceVersion:125431,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f b135db48-0d53-4224-8474-3b91d7532810 0xc002a9f6a0 0xc002a9f6a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-7b8c6f4498-45drg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-45drg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-45drg,UID:7434db05-d5de-4c91-ae34-a7923a81cf9d,ResourceVersion:125407,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9f800 0xc002a9f801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-7b8c6f4498-48h64" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-48h64,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-48h64,UID:0be69b37-f192-4dd2-a5a5-733370992d29,ResourceVersion:125284,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9f940 0xc002a9f941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9f9a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9f9c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:10.56.1.59,StartTime:2020-02-19 22:53:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://812579848f93e7a744cf5b4f68d3836e81ff80e689d64de7f22b87c0a77ec323}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.969: INFO: Pod "nginx-deployment-7b8c6f4498-7vmr6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7vmr6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-7vmr6,UID:8ff9ab79-e597-4ce3-a6ec-d9d36ee77b7f,ResourceVersion:125432,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9fa90 0xc002a9fa91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9faf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9fb10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.970: INFO: Pod "nginx-deployment-7b8c6f4498-ct9rl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ct9rl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-ct9rl,UID:d447f9b1-7f2a-46e6-b2e4-1db65b8401c7,ResourceVersion:125458,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9fbd0 0xc002a9fbd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9fc30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9fc50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.971: INFO: Pod "nginx-deployment-7b8c6f4498-fxwsx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fxwsx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-fxwsx,UID:27c47d48-e603-4e2c-b268-66b53309e6f8,ResourceVersion:125465,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9fd10 0xc002a9fd11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9fd70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9fd90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:31 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.971: INFO: Pod "nginx-deployment-7b8c6f4498-gsqpx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gsqpx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-gsqpx,UID:f4387739-bebf-4516-9318-2910e64aa8d9,ResourceVersion:125301,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9fe50 0xc002a9fe51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a9feb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a9fed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:10.56.0.65,StartTime:2020-02-19 22:53:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://6686ebbb8aee4604a61e6dfdf1de90af5fca4fb8a2dae25047be728f888deb95}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.971: INFO: Pod "nginx-deployment-7b8c6f4498-j8sql" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j8sql,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-j8sql,UID:2e762b72-f315-41c0-9f3c-3af87716142b,ResourceVersion:125424,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002a9ffa0 0xc002a9ffa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.971: INFO: Pod "nginx-deployment-7b8c6f4498-jmpgm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jmpgm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-jmpgm,UID:99479fd3-da90-44ce-aeb5-46b937f8fa91,ResourceVersion:125390,Generation:0,CreationTimestamp:2020-02-19 22:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c0e0 0xc002c5c0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.971: INFO: Pod "nginx-deployment-7b8c6f4498-k9nfq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-k9nfq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-k9nfq,UID:c383b740-332f-4d2f-bfae-6cafd01a2c5a,ResourceVersion:125313,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c220 0xc002c5c221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c2a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:10.56.2.231,StartTime:2020-02-19 22:53:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://c4afb66915381229cbc2294d829aaa068d65f16c8c51ba9d26d63eb0c424d54b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.971: INFO: Pod "nginx-deployment-7b8c6f4498-kk2f6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kk2f6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-kk2f6,UID:6b225b4f-638d-4e8c-90ac-a4b177c08f86,ResourceVersion:125290,Generation:0,CreationTimestamp:2020-02-19 22:53:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c370 0xc002c5c371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:22 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:10.56.1.60,StartTime:2020-02-19 22:53:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://16fd910f23dfece80398dd058e9bf5fbdf48eb1461de6d1bcc6411b3bf6dbe4b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.972: INFO: Pod "nginx-deployment-7b8c6f4498-kw52p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kw52p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-kw52p,UID:c0fd585e-5ea7-4e26-b9a6-e8df29158bb0,ResourceVersion:125459,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c4c0 0xc002c5c4c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.972: INFO: Pod "nginx-deployment-7b8c6f4498-m72wg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m72wg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-m72wg,UID:c2d1d7df-0b35-468d-9f8d-9045f2addaa4,ResourceVersion:125450,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c600 0xc002c5c601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.972: INFO: Pod "nginx-deployment-7b8c6f4498-mz92k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mz92k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-mz92k,UID:2227c632-f18c-4303-9991-cbcc3942a77f,ResourceVersion:125455,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c740 0xc002c5c741}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c7c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.972: INFO: Pod "nginx-deployment-7b8c6f4498-nzjr4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nzjr4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-nzjr4,UID:47db5f65-e6b5-41db-8554-f8a4247d74a0,ResourceVersion:125463,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c880 0xc002c5c881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5c8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5c900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.974: INFO: Pod "nginx-deployment-7b8c6f4498-q9knh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-q9knh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-q9knh,UID:e6e96f2b-2f64-49f7-9249-355ffe2d28e9,ResourceVersion:125287,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5c9c0 0xc002c5c9c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-qclh,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5ca20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5ca40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:24 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:24 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.2,PodIP:10.56.1.61,StartTime:2020-02-19 22:53:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://78478ea2c9bd91802b0177445898e4deaada5f728d91d2636dff12454f4f0cc2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.974: INFO: Pod "nginx-deployment-7b8c6f4498-rk4sk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rk4sk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-rk4sk,UID:eabcea82-0e60-46c3-be95-ba117e17a9ea,ResourceVersion:125305,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5cb10 0xc002c5cb11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5cb70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5cb90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:10.56.2.232,StartTime:2020-02-19 22:53:22 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://b3e21d6104e6c8d13a6c1ffa7fb9648af6041e3b6fd52e70cef679ed0a50b1cd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.974: INFO: Pod "nginx-deployment-7b8c6f4498-vhhzp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vhhzp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-vhhzp,UID:2badee1e-d0de-417e-9a0e-4efcec99be54,ResourceVersion:125460,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5cc60 0xc002c5cc61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5ccc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5cce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.974: INFO: Pod "nginx-deployment-7b8c6f4498-wkg85" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wkg85,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-wkg85,UID:5e99b317-9b91-4518-920f-8f958e6aa4ba,ResourceVersion:125406,Generation:0,CreationTimestamp:2020-02-19 22:53:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5cda0 0xc002c5cda1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5ce00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5ce20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:30 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:,StartTime:2020-02-19 22:53:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.974: INFO: Pod "nginx-deployment-7b8c6f4498-xjrrs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xjrrs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-xjrrs,UID:302d0e41-7914-405b-b48d-81fc5469e0a4,ResourceVersion:125299,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5cee0 0xc002c5cee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-vhvp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5cf40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5cf60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.3,PodIP:10.56.0.66,StartTime:2020-02-19 22:53:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://0be5d80beebec18d30f29805a58649f41a7c0950b451b6c5c360610ae592cc05}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 19 22:53:31.974: INFO: Pod "nginx-deployment-7b8c6f4498-xppkm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xppkm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-2906,SelfLink:/api/v1/namespaces/deployment-2906/pods/nginx-deployment-7b8c6f4498-xppkm,UID:5b70fb14-83bb-4936-a23e-5e5d36ac1529,ResourceVersion:125311,Generation:0,CreationTimestamp:2020-02-19 22:53:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5994dba6-07eb-4f2a-bf69-999bdab13c27 0xc002c5d030 0xc002c5d031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t8crc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t8crc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-t8crc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-c115-default-pool-249bf33f-nfnp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5d090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5d0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-19 22:53:21 +0000 UTC  }],Message:,Reason:,HostIP:10.142.0.4,PodIP:10.56.2.234,StartTime:2020-02-19 22:53:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-02-19 22:53:24 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 docker://ee9ec2f097a422316edda20c010bdbbf5efce8e5667d39895360f16b52495440}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:53:31.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2906" for this suite.
Feb 19 22:53:40.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:53:41.069: INFO: namespace deployment-2906 deletion completed in 9.091426298s

• [SLOW TEST:19.339 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:53:41.074: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-576180c5-b95f-40bb-b363-606cb56c4a2d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-576180c5-b95f-40bb-b363-606cb56c4a2d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:53:45.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7330" for this suite.
Feb 19 22:54:07.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:54:07.518: INFO: namespace projected-7330 deletion completed in 22.260281544s

• [SLOW TEST:26.445 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Feb 19 22:54:07.527: INFO: >>> kubeConfig: /tmp/kubeconfig-365921390
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 19 22:54:07.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3337'
Feb 19 22:54:07.667: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 19 22:54:07.667: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Feb 19 22:54:09.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-365921390 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3337'
Feb 19 22:54:10.223: INFO: stderr: ""
Feb 19 22:54:10.223: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Feb 19 22:54:10.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3337" for this suite.
Feb 19 22:54:16.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 19 22:54:16.765: INFO: namespace kubectl-3337 deletion completed in 6.425017429s

• [SLOW TEST:9.239 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.9-beta.0.3+2e808b7cb054ee/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSFeb 19 22:54:16.766: INFO: Running AfterSuite actions on all nodes
Feb 19 22:54:16.767: INFO: Running AfterSuite actions on node 1
Feb 19 22:54:16.767: INFO: Skipping dumping logs from cluster

Ran 215 of 4412 Specs in 5670.221 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4197 Skipped
PASS

Ginkgo ran 1 suite in 1h34m32.267552944s
Test Suite Passed
