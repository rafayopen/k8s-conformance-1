I1219 06:40:02.271550      17 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-317660674
I1219 06:40:02.273150      17 e2e.go:241] Starting e2e run "fca76fc9-7bfc-44ba-8e5e-5852ecac9621" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576737600 - Will randomize all specs
Will run 215 of 4411 specs

Dec 19 06:40:02.335: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 06:40:02.341: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 19 06:40:02.377: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 19 06:40:02.392: INFO: 6 / 6 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 19 06:40:02.392: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec 19 06:40:02.392: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 19 06:40:02.397: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'everest-csi-driver' (0 seconds elapsed)
Dec 19 06:40:02.397: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'icagent' (0 seconds elapsed)
Dec 19 06:40:02.397: INFO: e2e test version: v1.15.0
Dec 19 06:40:02.398: INFO: kube-apiserver version: v1.15.6-r0-CCE2.0.26.B001
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:40:02.398: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
Dec 19 06:40:02.432: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 06:40:02.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec" in namespace "projected-8029" to be "success or failure"
Dec 19 06:40:02.519: INFO: Pod "downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec": Phase="Pending", Reason="", readiness=false. Elapsed: 70.818314ms
Dec 19 06:40:04.522: INFO: Pod "downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073459278s
Dec 19 06:40:06.524: INFO: Pod "downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076063021s
STEP: Saw pod success
Dec 19 06:40:06.524: INFO: Pod "downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec" satisfied condition "success or failure"
Dec 19 06:40:06.526: INFO: Trying to get logs from node 192.168.0.205 pod downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec container client-container: <nil>
STEP: delete the pod
Dec 19 06:40:06.537: INFO: Waiting for pod downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec to disappear
Dec 19 06:40:06.540: INFO: Pod downwardapi-volume-230eb868-2878-42ad-8568-f32df15c10ec no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:40:06.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8029" for this suite.
Dec 19 06:40:12.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:40:12.592: INFO: namespace projected-8029 deletion completed in 6.050569372s

• [SLOW TEST:10.194 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:40:12.592: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1118
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 19 06:40:12.609: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 19 06:40:36.649: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.0.22:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1118 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 06:40:36.649: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 06:40:36.748: INFO: Found all expected endpoints: [netserver-0]
Dec 19 06:40:36.750: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.0.61:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1118 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 06:40:36.750: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 06:40:36.811: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:40:36.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1118" for this suite.
Dec 19 06:40:58.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:40:58.864: INFO: namespace pod-network-test-1118 deletion completed in 22.049656672s

• [SLOW TEST:46.272 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:40:58.864: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec 19 06:40:58.885: INFO: Waiting up to 5m0s for pod "var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85" in namespace "var-expansion-6663" to be "success or failure"
Dec 19 06:40:58.889: INFO: Pod "var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090269ms
Dec 19 06:41:00.892: INFO: Pod "var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006420669s
Dec 19 06:41:02.894: INFO: Pod "var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008633859s
STEP: Saw pod success
Dec 19 06:41:02.894: INFO: Pod "var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85" satisfied condition "success or failure"
Dec 19 06:41:02.895: INFO: Trying to get logs from node 192.168.0.132 pod var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85 container dapi-container: <nil>
STEP: delete the pod
Dec 19 06:41:02.906: INFO: Waiting for pod var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85 to disappear
Dec 19 06:41:02.907: INFO: Pod var-expansion-7a34429b-d042-4179-b677-6aeee2fdbe85 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:41:02.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6663" for this suite.
Dec 19 06:41:08.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:41:08.959: INFO: namespace var-expansion-6663 deletion completed in 6.049979713s

• [SLOW TEST:10.095 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:41:08.959: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 06:41:08.979: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2" in namespace "projected-4682" to be "success or failure"
Dec 19 06:41:08.982: INFO: Pod "downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731197ms
Dec 19 06:41:10.984: INFO: Pod "downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004934706s
STEP: Saw pod success
Dec 19 06:41:10.984: INFO: Pod "downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2" satisfied condition "success or failure"
Dec 19 06:41:10.986: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2 container client-container: <nil>
STEP: delete the pod
Dec 19 06:41:10.996: INFO: Waiting for pod downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2 to disappear
Dec 19 06:41:10.997: INFO: Pod downwardapi-volume-d49dfd61-164e-4a9d-81e8-945b69d573a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:41:10.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4682" for this suite.
Dec 19 06:41:17.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:41:17.051: INFO: namespace projected-4682 deletion completed in 6.052239641s

• [SLOW TEST:8.092 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:41:17.052: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9cffa0b1-6774-4a5c-a90e-0d9748119661
STEP: Creating a pod to test consume configMaps
Dec 19 06:41:17.152: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186" in namespace "configmap-7166" to be "success or failure"
Dec 19 06:41:17.154: INFO: Pod "pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186": Phase="Pending", Reason="", readiness=false. Elapsed: 1.844571ms
Dec 19 06:41:19.156: INFO: Pod "pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004047364s
STEP: Saw pod success
Dec 19 06:41:19.156: INFO: Pod "pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186" satisfied condition "success or failure"
Dec 19 06:41:19.158: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 06:41:19.170: INFO: Waiting for pod pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186 to disappear
Dec 19 06:41:19.174: INFO: Pod pod-configmaps-e9132b70-daba-4264-8034-ea4b4572f186 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:41:19.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7166" for this suite.
Dec 19 06:41:25.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:41:25.225: INFO: namespace configmap-7166 deletion completed in 6.048403162s

• [SLOW TEST:8.173 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:41:25.225: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3391
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 19 06:41:25.252: INFO: Found 0 stateful pods, waiting for 3
Dec 19 06:41:35.255: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 06:41:35.255: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 06:41:35.255: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 19 06:41:35.274: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 19 06:41:45.299: INFO: Updating stateful set ss2
Dec 19 06:41:45.307: INFO: Waiting for Pod statefulset-3391/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec 19 06:41:55.342: INFO: Found 2 stateful pods, waiting for 3
Dec 19 06:42:05.344: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 06:42:05.344: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 06:42:05.344: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 19 06:42:05.361: INFO: Updating stateful set ss2
Dec 19 06:42:05.365: INFO: Waiting for Pod statefulset-3391/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 19 06:42:15.381: INFO: Waiting for Pod statefulset-3391/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 19 06:42:25.384: INFO: Updating stateful set ss2
Dec 19 06:42:25.395: INFO: Waiting for StatefulSet statefulset-3391/ss2 to complete update
Dec 19 06:42:25.395: INFO: Waiting for Pod statefulset-3391/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 19 06:42:35.399: INFO: Waiting for StatefulSet statefulset-3391/ss2 to complete update
Dec 19 06:42:35.399: INFO: Waiting for Pod statefulset-3391/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 19 06:42:45.439: INFO: Deleting all statefulset in ns statefulset-3391
Dec 19 06:42:45.442: INFO: Scaling statefulset ss2 to 0
Dec 19 06:43:05.461: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 06:43:05.463: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:43:05.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3391" for this suite.
Dec 19 06:43:11.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:43:11.521: INFO: namespace statefulset-3391 deletion completed in 6.049888875s

• [SLOW TEST:106.296 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:43:11.521: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 19 06:43:11.552: INFO: Waiting up to 5m0s for pod "pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97" in namespace "emptydir-4034" to be "success or failure"
Dec 19 06:43:11.554: INFO: Pod "pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.603393ms
Dec 19 06:43:13.556: INFO: Pod "pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00384766s
Dec 19 06:43:15.558: INFO: Pod "pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006056544s
STEP: Saw pod success
Dec 19 06:43:15.558: INFO: Pod "pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97" satisfied condition "success or failure"
Dec 19 06:43:15.559: INFO: Trying to get logs from node 192.168.0.132 pod pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97 container test-container: <nil>
STEP: delete the pod
Dec 19 06:43:15.570: INFO: Waiting for pod pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97 to disappear
Dec 19 06:43:15.572: INFO: Pod pod-af4bda36-9ecb-4774-ae2b-860ac4a27e97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:43:15.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4034" for this suite.
Dec 19 06:43:21.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:43:21.632: INFO: namespace emptydir-4034 deletion completed in 6.058112375s

• [SLOW TEST:10.111 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:43:21.632: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-caac672b-2163-413a-b1d1-5b69314f7ee9
STEP: Creating a pod to test consume configMaps
Dec 19 06:43:21.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff" in namespace "configmap-6398" to be "success or failure"
Dec 19 06:43:21.664: INFO: Pod "pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.686545ms
Dec 19 06:43:23.667: INFO: Pod "pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004011961s
Dec 19 06:43:25.669: INFO: Pod "pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006300776s
STEP: Saw pod success
Dec 19 06:43:25.669: INFO: Pod "pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff" satisfied condition "success or failure"
Dec 19 06:43:25.671: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 06:43:25.682: INFO: Waiting for pod pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff to disappear
Dec 19 06:43:25.684: INFO: Pod pod-configmaps-de8b10d0-f6c9-4a15-90e3-3a13785d4bff no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:43:25.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6398" for this suite.
Dec 19 06:43:31.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:43:31.734: INFO: namespace configmap-6398 deletion completed in 6.048494071s

• [SLOW TEST:10.102 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:43:31.734: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 06:43:31.761: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 19 06:43:31.765: INFO: Number of nodes with available pods: 0
Dec 19 06:43:31.765: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 19 06:43:31.776: INFO: Number of nodes with available pods: 0
Dec 19 06:43:31.776: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:32.796: INFO: Number of nodes with available pods: 0
Dec 19 06:43:32.796: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:33.778: INFO: Number of nodes with available pods: 0
Dec 19 06:43:33.778: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:34.778: INFO: Number of nodes with available pods: 1
Dec 19 06:43:34.778: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 19 06:43:34.787: INFO: Number of nodes with available pods: 1
Dec 19 06:43:34.787: INFO: Number of running nodes: 0, number of available pods: 1
Dec 19 06:43:35.789: INFO: Number of nodes with available pods: 0
Dec 19 06:43:35.789: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 19 06:43:35.800: INFO: Number of nodes with available pods: 0
Dec 19 06:43:35.800: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:36.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:36.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:37.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:37.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:38.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:38.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:39.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:39.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:40.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:40.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:41.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:41.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:42.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:42.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:43.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:43.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:44.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:44.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:45.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:45.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:46.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:46.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:47.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:47.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:48.802: INFO: Number of nodes with available pods: 0
Dec 19 06:43:48.802: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:43:49.802: INFO: Number of nodes with available pods: 1
Dec 19 06:43:49.802: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1205, will wait for the garbage collector to delete the pods
Dec 19 06:43:49.861: INFO: Deleting DaemonSet.extensions daemon-set took: 4.354646ms
Dec 19 06:43:49.961: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.138928ms
Dec 19 06:43:53.463: INFO: Number of nodes with available pods: 0
Dec 19 06:43:53.463: INFO: Number of running nodes: 0, number of available pods: 0
Dec 19 06:43:53.465: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1205/daemonsets","resourceVersion":"655506"},"items":null}

Dec 19 06:43:53.467: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1205/pods","resourceVersion":"655506"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:43:53.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1205" for this suite.
Dec 19 06:43:59.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:43:59.546: INFO: namespace daemonsets-1205 deletion completed in 6.055260491s

• [SLOW TEST:27.811 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:43:59.546: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Dec 19 06:43:59.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-6521'
Dec 19 06:44:00.230: INFO: stderr: ""
Dec 19 06:44:00.230: INFO: stdout: "pod/pause created\n"
Dec 19 06:44:00.230: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 19 06:44:00.231: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6521" to be "running and ready"
Dec 19 06:44:00.233: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882938ms
Dec 19 06:44:02.236: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005033838s
Dec 19 06:44:04.238: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.007337402s
Dec 19 06:44:04.238: INFO: Pod "pause" satisfied condition "running and ready"
Dec 19 06:44:04.238: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 19 06:44:04.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 label pods pause testing-label=testing-label-value --namespace=kubectl-6521'
Dec 19 06:44:04.295: INFO: stderr: ""
Dec 19 06:44:04.295: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 19 06:44:04.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pod pause -L testing-label --namespace=kubectl-6521'
Dec 19 06:44:04.349: INFO: stderr: ""
Dec 19 06:44:04.349: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 19 06:44:04.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 label pods pause testing-label- --namespace=kubectl-6521'
Dec 19 06:44:04.404: INFO: stderr: ""
Dec 19 06:44:04.404: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 19 06:44:04.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pod pause -L testing-label --namespace=kubectl-6521'
Dec 19 06:44:04.455: INFO: stderr: ""
Dec 19 06:44:04.455: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Dec 19 06:44:04.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-6521'
Dec 19 06:44:04.511: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:44:04.511: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 19 06:44:04.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get rc,svc -l name=pause --no-headers --namespace=kubectl-6521'
Dec 19 06:44:04.567: INFO: stderr: "No resources found.\n"
Dec 19 06:44:04.567: INFO: stdout: ""
Dec 19 06:44:04.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -l name=pause --namespace=kubectl-6521 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 19 06:44:04.617: INFO: stderr: ""
Dec 19 06:44:04.617: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:44:04.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6521" for this suite.
Dec 19 06:44:10.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:44:10.672: INFO: namespace kubectl-6521 deletion completed in 6.051851941s

• [SLOW TEST:11.126 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:44:10.672: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 06:44:10.688: INFO: Creating deployment "test-recreate-deployment"
Dec 19 06:44:10.690: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 19 06:44:10.694: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 19 06:44:12.698: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 19 06:44:12.699: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 19 06:44:12.703: INFO: Updating deployment test-recreate-deployment
Dec 19 06:44:12.703: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 19 06:44:12.754: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7687,SelfLink:/apis/apps/v1/namespaces/deployment-7687/deployments/test-recreate-deployment,UID:9359257a-ac8c-4395-ab9c-7d1d07dd3ab6,ResourceVersion:655623,Generation:2,CreationTimestamp:2019-12-19 06:44:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-19 06:44:12 +0000 UTC 2019-12-19 06:44:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-19 06:44:12 +0000 UTC 2019-12-19 06:44:10 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec 19 06:44:12.758: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-7687,SelfLink:/apis/apps/v1/namespaces/deployment-7687/replicasets/test-recreate-deployment-5c8c9cc69d,UID:95825b08-55dd-4037-8256-a8b34314e8f3,ResourceVersion:655622,Generation:1,CreationTimestamp:2019-12-19 06:44:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9359257a-ac8c-4395-ab9c-7d1d07dd3ab6 0xc0021bc787 0xc0021bc788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 06:44:12.758: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 19 06:44:12.758: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-7687,SelfLink:/apis/apps/v1/namespaces/deployment-7687/replicasets/test-recreate-deployment-6df85df6b9,UID:686d44dc-03a0-4296-857e-74ed030547bb,ResourceVersion:655615,Generation:2,CreationTimestamp:2019-12-19 06:44:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9359257a-ac8c-4395-ab9c-7d1d07dd3ab6 0xc0021bc857 0xc0021bc858}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 06:44:12.759: INFO: Pod "test-recreate-deployment-5c8c9cc69d-dt844" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-dt844,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-7687,SelfLink:/api/v1/namespaces/deployment-7687/pods/test-recreate-deployment-5c8c9cc69d-dt844,UID:3f9dacb1-f478-4bb1-91a9-003ce253e2a1,ResourceVersion:655624,Generation:0,CreationTimestamp:2019-12-19 06:44:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 95825b08-55dd-4037-8256-a8b34314e8f3 0xc0021bd137 0xc0021bd138}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-99tcv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-99tcv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-99tcv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021bd1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021bd1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc003002c70} {timeout 0xc003002c80}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:44:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:44:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:44:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:44:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2019-12-19 06:44:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:44:12.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7687" for this suite.
Dec 19 06:44:18.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:44:18.819: INFO: namespace deployment-7687 deletion completed in 6.057372913s

• [SLOW TEST:8.147 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:44:18.819: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 19 06:44:18.847: INFO: Waiting up to 5m0s for pod "downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4" in namespace "downward-api-1827" to be "success or failure"
Dec 19 06:44:18.850: INFO: Pod "downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191093ms
Dec 19 06:44:20.852: INFO: Pod "downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005276057s
Dec 19 06:44:22.855: INFO: Pod "downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007899888s
STEP: Saw pod success
Dec 19 06:44:22.855: INFO: Pod "downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4" satisfied condition "success or failure"
Dec 19 06:44:22.856: INFO: Trying to get logs from node 192.168.0.132 pod downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4 container dapi-container: <nil>
STEP: delete the pod
Dec 19 06:44:22.870: INFO: Waiting for pod downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4 to disappear
Dec 19 06:44:22.873: INFO: Pod downward-api-ea7fb57f-37bb-453d-b3ab-86179b582cc4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:44:22.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1827" for this suite.
Dec 19 06:44:28.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:44:28.922: INFO: namespace downward-api-1827 deletion completed in 6.047834872s

• [SLOW TEST:10.104 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:44:28.923: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 19 06:44:28.946: INFO: Waiting up to 5m0s for pod "pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f" in namespace "emptydir-2717" to be "success or failure"
Dec 19 06:44:28.947: INFO: Pod "pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548492ms
Dec 19 06:44:30.949: INFO: Pod "pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00371894s
Dec 19 06:44:32.952: INFO: Pod "pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005989461s
STEP: Saw pod success
Dec 19 06:44:32.952: INFO: Pod "pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f" satisfied condition "success or failure"
Dec 19 06:44:32.953: INFO: Trying to get logs from node 192.168.0.132 pod pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f container test-container: <nil>
STEP: delete the pod
Dec 19 06:44:32.964: INFO: Waiting for pod pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f to disappear
Dec 19 06:44:32.965: INFO: Pod pod-945a2dfe-f620-441b-b410-3a9b3dd0d50f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:44:32.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2717" for this suite.
Dec 19 06:44:38.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:44:39.014: INFO: namespace emptydir-2717 deletion completed in 6.047572949s

• [SLOW TEST:10.092 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:44:39.015: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 19 06:44:39.043: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6421,SelfLink:/api/v1/namespaces/watch-6421/configmaps/e2e-watch-test-label-changed,UID:8f4f2b94-d723-4276-97a3-800ae58e9a0f,ResourceVersion:655758,Generation:0,CreationTimestamp:2019-12-19 06:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 19 06:44:39.043: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6421,SelfLink:/api/v1/namespaces/watch-6421/configmaps/e2e-watch-test-label-changed,UID:8f4f2b94-d723-4276-97a3-800ae58e9a0f,ResourceVersion:655759,Generation:0,CreationTimestamp:2019-12-19 06:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 19 06:44:39.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6421,SelfLink:/api/v1/namespaces/watch-6421/configmaps/e2e-watch-test-label-changed,UID:8f4f2b94-d723-4276-97a3-800ae58e9a0f,ResourceVersion:655760,Generation:0,CreationTimestamp:2019-12-19 06:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 19 06:44:49.059: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6421,SelfLink:/api/v1/namespaces/watch-6421/configmaps/e2e-watch-test-label-changed,UID:8f4f2b94-d723-4276-97a3-800ae58e9a0f,ResourceVersion:655796,Generation:0,CreationTimestamp:2019-12-19 06:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 19 06:44:49.059: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6421,SelfLink:/api/v1/namespaces/watch-6421/configmaps/e2e-watch-test-label-changed,UID:8f4f2b94-d723-4276-97a3-800ae58e9a0f,ResourceVersion:655797,Generation:0,CreationTimestamp:2019-12-19 06:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 19 06:44:49.059: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6421,SelfLink:/api/v1/namespaces/watch-6421/configmaps/e2e-watch-test-label-changed,UID:8f4f2b94-d723-4276-97a3-800ae58e9a0f,ResourceVersion:655798,Generation:0,CreationTimestamp:2019-12-19 06:44:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:44:49.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6421" for this suite.
Dec 19 06:44:55.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:44:55.116: INFO: namespace watch-6421 deletion completed in 6.05380586s

• [SLOW TEST:16.101 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:44:55.116: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 19 06:44:55.139: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 19 06:44:55.142: INFO: Waiting for terminating namespaces to be deleted...
Dec 19 06:44:55.143: INFO: 
Logging pods the kubelet thinks is on node 192.168.0.132 before test
Dec 19 06:44:55.147: INFO: icagent-kh9cq from kube-system started at 2019-12-19 01:14:36 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container icagent ready: true, restart count 1
Dec 19 06:44:55.147: INFO: test-liyi-6668b4c9f8-pkxvx from default started at 2019-12-19 02:35:17 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:44:55.147: INFO: test-liyi-6668b4c9f8-xkqdd from default started at 2019-12-19 05:41:50 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:44:55.147: INFO: test-liyi-6668b4c9f8-c2fsf from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:44:55.147: INFO: test-liyi-6668b4c9f8-4qzm2 from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:44:55.147: INFO: everest-csi-driver-w2xrr from kube-system started at 2019-12-19 06:18:26 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 06:44:55.147: INFO: sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-8b4v2 from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 06:44:55.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 06:44:55.147: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 19 06:44:55.147: INFO: 
Logging pods the kubelet thinks is on node 192.168.0.205 before test
Dec 19 06:44:55.154: INFO: test-liyi-6668b4c9f8-zzwm7 from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:44:55.154: INFO: test-liyi-6668b4c9f8-sp92n from default started at 2019-12-18 11:46:57 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:44:55.154: INFO: icagent-2tb9d from kube-system started at 2019-12-17 03:05:53 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container icagent ready: true, restart count 0
Dec 19 06:44:55.154: INFO: everest-csi-driver-5gw2v from kube-system started at 2019-12-19 06:18:37 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 06:44:55.154: INFO: sonobuoy-e2e-job-9bbd16c8926f47ce from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container e2e ready: true, restart count 0
Dec 19 06:44:55.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 06:44:55.154: INFO: sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-sbhz7 from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 06:44:55.154: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 19 06:44:55.154: INFO: web-terminal-f79ff6644-6w8n5 from default started at 2019-12-17 03:05:00 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container web-terminal ready: true, restart count 0
Dec 19 06:44:55.154: INFO: sonobuoy from sonobuoy started at 2019-12-19 06:39:49 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 19 06:44:55.154: INFO: everest-csi-controller-b9cddb7d6-4ljp8 from kube-system started at 2019-12-18 13:31:00 +0000 UTC (2 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container everest-csi-controller ready: true, restart count 0
Dec 19 06:44:55.154: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 06:44:55.154: INFO: coredns-59d5fcd5dc-bgx6r from kube-system started at 2019-12-18 13:27:54 +0000 UTC (1 container statuses recorded)
Dec 19 06:44:55.154: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e1b2c301174798], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:44:56.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8915" for this suite.
Dec 19 06:45:02.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:45:02.221: INFO: namespace sched-pred-8915 deletion completed in 6.05264233s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.105 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:45:02.221: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 19 06:45:02.240: INFO: Waiting up to 5m0s for pod "downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82" in namespace "downward-api-7539" to be "success or failure"
Dec 19 06:45:02.245: INFO: Pod "downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489371ms
Dec 19 06:45:04.247: INFO: Pod "downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006606147s
STEP: Saw pod success
Dec 19 06:45:04.247: INFO: Pod "downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82" satisfied condition "success or failure"
Dec 19 06:45:04.248: INFO: Trying to get logs from node 192.168.0.132 pod downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82 container dapi-container: <nil>
STEP: delete the pod
Dec 19 06:45:04.258: INFO: Waiting for pod downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82 to disappear
Dec 19 06:45:04.259: INFO: Pod downward-api-e2c4e0fe-a2b4-457d-8fdc-8519545b7d82 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:45:04.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7539" for this suite.
Dec 19 06:45:10.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:45:10.310: INFO: namespace downward-api-7539 deletion completed in 6.048717461s

• [SLOW TEST:8.089 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:45:10.310: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 06:45:10.332: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71" in namespace "projected-115" to be "success or failure"
Dec 19 06:45:10.334: INFO: Pod "downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389214ms
Dec 19 06:45:12.336: INFO: Pod "downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004611186s
STEP: Saw pod success
Dec 19 06:45:12.336: INFO: Pod "downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71" satisfied condition "success or failure"
Dec 19 06:45:12.338: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71 container client-container: <nil>
STEP: delete the pod
Dec 19 06:45:12.346: INFO: Waiting for pod downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71 to disappear
Dec 19 06:45:12.347: INFO: Pod downwardapi-volume-0b91c1ce-8430-4db1-a8b2-70f7a7e6cd71 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:45:12.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-115" for this suite.
Dec 19 06:45:18.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:45:18.402: INFO: namespace projected-115 deletion completed in 6.052897211s

• [SLOW TEST:8.092 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:45:18.402: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 19 06:45:18.419: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 19 06:45:18.425: INFO: Waiting for terminating namespaces to be deleted...
Dec 19 06:45:18.426: INFO: 
Logging pods the kubelet thinks is on node 192.168.0.132 before test
Dec 19 06:45:18.430: INFO: test-liyi-6668b4c9f8-c2fsf from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:45:18.430: INFO: test-liyi-6668b4c9f8-xkqdd from default started at 2019-12-19 05:41:50 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:45:18.430: INFO: test-liyi-6668b4c9f8-4qzm2 from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:45:18.430: INFO: everest-csi-driver-w2xrr from kube-system started at 2019-12-19 06:18:26 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 06:45:18.430: INFO: sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-8b4v2 from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 06:45:18.430: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 19 06:45:18.430: INFO: icagent-kh9cq from kube-system started at 2019-12-19 01:14:36 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container icagent ready: true, restart count 1
Dec 19 06:45:18.430: INFO: test-liyi-6668b4c9f8-pkxvx from default started at 2019-12-19 02:35:17 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.430: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:45:18.430: INFO: 
Logging pods the kubelet thinks is on node 192.168.0.205 before test
Dec 19 06:45:18.434: INFO: icagent-2tb9d from kube-system started at 2019-12-17 03:05:53 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container icagent ready: true, restart count 0
Dec 19 06:45:18.434: INFO: everest-csi-driver-5gw2v from kube-system started at 2019-12-19 06:18:37 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 06:45:18.434: INFO: sonobuoy-e2e-job-9bbd16c8926f47ce from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container e2e ready: true, restart count 0
Dec 19 06:45:18.434: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 06:45:18.434: INFO: web-terminal-f79ff6644-6w8n5 from default started at 2019-12-17 03:05:00 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container web-terminal ready: true, restart count 0
Dec 19 06:45:18.434: INFO: sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-sbhz7 from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 06:45:18.434: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 19 06:45:18.434: INFO: everest-csi-controller-b9cddb7d6-4ljp8 from kube-system started at 2019-12-18 13:31:00 +0000 UTC (2 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container everest-csi-controller ready: true, restart count 0
Dec 19 06:45:18.434: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 06:45:18.434: INFO: sonobuoy from sonobuoy started at 2019-12-19 06:39:49 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 19 06:45:18.434: INFO: coredns-59d5fcd5dc-bgx6r from kube-system started at 2019-12-18 13:27:54 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container coredns ready: true, restart count 0
Dec 19 06:45:18.434: INFO: test-liyi-6668b4c9f8-sp92n from default started at 2019-12-18 11:46:57 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 06:45:18.434: INFO: test-liyi-6668b4c9f8-zzwm7 from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 06:45:18.434: INFO: 	Container container-0 ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3ce3bcd7-dcf0-4517-b032-0c781c461635 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-3ce3bcd7-dcf0-4517-b032-0c781c461635 off the node 192.168.0.132
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3ce3bcd7-dcf0-4517-b032-0c781c461635
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:45:24.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9328" for this suite.
Dec 19 06:45:32.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:45:32.533: INFO: namespace sched-pred-9328 deletion completed in 8.05054888s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:14.131 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:45:32.533: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c
Dec 19 06:45:32.554: INFO: Pod name my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c: Found 0 pods out of 1
Dec 19 06:45:37.557: INFO: Pod name my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c: Found 1 pods out of 1
Dec 19 06:45:37.557: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c" are running
Dec 19 06:45:37.558: INFO: Pod "my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c-9kd68" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 06:45:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 06:45:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 06:45:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 06:45:32 +0000 UTC Reason: Message:}])
Dec 19 06:45:37.558: INFO: Trying to dial the pod
Dec 19 06:45:42.568: INFO: Controller my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c: Got expected result from replica 1 [my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c-9kd68]: "my-hostname-basic-14de1bbb-d905-4217-b39b-588ca169804c-9kd68", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:45:42.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2941" for this suite.
Dec 19 06:45:48.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:45:48.618: INFO: namespace replication-controller-2941 deletion completed in 6.048432098s

• [SLOW TEST:16.085 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:45:48.618: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-26, will wait for the garbage collector to delete the pods
Dec 19 06:45:52.696: INFO: Deleting Job.batch foo took: 3.717174ms
Dec 19 06:45:52.796: INFO: Terminating Job.batch foo pods took: 100.166141ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:46:36.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-26" for this suite.
Dec 19 06:46:42.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:46:42.749: INFO: namespace job-26 deletion completed in 6.049054167s

• [SLOW TEST:54.131 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:46:42.750: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 19 06:46:42.770: INFO: Waiting up to 5m0s for pod "pod-ef269366-f742-4221-b140-c078d8e08ccd" in namespace "emptydir-2196" to be "success or failure"
Dec 19 06:46:42.772: INFO: Pod "pod-ef269366-f742-4221-b140-c078d8e08ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.609971ms
Dec 19 06:46:44.774: INFO: Pod "pod-ef269366-f742-4221-b140-c078d8e08ccd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003788247s
Dec 19 06:46:46.776: INFO: Pod "pod-ef269366-f742-4221-b140-c078d8e08ccd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0060039s
STEP: Saw pod success
Dec 19 06:46:46.776: INFO: Pod "pod-ef269366-f742-4221-b140-c078d8e08ccd" satisfied condition "success or failure"
Dec 19 06:46:46.778: INFO: Trying to get logs from node 192.168.0.132 pod pod-ef269366-f742-4221-b140-c078d8e08ccd container test-container: <nil>
STEP: delete the pod
Dec 19 06:46:46.789: INFO: Waiting for pod pod-ef269366-f742-4221-b140-c078d8e08ccd to disappear
Dec 19 06:46:46.791: INFO: Pod pod-ef269366-f742-4221-b140-c078d8e08ccd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:46:46.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2196" for this suite.
Dec 19 06:46:52.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:46:52.841: INFO: namespace emptydir-2196 deletion completed in 6.048954524s

• [SLOW TEST:10.092 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:46:52.842: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 19 06:46:52.866: INFO: namespace kubectl-417
Dec 19 06:46:52.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-417'
Dec 19 06:46:53.002: INFO: stderr: ""
Dec 19 06:46:53.002: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 19 06:46:54.004: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:46:54.004: INFO: Found 0 / 1
Dec 19 06:46:55.004: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:46:55.004: INFO: Found 0 / 1
Dec 19 06:46:56.004: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:46:56.004: INFO: Found 1 / 1
Dec 19 06:46:56.004: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 19 06:46:56.006: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:46:56.006: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 19 06:46:56.006: INFO: wait on redis-master startup in kubectl-417 
Dec 19 06:46:56.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 logs redis-master-9szbv redis-master --namespace=kubectl-417'
Dec 19 06:46:56.065: INFO: stderr: ""
Dec 19 06:46:56.065: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Dec 06:46:54.729 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Dec 06:46:54.730 # Server started, Redis version 3.2.12\n1:M 19 Dec 06:46:54.730 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Dec 06:46:54.730 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec 19 06:46:56.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-417'
Dec 19 06:46:56.128: INFO: stderr: ""
Dec 19 06:46:56.128: INFO: stdout: "service/rm2 exposed\n"
Dec 19 06:46:56.132: INFO: Service rm2 in namespace kubectl-417 found.
STEP: exposing service
Dec 19 06:46:58.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-417'
Dec 19 06:46:58.195: INFO: stderr: ""
Dec 19 06:46:58.195: INFO: stdout: "service/rm3 exposed\n"
Dec 19 06:46:58.197: INFO: Service rm3 in namespace kubectl-417 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:47:00.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-417" for this suite.
Dec 19 06:47:22.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:47:22.248: INFO: namespace kubectl-417 deletion completed in 22.045849814s

• [SLOW TEST:29.406 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:47:22.248: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 06:47:22.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6005'
Dec 19 06:47:22.322: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 19 06:47:22.322: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec 19 06:47:22.328: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Dec 19 06:47:22.334: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec 19 06:47:22.338: INFO: scanned /root for discovery docs: <nil>
Dec 19 06:47:22.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6005'
Dec 19 06:47:35.039: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 19 06:47:35.040: INFO: stdout: "Created e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3\nScaling up e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec 19 06:47:35.040: INFO: stdout: "Created e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3\nScaling up e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec 19 06:47:35.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6005'
Dec 19 06:47:35.092: INFO: stderr: ""
Dec 19 06:47:35.092: INFO: stdout: "e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3-psk5z "
Dec 19 06:47:35.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3-psk5z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6005'
Dec 19 06:47:35.142: INFO: stderr: ""
Dec 19 06:47:35.142: INFO: stdout: "true"
Dec 19 06:47:35.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3-psk5z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6005'
Dec 19 06:47:35.192: INFO: stderr: ""
Dec 19 06:47:35.192: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec 19 06:47:35.192: INFO: e2e-test-nginx-rc-db8d2d44c46fd209df7f6c74d9de08b3-psk5z is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Dec 19 06:47:35.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete rc e2e-test-nginx-rc --namespace=kubectl-6005'
Dec 19 06:47:35.246: INFO: stderr: ""
Dec 19 06:47:35.246: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:47:35.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6005" for this suite.
Dec 19 06:47:41.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:47:41.297: INFO: namespace kubectl-6005 deletion completed in 6.047551322s

• [SLOW TEST:19.048 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:47:41.297: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 19 06:47:41.323: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9655,SelfLink:/api/v1/namespaces/watch-9655/configmaps/e2e-watch-test-watch-closed,UID:63c562cf-c3e0-4459-9ffc-a208b64533a7,ResourceVersion:656613,Generation:0,CreationTimestamp:2019-12-19 06:47:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 19 06:47:41.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9655,SelfLink:/api/v1/namespaces/watch-9655/configmaps/e2e-watch-test-watch-closed,UID:63c562cf-c3e0-4459-9ffc-a208b64533a7,ResourceVersion:656614,Generation:0,CreationTimestamp:2019-12-19 06:47:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 19 06:47:41.329: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9655,SelfLink:/api/v1/namespaces/watch-9655/configmaps/e2e-watch-test-watch-closed,UID:63c562cf-c3e0-4459-9ffc-a208b64533a7,ResourceVersion:656615,Generation:0,CreationTimestamp:2019-12-19 06:47:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 19 06:47:41.329: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9655,SelfLink:/api/v1/namespaces/watch-9655/configmaps/e2e-watch-test-watch-closed,UID:63c562cf-c3e0-4459-9ffc-a208b64533a7,ResourceVersion:656616,Generation:0,CreationTimestamp:2019-12-19 06:47:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:47:41.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9655" for this suite.
Dec 19 06:47:47.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:47:47.387: INFO: namespace watch-9655 deletion completed in 6.056043038s

• [SLOW TEST:6.090 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:47:47.387: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9702.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9702.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9702.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9702.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9702.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 117.68.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.68.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.68.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.68.117_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9702.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9702.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9702.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9702.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9702.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9702.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 117.68.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.68.117_udp@PTR;check="$$(dig +tcp +noall +answer +search 117.68.247.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.247.68.117_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 19 06:47:51.440: INFO: Unable to read wheezy_udp@dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.443: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.444: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.446: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.461: INFO: Unable to read jessie_udp@dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.463: INFO: Unable to read jessie_tcp@dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.465: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.466: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local from pod dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442: the server could not find the requested resource (get pods dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442)
Dec 19 06:47:51.476: INFO: Lookups using dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442 failed for: [wheezy_udp@dns-test-service.dns-9702.svc.cluster.local wheezy_tcp@dns-test-service.dns-9702.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local jessie_udp@dns-test-service.dns-9702.svc.cluster.local jessie_tcp@dns-test-service.dns-9702.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9702.svc.cluster.local]

Dec 19 06:47:56.513: INFO: DNS probes using dns-9702/dns-test-9a57b2f9-9ced-4299-babf-3ae4ba3f9442 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:47:56.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9702" for this suite.
Dec 19 06:48:02.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:48:02.601: INFO: namespace dns-9702 deletion completed in 6.056626796s

• [SLOW TEST:15.214 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:48:02.601: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 19 06:48:05.133: INFO: Successfully updated pod "pod-update-5877e92d-04e7-4a71-b2c0-9f4300601811"
STEP: verifying the updated pod is in kubernetes
Dec 19 06:48:05.136: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:48:05.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8666" for this suite.
Dec 19 06:48:27.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:48:27.195: INFO: namespace pods-8666 deletion completed in 22.057300612s

• [SLOW TEST:24.595 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:48:27.196: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 19 06:48:27.216: INFO: Waiting up to 5m0s for pod "downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992" in namespace "downward-api-6484" to be "success or failure"
Dec 19 06:48:27.220: INFO: Pod "downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992": Phase="Pending", Reason="", readiness=false. Elapsed: 4.390538ms
Dec 19 06:48:29.222: INFO: Pod "downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006643454s
Dec 19 06:48:31.224: INFO: Pod "downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00890661s
STEP: Saw pod success
Dec 19 06:48:31.224: INFO: Pod "downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992" satisfied condition "success or failure"
Dec 19 06:48:31.226: INFO: Trying to get logs from node 192.168.0.132 pod downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992 container dapi-container: <nil>
STEP: delete the pod
Dec 19 06:48:31.239: INFO: Waiting for pod downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992 to disappear
Dec 19 06:48:31.241: INFO: Pod downward-api-976a0169-047c-40d2-a5ec-ec9aaec70992 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:48:31.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6484" for this suite.
Dec 19 06:48:37.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:48:37.291: INFO: namespace downward-api-6484 deletion completed in 6.048337612s

• [SLOW TEST:10.095 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:48:37.291: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fdb7aacb-568c-45de-b7c6-c890a66a5baf
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fdb7aacb-568c-45de-b7c6-c890a66a5baf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:49:49.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6337" for this suite.
Dec 19 06:50:11.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:50:11.658: INFO: namespace projected-6337 deletion completed in 22.060819042s

• [SLOW TEST:94.367 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:50:11.658: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 06:50:11.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 version'
Dec 19 06:50:11.735: INFO: stderr: ""
Dec 19 06:50:11.735: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.6-r0-CCE2.0.26.B001\", GitCommit:\"d2c5343ff3f0a7ad8c9872bbe1bb99cd7bf38ed0\", GitTreeState:\"clean\", BuildDate:\"2019-12-12T09:15:24Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:50:11.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7565" for this suite.
Dec 19 06:50:17.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:50:17.788: INFO: namespace kubectl-7565 deletion completed in 6.05151046s

• [SLOW TEST:6.130 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:50:17.789: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec 19 06:50:17.805: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 19 06:50:17.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-551'
Dec 19 06:50:17.934: INFO: stderr: ""
Dec 19 06:50:17.934: INFO: stdout: "service/redis-slave created\n"
Dec 19 06:50:17.934: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 19 06:50:17.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-551'
Dec 19 06:50:18.065: INFO: stderr: ""
Dec 19 06:50:18.065: INFO: stdout: "service/redis-master created\n"
Dec 19 06:50:18.065: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 19 06:50:18.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-551'
Dec 19 06:50:18.208: INFO: stderr: ""
Dec 19 06:50:18.208: INFO: stdout: "service/frontend created\n"
Dec 19 06:50:18.208: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 19 06:50:18.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-551'
Dec 19 06:50:18.330: INFO: stderr: ""
Dec 19 06:50:18.330: INFO: stdout: "deployment.apps/frontend created\n"
Dec 19 06:50:18.330: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 19 06:50:18.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-551'
Dec 19 06:50:18.460: INFO: stderr: ""
Dec 19 06:50:18.460: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 19 06:50:18.460: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 19 06:50:18.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-551'
Dec 19 06:50:18.607: INFO: stderr: ""
Dec 19 06:50:18.607: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 19 06:50:18.607: INFO: Waiting for all frontend pods to be Running.
Dec 19 06:50:23.657: INFO: Waiting for frontend to serve content.
Dec 19 06:50:23.702: INFO: Trying to add a new entry to the guestbook.
Dec 19 06:50:24.723: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 19 06:50:24.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-551'
Dec 19 06:50:24.844: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:50:24.844: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 19 06:50:24.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-551'
Dec 19 06:50:24.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:50:24.903: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 19 06:50:24.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-551'
Dec 19 06:50:24.959: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:50:24.959: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 19 06:50:24.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-551'
Dec 19 06:50:25.015: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:50:25.015: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 19 06:50:25.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-551'
Dec 19 06:50:25.071: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:50:25.071: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 19 06:50:25.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-551'
Dec 19 06:50:25.129: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 06:50:25.129: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:50:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-551" for this suite.
Dec 19 06:51:09.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:51:09.186: INFO: namespace kubectl-551 deletion completed in 44.055430202s

• [SLOW TEST:51.398 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:51:09.187: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-ab865cc5-afc2-467d-84bd-54541a0ed429
STEP: Creating a pod to test consume secrets
Dec 19 06:51:09.211: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda" in namespace "projected-1770" to be "success or failure"
Dec 19 06:51:09.219: INFO: Pod "pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda": Phase="Pending", Reason="", readiness=false. Elapsed: 7.740033ms
Dec 19 06:51:11.221: INFO: Pod "pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009969887s
STEP: Saw pod success
Dec 19 06:51:11.221: INFO: Pod "pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda" satisfied condition "success or failure"
Dec 19 06:51:11.222: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 19 06:51:11.231: INFO: Waiting for pod pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda to disappear
Dec 19 06:51:11.233: INFO: Pod pod-projected-secrets-78446ee0-1b2e-4562-b779-4bdce8ffacda no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:51:11.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1770" for this suite.
Dec 19 06:51:17.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:51:17.282: INFO: namespace projected-1770 deletion completed in 6.047469781s

• [SLOW TEST:8.096 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:51:17.282: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2wlgw in namespace proxy-4740
I1219 06:51:17.305064      17 runners.go:180] Created replication controller with name: proxy-service-2wlgw, namespace: proxy-4740, replica count: 1
I1219 06:51:18.355342      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1219 06:51:19.355475      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:20.355621      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:21.355759      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:22.355927      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:23.356073      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:24.356200      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:25.356355      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:26.356481      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:27.356652      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1219 06:51:28.356791      17 runners.go:180] proxy-service-2wlgw Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 19 06:51:28.358: INFO: setup took 11.059996382s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 19 06:51:28.367: INFO: (0) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 8.873704ms)
Dec 19 06:51:28.367: INFO: (0) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 8.832458ms)
Dec 19 06:51:28.367: INFO: (0) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 9.029972ms)
Dec 19 06:51:28.368: INFO: (0) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 9.63698ms)
Dec 19 06:51:28.368: INFO: (0) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 9.960905ms)
Dec 19 06:51:28.368: INFO: (0) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 10.003383ms)
Dec 19 06:51:28.368: INFO: (0) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 10.224654ms)
Dec 19 06:51:28.369: INFO: (0) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 10.559359ms)
Dec 19 06:51:28.369: INFO: (0) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 10.78967ms)
Dec 19 06:51:28.369: INFO: (0) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 10.899869ms)
Dec 19 06:51:28.369: INFO: (0) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 11.046759ms)
Dec 19 06:51:28.370: INFO: (0) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 11.939972ms)
Dec 19 06:51:28.373: INFO: (0) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 14.844208ms)
Dec 19 06:51:28.373: INFO: (0) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 14.935321ms)
Dec 19 06:51:28.373: INFO: (0) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 14.960519ms)
Dec 19 06:51:28.373: INFO: (0) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 15.062857ms)
Dec 19 06:51:28.376: INFO: (1) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 2.317753ms)
Dec 19 06:51:28.376: INFO: (1) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 2.695297ms)
Dec 19 06:51:28.376: INFO: (1) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 2.98472ms)
Dec 19 06:51:28.376: INFO: (1) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.064004ms)
Dec 19 06:51:28.377: INFO: (1) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.480466ms)
Dec 19 06:51:28.377: INFO: (1) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.495867ms)
Dec 19 06:51:28.377: INFO: (1) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.532434ms)
Dec 19 06:51:28.377: INFO: (1) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.547648ms)
Dec 19 06:51:28.377: INFO: (1) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.810443ms)
Dec 19 06:51:28.377: INFO: (1) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.740645ms)
Dec 19 06:51:28.378: INFO: (1) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.445216ms)
Dec 19 06:51:28.378: INFO: (1) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.475526ms)
Dec 19 06:51:28.378: INFO: (1) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.560699ms)
Dec 19 06:51:28.378: INFO: (1) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.593043ms)
Dec 19 06:51:28.378: INFO: (1) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.642478ms)
Dec 19 06:51:28.379: INFO: (1) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.594782ms)
Dec 19 06:51:28.381: INFO: (2) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 1.681487ms)
Dec 19 06:51:28.381: INFO: (2) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 1.809116ms)
Dec 19 06:51:28.382: INFO: (2) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.136684ms)
Dec 19 06:51:28.383: INFO: (2) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.364879ms)
Dec 19 06:51:28.383: INFO: (2) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.527215ms)
Dec 19 06:51:28.383: INFO: (2) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 4.462802ms)
Dec 19 06:51:28.383: INFO: (2) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 4.459212ms)
Dec 19 06:51:28.383: INFO: (2) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.510651ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.558464ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.572796ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 4.653902ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.772521ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 4.797918ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.799832ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 5.047748ms)
Dec 19 06:51:28.384: INFO: (2) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 5.007577ms)
Dec 19 06:51:28.387: INFO: (3) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.371711ms)
Dec 19 06:51:28.387: INFO: (3) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.290639ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.461243ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.435808ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.605224ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.601193ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 4.21817ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.26052ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.303576ms)
Dec 19 06:51:28.388: INFO: (3) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.3382ms)
Dec 19 06:51:28.389: INFO: (3) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.391657ms)
Dec 19 06:51:28.389: INFO: (3) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.535459ms)
Dec 19 06:51:28.389: INFO: (3) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 4.549217ms)
Dec 19 06:51:28.389: INFO: (3) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.650192ms)
Dec 19 06:51:28.389: INFO: (3) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.697123ms)
Dec 19 06:51:28.389: INFO: (3) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 5.392783ms)
Dec 19 06:51:28.391: INFO: (4) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 1.753526ms)
Dec 19 06:51:28.391: INFO: (4) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 1.93785ms)
Dec 19 06:51:28.392: INFO: (4) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 2.791959ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.074338ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.026681ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.126426ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.113522ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.232016ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 3.69481ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.677217ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.008915ms)
Dec 19 06:51:28.393: INFO: (4) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.94492ms)
Dec 19 06:51:28.394: INFO: (4) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.04084ms)
Dec 19 06:51:28.394: INFO: (4) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.055433ms)
Dec 19 06:51:28.394: INFO: (4) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.282298ms)
Dec 19 06:51:28.394: INFO: (4) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.267018ms)
Dec 19 06:51:28.396: INFO: (5) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 2.262563ms)
Dec 19 06:51:28.396: INFO: (5) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 2.32184ms)
Dec 19 06:51:28.396: INFO: (5) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 2.322555ms)
Dec 19 06:51:28.397: INFO: (5) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.531575ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.666255ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.650987ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.786779ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.779798ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 3.830157ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.835481ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.879235ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.300383ms)
Dec 19 06:51:28.398: INFO: (5) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.428618ms)
Dec 19 06:51:28.399: INFO: (5) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.468071ms)
Dec 19 06:51:28.399: INFO: (5) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 5.496808ms)
Dec 19 06:51:28.399: INFO: (5) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.555312ms)
Dec 19 06:51:28.402: INFO: (6) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 2.805938ms)
Dec 19 06:51:28.402: INFO: (6) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 2.937621ms)
Dec 19 06:51:28.402: INFO: (6) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.030817ms)
Dec 19 06:51:28.402: INFO: (6) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 2.970983ms)
Dec 19 06:51:28.402: INFO: (6) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.037097ms)
Dec 19 06:51:28.402: INFO: (6) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 2.991196ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.778875ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.780003ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 3.950504ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.877143ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 3.896202ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 3.988485ms)
Dec 19 06:51:28.403: INFO: (6) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.033013ms)
Dec 19 06:51:28.404: INFO: (6) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.052816ms)
Dec 19 06:51:28.404: INFO: (6) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.125736ms)
Dec 19 06:51:28.404: INFO: (6) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.91161ms)
Dec 19 06:51:28.408: INFO: (7) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.905024ms)
Dec 19 06:51:28.408: INFO: (7) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.98762ms)
Dec 19 06:51:28.408: INFO: (7) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.969923ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.153182ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 4.099929ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.095132ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.126827ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 4.25463ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.270657ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.371781ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.419949ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.488381ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.611098ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.764768ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.880743ms)
Dec 19 06:51:28.409: INFO: (7) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 4.912187ms)
Dec 19 06:51:28.413: INFO: (8) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.318432ms)
Dec 19 06:51:28.413: INFO: (8) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.463523ms)
Dec 19 06:51:28.413: INFO: (8) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.739166ms)
Dec 19 06:51:28.414: INFO: (8) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 5.024638ms)
Dec 19 06:51:28.415: INFO: (8) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 5.098349ms)
Dec 19 06:51:28.415: INFO: (8) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.258839ms)
Dec 19 06:51:28.415: INFO: (8) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.32389ms)
Dec 19 06:51:28.415: INFO: (8) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 5.413072ms)
Dec 19 06:51:28.415: INFO: (8) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 5.503789ms)
Dec 19 06:51:28.415: INFO: (8) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.997654ms)
Dec 19 06:51:28.416: INFO: (8) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 6.269985ms)
Dec 19 06:51:28.416: INFO: (8) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 6.294666ms)
Dec 19 06:51:28.416: INFO: (8) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 6.244415ms)
Dec 19 06:51:28.416: INFO: (8) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 6.230613ms)
Dec 19 06:51:28.416: INFO: (8) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 6.286427ms)
Dec 19 06:51:28.416: INFO: (8) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 6.507725ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.005181ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.06003ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.007164ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.101294ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.018856ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.116978ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.248118ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.347ms)
Dec 19 06:51:28.419: INFO: (9) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.306064ms)
Dec 19 06:51:28.420: INFO: (9) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.058661ms)
Dec 19 06:51:28.420: INFO: (9) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.1255ms)
Dec 19 06:51:28.420: INFO: (9) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.236036ms)
Dec 19 06:51:28.420: INFO: (9) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.235743ms)
Dec 19 06:51:28.420: INFO: (9) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.378703ms)
Dec 19 06:51:28.421: INFO: (9) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.68378ms)
Dec 19 06:51:28.421: INFO: (9) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 5.354861ms)
Dec 19 06:51:28.423: INFO: (10) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 1.870725ms)
Dec 19 06:51:28.423: INFO: (10) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 1.963722ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.32451ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.344793ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.303552ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.310839ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.332249ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.335909ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.411858ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.533566ms)
Dec 19 06:51:28.425: INFO: (10) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 3.650449ms)
Dec 19 06:51:28.426: INFO: (10) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.844586ms)
Dec 19 06:51:28.426: INFO: (10) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.852581ms)
Dec 19 06:51:28.426: INFO: (10) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.842038ms)
Dec 19 06:51:28.426: INFO: (10) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.950254ms)
Dec 19 06:51:28.426: INFO: (10) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.05355ms)
Dec 19 06:51:28.430: INFO: (11) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.494658ms)
Dec 19 06:51:28.430: INFO: (11) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.93293ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.187505ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 4.15655ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 4.3965ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.739143ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.668075ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 4.846839ms)
Dec 19 06:51:28.431: INFO: (11) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.835898ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.999233ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 5.114222ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.062075ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 5.147045ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 5.233434ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 5.20231ms)
Dec 19 06:51:28.432: INFO: (11) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.259703ms)
Dec 19 06:51:28.435: INFO: (12) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 2.940764ms)
Dec 19 06:51:28.435: INFO: (12) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 2.959255ms)
Dec 19 06:51:28.435: INFO: (12) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.026813ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.815443ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.806163ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.902406ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.965575ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.021059ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 4.054861ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 4.098724ms)
Dec 19 06:51:28.436: INFO: (12) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.126714ms)
Dec 19 06:51:28.437: INFO: (12) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.721244ms)
Dec 19 06:51:28.437: INFO: (12) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.884116ms)
Dec 19 06:51:28.437: INFO: (12) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.958378ms)
Dec 19 06:51:28.437: INFO: (12) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.972441ms)
Dec 19 06:51:28.437: INFO: (12) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.068708ms)
Dec 19 06:51:28.441: INFO: (13) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 4.261645ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.47291ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.525217ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.513512ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.529017ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 4.551924ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 4.567952ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 4.528947ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.622436ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.900589ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.944694ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 5.089137ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 5.130682ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 5.107936ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 5.260084ms)
Dec 19 06:51:28.442: INFO: (13) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.180783ms)
Dec 19 06:51:28.445: INFO: (14) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 2.913674ms)
Dec 19 06:51:28.446: INFO: (14) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.18496ms)
Dec 19 06:51:28.446: INFO: (14) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.207463ms)
Dec 19 06:51:28.446: INFO: (14) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.347131ms)
Dec 19 06:51:28.446: INFO: (14) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.32461ms)
Dec 19 06:51:28.446: INFO: (14) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.379397ms)
Dec 19 06:51:28.447: INFO: (14) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 5.01795ms)
Dec 19 06:51:28.447: INFO: (14) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 5.009212ms)
Dec 19 06:51:28.447: INFO: (14) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 5.002486ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 5.135301ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 5.17581ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 5.222115ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.299849ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.40594ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 5.584426ms)
Dec 19 06:51:28.448: INFO: (14) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.610765ms)
Dec 19 06:51:28.450: INFO: (15) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 1.736176ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 2.803095ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.021724ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.088457ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.047947ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 3.219752ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.240623ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.258604ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.329273ms)
Dec 19 06:51:28.451: INFO: (15) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.377699ms)
Dec 19 06:51:28.453: INFO: (15) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 5.087428ms)
Dec 19 06:51:28.453: INFO: (15) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.019395ms)
Dec 19 06:51:28.453: INFO: (15) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.109604ms)
Dec 19 06:51:28.453: INFO: (15) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 5.236358ms)
Dec 19 06:51:28.453: INFO: (15) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 5.286437ms)
Dec 19 06:51:28.453: INFO: (15) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.278595ms)
Dec 19 06:51:28.456: INFO: (16) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 2.348986ms)
Dec 19 06:51:28.457: INFO: (16) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.926748ms)
Dec 19 06:51:28.457: INFO: (16) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 3.979056ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.099001ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.21202ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 4.23107ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.294747ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.462011ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.409142ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 4.406594ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 4.526662ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.531004ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.692458ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 4.885228ms)
Dec 19 06:51:28.458: INFO: (16) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 4.984097ms)
Dec 19 06:51:28.459: INFO: (16) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.054394ms)
Dec 19 06:51:28.461: INFO: (17) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 2.078906ms)
Dec 19 06:51:28.461: INFO: (17) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 2.225064ms)
Dec 19 06:51:28.461: INFO: (17) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 2.30059ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 4.867974ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.951084ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 5.001096ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.957799ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 5.14034ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 5.158221ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 5.300716ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.422283ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.425766ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 5.455057ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 5.527908ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 5.546923ms)
Dec 19 06:51:28.464: INFO: (17) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 5.661951ms)
Dec 19 06:51:28.467: INFO: (18) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 2.526491ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 3.445753ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 3.750656ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 3.69719ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 3.753345ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 3.748463ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 3.918042ms)
Dec 19 06:51:28.468: INFO: (18) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 4.10546ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 4.330425ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.476552ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.465905ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 4.6199ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 4.765097ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 4.822993ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.956777ms)
Dec 19 06:51:28.469: INFO: (18) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 4.983632ms)
Dec 19 06:51:28.471: INFO: (19) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 1.743411ms)
Dec 19 06:51:28.472: INFO: (19) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:462/proxy/: tls qux (200; 2.680781ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj/proxy/rewriteme">test</a> (200; 4.105405ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.08466ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:460/proxy/: tls baz (200; 4.359331ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:162/proxy/: bar (200; 4.316956ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/https:proxy-service-2wlgw-gdzwj:443/proxy/tlsrewritem... (200; 4.326838ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/http:proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">... (200; 4.353879ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:160/proxy/: foo (200; 4.507057ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname1/proxy/: foo (200; 4.537181ms)
Dec 19 06:51:28.474: INFO: (19) /api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-4740/pods/proxy-service-2wlgw-gdzwj:1080/proxy/rewriteme">test<... (200; 4.528604ms)
Dec 19 06:51:28.475: INFO: (19) /api/v1/namespaces/proxy-4740/services/http:proxy-service-2wlgw:portname2/proxy/: bar (200; 5.13066ms)
Dec 19 06:51:28.475: INFO: (19) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname1/proxy/: foo (200; 5.442657ms)
Dec 19 06:51:28.475: INFO: (19) /api/v1/namespaces/proxy-4740/services/proxy-service-2wlgw:portname2/proxy/: bar (200; 5.404942ms)
Dec 19 06:51:28.475: INFO: (19) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname2/proxy/: tls qux (200; 5.60272ms)
Dec 19 06:51:28.475: INFO: (19) /api/v1/namespaces/proxy-4740/services/https:proxy-service-2wlgw:tlsportname1/proxy/: tls baz (200; 5.684838ms)
STEP: deleting ReplicationController proxy-service-2wlgw in namespace proxy-4740, will wait for the garbage collector to delete the pods
Dec 19 06:51:28.530: INFO: Deleting ReplicationController proxy-service-2wlgw took: 3.274624ms
Dec 19 06:51:28.630: INFO: Terminating ReplicationController proxy-service-2wlgw pods took: 100.171271ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:51:36.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4740" for this suite.
Dec 19 06:51:42.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:51:42.686: INFO: namespace proxy-4740 deletion completed in 6.053518688s

• [SLOW TEST:25.403 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:51:42.686: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-k4tv
STEP: Creating a pod to test atomic-volume-subpath
Dec 19 06:51:42.713: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-k4tv" in namespace "subpath-3759" to be "success or failure"
Dec 19 06:51:42.717: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.350923ms
Dec 19 06:51:44.719: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005997061s
Dec 19 06:51:46.721: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 4.008134357s
Dec 19 06:51:48.723: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 6.010193086s
Dec 19 06:51:50.725: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 8.012299228s
Dec 19 06:51:52.728: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 10.014587757s
Dec 19 06:51:54.730: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 12.017255043s
Dec 19 06:51:56.732: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 14.019464839s
Dec 19 06:51:58.734: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 16.021565835s
Dec 19 06:52:00.737: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 18.023657277s
Dec 19 06:52:02.739: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 20.025831385s
Dec 19 06:52:04.741: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Running", Reason="", readiness=true. Elapsed: 22.027883901s
Dec 19 06:52:06.743: INFO: Pod "pod-subpath-test-configmap-k4tv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.030060392s
STEP: Saw pod success
Dec 19 06:52:06.743: INFO: Pod "pod-subpath-test-configmap-k4tv" satisfied condition "success or failure"
Dec 19 06:52:06.745: INFO: Trying to get logs from node 192.168.0.132 pod pod-subpath-test-configmap-k4tv container test-container-subpath-configmap-k4tv: <nil>
STEP: delete the pod
Dec 19 06:52:06.755: INFO: Waiting for pod pod-subpath-test-configmap-k4tv to disappear
Dec 19 06:52:06.756: INFO: Pod pod-subpath-test-configmap-k4tv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-k4tv
Dec 19 06:52:06.757: INFO: Deleting pod "pod-subpath-test-configmap-k4tv" in namespace "subpath-3759"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:52:06.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3759" for this suite.
Dec 19 06:52:12.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:52:12.809: INFO: namespace subpath-3759 deletion completed in 6.048757941s

• [SLOW TEST:30.123 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:52:12.809: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 06:52:12.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450" in namespace "projected-3495" to be "success or failure"
Dec 19 06:52:12.848: INFO: Pod "downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450": Phase="Pending", Reason="", readiness=false. Elapsed: 9.419313ms
Dec 19 06:52:14.851: INFO: Pod "downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011767085s
STEP: Saw pod success
Dec 19 06:52:14.851: INFO: Pod "downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450" satisfied condition "success or failure"
Dec 19 06:52:14.852: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450 container client-container: <nil>
STEP: delete the pod
Dec 19 06:52:14.863: INFO: Waiting for pod downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450 to disappear
Dec 19 06:52:14.865: INFO: Pod downwardapi-volume-bd9bfe3a-d534-4d46-b349-b23b3cc74450 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:52:14.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3495" for this suite.
Dec 19 06:52:20.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:52:20.919: INFO: namespace projected-3495 deletion completed in 6.052237043s

• [SLOW TEST:8.109 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:52:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 06:52:20.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-732'
Dec 19 06:52:21.064: INFO: stderr: ""
Dec 19 06:52:21.064: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 19 06:52:21.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-732'
Dec 19 06:52:21.188: INFO: stderr: ""
Dec 19 06:52:21.188: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 19 06:52:22.191: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:52:22.191: INFO: Found 0 / 1
Dec 19 06:52:23.191: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:52:23.191: INFO: Found 0 / 1
Dec 19 06:52:24.191: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:52:24.191: INFO: Found 1 / 1
Dec 19 06:52:24.191: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 19 06:52:24.193: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 06:52:24.193: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 19 06:52:24.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 describe pod redis-master-5bt6c --namespace=kubectl-732'
Dec 19 06:52:24.254: INFO: stderr: ""
Dec 19 06:52:24.254: INFO: stdout: "Name:           redis-master-5bt6c\nNamespace:      kubectl-732\nPriority:       0\nNode:           192.168.0.132/192.168.0.132\nStart Time:     Thu, 19 Dec 2019 06:52:21 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.16.0.30\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://7df19a1b1f0a694df687213518cff8eb61c8daca4f065d30d58f97ece757f1e3\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Dec 2019 06:52:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-q62j9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-q62j9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-q62j9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason                 Age              From                    Message\n  ----    ------                 ----             ----                    -------\n  Normal  Scheduled              3s               default-scheduler       Successfully assigned kubectl-732/redis-master-5bt6c to 192.168.0.132\n  Normal  Pulled                 2s               kubelet, 192.168.0.132  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  SuccessfulCreate       2s               kubelet, 192.168.0.132  Created container redis-master\n  Normal  Started                1s               kubelet, 192.168.0.132  Started container redis-master\n  Normal  SuccessfulMountVolume  0s (x2 over 3s)  kubelet, 192.168.0.132  Successfully mounted volumes for pod \"redis-master-5bt6c_kubectl-732(d58ca55a-3f58-45a5-a56d-345272891be5)\"\n"
Dec 19 06:52:24.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 describe rc redis-master --namespace=kubectl-732'
Dec 19 06:52:24.317: INFO: stderr: ""
Dec 19 06:52:24.317: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-732\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-5bt6c\n"
Dec 19 06:52:24.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 describe service redis-master --namespace=kubectl-732'
Dec 19 06:52:24.375: INFO: stderr: ""
Dec 19 06:52:24.375: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-732\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.247.207.134\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.0.30:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 19 06:52:24.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 describe node 192.168.0.132'
Dec 19 06:52:24.445: INFO: stderr: ""
Dec 19 06:52:24.445: INFO: stdout: "Name:               192.168.0.132\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/is-baremetal=false\n                    failure-domain.beta.kubernetes.io/region=cn-north-4\n                    failure-domain.beta.kubernetes.io/zone=cn-north-4a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/availablezone=cn-north-4a\n                    kubernetes.io/eniquota=2\n                    kubernetes.io/hostname=192.168.0.132\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/subnetid=e0590121-d106-43cd-976d-ca98d4e03347\n                    os.architecture=amd64\n                    os.name=CentOS_Linux_7_Core\n                    os.version=3.10.0-1062.1.1.el7.x86_64\nAnnotations:        cce.io/gpu-status: []\n                    csi.volume.kubernetes.io/nodeid: {\"sfsturbo.csi.everest.io\":\"4063dd9f-265a-4cbc-93f8-7b7c11a8c8eb\"}\n                    node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Thu, 19 Dec 2019 01:14:36 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Dec 2019 06:51:45 +0000   Thu, 19 Dec 2019 01:14:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Dec 2019 06:51:45 +0000   Thu, 19 Dec 2019 01:14:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Dec 2019 06:51:45 +0000   Thu, 19 Dec 2019 01:14:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Dec 2019 06:51:45 +0000   Thu, 19 Dec 2019 01:14:36 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.132\n  Hostname:    192.168.0.132\nCapacity:\n cce/eni:            1\n cpu:                8\n ephemeral-storage:  10186004Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16265268Ki\n pods:               110\nAllocatable:\n cce/eni:            1\n cpu:                7910m\n ephemeral-storage:  9387421271\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13749300Ki\n pods:               110\nSystem Info:\n Machine ID:                 4063dd9f-265a-4cbc-93f8-7b7c11a8c8eb\n System UUID:                B4C437FD-9CB5-4E42-9CE8-91B22155339E\n Boot ID:                    3da76990-dddc-466c-82fc-4f6b890e515e\n Kernel Version:             3.10.0-1062.1.1.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.0\n Kubelet Version:            v1.15.6-r0-CCE2.0.26.B001\n Kube-Proxy Version:         v1.15.6-r0-CCE2.0.26.B001\nProviderID:                  b61ff720-21fc-11ea-970a-0255ac101527\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    aaaaaaa-cq9f6                                              250m (3%)     250m (3%)   512Mi (3%)       512Mi (3%)     2m48s\n  default                    test-liyi-6668b4c9f8-4qzm2                                 250m (3%)     250m (3%)   512Mi (3%)       512Mi (3%)     66m\n  default                    test-liyi-6668b4c9f8-c2fsf                                 250m (3%)     250m (3%)   512Mi (3%)       512Mi (3%)     66m\n  default                    test-liyi-6668b4c9f8-pkxvx                                 250m (3%)     250m (3%)   512Mi (3%)       512Mi (3%)     4h17m\n  default                    test-liyi-6668b4c9f8-xkqdd                                 250m (3%)     250m (3%)   512Mi (3%)       512Mi (3%)     70m\n  kube-system                everest-csi-driver-w2xrr                                   100m (1%)     100m (1%)   300Mi (2%)       300Mi (2%)     33m\n  kube-system                icagent-kh9cq                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h37m\n  kubectl-732                redis-master-5bt6c                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-8b4v2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         12m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1350m (17%)   1350m (17%)\n  memory             2860Mi (21%)  2860Mi (21%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  cce/eni            0             0\nEvents:              <none>\n"
Dec 19 06:52:24.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 describe namespace kubectl-732'
Dec 19 06:52:24.503: INFO: stderr: ""
Dec 19 06:52:24.503: INFO: stdout: "Name:         kubectl-732\nLabels:       e2e-framework=kubectl\n              e2e-run=fca76fc9-7bfc-44ba-8e5e-5852ecac9621\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:52:24.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-732" for this suite.
Dec 19 06:52:46.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:52:46.564: INFO: namespace kubectl-732 deletion completed in 22.060000423s

• [SLOW TEST:25.646 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:52:46.565: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 06:52:46.595: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e223c82e-f731-489d-8a65-3b0a9bdfafb7", Controller:(*bool)(0xc0008dcc86), BlockOwnerDeletion:(*bool)(0xc0008dcc87)}}
Dec 19 06:52:46.601: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a30f3454-0f29-4f83-9681-f8e0d7e50492", Controller:(*bool)(0xc00258a6a6), BlockOwnerDeletion:(*bool)(0xc00258a6a7)}}
Dec 19 06:52:46.605: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d45f0994-e3d9-445d-a644-bc285d3bdba2", Controller:(*bool)(0xc0008dce72), BlockOwnerDeletion:(*bool)(0xc0008dce73)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:52:51.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-934" for this suite.
Dec 19 06:52:57.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:52:57.668: INFO: namespace gc-934 deletion completed in 6.054061057s

• [SLOW TEST:11.104 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:52:57.669: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6731
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6731 to expose endpoints map[]
Dec 19 06:52:57.722: INFO: Get endpoints failed (5.24329ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 19 06:52:58.724: INFO: successfully validated that service multi-endpoint-test in namespace services-6731 exposes endpoints map[] (1.007267347s elapsed)
STEP: Creating pod pod1 in namespace services-6731
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6731 to expose endpoints map[pod1:[100]]
Dec 19 06:53:01.746: INFO: successfully validated that service multi-endpoint-test in namespace services-6731 exposes endpoints map[pod1:[100]] (3.017091837s elapsed)
STEP: Creating pod pod2 in namespace services-6731
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6731 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 19 06:53:03.765: INFO: successfully validated that service multi-endpoint-test in namespace services-6731 exposes endpoints map[pod1:[100] pod2:[101]] (2.017202572s elapsed)
STEP: Deleting pod pod1 in namespace services-6731
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6731 to expose endpoints map[pod2:[101]]
Dec 19 06:53:04.776: INFO: successfully validated that service multi-endpoint-test in namespace services-6731 exposes endpoints map[pod2:[101]] (1.008484722s elapsed)
STEP: Deleting pod pod2 in namespace services-6731
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6731 to expose endpoints map[]
Dec 19 06:53:05.782: INFO: successfully validated that service multi-endpoint-test in namespace services-6731 exposes endpoints map[] (1.003454108s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:53:05.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6731" for this suite.
Dec 19 06:53:27.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:53:27.852: INFO: namespace services-6731 deletion completed in 22.057281005s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:30.183 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:53:27.852: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 19 06:53:31.879: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8f0da0b4-2968-4bf3-a3e8-e4e29034cf80,GenerateName:,Namespace:events-8088,SelfLink:/api/v1/namespaces/events-8088/pods/send-events-8f0da0b4-2968-4bf3-a3e8-e4e29034cf80,UID:7e11511c-db07-4151-80ef-19cc7e5a9404,ResourceVersion:658222,Generation:0,CreationTimestamp:2019-12-19 06:53:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 869459816,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t6x4x {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t6x4x,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-t6x4x true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028c3860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028c3880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{timeout 0xc00027b170} {single-request-reopen 0xc00027b1d0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:53:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:53:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:53:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 06:53:27 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.20,StartTime:2019-12-19 06:53:27 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-19 06:53:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://3beb8830e9b53c0f5a9250d5210a94bf63b079b1b0ff7dee98785aca95498408}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec 19 06:53:33.882: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 19 06:53:35.884: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:53:35.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8088" for this suite.
Dec 19 06:54:17.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:54:17.941: INFO: namespace events-8088 deletion completed in 42.05265031s

• [SLOW TEST:50.089 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:54:17.941: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:54:23.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4771" for this suite.
Dec 19 06:54:29.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:54:29.596: INFO: namespace watch-4771 deletion completed in 6.151601959s

• [SLOW TEST:11.655 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:54:29.596: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 19 06:54:29.615: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 19 06:54:34.617: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:54:35.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-618" for this suite.
Dec 19 06:54:41.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:54:41.678: INFO: namespace replication-controller-618 deletion completed in 6.048057546s

• [SLOW TEST:12.082 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:54:41.678: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-167c4e08-c945-4a6a-b7a7-32fa24d4148f
STEP: Creating a pod to test consume secrets
Dec 19 06:54:41.700: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645" in namespace "projected-9947" to be "success or failure"
Dec 19 06:54:41.702: INFO: Pod "pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645": Phase="Pending", Reason="", readiness=false. Elapsed: 1.932453ms
Dec 19 06:54:43.704: INFO: Pod "pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004085524s
Dec 19 06:54:45.707: INFO: Pod "pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006728229s
STEP: Saw pod success
Dec 19 06:54:45.707: INFO: Pod "pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645" satisfied condition "success or failure"
Dec 19 06:54:45.708: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 19 06:54:45.719: INFO: Waiting for pod pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645 to disappear
Dec 19 06:54:45.720: INFO: Pod pod-projected-secrets-4d6fc6e2-10fc-40a0-a73b-1123e411e645 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:54:45.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9947" for this suite.
Dec 19 06:54:51.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:54:51.776: INFO: namespace projected-9947 deletion completed in 6.05423803s

• [SLOW TEST:10.098 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:54:51.776: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 19 06:54:51.796: INFO: Waiting up to 5m0s for pod "pod-44c5ffb0-4033-4a93-abca-268f01faef1b" in namespace "emptydir-2214" to be "success or failure"
Dec 19 06:54:51.799: INFO: Pod "pod-44c5ffb0-4033-4a93-abca-268f01faef1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.365257ms
Dec 19 06:54:53.801: INFO: Pod "pod-44c5ffb0-4033-4a93-abca-268f01faef1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004573777s
Dec 19 06:54:55.803: INFO: Pod "pod-44c5ffb0-4033-4a93-abca-268f01faef1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006577467s
STEP: Saw pod success
Dec 19 06:54:55.803: INFO: Pod "pod-44c5ffb0-4033-4a93-abca-268f01faef1b" satisfied condition "success or failure"
Dec 19 06:54:55.804: INFO: Trying to get logs from node 192.168.0.132 pod pod-44c5ffb0-4033-4a93-abca-268f01faef1b container test-container: <nil>
STEP: delete the pod
Dec 19 06:54:55.812: INFO: Waiting for pod pod-44c5ffb0-4033-4a93-abca-268f01faef1b to disappear
Dec 19 06:54:55.813: INFO: Pod pod-44c5ffb0-4033-4a93-abca-268f01faef1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:54:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2214" for this suite.
Dec 19 06:55:01.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:55:01.867: INFO: namespace emptydir-2214 deletion completed in 6.05169663s

• [SLOW TEST:10.091 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:55:01.867: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:55:28.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6250" for this suite.
Dec 19 06:55:34.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:55:34.061: INFO: namespace container-runtime-6250 deletion completed in 6.049062881s

• [SLOW TEST:32.194 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:55:34.061: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 19 06:55:36.093: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:55:36.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2514" for this suite.
Dec 19 06:55:42.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:55:42.160: INFO: namespace container-runtime-2514 deletion completed in 6.055795705s

• [SLOW TEST:8.099 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:55:42.161: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7563
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7563
STEP: Creating statefulset with conflicting port in namespace statefulset-7563
STEP: Waiting until pod test-pod will start running in namespace statefulset-7563
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7563
Dec 19 06:55:44.201: INFO: Observed stateful pod in namespace: statefulset-7563, name: ss-0, uid: 17e38044-381f-4474-a32f-e362c3700c0a, status phase: Pending. Waiting for statefulset controller to delete.
Dec 19 06:55:46.624: INFO: Observed stateful pod in namespace: statefulset-7563, name: ss-0, uid: 17e38044-381f-4474-a32f-e362c3700c0a, status phase: Failed. Waiting for statefulset controller to delete.
Dec 19 06:55:46.629: INFO: Observed stateful pod in namespace: statefulset-7563, name: ss-0, uid: 17e38044-381f-4474-a32f-e362c3700c0a, status phase: Failed. Waiting for statefulset controller to delete.
Dec 19 06:55:46.631: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7563
STEP: Removing pod with conflicting port in namespace statefulset-7563
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7563 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 19 06:55:50.649: INFO: Deleting all statefulset in ns statefulset-7563
Dec 19 06:55:50.650: INFO: Scaling statefulset ss to 0
Dec 19 06:56:00.658: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 06:56:00.659: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:56:00.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7563" for this suite.
Dec 19 06:56:06.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:56:06.719: INFO: namespace statefulset-7563 deletion completed in 6.050735514s

• [SLOW TEST:24.559 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:56:06.719: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 06:56:06.747: INFO: Create a RollingUpdate DaemonSet
Dec 19 06:56:06.749: INFO: Check that daemon pods launch on every node of the cluster
Dec 19 06:56:06.752: INFO: Number of nodes with available pods: 0
Dec 19 06:56:06.752: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:56:07.757: INFO: Number of nodes with available pods: 0
Dec 19 06:56:07.757: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 06:56:08.757: INFO: Number of nodes with available pods: 1
Dec 19 06:56:08.757: INFO: Node 192.168.0.205 is running more than one daemon pod
Dec 19 06:56:09.756: INFO: Number of nodes with available pods: 2
Dec 19 06:56:09.757: INFO: Number of running nodes: 2, number of available pods: 2
Dec 19 06:56:09.757: INFO: Update the DaemonSet to trigger a rollout
Dec 19 06:56:09.760: INFO: Updating DaemonSet daemon-set
Dec 19 06:56:18.769: INFO: Roll back the DaemonSet before rollout is complete
Dec 19 06:56:18.773: INFO: Updating DaemonSet daemon-set
Dec 19 06:56:18.773: INFO: Make sure DaemonSet rollback is complete
Dec 19 06:56:18.780: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:18.780: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:19.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:19.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:20.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:20.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:21.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:21.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:22.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:22.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:23.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:23.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:24.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:24.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:25.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:25.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:26.784: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:26.784: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:27.785: INFO: Wrong image for pod: daemon-set-vjkbq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 19 06:56:27.785: INFO: Pod daemon-set-vjkbq is not available
Dec 19 06:56:28.784: INFO: Pod daemon-set-hm45d is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-278, will wait for the garbage collector to delete the pods
Dec 19 06:56:28.843: INFO: Deleting DaemonSet.extensions daemon-set took: 3.333677ms
Dec 19 06:56:28.944: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.127471ms
Dec 19 06:56:30.846: INFO: Number of nodes with available pods: 0
Dec 19 06:56:30.846: INFO: Number of running nodes: 0, number of available pods: 0
Dec 19 06:56:30.847: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-278/daemonsets","resourceVersion":"659533"},"items":null}

Dec 19 06:56:30.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-278/pods","resourceVersion":"659533"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:56:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-278" for this suite.
Dec 19 06:56:36.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:56:36.905: INFO: namespace daemonsets-278 deletion completed in 6.050022994s

• [SLOW TEST:30.186 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:56:36.905: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:56:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9039" for this suite.
Dec 19 06:56:46.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:56:46.983: INFO: namespace kubelet-test-9039 deletion completed in 6.051692509s

• [SLOW TEST:10.078 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:56:46.983: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-fc983c7f-8a45-4581-a0f3-02d9e80bc905 in namespace container-probe-461
Dec 19 06:56:49.008: INFO: Started pod busybox-fc983c7f-8a45-4581-a0f3-02d9e80bc905 in namespace container-probe-461
STEP: checking the pod's current state and verifying that restartCount is present
Dec 19 06:56:49.009: INFO: Initial restart count of pod busybox-fc983c7f-8a45-4581-a0f3-02d9e80bc905 is 0
Dec 19 06:57:43.069: INFO: Restart count of pod container-probe-461/busybox-fc983c7f-8a45-4581-a0f3-02d9e80bc905 is now 1 (54.059780439s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:57:43.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-461" for this suite.
Dec 19 06:57:49.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:57:49.129: INFO: namespace container-probe-461 deletion completed in 6.050769362s

• [SLOW TEST:62.146 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:57:49.130: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1219 06:57:59.209771      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 19 06:57:59.209: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:57:59.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7584" for this suite.
Dec 19 06:58:05.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 06:58:05.270: INFO: namespace gc-7584 deletion completed in 6.059079213s

• [SLOW TEST:16.141 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 06:58:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 06:58:05.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-9197'
Dec 19 06:58:05.634: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 19 06:58:05.634: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Dec 19 06:58:07.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9197'
Dec 19 06:58:07.703: INFO: stderr: ""
Dec 19 06:58:07.703: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 06:58:07.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9197" for this suite.
Dec 19 07:00:07.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:00:07.754: INFO: namespace kubectl-9197 deletion completed in 2m0.048826392s

• [SLOW TEST:122.484 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:00:07.754: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec 19 07:00:08.283: INFO: created pod pod-service-account-defaultsa
Dec 19 07:00:08.283: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 19 07:00:08.286: INFO: created pod pod-service-account-mountsa
Dec 19 07:00:08.286: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 19 07:00:08.288: INFO: created pod pod-service-account-nomountsa
Dec 19 07:00:08.288: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 19 07:00:08.291: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 19 07:00:08.291: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 19 07:00:08.294: INFO: created pod pod-service-account-mountsa-mountspec
Dec 19 07:00:08.294: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 19 07:00:08.307: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 19 07:00:08.307: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 19 07:00:08.312: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 19 07:00:08.312: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 19 07:00:08.315: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 19 07:00:08.315: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 19 07:00:08.321: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 19 07:00:08.321: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:00:08.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7192" for this suite.
Dec 19 07:00:30.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:00:30.387: INFO: namespace svcaccounts-7192 deletion completed in 22.062267699s

• [SLOW TEST:22.633 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:00:30.387: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1219 07:01:10.422997      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 19 07:01:10.423: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:01:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-50" for this suite.
Dec 19 07:01:16.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:01:16.474: INFO: namespace gc-50 deletion completed in 6.049432174s

• [SLOW TEST:46.087 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:01:16.474: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-71067dc5-7011-4358-a464-f25d2f9d5e1e
STEP: Creating a pod to test consume configMaps
Dec 19 07:01:16.506: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45" in namespace "configmap-7727" to be "success or failure"
Dec 19 07:01:16.511: INFO: Pod "pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.404854ms
Dec 19 07:01:18.513: INFO: Pod "pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006432471s
Dec 19 07:01:20.515: INFO: Pod "pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008586651s
STEP: Saw pod success
Dec 19 07:01:20.515: INFO: Pod "pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45" satisfied condition "success or failure"
Dec 19 07:01:20.516: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:01:20.531: INFO: Waiting for pod pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45 to disappear
Dec 19 07:01:20.533: INFO: Pod pod-configmaps-8b676328-8ba5-46aa-878f-0e4491e28f45 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:01:20.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7727" for this suite.
Dec 19 07:01:26.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:01:26.585: INFO: namespace configmap-7727 deletion completed in 6.04972518s

• [SLOW TEST:10.111 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:01:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:01:48.613: INFO: Container started at 2019-12-19 07:01:28 +0000 UTC, pod became ready at 2019-12-19 07:01:47 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:01:48.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2785" for this suite.
Dec 19 07:02:10.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:02:10.665: INFO: namespace container-probe-2785 deletion completed in 22.049635585s

• [SLOW TEST:44.080 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:02:10.665: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:02:10.692: INFO: (0) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.346376ms)
Dec 19 07:02:10.694: INFO: (1) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.073672ms)
Dec 19 07:02:10.696: INFO: (2) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.00175ms)
Dec 19 07:02:10.698: INFO: (3) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.825552ms)
Dec 19 07:02:10.700: INFO: (4) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.89937ms)
Dec 19 07:02:10.702: INFO: (5) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.937448ms)
Dec 19 07:02:10.704: INFO: (6) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.741074ms)
Dec 19 07:02:10.706: INFO: (7) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.875041ms)
Dec 19 07:02:10.707: INFO: (8) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.717491ms)
Dec 19 07:02:10.709: INFO: (9) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.778233ms)
Dec 19 07:02:10.711: INFO: (10) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.875693ms)
Dec 19 07:02:10.713: INFO: (11) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.765239ms)
Dec 19 07:02:10.715: INFO: (12) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.966146ms)
Dec 19 07:02:10.717: INFO: (13) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.847279ms)
Dec 19 07:02:10.718: INFO: (14) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.735173ms)
Dec 19 07:02:10.720: INFO: (15) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.884231ms)
Dec 19 07:02:10.722: INFO: (16) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.908193ms)
Dec 19 07:02:10.724: INFO: (17) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.765114ms)
Dec 19 07:02:10.726: INFO: (18) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.904291ms)
Dec 19 07:02:10.728: INFO: (19) /api/v1/nodes/192.168.0.132/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.829133ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:02:10.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1026" for this suite.
Dec 19 07:02:16.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:02:16.779: INFO: namespace proxy-1026 deletion completed in 6.049450475s

• [SLOW TEST:6.114 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:02:16.779: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2871
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 19 07:02:16.801: INFO: Found 0 stateful pods, waiting for 3
Dec 19 07:02:26.804: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 07:02:26.804: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 07:02:26.804: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 07:02:26.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-2871 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 07:02:26.989: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 07:02:26.989: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 07:02:26.989: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 19 07:02:37.010: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 19 07:02:47.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-2871 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 07:02:47.128: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 07:02:47.128: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 07:02:47.128: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 07:02:57.138: INFO: Waiting for StatefulSet statefulset-2871/ss2 to complete update
Dec 19 07:02:57.138: INFO: Waiting for Pod statefulset-2871/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 19 07:02:57.138: INFO: Waiting for Pod statefulset-2871/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 19 07:03:07.144: INFO: Waiting for StatefulSet statefulset-2871/ss2 to complete update
Dec 19 07:03:07.144: INFO: Waiting for Pod statefulset-2871/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 19 07:03:07.144: INFO: Waiting for Pod statefulset-2871/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec 19 07:03:17.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-2871 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 07:03:17.323: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 07:03:17.323: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 07:03:17.323: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 07:03:27.345: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 19 07:03:37.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-2871 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 07:03:37.466: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 07:03:37.466: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 07:03:37.466: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 07:03:47.476: INFO: Waiting for StatefulSet statefulset-2871/ss2 to complete update
Dec 19 07:03:47.477: INFO: Waiting for Pod statefulset-2871/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec 19 07:03:47.477: INFO: Waiting for Pod statefulset-2871/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec 19 07:03:57.481: INFO: Waiting for StatefulSet statefulset-2871/ss2 to complete update
Dec 19 07:03:57.481: INFO: Waiting for Pod statefulset-2871/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec 19 07:03:57.481: INFO: Waiting for Pod statefulset-2871/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec 19 07:04:07.481: INFO: Waiting for StatefulSet statefulset-2871/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 19 07:04:17.480: INFO: Deleting all statefulset in ns statefulset-2871
Dec 19 07:04:17.482: INFO: Scaling statefulset ss2 to 0
Dec 19 07:04:37.491: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 07:04:37.492: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:04:37.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2871" for this suite.
Dec 19 07:04:43.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:04:43.552: INFO: namespace statefulset-2871 deletion completed in 6.051087092s

• [SLOW TEST:146.773 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:04:43.552: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-6707
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6707
STEP: Deleting pre-stop pod
Dec 19 07:04:56.590: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:04:56.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6707" for this suite.
Dec 19 07:05:34.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:05:34.644: INFO: namespace prestop-6707 deletion completed in 38.048690543s

• [SLOW TEST:51.092 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:05:34.644: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec 19 07:05:34.660: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-317660674 proxy --unix-socket=/tmp/kubectl-proxy-unix097517433/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:05:34.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9467" for this suite.
Dec 19 07:05:40.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:05:40.752: INFO: namespace kubectl-9467 deletion completed in 6.050030514s

• [SLOW TEST:6.108 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:05:40.752: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 19 07:05:40.774: INFO: Waiting up to 5m0s for pod "pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe" in namespace "emptydir-5730" to be "success or failure"
Dec 19 07:05:40.778: INFO: Pod "pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.570983ms
Dec 19 07:05:42.780: INFO: Pod "pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005603883s
Dec 19 07:05:44.782: INFO: Pod "pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007720162s
STEP: Saw pod success
Dec 19 07:05:44.782: INFO: Pod "pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe" satisfied condition "success or failure"
Dec 19 07:05:44.783: INFO: Trying to get logs from node 192.168.0.132 pod pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe container test-container: <nil>
STEP: delete the pod
Dec 19 07:05:44.795: INFO: Waiting for pod pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe to disappear
Dec 19 07:05:44.796: INFO: Pod pod-1a14694a-2dcf-40e1-8cd2-a900eff9cfbe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:05:44.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5730" for this suite.
Dec 19 07:05:50.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:05:50.848: INFO: namespace emptydir-5730 deletion completed in 6.04977569s

• [SLOW TEST:10.096 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:05:50.848: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 19 07:05:56.896: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:05:56.897: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:05:58.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:05:58.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:00.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:00.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:02.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:02.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:04.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:04.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:06.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:06.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:08.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:08.901: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:10.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:10.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:12.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:12.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:14.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:14.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:16.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:16.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:18.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:18.901: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:20.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:20.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:22.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:22.900: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:24.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:24.899: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 19 07:06:26.897: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 19 07:06:26.899: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:06:26.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9701" for this suite.
Dec 19 07:06:48.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:06:48.957: INFO: namespace container-lifecycle-hook-9701 deletion completed in 22.055671757s

• [SLOW TEST:58.109 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:06:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:06:48.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9180" for this suite.
Dec 19 07:06:54.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:06:55.040: INFO: namespace kubelet-test-9180 deletion completed in 6.049376697s

• [SLOW TEST:6.083 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:06:55.040: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 19 07:06:59.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec pod-sharedvolume-867b2031-0568-43ad-ab9f-a26a33a28166 -c busybox-main-container --namespace=emptydir-8873 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 19 07:06:59.174: INFO: stderr: ""
Dec 19 07:06:59.174: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:06:59.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8873" for this suite.
Dec 19 07:07:05.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:07:05.230: INFO: namespace emptydir-8873 deletion completed in 6.05345793s

• [SLOW TEST:10.190 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:07:05.230: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 19 07:07:05.250: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 19 07:07:12.268: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:07:12.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1618" for this suite.
Dec 19 07:07:18.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:07:18.320: INFO: namespace pods-1618 deletion completed in 6.048709923s

• [SLOW TEST:13.090 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:07:18.320: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-e3f4d047-09a1-4c89-89dd-2f616aea82aa
STEP: Creating a pod to test consume secrets
Dec 19 07:07:18.345: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149" in namespace "projected-2452" to be "success or failure"
Dec 19 07:07:18.348: INFO: Pod "pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149": Phase="Pending", Reason="", readiness=false. Elapsed: 3.32469ms
Dec 19 07:07:20.350: INFO: Pod "pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005237584s
STEP: Saw pod success
Dec 19 07:07:20.350: INFO: Pod "pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149" satisfied condition "success or failure"
Dec 19 07:07:20.351: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:07:20.363: INFO: Waiting for pod pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149 to disappear
Dec 19 07:07:20.365: INFO: Pod pod-projected-secrets-ca05b715-2517-4e85-a0e4-0d4397216149 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:07:20.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2452" for this suite.
Dec 19 07:07:26.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:07:26.418: INFO: namespace projected-2452 deletion completed in 6.051033861s

• [SLOW TEST:8.098 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:07:26.418: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:07:26.439: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c" in namespace "projected-1856" to be "success or failure"
Dec 19 07:07:26.442: INFO: Pod "downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.157215ms
Dec 19 07:07:28.444: INFO: Pod "downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005289654s
Dec 19 07:07:30.446: INFO: Pod "downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007443146s
STEP: Saw pod success
Dec 19 07:07:30.446: INFO: Pod "downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c" satisfied condition "success or failure"
Dec 19 07:07:30.447: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c container client-container: <nil>
STEP: delete the pod
Dec 19 07:07:30.461: INFO: Waiting for pod downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c to disappear
Dec 19 07:07:30.463: INFO: Pod downwardapi-volume-fbf3bf9c-97a4-4dda-9494-a63cad9b1e2c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:07:30.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1856" for this suite.
Dec 19 07:07:36.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:07:36.515: INFO: namespace projected-1856 deletion completed in 6.05079448s

• [SLOW TEST:10.097 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:07:36.516: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:07:36.533: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:07:37.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4623" for this suite.
Dec 19 07:07:43.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:07:43.612: INFO: namespace custom-resource-definition-4623 deletion completed in 6.050339582s

• [SLOW TEST:7.096 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:07:43.612: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 07:07:43.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7145'
Dec 19 07:07:43.685: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 19 07:07:43.685: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Dec 19 07:07:45.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7145'
Dec 19 07:07:45.748: INFO: stderr: ""
Dec 19 07:07:45.748: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:07:45.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7145" for this suite.
Dec 19 07:07:51.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:07:51.803: INFO: namespace kubectl-7145 deletion completed in 6.052539294s

• [SLOW TEST:8.191 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:07:51.803: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:07:51.826: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16" in namespace "downward-api-8773" to be "success or failure"
Dec 19 07:07:51.830: INFO: Pod "downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94189ms
Dec 19 07:07:53.833: INFO: Pod "downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00638671s
Dec 19 07:07:55.835: INFO: Pod "downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009000301s
STEP: Saw pod success
Dec 19 07:07:55.835: INFO: Pod "downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16" satisfied condition "success or failure"
Dec 19 07:07:55.837: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16 container client-container: <nil>
STEP: delete the pod
Dec 19 07:07:55.854: INFO: Waiting for pod downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16 to disappear
Dec 19 07:07:55.856: INFO: Pod downwardapi-volume-c2ab5ace-6fbd-462e-9c8d-464c3767ce16 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:07:55.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8773" for this suite.
Dec 19 07:08:01.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:08:01.908: INFO: namespace downward-api-8773 deletion completed in 6.050301515s

• [SLOW TEST:10.105 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:08:01.908: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 07:08:01.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2130'
Dec 19 07:08:01.982: INFO: stderr: ""
Dec 19 07:08:01.982: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Dec 19 07:08:01.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete pods e2e-test-nginx-pod --namespace=kubectl-2130'
Dec 19 07:08:16.631: INFO: stderr: ""
Dec 19 07:08:16.631: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:08:16.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2130" for this suite.
Dec 19 07:08:22.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:08:22.684: INFO: namespace kubectl-2130 deletion completed in 6.050856864s

• [SLOW TEST:20.776 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:08:22.684: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 19 07:08:22.704: INFO: Waiting up to 5m0s for pod "pod-9a41615b-8367-420c-897d-951610de3d63" in namespace "emptydir-3713" to be "success or failure"
Dec 19 07:08:22.706: INFO: Pod "pod-9a41615b-8367-420c-897d-951610de3d63": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897454ms
Dec 19 07:08:24.709: INFO: Pod "pod-9a41615b-8367-420c-897d-951610de3d63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004265821s
Dec 19 07:08:26.711: INFO: Pod "pod-9a41615b-8367-420c-897d-951610de3d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006363127s
STEP: Saw pod success
Dec 19 07:08:26.711: INFO: Pod "pod-9a41615b-8367-420c-897d-951610de3d63" satisfied condition "success or failure"
Dec 19 07:08:26.712: INFO: Trying to get logs from node 192.168.0.132 pod pod-9a41615b-8367-420c-897d-951610de3d63 container test-container: <nil>
STEP: delete the pod
Dec 19 07:08:26.720: INFO: Waiting for pod pod-9a41615b-8367-420c-897d-951610de3d63 to disappear
Dec 19 07:08:26.722: INFO: Pod pod-9a41615b-8367-420c-897d-951610de3d63 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:08:26.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3713" for this suite.
Dec 19 07:08:32.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:08:32.802: INFO: namespace emptydir-3713 deletion completed in 6.077813168s

• [SLOW TEST:10.118 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:08:32.802: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 19 07:08:32.839: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:08:36.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7275" for this suite.
Dec 19 07:08:42.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:08:42.363: INFO: namespace init-container-7275 deletion completed in 6.051395465s

• [SLOW TEST:9.561 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:08:42.363: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 19 07:08:44.393: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:08:44.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6062" for this suite.
Dec 19 07:08:50.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:08:50.457: INFO: namespace container-runtime-6062 deletion completed in 6.051410693s

• [SLOW TEST:8.094 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:08:50.457: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-f9f230eb-5511-4dc1-9675-5baeb700f3bb
STEP: Creating a pod to test consume configMaps
Dec 19 07:08:50.482: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933" in namespace "projected-7722" to be "success or failure"
Dec 19 07:08:50.484: INFO: Pod "pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933": Phase="Pending", Reason="", readiness=false. Elapsed: 1.796074ms
Dec 19 07:08:52.486: INFO: Pod "pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004123221s
STEP: Saw pod success
Dec 19 07:08:52.486: INFO: Pod "pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933" satisfied condition "success or failure"
Dec 19 07:08:52.487: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:08:52.500: INFO: Waiting for pod pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933 to disappear
Dec 19 07:08:52.502: INFO: Pod pod-projected-configmaps-f9af9c5a-a65b-41ab-94dd-1ea49dc26933 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:08:52.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7722" for this suite.
Dec 19 07:08:58.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:08:58.552: INFO: namespace projected-7722 deletion completed in 6.04855473s

• [SLOW TEST:8.095 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:08:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:08:58.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4669" for this suite.
Dec 19 07:09:04.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:09:04.706: INFO: namespace services-4669 deletion completed in 6.13082151s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.154 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:09:04.706: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-127512fe-6d1e-47ee-971e-c87a2544464c
STEP: Creating a pod to test consume configMaps
Dec 19 07:09:04.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0" in namespace "configmap-387" to be "success or failure"
Dec 19 07:09:04.734: INFO: Pod "pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.448197ms
Dec 19 07:09:06.736: INFO: Pod "pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003416982s
STEP: Saw pod success
Dec 19 07:09:06.736: INFO: Pod "pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0" satisfied condition "success or failure"
Dec 19 07:09:06.738: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:09:06.748: INFO: Waiting for pod pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0 to disappear
Dec 19 07:09:06.750: INFO: Pod pod-configmaps-dc84f8bf-7e80-4fb0-9814-707340dd78c0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:09:06.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-387" for this suite.
Dec 19 07:09:12.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:09:12.801: INFO: namespace configmap-387 deletion completed in 6.049260265s

• [SLOW TEST:8.094 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:09:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 19 07:09:16.839: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 19 07:09:16.853: INFO: Pod pod-with-prestop-http-hook still exists
Dec 19 07:09:18.854: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 19 07:09:18.856: INFO: Pod pod-with-prestop-http-hook still exists
Dec 19 07:09:20.854: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 19 07:09:20.856: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:09:20.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7722" for this suite.
Dec 19 07:09:42.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:09:42.920: INFO: namespace container-lifecycle-hook-7722 deletion completed in 22.057569977s

• [SLOW TEST:30.119 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:09:42.920: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Dec 19 07:09:42.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-1273'
Dec 19 07:09:43.396: INFO: stderr: ""
Dec 19 07:09:43.396: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec 19 07:09:44.399: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:09:44.399: INFO: Found 0 / 1
Dec 19 07:09:45.399: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:09:45.399: INFO: Found 0 / 1
Dec 19 07:09:46.399: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:09:46.399: INFO: Found 1 / 1
Dec 19 07:09:46.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 19 07:09:46.401: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:09:46.401: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec 19 07:09:46.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 logs redis-master-chvjp redis-master --namespace=kubectl-1273'
Dec 19 07:09:46.461: INFO: stderr: ""
Dec 19 07:09:46.461: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Dec 07:09:45.079 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Dec 07:09:45.080 # Server started, Redis version 3.2.12\n1:M 19 Dec 07:09:45.080 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Dec 07:09:45.080 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec 19 07:09:46.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 log redis-master-chvjp redis-master --namespace=kubectl-1273 --tail=1'
Dec 19 07:09:46.531: INFO: stderr: ""
Dec 19 07:09:46.531: INFO: stdout: "1:M 19 Dec 07:09:45.080 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec 19 07:09:46.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 log redis-master-chvjp redis-master --namespace=kubectl-1273 --limit-bytes=1'
Dec 19 07:09:46.590: INFO: stderr: ""
Dec 19 07:09:46.590: INFO: stdout: " "
STEP: exposing timestamps
Dec 19 07:09:46.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 log redis-master-chvjp redis-master --namespace=kubectl-1273 --tail=1 --timestamps'
Dec 19 07:09:46.662: INFO: stderr: ""
Dec 19 07:09:46.662: INFO: stdout: "2019-12-19T07:09:45.080171193Z 1:M 19 Dec 07:09:45.080 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec 19 07:09:49.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 log redis-master-chvjp redis-master --namespace=kubectl-1273 --since=1s'
Dec 19 07:09:49.221: INFO: stderr: ""
Dec 19 07:09:49.221: INFO: stdout: ""
Dec 19 07:09:49.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 log redis-master-chvjp redis-master --namespace=kubectl-1273 --since=24h'
Dec 19 07:09:49.280: INFO: stderr: ""
Dec 19 07:09:49.280: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 19 Dec 07:09:45.079 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 19 Dec 07:09:45.080 # Server started, Redis version 3.2.12\n1:M 19 Dec 07:09:45.080 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 19 Dec 07:09:45.080 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Dec 19 07:09:49.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-1273'
Dec 19 07:09:49.334: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 07:09:49.334: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec 19 07:09:49.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get rc,svc -l name=nginx --no-headers --namespace=kubectl-1273'
Dec 19 07:09:49.388: INFO: stderr: "No resources found.\n"
Dec 19 07:09:49.388: INFO: stdout: ""
Dec 19 07:09:49.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -l name=nginx --namespace=kubectl-1273 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 19 07:09:49.438: INFO: stderr: ""
Dec 19 07:09:49.438: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:09:49.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1273" for this suite.
Dec 19 07:09:55.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:09:55.496: INFO: namespace kubectl-1273 deletion completed in 6.05600037s

• [SLOW TEST:12.576 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:09:55.496: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-4dd01adc-a833-4cae-8a26-ca78d1f8f1d1
STEP: Creating secret with name secret-projected-all-test-volume-ddb97ae8-407a-4b56-b998-698bebf7a492
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 19 07:09:55.532: INFO: Waiting up to 5m0s for pod "projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1" in namespace "projected-8116" to be "success or failure"
Dec 19 07:09:55.533: INFO: Pod "projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.39449ms
Dec 19 07:09:57.535: INFO: Pod "projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003549938s
Dec 19 07:09:59.538: INFO: Pod "projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005721445s
STEP: Saw pod success
Dec 19 07:09:59.538: INFO: Pod "projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1" satisfied condition "success or failure"
Dec 19 07:09:59.539: INFO: Trying to get logs from node 192.168.0.132 pod projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 19 07:09:59.549: INFO: Waiting for pod projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1 to disappear
Dec 19 07:09:59.550: INFO: Pod projected-volume-78fd23ce-71ab-4308-97fc-fee895140dc1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:09:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8116" for this suite.
Dec 19 07:10:05.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:10:05.603: INFO: namespace projected-8116 deletion completed in 6.051398779s

• [SLOW TEST:10.107 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:10:05.603: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2b7de342-97c1-4625-8980-880d9843dc5a
STEP: Creating a pod to test consume secrets
Dec 19 07:10:05.625: INFO: Waiting up to 5m0s for pod "pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd" in namespace "secrets-618" to be "success or failure"
Dec 19 07:10:05.626: INFO: Pod "pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.745755ms
Dec 19 07:10:07.629: INFO: Pod "pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003879811s
Dec 19 07:10:09.631: INFO: Pod "pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005962599s
STEP: Saw pod success
Dec 19 07:10:09.631: INFO: Pod "pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd" satisfied condition "success or failure"
Dec 19 07:10:09.632: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:10:09.640: INFO: Waiting for pod pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd to disappear
Dec 19 07:10:09.642: INFO: Pod pod-secrets-8a9977ef-cfff-46fa-be81-1695ff6753fd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:10:09.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-618" for this suite.
Dec 19 07:10:15.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:10:15.696: INFO: namespace secrets-618 deletion completed in 6.053050035s

• [SLOW TEST:10.093 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:10:15.697: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 19 07:10:15.719: INFO: Waiting up to 5m0s for pod "pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc" in namespace "emptydir-4459" to be "success or failure"
Dec 19 07:10:15.722: INFO: Pod "pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.94683ms
Dec 19 07:10:17.724: INFO: Pod "pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005113741s
Dec 19 07:10:19.727: INFO: Pod "pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007230195s
STEP: Saw pod success
Dec 19 07:10:19.727: INFO: Pod "pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc" satisfied condition "success or failure"
Dec 19 07:10:19.728: INFO: Trying to get logs from node 192.168.0.132 pod pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc container test-container: <nil>
STEP: delete the pod
Dec 19 07:10:19.746: INFO: Waiting for pod pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc to disappear
Dec 19 07:10:19.747: INFO: Pod pod-ca412fb0-f0e3-4cdc-8bd0-65346ed57acc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:10:19.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4459" for this suite.
Dec 19 07:10:25.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:10:25.799: INFO: namespace emptydir-4459 deletion completed in 6.050478609s

• [SLOW TEST:10.103 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:10:25.799: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1219 07:10:56.344031      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 19 07:10:56.344: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:10:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6996" for this suite.
Dec 19 07:11:02.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:11:02.394: INFO: namespace gc-6996 deletion completed in 6.048615139s

• [SLOW TEST:36.595 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:11:02.394: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:11:02.418: INFO: (0) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.063877ms)
Dec 19 07:11:02.420: INFO: (1) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.848558ms)
Dec 19 07:11:02.422: INFO: (2) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.74586ms)
Dec 19 07:11:02.423: INFO: (3) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.720943ms)
Dec 19 07:11:02.425: INFO: (4) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.71369ms)
Dec 19 07:11:02.427: INFO: (5) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.729896ms)
Dec 19 07:11:02.428: INFO: (6) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.650976ms)
Dec 19 07:11:02.430: INFO: (7) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.866939ms)
Dec 19 07:11:02.432: INFO: (8) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.718225ms)
Dec 19 07:11:02.434: INFO: (9) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.788699ms)
Dec 19 07:11:02.436: INFO: (10) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.706245ms)
Dec 19 07:11:02.437: INFO: (11) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.769632ms)
Dec 19 07:11:02.439: INFO: (12) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.747284ms)
Dec 19 07:11:02.441: INFO: (13) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.804453ms)
Dec 19 07:11:02.443: INFO: (14) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.709686ms)
Dec 19 07:11:02.444: INFO: (15) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.741082ms)
Dec 19 07:11:02.446: INFO: (16) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.818208ms)
Dec 19 07:11:02.448: INFO: (17) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.880161ms)
Dec 19 07:11:02.450: INFO: (18) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.717494ms)
Dec 19 07:11:02.452: INFO: (19) /api/v1/nodes/192.168.0.132:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.773536ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:11:02.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4353" for this suite.
Dec 19 07:11:08.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:11:08.505: INFO: namespace proxy-4353 deletion completed in 6.052029612s

• [SLOW TEST:6.111 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:11:08.506: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 19 07:11:08.522: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:11:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1238" for this suite.
Dec 19 07:11:18.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:11:18.509: INFO: namespace init-container-1238 deletion completed in 6.050651681s

• [SLOW TEST:10.003 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:11:18.509: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:11:18.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511" in namespace "downward-api-1999" to be "success or failure"
Dec 19 07:11:18.533: INFO: Pod "downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511": Phase="Pending", Reason="", readiness=false. Elapsed: 1.567364ms
Dec 19 07:11:20.535: INFO: Pod "downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003904366s
Dec 19 07:11:22.538: INFO: Pod "downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006184489s
STEP: Saw pod success
Dec 19 07:11:22.538: INFO: Pod "downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511" satisfied condition "success or failure"
Dec 19 07:11:22.539: INFO: Trying to get logs from node 192.168.0.205 pod downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511 container client-container: <nil>
STEP: delete the pod
Dec 19 07:11:22.550: INFO: Waiting for pod downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511 to disappear
Dec 19 07:11:22.551: INFO: Pod downwardapi-volume-49e755da-d385-4099-977f-1084fbfc5511 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:11:22.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1999" for this suite.
Dec 19 07:11:28.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:11:28.604: INFO: namespace downward-api-1999 deletion completed in 6.050858794s

• [SLOW TEST:10.095 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:11:28.604: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 19 07:11:28.624: INFO: Waiting up to 5m0s for pod "downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96" in namespace "downward-api-4471" to be "success or failure"
Dec 19 07:11:28.626: INFO: Pod "downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.654119ms
Dec 19 07:11:30.628: INFO: Pod "downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003762549s
STEP: Saw pod success
Dec 19 07:11:30.628: INFO: Pod "downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96" satisfied condition "success or failure"
Dec 19 07:11:30.629: INFO: Trying to get logs from node 192.168.0.132 pod downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96 container dapi-container: <nil>
STEP: delete the pod
Dec 19 07:11:30.639: INFO: Waiting for pod downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96 to disappear
Dec 19 07:11:30.641: INFO: Pod downward-api-eb7213e1-2ed8-4643-8933-762e0ae4dd96 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:11:30.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4471" for this suite.
Dec 19 07:11:36.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:11:36.689: INFO: namespace downward-api-4471 deletion completed in 6.046493821s

• [SLOW TEST:8.085 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:11:36.690: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:11:36.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5" in namespace "downward-api-4637" to be "success or failure"
Dec 19 07:11:36.709: INFO: Pod "downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.061461ms
Dec 19 07:11:38.711: INFO: Pod "downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003495415s
Dec 19 07:11:40.713: INFO: Pod "downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00577394s
STEP: Saw pod success
Dec 19 07:11:40.713: INFO: Pod "downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5" satisfied condition "success or failure"
Dec 19 07:11:40.715: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5 container client-container: <nil>
STEP: delete the pod
Dec 19 07:11:40.726: INFO: Waiting for pod downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5 to disappear
Dec 19 07:11:40.728: INFO: Pod downwardapi-volume-0e27cd26-7860-4c49-8171-e33e7f9973e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:11:40.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4637" for this suite.
Dec 19 07:11:46.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:11:46.780: INFO: namespace downward-api-4637 deletion completed in 6.050124551s

• [SLOW TEST:10.090 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:11:46.780: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 19 07:11:54.816: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:11:54.818: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:11:56.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:11:56.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:11:58.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:11:58.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:00.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:00.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:02.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:02.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:04.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:04.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:06.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:06.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:08.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:08.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:10.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:10.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:12.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:12.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:14.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:14.821: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 19 07:12:16.819: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 19 07:12:16.821: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:12:16.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4001" for this suite.
Dec 19 07:12:38.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:12:38.878: INFO: namespace container-lifecycle-hook-4001 deletion completed in 22.050480257s

• [SLOW TEST:52.098 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:12:38.878: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2363
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 19 07:12:38.897: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 19 07:13:04.938: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.20:8080/dial?request=hostName&protocol=http&host=172.16.0.50&port=8080&tries=1'] Namespace:pod-network-test-2363 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:13:04.938: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:13:05.045: INFO: Waiting for endpoints: map[]
Dec 19 07:13:05.047: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.20:8080/dial?request=hostName&protocol=http&host=172.16.0.19&port=8080&tries=1'] Namespace:pod-network-test-2363 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:13:05.047: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:13:05.104: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:13:05.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2363" for this suite.
Dec 19 07:13:27.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:13:27.162: INFO: namespace pod-network-test-2363 deletion completed in 22.054968495s

• [SLOW TEST:48.284 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:13:27.162: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-7e279607-13ae-4070-b452-f8026a3cef29
STEP: Creating a pod to test consume secrets
Dec 19 07:13:27.195: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975" in namespace "projected-678" to be "success or failure"
Dec 19 07:13:27.196: INFO: Pod "pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975": Phase="Pending", Reason="", readiness=false. Elapsed: 1.58339ms
Dec 19 07:13:29.198: INFO: Pod "pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003845625s
Dec 19 07:13:31.201: INFO: Pod "pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005948819s
STEP: Saw pod success
Dec 19 07:13:31.201: INFO: Pod "pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975" satisfied condition "success or failure"
Dec 19 07:13:31.202: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:13:31.214: INFO: Waiting for pod pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975 to disappear
Dec 19 07:13:31.216: INFO: Pod pod-projected-secrets-d6d582fc-5799-4c6f-b826-8d835667c975 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:13:31.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-678" for this suite.
Dec 19 07:13:37.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:13:37.270: INFO: namespace projected-678 deletion completed in 6.052203246s

• [SLOW TEST:10.108 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:13:37.270: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3038
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-3038
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3038
Dec 19 07:13:37.293: INFO: Found 0 stateful pods, waiting for 1
Dec 19 07:13:47.296: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 19 07:13:47.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 07:13:47.429: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 07:13:47.429: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 07:13:47.429: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 07:13:47.431: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 19 07:13:57.434: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 07:13:57.434: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 07:13:57.446: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:13:57.446: INFO: ss-0  192.168.0.132  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:13:57.446: INFO: 
Dec 19 07:13:57.446: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 19 07:13:58.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992242836s
Dec 19 07:13:59.452: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98942026s
Dec 19 07:14:00.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986885516s
Dec 19 07:14:01.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984302014s
Dec 19 07:14:02.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981356835s
Dec 19 07:14:03.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978752134s
Dec 19 07:14:04.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976346856s
Dec 19 07:14:05.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973737676s
Dec 19 07:14:06.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.153903ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3038
Dec 19 07:14:07.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 07:14:07.597: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 07:14:07.597: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 07:14:07.597: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 07:14:07.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 07:14:07.757: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 19 07:14:07.757: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 07:14:07.757: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 07:14:07.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 07:14:07.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 19 07:14:07.881: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 07:14:07.881: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 07:14:07.883: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec 19 07:14:17.885: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 07:14:17.885: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 07:14:17.885: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 19 07:14:17.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 07:14:17.998: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 07:14:17.998: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 07:14:17.998: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 07:14:17.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 07:14:18.120: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 07:14:18.120: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 07:14:18.120: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 07:14:18.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-3038 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 07:14:18.229: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 07:14:18.229: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 07:14:18.229: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 07:14:18.229: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 07:14:18.231: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 19 07:14:28.234: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 07:14:28.234: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 07:14:28.234: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 07:14:28.239: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:28.239: INFO: ss-0  192.168.0.132  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:28.239: INFO: ss-1  192.168.0.205  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:28.239: INFO: ss-2  192.168.0.132  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:28.239: INFO: 
Dec 19 07:14:28.239: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:29.242: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:29.242: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:29.242: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:29.242: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:29.242: INFO: 
Dec 19 07:14:29.242: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:30.244: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:30.244: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:30.244: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:30.244: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:30.244: INFO: 
Dec 19 07:14:30.244: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:31.247: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:31.247: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:31.247: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:31.247: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:31.247: INFO: 
Dec 19 07:14:31.247: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:32.249: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:32.249: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:32.249: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:32.249: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:32.249: INFO: 
Dec 19 07:14:32.249: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:33.252: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:33.252: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:33.252: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:33.252: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:33.252: INFO: 
Dec 19 07:14:33.252: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:34.254: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:34.254: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:34.254: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:34.254: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:34.254: INFO: 
Dec 19 07:14:34.254: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:35.257: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:35.257: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:35.257: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:35.257: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:35.257: INFO: 
Dec 19 07:14:35.257: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:36.259: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:36.259: INFO: ss-0  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:37 +0000 UTC  }]
Dec 19 07:14:36.259: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:36.259: INFO: ss-2  192.168.0.132  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:36.259: INFO: 
Dec 19 07:14:36.259: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 19 07:14:37.261: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Dec 19 07:14:37.261: INFO: ss-1  192.168.0.205  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:14:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:13:57 +0000 UTC  }]
Dec 19 07:14:37.261: INFO: 
Dec 19 07:14:37.261: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3038
Dec 19 07:14:38.263: INFO: Scaling statefulset ss to 0
Dec 19 07:14:38.268: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 19 07:14:38.269: INFO: Deleting all statefulset in ns statefulset-3038
Dec 19 07:14:38.270: INFO: Scaling statefulset ss to 0
Dec 19 07:14:38.279: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 07:14:38.281: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:14:38.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3038" for this suite.
Dec 19 07:14:44.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:14:44.339: INFO: namespace statefulset-3038 deletion completed in 6.049944607s

• [SLOW TEST:67.069 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:14:44.339: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-01cd6da3-484c-4374-a1b8-cbe0feedcd41
STEP: Creating a pod to test consume secrets
Dec 19 07:14:44.376: INFO: Waiting up to 5m0s for pod "pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4" in namespace "secrets-5149" to be "success or failure"
Dec 19 07:14:44.377: INFO: Pod "pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.360214ms
Dec 19 07:14:46.379: INFO: Pod "pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003356023s
STEP: Saw pod success
Dec 19 07:14:46.379: INFO: Pod "pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4" satisfied condition "success or failure"
Dec 19 07:14:46.381: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4 container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:14:46.393: INFO: Waiting for pod pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4 to disappear
Dec 19 07:14:46.394: INFO: Pod pod-secrets-721aa559-4dfa-4c03-ba8b-82b432e4adf4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:14:46.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5149" for this suite.
Dec 19 07:14:52.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:14:52.468: INFO: namespace secrets-5149 deletion completed in 6.071960281s

• [SLOW TEST:8.129 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:14:52.468: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:14:52.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5398" for this suite.
Dec 19 07:15:14.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:15:14.545: INFO: namespace pods-5398 deletion completed in 22.05178757s

• [SLOW TEST:22.077 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:15:14.545: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 19 07:15:14.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-8196'
Dec 19 07:15:14.687: INFO: stderr: ""
Dec 19 07:15:14.687: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 19 07:15:14.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:14.741: INFO: stderr: ""
Dec 19 07:15:14.741: INFO: stdout: "update-demo-nautilus-glvnj update-demo-nautilus-wmpkj "
Dec 19 07:15:14.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-glvnj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:14.790: INFO: stderr: ""
Dec 19 07:15:14.790: INFO: stdout: ""
Dec 19 07:15:14.790: INFO: update-demo-nautilus-glvnj is created but not running
Dec 19 07:15:19.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:19.845: INFO: stderr: ""
Dec 19 07:15:19.845: INFO: stdout: "update-demo-nautilus-glvnj update-demo-nautilus-wmpkj "
Dec 19 07:15:19.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-glvnj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:19.894: INFO: stderr: ""
Dec 19 07:15:19.894: INFO: stdout: "true"
Dec 19 07:15:19.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-glvnj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:19.942: INFO: stderr: ""
Dec 19 07:15:19.942: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:15:19.942: INFO: validating pod update-demo-nautilus-glvnj
Dec 19 07:15:19.954: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:15:19.954: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:15:19.954: INFO: update-demo-nautilus-glvnj is verified up and running
Dec 19 07:15:19.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wmpkj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:20.006: INFO: stderr: ""
Dec 19 07:15:20.006: INFO: stdout: "true"
Dec 19 07:15:20.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wmpkj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:20.055: INFO: stderr: ""
Dec 19 07:15:20.055: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:15:20.055: INFO: validating pod update-demo-nautilus-wmpkj
Dec 19 07:15:20.095: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:15:20.095: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:15:20.095: INFO: update-demo-nautilus-wmpkj is verified up and running
STEP: scaling down the replication controller
Dec 19 07:15:20.095: INFO: scanned /root for discovery docs: <nil>
Dec 19 07:15:20.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8196'
Dec 19 07:15:21.158: INFO: stderr: ""
Dec 19 07:15:21.158: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 19 07:15:21.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:21.211: INFO: stderr: ""
Dec 19 07:15:21.211: INFO: stdout: "update-demo-nautilus-glvnj update-demo-nautilus-wmpkj "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 19 07:15:26.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:26.278: INFO: stderr: ""
Dec 19 07:15:26.279: INFO: stdout: "update-demo-nautilus-glvnj update-demo-nautilus-wmpkj "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 19 07:15:31.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:31.332: INFO: stderr: ""
Dec 19 07:15:31.332: INFO: stdout: "update-demo-nautilus-wmpkj "
Dec 19 07:15:31.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wmpkj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:31.384: INFO: stderr: ""
Dec 19 07:15:31.384: INFO: stdout: "true"
Dec 19 07:15:31.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wmpkj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:31.473: INFO: stderr: ""
Dec 19 07:15:31.473: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:15:31.473: INFO: validating pod update-demo-nautilus-wmpkj
Dec 19 07:15:31.476: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:15:31.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:15:31.476: INFO: update-demo-nautilus-wmpkj is verified up and running
STEP: scaling up the replication controller
Dec 19 07:15:31.476: INFO: scanned /root for discovery docs: <nil>
Dec 19 07:15:31.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8196'
Dec 19 07:15:32.539: INFO: stderr: ""
Dec 19 07:15:32.539: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 19 07:15:32.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:32.591: INFO: stderr: ""
Dec 19 07:15:32.591: INFO: stdout: "update-demo-nautilus-dw5p5 update-demo-nautilus-wmpkj "
Dec 19 07:15:32.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-dw5p5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:32.642: INFO: stderr: ""
Dec 19 07:15:32.642: INFO: stdout: ""
Dec 19 07:15:32.642: INFO: update-demo-nautilus-dw5p5 is created but not running
Dec 19 07:15:37.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8196'
Dec 19 07:15:37.695: INFO: stderr: ""
Dec 19 07:15:37.695: INFO: stdout: "update-demo-nautilus-dw5p5 update-demo-nautilus-wmpkj "
Dec 19 07:15:37.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-dw5p5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:37.747: INFO: stderr: ""
Dec 19 07:15:37.747: INFO: stdout: "true"
Dec 19 07:15:37.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-dw5p5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:37.797: INFO: stderr: ""
Dec 19 07:15:37.797: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:15:37.797: INFO: validating pod update-demo-nautilus-dw5p5
Dec 19 07:15:37.813: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:15:37.813: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:15:37.813: INFO: update-demo-nautilus-dw5p5 is verified up and running
Dec 19 07:15:37.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wmpkj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:37.867: INFO: stderr: ""
Dec 19 07:15:37.867: INFO: stdout: "true"
Dec 19 07:15:37.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wmpkj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8196'
Dec 19 07:15:37.920: INFO: stderr: ""
Dec 19 07:15:37.920: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:15:37.920: INFO: validating pod update-demo-nautilus-wmpkj
Dec 19 07:15:37.923: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:15:37.923: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:15:37.923: INFO: update-demo-nautilus-wmpkj is verified up and running
STEP: using delete to clean up resources
Dec 19 07:15:37.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-8196'
Dec 19 07:15:37.990: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 07:15:37.990: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 19 07:15:37.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8196'
Dec 19 07:15:38.043: INFO: stderr: "No resources found.\n"
Dec 19 07:15:38.043: INFO: stdout: ""
Dec 19 07:15:38.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -l name=update-demo --namespace=kubectl-8196 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 19 07:15:38.100: INFO: stderr: ""
Dec 19 07:15:38.100: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:15:38.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8196" for this suite.
Dec 19 07:16:00.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:16:00.161: INFO: namespace kubectl-8196 deletion completed in 22.05886297s

• [SLOW TEST:45.616 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:16:00.161: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 19 07:16:00.184: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664847,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 19 07:16:00.184: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664847,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 19 07:16:10.188: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664882,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 19 07:16:10.188: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664882,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 19 07:16:20.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664916,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 19 07:16:20.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664916,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 19 07:16:30.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664950,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 19 07:16:30.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-a,UID:f765b943-3f31-4681-b0d6-24c9c2267e44,ResourceVersion:664950,Generation:0,CreationTimestamp:2019-12-19 07:16:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 19 07:16:40.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-b,UID:1bb503cc-a657-45a4-8e14-1517aa874491,ResourceVersion:664986,Generation:0,CreationTimestamp:2019-12-19 07:16:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 19 07:16:40.199: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-b,UID:1bb503cc-a657-45a4-8e14-1517aa874491,ResourceVersion:664986,Generation:0,CreationTimestamp:2019-12-19 07:16:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 19 07:16:50.216: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-b,UID:1bb503cc-a657-45a4-8e14-1517aa874491,ResourceVersion:665029,Generation:0,CreationTimestamp:2019-12-19 07:16:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 19 07:16:50.216: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6141,SelfLink:/api/v1/namespaces/watch-6141/configmaps/e2e-watch-test-configmap-b,UID:1bb503cc-a657-45a4-8e14-1517aa874491,ResourceVersion:665029,Generation:0,CreationTimestamp:2019-12-19 07:16:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:17:00.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6141" for this suite.
Dec 19 07:17:06.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:17:06.269: INFO: namespace watch-6141 deletion completed in 6.051399247s

• [SLOW TEST:66.108 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:17:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4c5a00b5-f57d-4553-b102-4c427c545581
STEP: Creating a pod to test consume configMaps
Dec 19 07:17:06.305: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007" in namespace "projected-6752" to be "success or failure"
Dec 19 07:17:06.307: INFO: Pod "pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007": Phase="Pending", Reason="", readiness=false. Elapsed: 1.749533ms
Dec 19 07:17:08.309: INFO: Pod "pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003719374s
STEP: Saw pod success
Dec 19 07:17:08.309: INFO: Pod "pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007" satisfied condition "success or failure"
Dec 19 07:17:08.310: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:17:08.321: INFO: Waiting for pod pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007 to disappear
Dec 19 07:17:08.322: INFO: Pod pod-projected-configmaps-b63ef4e1-bd27-429d-af51-0bf06fab0007 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:17:08.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6752" for this suite.
Dec 19 07:17:14.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:17:14.379: INFO: namespace projected-6752 deletion completed in 6.054673672s

• [SLOW TEST:8.109 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:17:14.379: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:18:14.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2966" for this suite.
Dec 19 07:18:36.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:18:36.453: INFO: namespace container-probe-2966 deletion completed in 22.048348849s

• [SLOW TEST:82.074 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:18:36.453: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-ch5j
STEP: Creating a pod to test atomic-volume-subpath
Dec 19 07:18:36.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ch5j" in namespace "subpath-6643" to be "success or failure"
Dec 19 07:18:36.478: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057669ms
Dec 19 07:18:38.480: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004195987s
Dec 19 07:18:40.483: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 4.006416888s
Dec 19 07:18:42.485: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 6.008432961s
Dec 19 07:18:44.487: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.01078761s
Dec 19 07:18:46.489: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.012991078s
Dec 19 07:18:48.491: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.014712958s
Dec 19 07:18:50.493: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.016828542s
Dec 19 07:18:52.495: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.019258179s
Dec 19 07:18:54.498: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.02143283s
Dec 19 07:18:56.500: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.02376427s
Dec 19 07:18:58.502: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Running", Reason="", readiness=true. Elapsed: 22.025939211s
Dec 19 07:19:00.504: INFO: Pod "pod-subpath-test-projected-ch5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028189866s
STEP: Saw pod success
Dec 19 07:19:00.504: INFO: Pod "pod-subpath-test-projected-ch5j" satisfied condition "success or failure"
Dec 19 07:19:00.506: INFO: Trying to get logs from node 192.168.0.132 pod pod-subpath-test-projected-ch5j container test-container-subpath-projected-ch5j: <nil>
STEP: delete the pod
Dec 19 07:19:00.516: INFO: Waiting for pod pod-subpath-test-projected-ch5j to disappear
Dec 19 07:19:00.517: INFO: Pod pod-subpath-test-projected-ch5j no longer exists
STEP: Deleting pod pod-subpath-test-projected-ch5j
Dec 19 07:19:00.517: INFO: Deleting pod "pod-subpath-test-projected-ch5j" in namespace "subpath-6643"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:19:00.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6643" for this suite.
Dec 19 07:19:06.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:19:06.568: INFO: namespace subpath-6643 deletion completed in 6.048541779s

• [SLOW TEST:30.115 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:19:06.568: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 19 07:19:06.586: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 19 07:19:06.589: INFO: Waiting for terminating namespaces to be deleted...
Dec 19 07:19:06.591: INFO: 
Logging pods the kubelet thinks is on node 192.168.0.132 before test
Dec 19 07:19:06.594: INFO: test-liyi-6668b4c9f8-4qzm2 from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 07:19:06.594: INFO: everest-csi-driver-w2xrr from kube-system started at 2019-12-19 06:18:26 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 07:19:06.594: INFO: sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-8b4v2 from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 07:19:06.594: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 19 07:19:06.594: INFO: icagent-kh9cq from kube-system started at 2019-12-19 01:14:36 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container icagent ready: true, restart count 1
Dec 19 07:19:06.594: INFO: test-liyi-6668b4c9f8-pkxvx from default started at 2019-12-19 02:35:17 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 07:19:06.594: INFO: test-liyi-6668b4c9f8-xkqdd from default started at 2019-12-19 05:41:50 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 07:19:06.594: INFO: test-liyi-6668b4c9f8-c2fsf from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.594: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 07:19:06.594: INFO: 
Logging pods the kubelet thinks is on node 192.168.0.205 before test
Dec 19 07:19:06.598: INFO: test-liyi-6668b4c9f8-zzwm7 from default started at 2019-12-19 05:45:28 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 07:19:06.598: INFO: test-liyi-6668b4c9f8-sp92n from default started at 2019-12-18 11:46:57 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container container-0 ready: true, restart count 0
Dec 19 07:19:06.598: INFO: icagent-2tb9d from kube-system started at 2019-12-17 03:05:53 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container icagent ready: true, restart count 0
Dec 19 07:19:06.598: INFO: everest-csi-driver-5gw2v from kube-system started at 2019-12-19 06:18:37 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 07:19:06.598: INFO: sonobuoy-e2e-job-9bbd16c8926f47ce from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container e2e ready: true, restart count 0
Dec 19 07:19:06.598: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 07:19:06.598: INFO: sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-sbhz7 from sonobuoy started at 2019-12-19 06:39:52 +0000 UTC (2 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 19 07:19:06.598: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 19 07:19:06.598: INFO: web-terminal-f79ff6644-6w8n5 from default started at 2019-12-17 03:05:00 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container web-terminal ready: true, restart count 0
Dec 19 07:19:06.598: INFO: sonobuoy from sonobuoy started at 2019-12-19 06:39:49 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 19 07:19:06.598: INFO: everest-csi-controller-b9cddb7d6-4ljp8 from kube-system started at 2019-12-18 13:31:00 +0000 UTC (2 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container everest-csi-controller ready: true, restart count 0
Dec 19 07:19:06.598: INFO: 	Container everest-csi-driver ready: true, restart count 0
Dec 19 07:19:06.598: INFO: coredns-59d5fcd5dc-bgx6r from kube-system started at 2019-12-18 13:27:54 +0000 UTC (1 container statuses recorded)
Dec 19 07:19:06.598: INFO: 	Container coredns ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node 192.168.0.132
STEP: verifying the node has the label node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod test-liyi-6668b4c9f8-4qzm2 requesting resource cpu=250m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod test-liyi-6668b4c9f8-c2fsf requesting resource cpu=250m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod test-liyi-6668b4c9f8-pkxvx requesting resource cpu=250m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod test-liyi-6668b4c9f8-sp92n requesting resource cpu=250m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod test-liyi-6668b4c9f8-xkqdd requesting resource cpu=250m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod test-liyi-6668b4c9f8-zzwm7 requesting resource cpu=250m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod web-terminal-f79ff6644-6w8n5 requesting resource cpu=100m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod coredns-59d5fcd5dc-bgx6r requesting resource cpu=500m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod everest-csi-controller-b9cddb7d6-4ljp8 requesting resource cpu=200m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod everest-csi-driver-5gw2v requesting resource cpu=100m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod everest-csi-driver-w2xrr requesting resource cpu=100m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod icagent-2tb9d requesting resource cpu=0m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod icagent-kh9cq requesting resource cpu=0m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod sonobuoy requesting resource cpu=0m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod sonobuoy-e2e-job-9bbd16c8926f47ce requesting resource cpu=0m on Node 192.168.0.205
Dec 19 07:19:06.618: INFO: Pod sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-8b4v2 requesting resource cpu=0m on Node 192.168.0.132
Dec 19 07:19:06.618: INFO: Pod sonobuoy-systemd-logs-daemon-set-76d2296af0d74196-sbhz7 requesting resource cpu=0m on Node 192.168.0.205
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7.15e1b4a0a5e79bd3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8759/filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7 to 192.168.0.132]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7.15e1b4a0b821ba1e], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volumes for pod "filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7_sched-pred-8759(0fd42569-2f2e-4187-b6fe-cfb0c88dfb46)"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7.15e1b4a0f17a71d5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7.15e1b4a105675583], Reason = [SuccessfulCreate], Message = [Created container filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7.15e1b4a10f49577f], Reason = [Started], Message = [Started container filler-pod-0145ae2e-86d5-4333-b9d7-cbc9fdfc68e7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c.15e1b4a0a5e7b85c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8759/filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c to 192.168.0.205]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c.15e1b4a0baf50750], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volumes for pod "filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c_sched-pred-8759(695fddbe-2187-4d8b-8f59-23227103e505)"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c.15e1b4a0f5c0de54], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c.15e1b4a10a55082e], Reason = [SuccessfulCreate], Message = [Created container filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c.15e1b4a113fb8bfe], Reason = [Started], Message = [Started container filler-pod-18b2f616-fc66-4cf9-93fd-eb6a29b1af8c]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e1b4a194e38e9a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node 192.168.0.132
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 192.168.0.205
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:19:11.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8759" for this suite.
Dec 19 07:19:17.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:19:17.971: INFO: namespace sched-pred-8759 deletion completed in 6.05835073s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.403 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:19:17.971: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-6597
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6597 to expose endpoints map[]
Dec 19 07:19:18.007: INFO: Get endpoints failed (1.411449ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 19 07:19:19.009: INFO: successfully validated that service endpoint-test2 in namespace services-6597 exposes endpoints map[] (1.003421188s elapsed)
STEP: Creating pod pod1 in namespace services-6597
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6597 to expose endpoints map[pod1:[80]]
Dec 19 07:19:22.028: INFO: successfully validated that service endpoint-test2 in namespace services-6597 exposes endpoints map[pod1:[80]] (3.014995794s elapsed)
STEP: Creating pod pod2 in namespace services-6597
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6597 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 19 07:19:25.053: INFO: successfully validated that service endpoint-test2 in namespace services-6597 exposes endpoints map[pod1:[80] pod2:[80]] (3.023120068s elapsed)
STEP: Deleting pod pod1 in namespace services-6597
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6597 to expose endpoints map[pod2:[80]]
Dec 19 07:19:26.065: INFO: successfully validated that service endpoint-test2 in namespace services-6597 exposes endpoints map[pod2:[80]] (1.0092046s elapsed)
STEP: Deleting pod pod2 in namespace services-6597
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6597 to expose endpoints map[]
Dec 19 07:19:27.074: INFO: successfully validated that service endpoint-test2 in namespace services-6597 exposes endpoints map[] (1.004791661s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:19:27.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6597" for this suite.
Dec 19 07:19:33.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:19:33.141: INFO: namespace services-6597 deletion completed in 6.053860121s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:15.170 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:19:33.141: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2663baf9-3dde-4de2-aeca-ccb41444cbd5
STEP: Creating a pod to test consume secrets
Dec 19 07:19:33.165: INFO: Waiting up to 5m0s for pod "pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8" in namespace "secrets-3656" to be "success or failure"
Dec 19 07:19:33.166: INFO: Pod "pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.309934ms
Dec 19 07:19:35.169: INFO: Pod "pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003511568s
Dec 19 07:19:37.171: INFO: Pod "pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005659214s
STEP: Saw pod success
Dec 19 07:19:37.171: INFO: Pod "pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8" satisfied condition "success or failure"
Dec 19 07:19:37.172: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8 container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:19:37.183: INFO: Waiting for pod pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8 to disappear
Dec 19 07:19:37.185: INFO: Pod pod-secrets-c84d430d-dd70-424f-98f1-ad4d832391d8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:19:37.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3656" for this suite.
Dec 19 07:19:43.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:19:43.235: INFO: namespace secrets-3656 deletion completed in 6.048857534s

• [SLOW TEST:10.094 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:19:43.235: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 19 07:19:47.771: INFO: Successfully updated pod "labelsupdate98f221f5-f01c-431b-8de3-2564679c0257"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:19:49.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-794" for this suite.
Dec 19 07:20:11.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:20:11.839: INFO: namespace downward-api-794 deletion completed in 22.055293022s

• [SLOW TEST:28.604 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:20:11.839: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 19 07:20:11.861: INFO: Waiting up to 5m0s for pod "pod-65335e6f-b4bd-4810-b50e-4c40de854b44" in namespace "emptydir-9330" to be "success or failure"
Dec 19 07:20:11.863: INFO: Pod "pod-65335e6f-b4bd-4810-b50e-4c40de854b44": Phase="Pending", Reason="", readiness=false. Elapsed: 1.612749ms
Dec 19 07:20:13.865: INFO: Pod "pod-65335e6f-b4bd-4810-b50e-4c40de854b44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003759755s
Dec 19 07:20:15.867: INFO: Pod "pod-65335e6f-b4bd-4810-b50e-4c40de854b44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00592494s
STEP: Saw pod success
Dec 19 07:20:15.867: INFO: Pod "pod-65335e6f-b4bd-4810-b50e-4c40de854b44" satisfied condition "success or failure"
Dec 19 07:20:15.869: INFO: Trying to get logs from node 192.168.0.132 pod pod-65335e6f-b4bd-4810-b50e-4c40de854b44 container test-container: <nil>
STEP: delete the pod
Dec 19 07:20:15.879: INFO: Waiting for pod pod-65335e6f-b4bd-4810-b50e-4c40de854b44 to disappear
Dec 19 07:20:15.881: INFO: Pod pod-65335e6f-b4bd-4810-b50e-4c40de854b44 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:20:15.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9330" for this suite.
Dec 19 07:20:21.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:20:21.932: INFO: namespace emptydir-9330 deletion completed in 6.050056673s

• [SLOW TEST:10.093 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:20:21.932: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 19 07:20:21.953: INFO: Waiting up to 5m0s for pod "pod-be245021-e2f0-4b73-ba41-5c71f5e5a730" in namespace "emptydir-5670" to be "success or failure"
Dec 19 07:20:21.958: INFO: Pod "pod-be245021-e2f0-4b73-ba41-5c71f5e5a730": Phase="Pending", Reason="", readiness=false. Elapsed: 4.857235ms
Dec 19 07:20:23.960: INFO: Pod "pod-be245021-e2f0-4b73-ba41-5c71f5e5a730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007114583s
Dec 19 07:20:25.962: INFO: Pod "pod-be245021-e2f0-4b73-ba41-5c71f5e5a730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009329408s
STEP: Saw pod success
Dec 19 07:20:25.962: INFO: Pod "pod-be245021-e2f0-4b73-ba41-5c71f5e5a730" satisfied condition "success or failure"
Dec 19 07:20:25.964: INFO: Trying to get logs from node 192.168.0.132 pod pod-be245021-e2f0-4b73-ba41-5c71f5e5a730 container test-container: <nil>
STEP: delete the pod
Dec 19 07:20:25.974: INFO: Waiting for pod pod-be245021-e2f0-4b73-ba41-5c71f5e5a730 to disappear
Dec 19 07:20:25.978: INFO: Pod pod-be245021-e2f0-4b73-ba41-5c71f5e5a730 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:20:25.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5670" for this suite.
Dec 19 07:20:31.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:20:32.040: INFO: namespace emptydir-5670 deletion completed in 6.060622209s

• [SLOW TEST:10.108 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:20:32.040: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-f9d68143-b683-46e0-97a6-2d000c381819
STEP: Creating a pod to test consume configMaps
Dec 19 07:20:32.067: INFO: Waiting up to 5m0s for pod "pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73" in namespace "configmap-6290" to be "success or failure"
Dec 19 07:20:32.069: INFO: Pod "pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73": Phase="Pending", Reason="", readiness=false. Elapsed: 1.840677ms
Dec 19 07:20:34.072: INFO: Pod "pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004231766s
Dec 19 07:20:36.074: INFO: Pod "pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006406019s
STEP: Saw pod success
Dec 19 07:20:36.074: INFO: Pod "pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73" satisfied condition "success or failure"
Dec 19 07:20:36.075: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:20:36.087: INFO: Waiting for pod pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73 to disappear
Dec 19 07:20:36.089: INFO: Pod pod-configmaps-6fa814b3-632f-4f9c-8156-003ec8a41c73 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:20:36.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6290" for this suite.
Dec 19 07:20:42.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:20:42.154: INFO: namespace configmap-6290 deletion completed in 6.06297892s

• [SLOW TEST:10.114 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:20:42.154: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3169
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 19 07:20:42.176: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 19 07:21:02.213: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.0.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3169 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:21:02.213: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:21:03.274: INFO: Found all expected endpoints: [netserver-0]
Dec 19 07:21:03.275: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.0.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3169 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:21:03.275: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:21:04.329: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:21:04.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3169" for this suite.
Dec 19 07:21:26.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:21:26.381: INFO: namespace pod-network-test-3169 deletion completed in 22.050360458s

• [SLOW TEST:44.227 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:21:26.382: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-5ce7714d-fdbf-4e71-9e60-550aed75f3af in namespace container-probe-920
Dec 19 07:21:28.407: INFO: Started pod liveness-5ce7714d-fdbf-4e71-9e60-550aed75f3af in namespace container-probe-920
STEP: checking the pod's current state and verifying that restartCount is present
Dec 19 07:21:28.409: INFO: Initial restart count of pod liveness-5ce7714d-fdbf-4e71-9e60-550aed75f3af is 0
Dec 19 07:21:44.428: INFO: Restart count of pod container-probe-920/liveness-5ce7714d-fdbf-4e71-9e60-550aed75f3af is now 1 (16.018695556s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:21:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-920" for this suite.
Dec 19 07:21:50.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:21:50.482: INFO: namespace container-probe-920 deletion completed in 6.047316693s

• [SLOW TEST:24.100 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:21:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-g86g
STEP: Creating a pod to test atomic-volume-subpath
Dec 19 07:21:50.508: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g86g" in namespace "subpath-2842" to be "success or failure"
Dec 19 07:21:50.509: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469516ms
Dec 19 07:21:52.511: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 2.003574003s
Dec 19 07:21:54.516: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 4.00856425s
Dec 19 07:21:56.518: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 6.010723277s
Dec 19 07:21:58.520: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 8.012951855s
Dec 19 07:22:00.523: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 10.015169049s
Dec 19 07:22:02.525: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 12.017313998s
Dec 19 07:22:04.527: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 14.018998042s
Dec 19 07:22:06.529: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 16.021118691s
Dec 19 07:22:08.531: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 18.02334978s
Dec 19 07:22:10.533: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 20.025524392s
Dec 19 07:22:12.535: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Running", Reason="", readiness=true. Elapsed: 22.027524687s
Dec 19 07:22:14.537: INFO: Pod "pod-subpath-test-secret-g86g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.029674053s
STEP: Saw pod success
Dec 19 07:22:14.537: INFO: Pod "pod-subpath-test-secret-g86g" satisfied condition "success or failure"
Dec 19 07:22:14.539: INFO: Trying to get logs from node 192.168.0.132 pod pod-subpath-test-secret-g86g container test-container-subpath-secret-g86g: <nil>
STEP: delete the pod
Dec 19 07:22:14.549: INFO: Waiting for pod pod-subpath-test-secret-g86g to disappear
Dec 19 07:22:14.551: INFO: Pod pod-subpath-test-secret-g86g no longer exists
STEP: Deleting pod pod-subpath-test-secret-g86g
Dec 19 07:22:14.551: INFO: Deleting pod "pod-subpath-test-secret-g86g" in namespace "subpath-2842"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:22:14.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2842" for this suite.
Dec 19 07:22:20.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:22:20.602: INFO: namespace subpath-2842 deletion completed in 6.048814914s

• [SLOW TEST:30.121 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:22:20.603: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 19 07:22:23.171: INFO: Successfully updated pod "annotationupdate58f06d41-45e7-4b60-8279-0df928861815"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:22:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4640" for this suite.
Dec 19 07:22:47.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:22:47.234: INFO: namespace downward-api-4640 deletion completed in 22.0502355s

• [SLOW TEST:26.631 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:22:47.234: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:22:47.255: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 19 07:22:52.258: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 19 07:22:52.258: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 19 07:22:52.283: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1002,SelfLink:/apis/apps/v1/namespaces/deployment-1002/deployments/test-cleanup-deployment,UID:5b96aa25-e20b-40c5-a081-8a63cd705801,ResourceVersion:666674,Generation:1,CreationTimestamp:2019-12-19 07:22:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:0,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[{Available True 2019-12-19 07:22:52 +0000 UTC 2019-12-19 07:22:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-19 07:22:52 +0000 UTC 2019-12-19 07:22:52 +0000 UTC ReplicaSetUpdated ReplicaSet "test-cleanup-deployment-55bbcbc84c" is progressing.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 19 07:22:52.286: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-1002,SelfLink:/apis/apps/v1/namespaces/deployment-1002/replicasets/test-cleanup-deployment-55bbcbc84c,UID:099ebae9-d318-4176-bb40-5a330298dbd0,ResourceVersion:666673,Generation:1,CreationTimestamp:2019-12-19 07:22:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5b96aa25-e20b-40c5-a081-8a63cd705801 0xc002763337 0xc002763338}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 07:22:52.286: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec 19 07:22:52.286: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-1002,SelfLink:/apis/apps/v1/namespaces/deployment-1002/replicasets/test-cleanup-controller,UID:751503ef-156a-485b-8a71-2ff21c5efb81,ResourceVersion:666667,Generation:1,CreationTimestamp:2019-12-19 07:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 5b96aa25-e20b-40c5-a081-8a63cd705801 0xc002763117 0xc002763118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 19 07:22:52.287: INFO: Pod "test-cleanup-controller-llpqf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-llpqf,GenerateName:test-cleanup-controller-,Namespace:deployment-1002,SelfLink:/api/v1/namespaces/deployment-1002/pods/test-cleanup-controller-llpqf,UID:26908036-677d-4e64-8847-b0f621270417,ResourceVersion:666656,Generation:0,CreationTimestamp:2019-12-19 07:22:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 751503ef-156a-485b-8a71-2ff21c5efb81 0xc0024602e7 0xc0024602e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tgb25 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tgb25,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-tgb25 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002460360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002460380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{timeout 0xc00264a330} {single-request-reopen 0xc00264a340}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:22:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:22:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:22:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:22:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.22,StartTime:2019-12-19 07:22:47 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:22:49 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://caf4e6014512b1a3712da559b4f73f1e4bafcc05239477804fc7ae73b6a753c3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:22:52.288: INFO: Pod "test-cleanup-deployment-55bbcbc84c-kbrbb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-kbrbb,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-1002,SelfLink:/api/v1/namespaces/deployment-1002/pods/test-cleanup-deployment-55bbcbc84c-kbrbb,UID:3b5eba72-948c-47f1-a5c9-e46889c27bcd,ResourceVersion:666672,Generation:0,CreationTimestamp:2019-12-19 07:22:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 099ebae9-d318-4176-bb40-5a330298dbd0 0xc002460457 0xc002460458}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-tgb25 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-tgb25,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-tgb25 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002460500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002460530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc00264a350} {timeout 0xc00264a360}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:22:52 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:22:52.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1002" for this suite.
Dec 19 07:22:58.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:22:58.336: INFO: namespace deployment-1002 deletion completed in 6.046917401s

• [SLOW TEST:11.103 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:22:58.350: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:22:58.367: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 19 07:22:58.371: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 19 07:23:03.373: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 19 07:23:03.373: INFO: Creating deployment "test-rolling-update-deployment"
Dec 19 07:23:03.376: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 19 07:23:03.378: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 19 07:23:05.382: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 19 07:23:05.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712336983, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712336983, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712336983, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712336983, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 07:23:07.385: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 19 07:23:07.391: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-4107,SelfLink:/apis/apps/v1/namespaces/deployment-4107/deployments/test-rolling-update-deployment,UID:2f722f7a-41d9-4705-89e6-abec1a71d3d1,ResourceVersion:666775,Generation:1,CreationTimestamp:2019-12-19 07:23:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-19 07:23:03 +0000 UTC 2019-12-19 07:23:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-19 07:23:06 +0000 UTC 2019-12-19 07:23:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 19 07:23:07.392: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-4107,SelfLink:/apis/apps/v1/namespaces/deployment-4107/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:d69afabf-46db-4d5f-9dc2-102dd5526653,ResourceVersion:666768,Generation:1,CreationTimestamp:2019-12-19 07:23:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2f722f7a-41d9-4705-89e6-abec1a71d3d1 0xc0031ec1b7 0xc0031ec1b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 19 07:23:07.392: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 19 07:23:07.392: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-4107,SelfLink:/apis/apps/v1/namespaces/deployment-4107/replicasets/test-rolling-update-controller,UID:a6277a99-6983-4fe5-9ad1-9a9c45f08157,ResourceVersion:666774,Generation:2,CreationTimestamp:2019-12-19 07:22:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2f722f7a-41d9-4705-89e6-abec1a71d3d1 0xc0031ec0e7 0xc0031ec0e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 07:23:07.394: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-6wzw4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-6wzw4,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-4107,SelfLink:/api/v1/namespaces/deployment-4107/pods/test-rolling-update-deployment-79f6b9d75c-6wzw4,UID:d29b9e81-8029-46a4-990a-28a30befe98c,ResourceVersion:666767,Generation:0,CreationTimestamp:2019-12-19 07:23:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c d69afabf-46db-4d5f-9dc2-102dd5526653 0xc0031ecac7 0xc0031ecac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kr26q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kr26q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-kr26q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031ecb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031ecb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc0008223f0} {timeout 0xc000822400}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:23:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:23:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:23:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:23:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.26,StartTime:2019-12-19 07:23:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-19 07:23:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://a3ba5b826c9b88e9430c21c0ec8b51e7ee316f056c8bb50f8b164e5085a89c9b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:23:07.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4107" for this suite.
Dec 19 07:23:13.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:23:13.446: INFO: namespace deployment-4107 deletion completed in 6.050523611s

• [SLOW TEST:15.097 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:23:13.447: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2858/configmap-test-50115fcd-4091-4079-b03c-122ef9e8af95
STEP: Creating a pod to test consume configMaps
Dec 19 07:23:13.475: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a" in namespace "configmap-2858" to be "success or failure"
Dec 19 07:23:13.478: INFO: Pod "pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.983509ms
Dec 19 07:23:15.479: INFO: Pod "pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004862329s
STEP: Saw pod success
Dec 19 07:23:15.479: INFO: Pod "pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a" satisfied condition "success or failure"
Dec 19 07:23:15.481: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a container env-test: <nil>
STEP: delete the pod
Dec 19 07:23:15.492: INFO: Waiting for pod pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a to disappear
Dec 19 07:23:15.493: INFO: Pod pod-configmaps-ff010e9d-3310-4b17-ad32-f067a787c27a no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:23:15.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2858" for this suite.
Dec 19 07:23:21.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:23:21.556: INFO: namespace configmap-2858 deletion completed in 6.060255362s

• [SLOW TEST:8.109 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:23:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-76fbec9a-41f4-4abb-8853-41cfc1f2b517 in namespace container-probe-6011
Dec 19 07:23:23.578: INFO: Started pod test-webserver-76fbec9a-41f4-4abb-8853-41cfc1f2b517 in namespace container-probe-6011
STEP: checking the pod's current state and verifying that restartCount is present
Dec 19 07:23:23.580: INFO: Initial restart count of pod test-webserver-76fbec9a-41f4-4abb-8853-41cfc1f2b517 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:27:23.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6011" for this suite.
Dec 19 07:27:29.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:27:29.904: INFO: namespace container-probe-6011 deletion completed in 6.050226327s

• [SLOW TEST:248.348 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:27:29.904: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 19 07:27:29.924: INFO: Waiting up to 5m0s for pod "downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9" in namespace "downward-api-7049" to be "success or failure"
Dec 19 07:27:29.926: INFO: Pod "downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604655ms
Dec 19 07:27:31.928: INFO: Pod "downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0039953s
Dec 19 07:27:33.930: INFO: Pod "downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006163038s
STEP: Saw pod success
Dec 19 07:27:33.930: INFO: Pod "downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9" satisfied condition "success or failure"
Dec 19 07:27:33.932: INFO: Trying to get logs from node 192.168.0.132 pod downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9 container dapi-container: <nil>
STEP: delete the pod
Dec 19 07:27:33.946: INFO: Waiting for pod downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9 to disappear
Dec 19 07:27:33.948: INFO: Pod downward-api-07226de7-1bec-4793-b3ee-4491ae838ff9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:27:33.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7049" for this suite.
Dec 19 07:27:39.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:27:39.999: INFO: namespace downward-api-7049 deletion completed in 6.049923669s

• [SLOW TEST:10.095 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:27:40.000: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 19 07:27:40.409: INFO: Pod name wrapped-volume-race-151353c3-ec6c-4fc0-9ca9-845d23fb6471: Found 0 pods out of 5
Dec 19 07:27:45.413: INFO: Pod name wrapped-volume-race-151353c3-ec6c-4fc0-9ca9-845d23fb6471: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-151353c3-ec6c-4fc0-9ca9-845d23fb6471 in namespace emptydir-wrapper-9804, will wait for the garbage collector to delete the pods
Dec 19 07:27:45.478: INFO: Deleting ReplicationController wrapped-volume-race-151353c3-ec6c-4fc0-9ca9-845d23fb6471 took: 5.103347ms
Dec 19 07:27:45.578: INFO: Terminating ReplicationController wrapped-volume-race-151353c3-ec6c-4fc0-9ca9-845d23fb6471 pods took: 100.155753ms
STEP: Creating RC which spawns configmap-volume pods
Dec 19 07:28:26.788: INFO: Pod name wrapped-volume-race-f6923d2b-873b-4a26-a7c3-a40bc6b1c363: Found 0 pods out of 5
Dec 19 07:28:31.792: INFO: Pod name wrapped-volume-race-f6923d2b-873b-4a26-a7c3-a40bc6b1c363: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f6923d2b-873b-4a26-a7c3-a40bc6b1c363 in namespace emptydir-wrapper-9804, will wait for the garbage collector to delete the pods
Dec 19 07:28:31.856: INFO: Deleting ReplicationController wrapped-volume-race-f6923d2b-873b-4a26-a7c3-a40bc6b1c363 took: 3.995435ms
Dec 19 07:28:31.956: INFO: Terminating ReplicationController wrapped-volume-race-f6923d2b-873b-4a26-a7c3-a40bc6b1c363 pods took: 100.129232ms
STEP: Creating RC which spawns configmap-volume pods
Dec 19 07:29:16.876: INFO: Pod name wrapped-volume-race-f6dc89b8-783d-40da-8705-38d06b0ebf21: Found 0 pods out of 5
Dec 19 07:29:21.880: INFO: Pod name wrapped-volume-race-f6dc89b8-783d-40da-8705-38d06b0ebf21: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f6dc89b8-783d-40da-8705-38d06b0ebf21 in namespace emptydir-wrapper-9804, will wait for the garbage collector to delete the pods
Dec 19 07:29:21.943: INFO: Deleting ReplicationController wrapped-volume-race-f6dc89b8-783d-40da-8705-38d06b0ebf21 took: 3.967341ms
Dec 19 07:29:22.044: INFO: Terminating ReplicationController wrapped-volume-race-f6dc89b8-783d-40da-8705-38d06b0ebf21 pods took: 100.155843ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:29:56.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9804" for this suite.
Dec 19 07:30:02.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:30:02.942: INFO: namespace emptydir-wrapper-9804 deletion completed in 6.071281206s

• [SLOW TEST:142.942 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:30:02.942: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2647
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 19 07:30:02.960: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 19 07:30:27.004: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.16:8080/dial?request=hostName&protocol=udp&host=172.16.0.53&port=8081&tries=1'] Namespace:pod-network-test-2647 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:30:27.004: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:30:27.095: INFO: Waiting for endpoints: map[]
Dec 19 07:30:27.097: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.0.16:8080/dial?request=hostName&protocol=udp&host=172.16.0.31&port=8081&tries=1'] Namespace:pod-network-test-2647 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:30:27.097: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:30:27.149: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:30:27.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2647" for this suite.
Dec 19 07:30:49.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:30:49.213: INFO: namespace pod-network-test-2647 deletion completed in 22.061354392s

• [SLOW TEST:46.271 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:30:49.213: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2378
I1219 07:30:49.235038      17 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2378, replica count: 1
I1219 07:30:50.285321      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1219 07:30:51.285452      17 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 19 07:30:51.393: INFO: Created: latency-svc-zmbgk
Dec 19 07:30:51.396: INFO: Got endpoints: latency-svc-zmbgk [11.201521ms]
Dec 19 07:30:51.403: INFO: Created: latency-svc-v77sp
Dec 19 07:30:51.408: INFO: Created: latency-svc-knmpz
Dec 19 07:30:51.409: INFO: Got endpoints: latency-svc-v77sp [12.815977ms]
Dec 19 07:30:51.420: INFO: Got endpoints: latency-svc-knmpz [23.093935ms]
Dec 19 07:30:51.420: INFO: Created: latency-svc-mt54x
Dec 19 07:30:51.426: INFO: Got endpoints: latency-svc-mt54x [29.922079ms]
Dec 19 07:30:51.450: INFO: Created: latency-svc-jwhsx
Dec 19 07:30:51.454: INFO: Created: latency-svc-6b2rk
Dec 19 07:30:51.454: INFO: Got endpoints: latency-svc-jwhsx [57.638431ms]
Dec 19 07:30:51.462: INFO: Created: latency-svc-666db
Dec 19 07:30:51.462: INFO: Got endpoints: latency-svc-6b2rk [65.803514ms]
Dec 19 07:30:51.471: INFO: Created: latency-svc-hx2q7
Dec 19 07:30:51.471: INFO: Got endpoints: latency-svc-666db [74.134048ms]
Dec 19 07:30:51.480: INFO: Got endpoints: latency-svc-hx2q7 [82.985758ms]
Dec 19 07:30:51.483: INFO: Created: latency-svc-zd72w
Dec 19 07:30:51.489: INFO: Got endpoints: latency-svc-zd72w [91.960234ms]
Dec 19 07:30:51.496: INFO: Created: latency-svc-kcj7t
Dec 19 07:30:51.518: INFO: Created: latency-svc-522db
Dec 19 07:30:51.519: INFO: Got endpoints: latency-svc-kcj7t [122.348022ms]
Dec 19 07:30:51.521: INFO: Got endpoints: latency-svc-522db [124.749143ms]
Dec 19 07:30:51.526: INFO: Created: latency-svc-h2tqz
Dec 19 07:30:51.534: INFO: Got endpoints: latency-svc-h2tqz [137.453738ms]
Dec 19 07:30:51.535: INFO: Created: latency-svc-hkctx
Dec 19 07:30:51.541: INFO: Got endpoints: latency-svc-hkctx [144.176527ms]
Dec 19 07:30:51.541: INFO: Created: latency-svc-6whvk
Dec 19 07:30:51.547: INFO: Got endpoints: latency-svc-6whvk [150.838773ms]
Dec 19 07:30:51.548: INFO: Created: latency-svc-ljfd9
Dec 19 07:30:51.551: INFO: Got endpoints: latency-svc-ljfd9 [154.708398ms]
Dec 19 07:30:51.555: INFO: Created: latency-svc-zbpvl
Dec 19 07:30:51.559: INFO: Got endpoints: latency-svc-zbpvl [161.918721ms]
Dec 19 07:30:51.560: INFO: Created: latency-svc-8zsjt
Dec 19 07:30:51.560: INFO: Got endpoints: latency-svc-8zsjt [151.202884ms]
Dec 19 07:30:51.564: INFO: Created: latency-svc-n64fs
Dec 19 07:30:51.569: INFO: Got endpoints: latency-svc-n64fs [149.228777ms]
Dec 19 07:30:51.570: INFO: Created: latency-svc-5q4hr
Dec 19 07:30:51.575: INFO: Got endpoints: latency-svc-5q4hr [148.154677ms]
Dec 19 07:30:51.575: INFO: Created: latency-svc-dv6vp
Dec 19 07:30:51.580: INFO: Got endpoints: latency-svc-dv6vp [125.550537ms]
Dec 19 07:30:51.584: INFO: Created: latency-svc-8j9z6
Dec 19 07:30:51.588: INFO: Got endpoints: latency-svc-8j9z6 [125.963048ms]
Dec 19 07:30:51.589: INFO: Created: latency-svc-28j7w
Dec 19 07:30:51.593: INFO: Got endpoints: latency-svc-28j7w [121.82719ms]
Dec 19 07:30:51.593: INFO: Created: latency-svc-bftjf
Dec 19 07:30:51.600: INFO: Got endpoints: latency-svc-bftjf [120.400926ms]
Dec 19 07:30:51.602: INFO: Created: latency-svc-vbbmt
Dec 19 07:30:51.602: INFO: Got endpoints: latency-svc-vbbmt [112.979061ms]
Dec 19 07:30:51.605: INFO: Created: latency-svc-8skkt
Dec 19 07:30:51.611: INFO: Got endpoints: latency-svc-8skkt [91.947407ms]
Dec 19 07:30:51.611: INFO: Created: latency-svc-4dwqz
Dec 19 07:30:51.617: INFO: Created: latency-svc-m5dwr
Dec 19 07:30:51.617: INFO: Got endpoints: latency-svc-4dwqz [95.983615ms]
Dec 19 07:30:51.621: INFO: Created: latency-svc-bmbfd
Dec 19 07:30:51.621: INFO: Got endpoints: latency-svc-m5dwr [86.442271ms]
Dec 19 07:30:51.626: INFO: Created: latency-svc-cmlxg
Dec 19 07:30:51.626: INFO: Got endpoints: latency-svc-bmbfd [84.782444ms]
Dec 19 07:30:51.630: INFO: Got endpoints: latency-svc-cmlxg [82.714462ms]
Dec 19 07:30:51.631: INFO: Created: latency-svc-4bqkm
Dec 19 07:30:51.635: INFO: Got endpoints: latency-svc-4bqkm [83.536878ms]
Dec 19 07:30:51.635: INFO: Created: latency-svc-q4t7w
Dec 19 07:30:51.639: INFO: Got endpoints: latency-svc-q4t7w [80.750525ms]
Dec 19 07:30:51.641: INFO: Created: latency-svc-2q98x
Dec 19 07:30:51.644: INFO: Got endpoints: latency-svc-2q98x [83.663251ms]
Dec 19 07:30:51.648: INFO: Created: latency-svc-bh46n
Dec 19 07:30:51.652: INFO: Got endpoints: latency-svc-bh46n [83.386472ms]
Dec 19 07:30:51.657: INFO: Created: latency-svc-wx5jz
Dec 19 07:30:51.662: INFO: Got endpoints: latency-svc-wx5jz [87.157025ms]
Dec 19 07:30:51.662: INFO: Created: latency-svc-bb5tw
Dec 19 07:30:51.667: INFO: Got endpoints: latency-svc-bb5tw [87.4725ms]
Dec 19 07:30:51.669: INFO: Created: latency-svc-rvlmk
Dec 19 07:30:51.671: INFO: Got endpoints: latency-svc-rvlmk [82.884115ms]
Dec 19 07:30:51.676: INFO: Created: latency-svc-pfn27
Dec 19 07:30:51.680: INFO: Created: latency-svc-fjgh2
Dec 19 07:30:51.680: INFO: Got endpoints: latency-svc-pfn27 [87.138873ms]
Dec 19 07:30:51.684: INFO: Created: latency-svc-d4hl7
Dec 19 07:30:51.684: INFO: Got endpoints: latency-svc-fjgh2 [83.902257ms]
Dec 19 07:30:51.689: INFO: Got endpoints: latency-svc-d4hl7 [86.927833ms]
Dec 19 07:30:51.689: INFO: Created: latency-svc-l5nqc
Dec 19 07:30:51.694: INFO: Got endpoints: latency-svc-l5nqc [82.802372ms]
Dec 19 07:30:51.694: INFO: Created: latency-svc-8bhzz
Dec 19 07:30:51.709: INFO: Got endpoints: latency-svc-8bhzz [91.936957ms]
Dec 19 07:30:51.714: INFO: Created: latency-svc-8dvqv
Dec 19 07:30:51.715: INFO: Got endpoints: latency-svc-8dvqv [94.554307ms]
Dec 19 07:30:51.716: INFO: Created: latency-svc-fkdxk
Dec 19 07:30:51.724: INFO: Got endpoints: latency-svc-fkdxk [97.948811ms]
Dec 19 07:30:51.725: INFO: Created: latency-svc-9qjdd
Dec 19 07:30:51.729: INFO: Got endpoints: latency-svc-9qjdd [98.328564ms]
Dec 19 07:30:51.729: INFO: Created: latency-svc-2vk7c
Dec 19 07:30:51.733: INFO: Got endpoints: latency-svc-2vk7c [98.031733ms]
Dec 19 07:30:51.734: INFO: Created: latency-svc-dg6l4
Dec 19 07:30:51.738: INFO: Created: latency-svc-4xpz7
Dec 19 07:30:51.743: INFO: Got endpoints: latency-svc-4xpz7 [98.826742ms]
Dec 19 07:30:51.743: INFO: Created: latency-svc-ht4c6
Dec 19 07:30:51.743: INFO: Got endpoints: latency-svc-dg6l4 [103.530328ms]
Dec 19 07:30:51.747: INFO: Got endpoints: latency-svc-ht4c6 [94.658606ms]
Dec 19 07:30:51.751: INFO: Created: latency-svc-b67rj
Dec 19 07:30:51.755: INFO: Got endpoints: latency-svc-b67rj [92.709482ms]
Dec 19 07:30:51.757: INFO: Created: latency-svc-4rhxq
Dec 19 07:30:51.760: INFO: Got endpoints: latency-svc-4rhxq [92.789391ms]
Dec 19 07:30:51.761: INFO: Created: latency-svc-rx5kr
Dec 19 07:30:51.765: INFO: Got endpoints: latency-svc-rx5kr [93.294388ms]
Dec 19 07:30:51.767: INFO: Created: latency-svc-7t6hc
Dec 19 07:30:51.772: INFO: Got endpoints: latency-svc-7t6hc [91.80909ms]
Dec 19 07:30:51.772: INFO: Created: latency-svc-zvc8j
Dec 19 07:30:51.776: INFO: Created: latency-svc-cgxns
Dec 19 07:30:51.776: INFO: Got endpoints: latency-svc-zvc8j [92.075195ms]
Dec 19 07:30:51.780: INFO: Created: latency-svc-8cbdw
Dec 19 07:30:51.782: INFO: Got endpoints: latency-svc-cgxns [92.933264ms]
Dec 19 07:30:51.784: INFO: Got endpoints: latency-svc-8cbdw [89.883694ms]
Dec 19 07:30:51.786: INFO: Created: latency-svc-27xw7
Dec 19 07:30:51.790: INFO: Got endpoints: latency-svc-27xw7 [80.883823ms]
Dec 19 07:30:51.792: INFO: Created: latency-svc-42vmv
Dec 19 07:30:51.795: INFO: Got endpoints: latency-svc-42vmv [79.337716ms]
Dec 19 07:30:51.795: INFO: Created: latency-svc-rkrxf
Dec 19 07:30:51.802: INFO: Got endpoints: latency-svc-rkrxf [78.541844ms]
Dec 19 07:30:51.808: INFO: Created: latency-svc-9fl4j
Dec 19 07:30:51.811: INFO: Created: latency-svc-hkz7f
Dec 19 07:30:51.811: INFO: Got endpoints: latency-svc-9fl4j [82.888847ms]
Dec 19 07:30:51.815: INFO: Created: latency-svc-l4q8r
Dec 19 07:30:51.815: INFO: Got endpoints: latency-svc-hkz7f [82.422755ms]
Dec 19 07:30:51.821: INFO: Got endpoints: latency-svc-l4q8r [78.242671ms]
Dec 19 07:30:51.822: INFO: Created: latency-svc-ktx4m
Dec 19 07:30:51.826: INFO: Got endpoints: latency-svc-ktx4m [82.667085ms]
Dec 19 07:30:51.826: INFO: Created: latency-svc-znw86
Dec 19 07:30:51.833: INFO: Got endpoints: latency-svc-znw86 [86.262793ms]
Dec 19 07:30:51.833: INFO: Created: latency-svc-tbkp4
Dec 19 07:30:51.837: INFO: Got endpoints: latency-svc-tbkp4 [82.562784ms]
Dec 19 07:30:51.837: INFO: Created: latency-svc-82247
Dec 19 07:30:51.843: INFO: Got endpoints: latency-svc-82247 [82.942346ms]
Dec 19 07:30:51.847: INFO: Created: latency-svc-5xt6l
Dec 19 07:30:51.850: INFO: Got endpoints: latency-svc-5xt6l [85.027424ms]
Dec 19 07:30:51.853: INFO: Created: latency-svc-kmkqz
Dec 19 07:30:51.857: INFO: Got endpoints: latency-svc-kmkqz [85.133144ms]
Dec 19 07:30:51.857: INFO: Created: latency-svc-zmdbx
Dec 19 07:30:51.862: INFO: Got endpoints: latency-svc-zmdbx [86.05691ms]
Dec 19 07:30:51.864: INFO: Created: latency-svc-dvxx9
Dec 19 07:30:51.867: INFO: Created: latency-svc-6f54t
Dec 19 07:30:51.867: INFO: Got endpoints: latency-svc-dvxx9 [85.886277ms]
Dec 19 07:30:51.872: INFO: Got endpoints: latency-svc-6f54t [88.577408ms]
Dec 19 07:30:51.873: INFO: Created: latency-svc-n25dt
Dec 19 07:30:51.879: INFO: Got endpoints: latency-svc-n25dt [88.344539ms]
Dec 19 07:30:51.879: INFO: Created: latency-svc-jj2ff
Dec 19 07:30:51.883: INFO: Got endpoints: latency-svc-jj2ff [88.322927ms]
Dec 19 07:30:51.883: INFO: Created: latency-svc-sz588
Dec 19 07:30:51.888: INFO: Got endpoints: latency-svc-sz588 [86.057931ms]
Dec 19 07:30:51.889: INFO: Created: latency-svc-tfwfq
Dec 19 07:30:51.891: INFO: Got endpoints: latency-svc-tfwfq [79.838629ms]
Dec 19 07:30:51.896: INFO: Created: latency-svc-7f86g
Dec 19 07:30:51.897: INFO: Got endpoints: latency-svc-7f86g [81.868263ms]
Dec 19 07:30:51.911: INFO: Created: latency-svc-dmqn2
Dec 19 07:30:51.917: INFO: Created: latency-svc-mbwng
Dec 19 07:30:51.918: INFO: Got endpoints: latency-svc-dmqn2 [96.695729ms]
Dec 19 07:30:51.920: INFO: Got endpoints: latency-svc-mbwng [94.164531ms]
Dec 19 07:30:51.923: INFO: Created: latency-svc-xtb8n
Dec 19 07:30:51.930: INFO: Created: latency-svc-8c5rj
Dec 19 07:30:51.931: INFO: Got endpoints: latency-svc-xtb8n [97.491283ms]
Dec 19 07:30:51.935: INFO: Got endpoints: latency-svc-8c5rj [97.436619ms]
Dec 19 07:30:51.935: INFO: Created: latency-svc-vbp5w
Dec 19 07:30:51.941: INFO: Got endpoints: latency-svc-vbp5w [97.823395ms]
Dec 19 07:30:51.941: INFO: Created: latency-svc-wghnq
Dec 19 07:30:51.944: INFO: Got endpoints: latency-svc-wghnq [94.522989ms]
Dec 19 07:30:51.949: INFO: Created: latency-svc-qt6sz
Dec 19 07:30:51.960: INFO: Got endpoints: latency-svc-qt6sz [103.147065ms]
Dec 19 07:30:51.960: INFO: Created: latency-svc-phrfb
Dec 19 07:30:51.971: INFO: Got endpoints: latency-svc-phrfb [108.539927ms]
Dec 19 07:30:51.971: INFO: Created: latency-svc-6th86
Dec 19 07:30:51.976: INFO: Got endpoints: latency-svc-6th86 [109.046899ms]
Dec 19 07:30:51.977: INFO: Created: latency-svc-vrw2b
Dec 19 07:30:51.979: INFO: Got endpoints: latency-svc-vrw2b [107.01022ms]
Dec 19 07:30:51.984: INFO: Created: latency-svc-rc5g8
Dec 19 07:30:51.987: INFO: Got endpoints: latency-svc-rc5g8 [108.134526ms]
Dec 19 07:30:51.992: INFO: Created: latency-svc-pqt8j
Dec 19 07:30:51.995: INFO: Created: latency-svc-xmlsn
Dec 19 07:30:51.995: INFO: Got endpoints: latency-svc-pqt8j [112.358026ms]
Dec 19 07:30:51.999: INFO: Got endpoints: latency-svc-xmlsn [110.958214ms]
Dec 19 07:30:52.000: INFO: Created: latency-svc-7kt44
Dec 19 07:30:52.014: INFO: Got endpoints: latency-svc-7kt44 [122.291811ms]
Dec 19 07:30:52.014: INFO: Created: latency-svc-78npd
Dec 19 07:30:52.018: INFO: Got endpoints: latency-svc-78npd [120.390367ms]
Dec 19 07:30:52.020: INFO: Created: latency-svc-f6wz7
Dec 19 07:30:52.025: INFO: Got endpoints: latency-svc-f6wz7 [106.992751ms]
Dec 19 07:30:52.025: INFO: Created: latency-svc-rfrzc
Dec 19 07:30:52.031: INFO: Got endpoints: latency-svc-rfrzc [110.856819ms]
Dec 19 07:30:52.031: INFO: Created: latency-svc-z8rc7
Dec 19 07:30:52.039: INFO: Got endpoints: latency-svc-z8rc7 [107.945414ms]
Dec 19 07:30:52.039: INFO: Created: latency-svc-b8p2p
Dec 19 07:30:52.043: INFO: Got endpoints: latency-svc-b8p2p [107.984158ms]
Dec 19 07:30:52.045: INFO: Created: latency-svc-rvxtt
Dec 19 07:30:52.049: INFO: Got endpoints: latency-svc-rvxtt [108.061948ms]
Dec 19 07:30:52.050: INFO: Created: latency-svc-kzjqz
Dec 19 07:30:52.057: INFO: Got endpoints: latency-svc-kzjqz [112.977006ms]
Dec 19 07:30:52.059: INFO: Created: latency-svc-lzf97
Dec 19 07:30:52.061: INFO: Got endpoints: latency-svc-lzf97 [100.818571ms]
Dec 19 07:30:52.066: INFO: Created: latency-svc-xqx24
Dec 19 07:30:52.070: INFO: Got endpoints: latency-svc-xqx24 [99.135092ms]
Dec 19 07:30:52.071: INFO: Created: latency-svc-sh6kd
Dec 19 07:30:52.076: INFO: Got endpoints: latency-svc-sh6kd [99.486315ms]
Dec 19 07:30:52.076: INFO: Created: latency-svc-d5tdj
Dec 19 07:30:52.081: INFO: Got endpoints: latency-svc-d5tdj [101.387469ms]
Dec 19 07:30:52.081: INFO: Created: latency-svc-mnwwz
Dec 19 07:30:52.085: INFO: Got endpoints: latency-svc-mnwwz [98.098243ms]
Dec 19 07:30:52.086: INFO: Created: latency-svc-7j8z9
Dec 19 07:30:52.090: INFO: Got endpoints: latency-svc-7j8z9 [94.966302ms]
Dec 19 07:30:52.091: INFO: Created: latency-svc-v4jt4
Dec 19 07:30:52.095: INFO: Created: latency-svc-j9dns
Dec 19 07:30:52.095: INFO: Got endpoints: latency-svc-v4jt4 [95.627595ms]
Dec 19 07:30:52.099: INFO: Got endpoints: latency-svc-j9dns [85.250471ms]
Dec 19 07:30:52.102: INFO: Created: latency-svc-9gfdv
Dec 19 07:30:52.106: INFO: Got endpoints: latency-svc-9gfdv [15.774974ms]
Dec 19 07:30:52.111: INFO: Created: latency-svc-6mpf6
Dec 19 07:30:52.119: INFO: Got endpoints: latency-svc-6mpf6 [100.972189ms]
Dec 19 07:30:52.121: INFO: Created: latency-svc-pxbh2
Dec 19 07:30:52.123: INFO: Got endpoints: latency-svc-pxbh2 [98.089586ms]
Dec 19 07:30:52.133: INFO: Created: latency-svc-ctz8r
Dec 19 07:30:52.144: INFO: Got endpoints: latency-svc-ctz8r [112.897151ms]
Dec 19 07:30:52.144: INFO: Created: latency-svc-rl8x7
Dec 19 07:30:52.151: INFO: Got endpoints: latency-svc-rl8x7 [111.834452ms]
Dec 19 07:30:52.151: INFO: Created: latency-svc-wnxcl
Dec 19 07:30:52.157: INFO: Got endpoints: latency-svc-wnxcl [114.598403ms]
Dec 19 07:30:52.162: INFO: Created: latency-svc-8rt48
Dec 19 07:30:52.166: INFO: Got endpoints: latency-svc-8rt48 [116.645016ms]
Dec 19 07:30:52.170: INFO: Created: latency-svc-5f4cx
Dec 19 07:30:52.173: INFO: Got endpoints: latency-svc-5f4cx [116.139409ms]
Dec 19 07:30:52.173: INFO: Created: latency-svc-rxbdw
Dec 19 07:30:52.178: INFO: Created: latency-svc-2dcwj
Dec 19 07:30:52.180: INFO: Got endpoints: latency-svc-rxbdw [119.607557ms]
Dec 19 07:30:52.186: INFO: Created: latency-svc-prrfn
Dec 19 07:30:52.187: INFO: Got endpoints: latency-svc-2dcwj [116.960365ms]
Dec 19 07:30:52.190: INFO: Got endpoints: latency-svc-prrfn [113.956985ms]
Dec 19 07:30:52.195: INFO: Created: latency-svc-6jqfn
Dec 19 07:30:52.199: INFO: Got endpoints: latency-svc-6jqfn [118.416943ms]
Dec 19 07:30:52.200: INFO: Created: latency-svc-n8f7q
Dec 19 07:30:52.202: INFO: Got endpoints: latency-svc-n8f7q [117.236687ms]
Dec 19 07:30:52.213: INFO: Created: latency-svc-5xc8g
Dec 19 07:30:52.221: INFO: Got endpoints: latency-svc-5xc8g [125.761294ms]
Dec 19 07:30:52.222: INFO: Created: latency-svc-4j564
Dec 19 07:30:52.225: INFO: Created: latency-svc-2f5xq
Dec 19 07:30:52.226: INFO: Got endpoints: latency-svc-4j564 [127.197455ms]
Dec 19 07:30:52.233: INFO: Created: latency-svc-6lb4q
Dec 19 07:30:52.233: INFO: Got endpoints: latency-svc-2f5xq [126.902445ms]
Dec 19 07:30:52.239: INFO: Created: latency-svc-f4xbj
Dec 19 07:30:52.239: INFO: Got endpoints: latency-svc-6lb4q [119.935665ms]
Dec 19 07:30:52.249: INFO: Got endpoints: latency-svc-f4xbj [125.850107ms]
Dec 19 07:30:52.251: INFO: Created: latency-svc-b729d
Dec 19 07:30:52.251: INFO: Got endpoints: latency-svc-b729d [107.661164ms]
Dec 19 07:30:52.255: INFO: Created: latency-svc-t5ns6
Dec 19 07:30:52.260: INFO: Got endpoints: latency-svc-t5ns6 [109.431916ms]
Dec 19 07:30:52.261: INFO: Created: latency-svc-jhcd4
Dec 19 07:30:52.268: INFO: Created: latency-svc-m9wfh
Dec 19 07:30:52.268: INFO: Got endpoints: latency-svc-jhcd4 [111.022601ms]
Dec 19 07:30:52.272: INFO: Got endpoints: latency-svc-m9wfh [106.096862ms]
Dec 19 07:30:52.273: INFO: Created: latency-svc-4ndmg
Dec 19 07:30:52.277: INFO: Got endpoints: latency-svc-4ndmg [103.760623ms]
Dec 19 07:30:52.278: INFO: Created: latency-svc-bwn22
Dec 19 07:30:52.280: INFO: Got endpoints: latency-svc-bwn22 [99.555898ms]
Dec 19 07:30:52.283: INFO: Created: latency-svc-5pz7p
Dec 19 07:30:52.286: INFO: Got endpoints: latency-svc-5pz7p [99.262841ms]
Dec 19 07:30:52.287: INFO: Created: latency-svc-8rxhn
Dec 19 07:30:52.298: INFO: Got endpoints: latency-svc-8rxhn [108.173759ms]
Dec 19 07:30:52.299: INFO: Created: latency-svc-4r2g6
Dec 19 07:30:52.304: INFO: Created: latency-svc-5pckp
Dec 19 07:30:52.304: INFO: Got endpoints: latency-svc-4r2g6 [104.584878ms]
Dec 19 07:30:52.307: INFO: Created: latency-svc-vwt7l
Dec 19 07:30:52.308: INFO: Got endpoints: latency-svc-5pckp [105.382414ms]
Dec 19 07:30:52.310: INFO: Got endpoints: latency-svc-vwt7l [89.535363ms]
Dec 19 07:30:52.313: INFO: Created: latency-svc-z9qmj
Dec 19 07:30:52.315: INFO: Got endpoints: latency-svc-z9qmj [89.351669ms]
Dec 19 07:30:52.320: INFO: Created: latency-svc-s7wgh
Dec 19 07:30:52.324: INFO: Got endpoints: latency-svc-s7wgh [91.25809ms]
Dec 19 07:30:52.325: INFO: Created: latency-svc-9k2cl
Dec 19 07:30:52.328: INFO: Created: latency-svc-jq5lp
Dec 19 07:30:52.330: INFO: Got endpoints: latency-svc-9k2cl [91.18333ms]
Dec 19 07:30:52.335: INFO: Created: latency-svc-q8nn5
Dec 19 07:30:52.335: INFO: Got endpoints: latency-svc-jq5lp [86.006423ms]
Dec 19 07:30:52.341: INFO: Got endpoints: latency-svc-q8nn5 [89.899469ms]
Dec 19 07:30:52.342: INFO: Created: latency-svc-6rfhr
Dec 19 07:30:52.347: INFO: Got endpoints: latency-svc-6rfhr [87.394613ms]
Dec 19 07:30:52.348: INFO: Created: latency-svc-l8mpr
Dec 19 07:30:52.353: INFO: Got endpoints: latency-svc-l8mpr [84.725159ms]
Dec 19 07:30:52.353: INFO: Created: latency-svc-wvsrx
Dec 19 07:30:52.357: INFO: Got endpoints: latency-svc-wvsrx [85.302854ms]
Dec 19 07:30:52.357: INFO: Created: latency-svc-csllq
Dec 19 07:30:52.360: INFO: Got endpoints: latency-svc-csllq [83.166708ms]
Dec 19 07:30:52.364: INFO: Created: latency-svc-2d8qd
Dec 19 07:30:52.373: INFO: Got endpoints: latency-svc-2d8qd [93.321807ms]
Dec 19 07:30:52.374: INFO: Created: latency-svc-25tvf
Dec 19 07:30:52.378: INFO: Got endpoints: latency-svc-25tvf [91.996632ms]
Dec 19 07:30:52.382: INFO: Created: latency-svc-8knws
Dec 19 07:30:52.386: INFO: Got endpoints: latency-svc-8knws [87.52484ms]
Dec 19 07:30:52.388: INFO: Created: latency-svc-grccp
Dec 19 07:30:52.392: INFO: Created: latency-svc-qsdq6
Dec 19 07:30:52.393: INFO: Got endpoints: latency-svc-grccp [88.967502ms]
Dec 19 07:30:52.398: INFO: Got endpoints: latency-svc-qsdq6 [90.405464ms]
Dec 19 07:30:52.398: INFO: Created: latency-svc-8qs4w
Dec 19 07:30:52.402: INFO: Got endpoints: latency-svc-8qs4w [92.144358ms]
Dec 19 07:30:52.403: INFO: Created: latency-svc-7qjb6
Dec 19 07:30:52.414: INFO: Got endpoints: latency-svc-7qjb6 [98.137023ms]
Dec 19 07:30:52.415: INFO: Created: latency-svc-qlwnm
Dec 19 07:30:52.419: INFO: Created: latency-svc-sgw2t
Dec 19 07:30:52.419: INFO: Got endpoints: latency-svc-qlwnm [95.218502ms]
Dec 19 07:30:52.423: INFO: Got endpoints: latency-svc-sgw2t [93.407807ms]
Dec 19 07:30:52.426: INFO: Created: latency-svc-4qhxx
Dec 19 07:30:52.434: INFO: Got endpoints: latency-svc-4qhxx [98.976283ms]
Dec 19 07:30:52.434: INFO: Created: latency-svc-7bjf6
Dec 19 07:30:52.439: INFO: Got endpoints: latency-svc-7bjf6 [97.691368ms]
Dec 19 07:30:52.439: INFO: Created: latency-svc-8bj7h
Dec 19 07:30:52.441: INFO: Got endpoints: latency-svc-8bj7h [94.00213ms]
Dec 19 07:30:52.445: INFO: Created: latency-svc-z55kw
Dec 19 07:30:52.448: INFO: Got endpoints: latency-svc-z55kw [95.178766ms]
Dec 19 07:30:52.453: INFO: Created: latency-svc-zv88c
Dec 19 07:30:52.457: INFO: Created: latency-svc-tx5jd
Dec 19 07:30:52.459: INFO: Got endpoints: latency-svc-tx5jd [98.644398ms]
Dec 19 07:30:52.459: INFO: Got endpoints: latency-svc-zv88c [101.903569ms]
Dec 19 07:30:52.463: INFO: Created: latency-svc-97stk
Dec 19 07:30:52.469: INFO: Got endpoints: latency-svc-97stk [96.08485ms]
Dec 19 07:30:52.470: INFO: Created: latency-svc-bjjnn
Dec 19 07:30:52.478: INFO: Got endpoints: latency-svc-bjjnn [99.688253ms]
Dec 19 07:30:52.478: INFO: Created: latency-svc-2mcwb
Dec 19 07:30:52.484: INFO: Got endpoints: latency-svc-2mcwb [98.289286ms]
Dec 19 07:30:52.488: INFO: Created: latency-svc-mwhtw
Dec 19 07:30:52.492: INFO: Created: latency-svc-cjwz6
Dec 19 07:30:52.492: INFO: Got endpoints: latency-svc-mwhtw [99.16718ms]
Dec 19 07:30:52.500: INFO: Got endpoints: latency-svc-cjwz6 [102.088407ms]
Dec 19 07:30:52.505: INFO: Created: latency-svc-mz526
Dec 19 07:30:52.508: INFO: Got endpoints: latency-svc-mz526 [105.902313ms]
Dec 19 07:30:52.510: INFO: Created: latency-svc-lh4sb
Dec 19 07:30:52.512: INFO: Created: latency-svc-kjmtr
Dec 19 07:30:52.513: INFO: Got endpoints: latency-svc-lh4sb [99.478085ms]
Dec 19 07:30:52.515: INFO: Got endpoints: latency-svc-kjmtr [95.325047ms]
Dec 19 07:30:52.518: INFO: Created: latency-svc-2cbkw
Dec 19 07:30:52.522: INFO: Created: latency-svc-2d8nd
Dec 19 07:30:52.522: INFO: Got endpoints: latency-svc-2cbkw [98.1601ms]
Dec 19 07:30:52.525: INFO: Got endpoints: latency-svc-2d8nd [91.448773ms]
Dec 19 07:30:52.526: INFO: Created: latency-svc-k45v6
Dec 19 07:30:52.530: INFO: Got endpoints: latency-svc-k45v6 [90.771451ms]
Dec 19 07:30:52.530: INFO: Created: latency-svc-7t7hd
Dec 19 07:30:52.537: INFO: Got endpoints: latency-svc-7t7hd [95.651177ms]
Dec 19 07:30:52.539: INFO: Created: latency-svc-vzb4z
Dec 19 07:30:52.542: INFO: Created: latency-svc-2t5zx
Dec 19 07:30:52.544: INFO: Got endpoints: latency-svc-vzb4z [95.902548ms]
Dec 19 07:30:52.549: INFO: Created: latency-svc-6nvrl
Dec 19 07:30:52.550: INFO: Got endpoints: latency-svc-2t5zx [90.883999ms]
Dec 19 07:30:52.554: INFO: Got endpoints: latency-svc-6nvrl [94.577241ms]
Dec 19 07:30:52.558: INFO: Created: latency-svc-gwncm
Dec 19 07:30:52.560: INFO: Created: latency-svc-fsh5q
Dec 19 07:30:52.560: INFO: Got endpoints: latency-svc-gwncm [90.918046ms]
Dec 19 07:30:52.565: INFO: Got endpoints: latency-svc-fsh5q [86.716477ms]
Dec 19 07:30:52.565: INFO: Created: latency-svc-dsfzt
Dec 19 07:30:52.571: INFO: Got endpoints: latency-svc-dsfzt [87.078786ms]
Dec 19 07:30:52.572: INFO: Created: latency-svc-5kf7n
Dec 19 07:30:52.577: INFO: Got endpoints: latency-svc-5kf7n [85.33992ms]
Dec 19 07:30:52.578: INFO: Created: latency-svc-64wt8
Dec 19 07:30:52.591: INFO: Got endpoints: latency-svc-64wt8 [90.449661ms]
Dec 19 07:30:52.591: INFO: Created: latency-svc-wbwxx
Dec 19 07:30:52.602: INFO: Created: latency-svc-z89gr
Dec 19 07:30:52.602: INFO: Got endpoints: latency-svc-wbwxx [93.780916ms]
Dec 19 07:30:52.607: INFO: Got endpoints: latency-svc-z89gr [94.330034ms]
Dec 19 07:30:52.608: INFO: Created: latency-svc-qd57j
Dec 19 07:30:52.611: INFO: Got endpoints: latency-svc-qd57j [96.124105ms]
Dec 19 07:30:52.613: INFO: Created: latency-svc-98sff
Dec 19 07:30:52.615: INFO: Got endpoints: latency-svc-98sff [93.761997ms]
Dec 19 07:30:52.617: INFO: Created: latency-svc-5wlmt
Dec 19 07:30:52.622: INFO: Got endpoints: latency-svc-5wlmt [96.811542ms]
Dec 19 07:30:52.623: INFO: Created: latency-svc-ph8ck
Dec 19 07:30:52.627: INFO: Created: latency-svc-nmlmg
Dec 19 07:30:52.627: INFO: Got endpoints: latency-svc-ph8ck [97.484619ms]
Dec 19 07:30:52.636: INFO: Got endpoints: latency-svc-nmlmg [98.661662ms]
Dec 19 07:30:52.637: INFO: Created: latency-svc-8stqw
Dec 19 07:30:52.642: INFO: Created: latency-svc-m8md4
Dec 19 07:30:52.642: INFO: Got endpoints: latency-svc-8stqw [97.961175ms]
Dec 19 07:30:52.647: INFO: Created: latency-svc-64x5d
Dec 19 07:30:52.647: INFO: Got endpoints: latency-svc-m8md4 [96.990251ms]
Dec 19 07:30:52.650: INFO: Got endpoints: latency-svc-64x5d [95.994153ms]
Dec 19 07:30:52.654: INFO: Created: latency-svc-2v622
Dec 19 07:30:52.658: INFO: Got endpoints: latency-svc-2v622 [97.540166ms]
Dec 19 07:30:52.659: INFO: Created: latency-svc-96c7v
Dec 19 07:30:52.660: INFO: Got endpoints: latency-svc-96c7v [95.20011ms]
Dec 19 07:30:52.663: INFO: Created: latency-svc-f5vtv
Dec 19 07:30:52.667: INFO: Got endpoints: latency-svc-f5vtv [95.632843ms]
Dec 19 07:30:52.667: INFO: Created: latency-svc-qgcdg
Dec 19 07:30:52.671: INFO: Got endpoints: latency-svc-qgcdg [93.921004ms]
Dec 19 07:30:52.672: INFO: Created: latency-svc-dqdvn
Dec 19 07:30:52.675: INFO: Got endpoints: latency-svc-dqdvn [84.433727ms]
Dec 19 07:30:52.675: INFO: Created: latency-svc-jrm8s
Dec 19 07:30:52.680: INFO: Created: latency-svc-dq6dh
Dec 19 07:30:52.680: INFO: Got endpoints: latency-svc-jrm8s [77.486562ms]
Dec 19 07:30:52.685: INFO: Created: latency-svc-jc7pv
Dec 19 07:30:52.685: INFO: Got endpoints: latency-svc-dq6dh [77.090311ms]
Dec 19 07:30:52.690: INFO: Got endpoints: latency-svc-jc7pv [78.762499ms]
Dec 19 07:30:52.690: INFO: Created: latency-svc-6n54h
Dec 19 07:30:52.697: INFO: Created: latency-svc-689cl
Dec 19 07:30:52.697: INFO: Got endpoints: latency-svc-6n54h [81.56382ms]
Dec 19 07:30:52.701: INFO: Got endpoints: latency-svc-689cl [78.77812ms]
Dec 19 07:30:52.701: INFO: Created: latency-svc-gll54
Dec 19 07:30:52.705: INFO: Got endpoints: latency-svc-gll54 [77.753611ms]
Dec 19 07:30:52.706: INFO: Created: latency-svc-mw9tn
Dec 19 07:30:52.708: INFO: Got endpoints: latency-svc-mw9tn [71.982892ms]
Dec 19 07:30:52.712: INFO: Created: latency-svc-kfcmw
Dec 19 07:30:52.715: INFO: Got endpoints: latency-svc-kfcmw [73.085043ms]
Dec 19 07:30:52.716: INFO: Created: latency-svc-k4mwm
Dec 19 07:30:52.718: INFO: Got endpoints: latency-svc-k4mwm [71.382248ms]
Dec 19 07:30:52.718: INFO: Latencies: [12.815977ms 15.774974ms 23.093935ms 29.922079ms 57.638431ms 65.803514ms 71.382248ms 71.982892ms 73.085043ms 74.134048ms 77.090311ms 77.486562ms 77.753611ms 78.242671ms 78.541844ms 78.762499ms 78.77812ms 79.337716ms 79.838629ms 80.750525ms 80.883823ms 81.56382ms 81.868263ms 82.422755ms 82.562784ms 82.667085ms 82.714462ms 82.802372ms 82.884115ms 82.888847ms 82.942346ms 82.985758ms 83.166708ms 83.386472ms 83.536878ms 83.663251ms 83.902257ms 84.433727ms 84.725159ms 84.782444ms 85.027424ms 85.133144ms 85.250471ms 85.302854ms 85.33992ms 85.886277ms 86.006423ms 86.05691ms 86.057931ms 86.262793ms 86.442271ms 86.716477ms 86.927833ms 87.078786ms 87.138873ms 87.157025ms 87.394613ms 87.4725ms 87.52484ms 88.322927ms 88.344539ms 88.577408ms 88.967502ms 89.351669ms 89.535363ms 89.883694ms 89.899469ms 90.405464ms 90.449661ms 90.771451ms 90.883999ms 90.918046ms 91.18333ms 91.25809ms 91.448773ms 91.80909ms 91.936957ms 91.947407ms 91.960234ms 91.996632ms 92.075195ms 92.144358ms 92.709482ms 92.789391ms 92.933264ms 93.294388ms 93.321807ms 93.407807ms 93.761997ms 93.780916ms 93.921004ms 94.00213ms 94.164531ms 94.330034ms 94.522989ms 94.554307ms 94.577241ms 94.658606ms 94.966302ms 95.178766ms 95.20011ms 95.218502ms 95.325047ms 95.627595ms 95.632843ms 95.651177ms 95.902548ms 95.983615ms 95.994153ms 96.08485ms 96.124105ms 96.695729ms 96.811542ms 96.990251ms 97.436619ms 97.484619ms 97.491283ms 97.540166ms 97.691368ms 97.823395ms 97.948811ms 97.961175ms 98.031733ms 98.089586ms 98.098243ms 98.137023ms 98.1601ms 98.289286ms 98.328564ms 98.644398ms 98.661662ms 98.826742ms 98.976283ms 99.135092ms 99.16718ms 99.262841ms 99.478085ms 99.486315ms 99.555898ms 99.688253ms 100.818571ms 100.972189ms 101.387469ms 101.903569ms 102.088407ms 103.147065ms 103.530328ms 103.760623ms 104.584878ms 105.382414ms 105.902313ms 106.096862ms 106.992751ms 107.01022ms 107.661164ms 107.945414ms 107.984158ms 108.061948ms 108.134526ms 108.173759ms 108.539927ms 109.046899ms 109.431916ms 110.856819ms 110.958214ms 111.022601ms 111.834452ms 112.358026ms 112.897151ms 112.977006ms 112.979061ms 113.956985ms 114.598403ms 116.139409ms 116.645016ms 116.960365ms 117.236687ms 118.416943ms 119.607557ms 119.935665ms 120.390367ms 120.400926ms 121.82719ms 122.291811ms 122.348022ms 124.749143ms 125.550537ms 125.761294ms 125.850107ms 125.963048ms 126.902445ms 127.197455ms 137.453738ms 144.176527ms 148.154677ms 149.228777ms 150.838773ms 151.202884ms 154.708398ms 161.918721ms]
Dec 19 07:30:52.718: INFO: 50 %ile: 95.20011ms
Dec 19 07:30:52.718: INFO: 90 %ile: 120.390367ms
Dec 19 07:30:52.718: INFO: 99 %ile: 154.708398ms
Dec 19 07:30:52.718: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:30:52.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2378" for this suite.
Dec 19 07:31:12.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:31:12.771: INFO: namespace svc-latency-2378 deletion completed in 20.050038077s

• [SLOW TEST:23.558 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:31:12.772: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:31:12.788: INFO: Creating deployment "nginx-deployment"
Dec 19 07:31:12.791: INFO: Waiting for observed generation 1
Dec 19 07:31:14.794: INFO: Waiting for all required pods to come up
Dec 19 07:31:14.797: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 19 07:31:18.802: INFO: Waiting for deployment "nginx-deployment" to complete
Dec 19 07:31:18.805: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec 19 07:31:18.808: INFO: Updating deployment nginx-deployment
Dec 19 07:31:18.809: INFO: Waiting for observed generation 2
Dec 19 07:31:20.813: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 19 07:31:20.814: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 19 07:31:20.816: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 19 07:31:20.820: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 19 07:31:20.820: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 19 07:31:20.822: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 19 07:31:20.824: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec 19 07:31:20.824: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec 19 07:31:20.828: INFO: Updating deployment nginx-deployment
Dec 19 07:31:20.828: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec 19 07:31:20.838: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 19 07:31:20.841: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 19 07:31:20.854: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9351,SelfLink:/apis/apps/v1/namespaces/deployment-9351/deployments/nginx-deployment,UID:64b34c46-42d4-43df-b80b-ba94ae38ed18,ResourceVersion:670133,Generation:3,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-12-19 07:31:18 +0000 UTC 2019-12-19 07:31:12 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-12-19 07:31:20 +0000 UTC 2019-12-19 07:31:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec 19 07:31:20.861: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-9351,SelfLink:/apis/apps/v1/namespaces/deployment-9351/replicasets/nginx-deployment-55fb7cb77f,UID:9019a186-2c83-40c8-a5d9-eda42214b1f7,ResourceVersion:670131,Generation:3,CreationTimestamp:2019-12-19 07:31:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 64b34c46-42d4-43df-b80b-ba94ae38ed18 0xc0019187f7 0xc0019187f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 07:31:20.861: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec 19 07:31:20.861: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-9351,SelfLink:/apis/apps/v1/namespaces/deployment-9351/replicasets/nginx-deployment-7b8c6f4498,UID:0b109fb0-aeca-40d7-abab-be11550aa442,ResourceVersion:670129,Generation:3,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 64b34c46-42d4-43df-b80b-ba94ae38ed18 0xc0019188c7 0xc0019188c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-8c2qd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8c2qd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-8c2qd,UID:bae2b2c5-40ed-4364-8b48-8a3de7b08a51,ResourceVersion:670138,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc001919257 0xc001919258}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019192c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019192e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738a10} {timeout 0xc002738a20}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-cp257" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cp257,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-cp257,UID:8bda0305-78d1-4738-a01a-337f8b508a6f,ResourceVersion:670103,Generation:0,CreationTimestamp:2019-12-19 07:31:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc001919350 0xc001919351}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019193d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019193f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738a30} {timeout 0xc002738a40}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2019-12-19 07:31:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-hckm2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hckm2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-hckm2,UID:db532e9d-67e2-442c-9408-23b85afa7d8c,ResourceVersion:670147,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc0019194c0 0xc0019194c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{timeout 0xc002738a50} {single-request-reopen 0xc002738a80}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-jtxqd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-jtxqd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-jtxqd,UID:4cc02834-bbe6-459a-bf7f-95485713c7ce,ResourceVersion:670108,Generation:0,CreationTimestamp:2019-12-19 07:31:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc0019195e0 0xc0019195e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738a90} {timeout 0xc002738aa0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2019-12-19 07:31:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-pg6zw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pg6zw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-pg6zw,UID:16bbb0a9-9427-4e46-9398-c570a7fc4d8f,ResourceVersion:670119,Generation:0,CreationTimestamp:2019-12-19 07:31:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc001919750 0xc001919751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019197d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019197f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738ab0} {timeout 0xc002738ac0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2019-12-19 07:31:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-qrbcm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qrbcm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-qrbcm,UID:1195eee4-0885-4cb1-87db-300a80b6c882,ResourceVersion:670115,Generation:0,CreationTimestamp:2019-12-19 07:31:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc0019198c0 0xc0019198c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738ad0} {timeout 0xc002738ae0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:,StartTime:2019-12-19 07:31:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-v9fgn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-v9fgn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-v9fgn,UID:b5087997-f79e-4c0e-b29c-e7dc70adf087,ResourceVersion:670117,Generation:0,CreationTimestamp:2019-12-19 07:31:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc001919a30 0xc001919a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738af0} {timeout 0xc002738b00}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.205,PodIP:,StartTime:2019-12-19 07:31:18 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.883: INFO: Pod "nginx-deployment-55fb7cb77f-zp77w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zp77w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-55fb7cb77f-zp77w,UID:416e5878-5c76-48ff-867b-d28fc7bcbd31,ResourceVersion:670139,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9019a186-2c83-40c8-a5d9-eda42214b1f7 0xc001919ba0 0xc001919ba1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919c20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919c40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738b10} {timeout 0xc002738b20}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-9w6f9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9w6f9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-9w6f9,UID:14337561-c0db-4732-bdae-6b7541941c1a,ResourceVersion:670052,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc001919cc0 0xc001919cc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919d70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738b30} {timeout 0xc002738b40}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.205,PodIP:172.16.0.58,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d580e63bb7c3db4a54c8c6ac2f580107a8f0fa437a947a0ea2998532ad3ba418}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-9wzhj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9wzhj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-9wzhj,UID:1519e6a6-6ad6-4043-be97-dfb35ef7acbd,ResourceVersion:670054,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc001919e40 0xc001919e41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001919eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001919ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738b50} {timeout 0xc002738b60}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.205,PodIP:172.16.0.60,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1bf420f43c33186dd50a35948d8c65effd657f2b802dda7edaa45fc1477dada4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-bhqs7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bhqs7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-bhqs7,UID:e94c5538-dac4-448a-a10e-34790ad619a3,ResourceVersion:670145,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc001919fa0 0xc001919fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738b70} {timeout 0xc002738b80}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-d2qkj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d2qkj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-d2qkj,UID:3519c9fd-aa92-4fea-ae86-933292e65b94,ResourceVersion:670144,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488090 0xc003488091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0034880f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738b90} {timeout 0xc002738ba0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-ddqvc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ddqvc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-ddqvc,UID:de9fb4d7-873a-4f41-b52b-4e7cda8e338e,ResourceVersion:670135,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488190 0xc003488191}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738bb0} {timeout 0xc002738bc0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-gsh4z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gsh4z,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-gsh4z,UID:dc79427a-90b5-40fe-906b-6fa688eca667,ResourceVersion:670061,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc0034882a0 0xc0034882a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{timeout 0xc002738bd0} {single-request-reopen 0xc002738be0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.21,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3292c313c2fd0be3a16d46b739531974dd1c9b08e0da33d0374280ed2329d0f9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-jqqqq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jqqqq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-jqqqq,UID:ca82a3c7-1bd6-4c3e-bf1b-ed8eca70f23f,ResourceVersion:670146,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488400 0xc003488401}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738bf0} {timeout 0xc002738c00}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-mqbgq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mqbgq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-mqbgq,UID:f10dcc3d-0e5d-494d-813f-a982bebe831a,ResourceVersion:670073,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc0034884f0 0xc0034884f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738c10} {timeout 0xc002738c20}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.23,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:17 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8db4683d06c9ed46ca51dd4769ee301c59601eccfb5751b3515fef3543fd2421}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-qn5vq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qn5vq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-qn5vq,UID:419a9bb5-69cc-4c41-a330-da5fc0869507,ResourceVersion:670141,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488650 0xc003488651}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0034886c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0034886e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{timeout 0xc002738c30} {single-request-reopen 0xc002738c40}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-r2p8l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r2p8l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-r2p8l,UID:dc773cb5-6d38-437b-b698-b549853e0c55,ResourceVersion:670066,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488760 0xc003488761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0034887d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0034887f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738c50} {timeout 0xc002738c60}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.205,PodIP:172.16.0.61,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://142e569dd64a964f87c357026b2bc97788689601c10c300c73c6e4c9b6a1a803}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-rlq4b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rlq4b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-rlq4b,UID:8a626a17-7ec7-472f-ac4f-7fe6bfa00054,ResourceVersion:670142,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc0034888c0 0xc0034888c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738c70} {timeout 0xc002738c80}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-sbg2f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sbg2f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-sbg2f,UID:b1899952-6c6e-4b91-8f1f-e424ac17012a,ResourceVersion:670078,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc0034889b0 0xc0034889b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738c90} {timeout 0xc002738ca0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.19,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4e0441f0a43da24ecf484663979f724b5548ca07a4051bc5180ac4209b235b33}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.884: INFO: Pod "nginx-deployment-7b8c6f4498-vx8dp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vx8dp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-vx8dp,UID:ee7193ac-eb29-43f2-acb6-f6de39a081b8,ResourceVersion:670143,Generation:0,CreationTimestamp:2019-12-19 07:31:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488b10 0xc003488b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738cb0} {timeout 0xc002738cc0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:20 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.885: INFO: Pod "nginx-deployment-7b8c6f4498-wc4wv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wc4wv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-wc4wv,UID:d89267ad-5f21-48c6-9ad0-9cc816cada68,ResourceVersion:670047,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488c20 0xc003488c21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.205,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488c90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488cb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738cd0} {timeout 0xc002738ce0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.205,PodIP:172.16.0.54,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3d7472c04e02639e5c6e91cad3121df50a2a275f0aa8833305098d4dac7ac4f4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 19 07:31:20.885: INFO: Pod "nginx-deployment-7b8c6f4498-ztbrz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ztbrz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9351,SelfLink:/api/v1/namespaces/deployment-9351/pods/nginx-deployment-7b8c6f4498-ztbrz,UID:48602643-3c86-4c6b-b838-a4b5ecf53e77,ResourceVersion:670076,Generation:0,CreationTimestamp:2019-12-19 07:31:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 0b109fb0-aeca-40d7-abab-be11550aa442 0xc003488d80 0xc003488d81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnq6p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnq6p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnq6p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003488df0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003488e10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002738cf0} {timeout 0xc002738d00}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 07:31:12 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.20,StartTime:2019-12-19 07:31:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-19 07:31:16 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://76d723ce468815c87713b6307e0a894ab4eb91721b1ec0c14c3eed6644bb1aa3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:31:20.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9351" for this suite.
Dec 19 07:31:26.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:31:26.958: INFO: namespace deployment-9351 deletion completed in 6.067704583s

• [SLOW TEST:14.187 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:31:26.958: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 19 07:31:37.001: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 19 07:31:37.002: INFO: Pod pod-with-poststart-http-hook still exists
Dec 19 07:31:39.002: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 19 07:31:39.004: INFO: Pod pod-with-poststart-http-hook still exists
Dec 19 07:31:41.002: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 19 07:31:41.004: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:31:41.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8346" for this suite.
Dec 19 07:32:03.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:32:03.064: INFO: namespace container-lifecycle-hook-8346 deletion completed in 22.057710132s

• [SLOW TEST:36.106 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:32:03.064: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 19 07:32:03.086: INFO: Waiting up to 5m0s for pod "pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156" in namespace "emptydir-3764" to be "success or failure"
Dec 19 07:32:03.088: INFO: Pod "pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584782ms
Dec 19 07:32:05.089: INFO: Pod "pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003132349s
Dec 19 07:32:07.091: INFO: Pod "pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004956196s
STEP: Saw pod success
Dec 19 07:32:07.091: INFO: Pod "pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156" satisfied condition "success or failure"
Dec 19 07:32:07.093: INFO: Trying to get logs from node 192.168.0.132 pod pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156 container test-container: <nil>
STEP: delete the pod
Dec 19 07:32:07.105: INFO: Waiting for pod pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156 to disappear
Dec 19 07:32:07.107: INFO: Pod pod-d48cfe92-6f7a-4e15-9f0c-b574ca172156 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:32:07.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3764" for this suite.
Dec 19 07:32:13.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:32:13.158: INFO: namespace emptydir-3764 deletion completed in 6.049042806s

• [SLOW TEST:10.093 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:32:13.158: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:32:13.182: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635" in namespace "projected-2501" to be "success or failure"
Dec 19 07:32:13.186: INFO: Pod "downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635": Phase="Pending", Reason="", readiness=false. Elapsed: 3.697064ms
Dec 19 07:32:15.188: INFO: Pod "downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005932926s
Dec 19 07:32:17.190: INFO: Pod "downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008078061s
STEP: Saw pod success
Dec 19 07:32:17.190: INFO: Pod "downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635" satisfied condition "success or failure"
Dec 19 07:32:17.192: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635 container client-container: <nil>
STEP: delete the pod
Dec 19 07:32:17.202: INFO: Waiting for pod downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635 to disappear
Dec 19 07:32:17.210: INFO: Pod downwardapi-volume-ae9dd2d0-1516-4bf9-b08e-e7a1c757e635 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:32:17.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2501" for this suite.
Dec 19 07:32:23.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:32:23.262: INFO: namespace projected-2501 deletion completed in 6.049218065s

• [SLOW TEST:10.104 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:32:23.263: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-cf392a10-0928-49a2-ba23-281e6d03c2d4
STEP: Creating a pod to test consume configMaps
Dec 19 07:32:23.286: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d" in namespace "projected-4868" to be "success or failure"
Dec 19 07:32:23.287: INFO: Pod "pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.140151ms
Dec 19 07:32:25.291: INFO: Pod "pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004762376s
Dec 19 07:32:27.293: INFO: Pod "pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007073666s
STEP: Saw pod success
Dec 19 07:32:27.293: INFO: Pod "pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d" satisfied condition "success or failure"
Dec 19 07:32:27.294: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:32:27.310: INFO: Waiting for pod pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d to disappear
Dec 19 07:32:27.312: INFO: Pod pod-projected-configmaps-dffd2c8d-19d0-4063-bb0a-238cfd8c154d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:32:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4868" for this suite.
Dec 19 07:32:33.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:32:33.373: INFO: namespace projected-4868 deletion completed in 6.059217422s

• [SLOW TEST:10.111 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:32:33.374: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-0abdadb6-10c3-40b0-b170-0e12827026d8
STEP: Creating a pod to test consume secrets
Dec 19 07:32:33.396: INFO: Waiting up to 5m0s for pod "pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70" in namespace "secrets-8287" to be "success or failure"
Dec 19 07:32:33.399: INFO: Pod "pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.199859ms
Dec 19 07:32:35.401: INFO: Pod "pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005214127s
STEP: Saw pod success
Dec 19 07:32:35.401: INFO: Pod "pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70" satisfied condition "success or failure"
Dec 19 07:32:35.403: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70 container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:32:35.411: INFO: Waiting for pod pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70 to disappear
Dec 19 07:32:35.413: INFO: Pod pod-secrets-99d228e9-8ed4-4a21-9976-34b24e113c70 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:32:35.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8287" for this suite.
Dec 19 07:32:41.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:32:41.463: INFO: namespace secrets-8287 deletion completed in 6.048571981s

• [SLOW TEST:8.090 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:32:41.464: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:32:43.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3902" for this suite.
Dec 19 07:32:49.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:32:49.568: INFO: namespace emptydir-wrapper-3902 deletion completed in 6.055860702s

• [SLOW TEST:8.105 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:32:49.568: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-qkn5
STEP: Creating a pod to test atomic-volume-subpath
Dec 19 07:32:49.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qkn5" in namespace "subpath-8659" to be "success or failure"
Dec 19 07:32:49.605: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.53061ms
Dec 19 07:32:51.607: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 2.003549753s
Dec 19 07:32:53.610: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 4.005811265s
Dec 19 07:32:55.612: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 6.007941121s
Dec 19 07:32:57.614: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 8.010323839s
Dec 19 07:32:59.616: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 10.01262425s
Dec 19 07:33:01.619: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 12.014740791s
Dec 19 07:33:03.621: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 14.01674196s
Dec 19 07:33:05.623: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 16.018727082s
Dec 19 07:33:07.625: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 18.020772335s
Dec 19 07:33:09.627: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 20.022879752s
Dec 19 07:33:11.628: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Running", Reason="", readiness=true. Elapsed: 22.024626002s
Dec 19 07:33:13.631: INFO: Pod "pod-subpath-test-downwardapi-qkn5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.026897629s
STEP: Saw pod success
Dec 19 07:33:13.631: INFO: Pod "pod-subpath-test-downwardapi-qkn5" satisfied condition "success or failure"
Dec 19 07:33:13.632: INFO: Trying to get logs from node 192.168.0.132 pod pod-subpath-test-downwardapi-qkn5 container test-container-subpath-downwardapi-qkn5: <nil>
STEP: delete the pod
Dec 19 07:33:13.642: INFO: Waiting for pod pod-subpath-test-downwardapi-qkn5 to disappear
Dec 19 07:33:13.645: INFO: Pod pod-subpath-test-downwardapi-qkn5 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qkn5
Dec 19 07:33:13.645: INFO: Deleting pod "pod-subpath-test-downwardapi-qkn5" in namespace "subpath-8659"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:33:13.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8659" for this suite.
Dec 19 07:33:19.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:33:19.700: INFO: namespace subpath-8659 deletion completed in 6.051734275s

• [SLOW TEST:30.132 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:33:19.700: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec 19 07:33:19.718: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-317660674 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:33:19.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9384" for this suite.
Dec 19 07:33:25.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:33:25.815: INFO: namespace kubectl-9384 deletion completed in 6.047073179s

• [SLOW TEST:6.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:33:25.816: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-7a33283a-0068-44c2-b8a7-156ca7fc63c7
STEP: Creating a pod to test consume configMaps
Dec 19 07:33:25.837: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9" in namespace "projected-7801" to be "success or failure"
Dec 19 07:33:25.840: INFO: Pod "pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.858105ms
Dec 19 07:33:27.842: INFO: Pod "pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004894826s
STEP: Saw pod success
Dec 19 07:33:27.842: INFO: Pod "pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9" satisfied condition "success or failure"
Dec 19 07:33:27.843: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:33:27.853: INFO: Waiting for pod pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9 to disappear
Dec 19 07:33:27.854: INFO: Pod pod-projected-configmaps-4fe02e6f-1b7d-4c9f-840f-0cacb2a47da9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:33:27.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7801" for this suite.
Dec 19 07:33:33.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:33:33.905: INFO: namespace projected-7801 deletion completed in 6.048638144s

• [SLOW TEST:8.089 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:33:33.905: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:33:33.922: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:33:35.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8598" for this suite.
Dec 19 07:34:13.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:34:13.996: INFO: namespace pods-8598 deletion completed in 38.05204249s

• [SLOW TEST:40.091 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:34:13.996: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4fb5eae1-dde5-402d-910f-2f86474e8945
STEP: Creating a pod to test consume configMaps
Dec 19 07:34:14.018: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6" in namespace "projected-5034" to be "success or failure"
Dec 19 07:34:14.021: INFO: Pod "pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257607ms
Dec 19 07:34:16.023: INFO: Pod "pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00437812s
STEP: Saw pod success
Dec 19 07:34:16.023: INFO: Pod "pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6" satisfied condition "success or failure"
Dec 19 07:34:16.024: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:34:16.037: INFO: Waiting for pod pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6 to disappear
Dec 19 07:34:16.040: INFO: Pod pod-projected-configmaps-cedba3b5-8f49-4b5c-8053-2fd4218f0db6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:34:16.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5034" for this suite.
Dec 19 07:34:22.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:34:22.092: INFO: namespace projected-5034 deletion completed in 6.050017586s

• [SLOW TEST:8.096 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:34:22.092: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 19 07:34:22.112: INFO: Waiting up to 5m0s for pod "pod-87523074-1492-4bee-a4a6-0ae91981064b" in namespace "emptydir-9825" to be "success or failure"
Dec 19 07:34:22.114: INFO: Pod "pod-87523074-1492-4bee-a4a6-0ae91981064b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740102ms
Dec 19 07:34:24.116: INFO: Pod "pod-87523074-1492-4bee-a4a6-0ae91981064b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004109383s
STEP: Saw pod success
Dec 19 07:34:24.116: INFO: Pod "pod-87523074-1492-4bee-a4a6-0ae91981064b" satisfied condition "success or failure"
Dec 19 07:34:24.119: INFO: Trying to get logs from node 192.168.0.132 pod pod-87523074-1492-4bee-a4a6-0ae91981064b container test-container: <nil>
STEP: delete the pod
Dec 19 07:34:24.131: INFO: Waiting for pod pod-87523074-1492-4bee-a4a6-0ae91981064b to disappear
Dec 19 07:34:24.132: INFO: Pod pod-87523074-1492-4bee-a4a6-0ae91981064b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:34:24.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9825" for this suite.
Dec 19 07:34:30.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:34:30.188: INFO: namespace emptydir-9825 deletion completed in 6.053821145s

• [SLOW TEST:8.095 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:34:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1783.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1783.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1783.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1783.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 19 07:34:34.228: INFO: DNS probes using dns-test-84c2c1e6-3cfd-4b1b-b7c7-3699a9e58d63 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1783.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1783.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1783.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1783.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 19 07:34:38.268: INFO: File wheezy_udp@dns-test-service-3.dns-1783.svc.cluster.local from pod  dns-1783/dns-test-d839ffcc-8592-4afd-abed-2ca739076d8d contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 19 07:34:38.271: INFO: Lookups using dns-1783/dns-test-d839ffcc-8592-4afd-abed-2ca739076d8d failed for: [wheezy_udp@dns-test-service-3.dns-1783.svc.cluster.local]

Dec 19 07:34:43.276: INFO: DNS probes using dns-test-d839ffcc-8592-4afd-abed-2ca739076d8d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1783.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1783.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1783.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1783.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 19 07:34:47.322: INFO: DNS probes using dns-test-046f3f91-9610-4009-aea1-7d43588c4370 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:34:47.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1783" for this suite.
Dec 19 07:34:53.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:34:53.400: INFO: namespace dns-1783 deletion completed in 6.055977862s

• [SLOW TEST:23.212 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:34:53.400: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 19 07:34:53.421: INFO: Waiting up to 5m0s for pod "pod-30d78d0c-0212-4a0e-86a3-e4cdae742506" in namespace "emptydir-689" to be "success or failure"
Dec 19 07:34:53.423: INFO: Pod "pod-30d78d0c-0212-4a0e-86a3-e4cdae742506": Phase="Pending", Reason="", readiness=false. Elapsed: 1.748255ms
Dec 19 07:34:55.425: INFO: Pod "pod-30d78d0c-0212-4a0e-86a3-e4cdae742506": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00386974s
Dec 19 07:34:57.427: INFO: Pod "pod-30d78d0c-0212-4a0e-86a3-e4cdae742506": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006155533s
STEP: Saw pod success
Dec 19 07:34:57.428: INFO: Pod "pod-30d78d0c-0212-4a0e-86a3-e4cdae742506" satisfied condition "success or failure"
Dec 19 07:34:57.429: INFO: Trying to get logs from node 192.168.0.132 pod pod-30d78d0c-0212-4a0e-86a3-e4cdae742506 container test-container: <nil>
STEP: delete the pod
Dec 19 07:34:57.443: INFO: Waiting for pod pod-30d78d0c-0212-4a0e-86a3-e4cdae742506 to disappear
Dec 19 07:34:57.444: INFO: Pod pod-30d78d0c-0212-4a0e-86a3-e4cdae742506 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:34:57.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-689" for this suite.
Dec 19 07:35:03.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:35:03.499: INFO: namespace emptydir-689 deletion completed in 6.053553821s

• [SLOW TEST:10.100 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:35:03.499: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 19 07:35:13.540: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1219 07:35:13.540890      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:35:13.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8981" for this suite.
Dec 19 07:35:19.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:35:19.592: INFO: namespace gc-8981 deletion completed in 6.049401363s

• [SLOW TEST:16.092 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:35:19.592: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 19 07:35:19.630: INFO: Number of nodes with available pods: 0
Dec 19 07:35:19.630: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:35:20.634: INFO: Number of nodes with available pods: 0
Dec 19 07:35:20.634: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:35:21.634: INFO: Number of nodes with available pods: 1
Dec 19 07:35:21.634: INFO: Node 192.168.0.205 is running more than one daemon pod
Dec 19 07:35:22.634: INFO: Number of nodes with available pods: 2
Dec 19 07:35:22.634: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 19 07:35:22.649: INFO: Number of nodes with available pods: 1
Dec 19 07:35:22.649: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:35:23.653: INFO: Number of nodes with available pods: 1
Dec 19 07:35:23.653: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:35:24.653: INFO: Number of nodes with available pods: 2
Dec 19 07:35:24.653: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2835, will wait for the garbage collector to delete the pods
Dec 19 07:35:24.711: INFO: Deleting DaemonSet.extensions daemon-set took: 3.59566ms
Dec 19 07:35:24.811: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.13299ms
Dec 19 07:35:36.713: INFO: Number of nodes with available pods: 0
Dec 19 07:35:36.713: INFO: Number of running nodes: 0, number of available pods: 0
Dec 19 07:35:36.714: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2835/daemonsets","resourceVersion":"671453"},"items":null}

Dec 19 07:35:36.716: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2835/pods","resourceVersion":"671453"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:35:36.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2835" for this suite.
Dec 19 07:35:42.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:35:42.770: INFO: namespace daemonsets-2835 deletion completed in 6.047790148s

• [SLOW TEST:23.178 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:35:42.770: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-deac14da-ef22-4e94-85dc-e70d7ecfae85
STEP: Creating secret with name s-test-opt-upd-f702b366-2e0c-4767-a330-dd3d1ba6fa2e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-deac14da-ef22-4e94-85dc-e70d7ecfae85
STEP: Updating secret s-test-opt-upd-f702b366-2e0c-4767-a330-dd3d1ba6fa2e
STEP: Creating secret with name s-test-opt-create-5ac37789-15d2-4568-abb3-0109d7d83e40
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:35:48.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8203" for this suite.
Dec 19 07:36:10.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:36:10.890: INFO: namespace secrets-8203 deletion completed in 22.046100797s

• [SLOW TEST:28.120 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:36:10.890: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:36:16.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2635" for this suite.
Dec 19 07:36:22.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:36:22.998: INFO: namespace namespaces-2635 deletion completed in 6.049855018s
STEP: Destroying namespace "nsdeletetest-6883" for this suite.
Dec 19 07:36:23.000: INFO: Namespace nsdeletetest-6883 was already deleted
STEP: Destroying namespace "nsdeletetest-4947" for this suite.
Dec 19 07:36:29.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:36:29.050: INFO: namespace nsdeletetest-4947 deletion completed in 6.050492428s

• [SLOW TEST:18.160 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:36:29.050: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:36:29.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349" in namespace "projected-4111" to be "success or failure"
Dec 19 07:36:29.075: INFO: Pod "downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452482ms
Dec 19 07:36:31.077: INFO: Pod "downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004499621s
Dec 19 07:36:33.079: INFO: Pod "downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006874302s
STEP: Saw pod success
Dec 19 07:36:33.079: INFO: Pod "downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349" satisfied condition "success or failure"
Dec 19 07:36:33.081: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349 container client-container: <nil>
STEP: delete the pod
Dec 19 07:36:33.094: INFO: Waiting for pod downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349 to disappear
Dec 19 07:36:33.096: INFO: Pod downwardapi-volume-cfadbdf9-11d7-4e96-91bd-745a216c2349 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:36:33.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4111" for this suite.
Dec 19 07:36:39.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:36:39.149: INFO: namespace projected-4111 deletion completed in 6.051638718s

• [SLOW TEST:10.099 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:36:39.149: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-11ab7686-3ede-48f5-9400-0e69feb96e51
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:36:43.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6961" for this suite.
Dec 19 07:37:05.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:37:05.240: INFO: namespace configmap-6961 deletion completed in 22.050404687s

• [SLOW TEST:26.091 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:37:05.240: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec 19 07:37:05.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-7210'
Dec 19 07:37:05.588: INFO: stderr: ""
Dec 19 07:37:05.588: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 19 07:37:05.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7210'
Dec 19 07:37:05.657: INFO: stderr: ""
Dec 19 07:37:05.657: INFO: stdout: "update-demo-nautilus-j99lp update-demo-nautilus-p86t2 "
Dec 19 07:37:05.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-j99lp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:05.724: INFO: stderr: ""
Dec 19 07:37:05.724: INFO: stdout: ""
Dec 19 07:37:05.724: INFO: update-demo-nautilus-j99lp is created but not running
Dec 19 07:37:10.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7210'
Dec 19 07:37:10.777: INFO: stderr: ""
Dec 19 07:37:10.777: INFO: stdout: "update-demo-nautilus-j99lp update-demo-nautilus-p86t2 "
Dec 19 07:37:10.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-j99lp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:10.828: INFO: stderr: ""
Dec 19 07:37:10.828: INFO: stdout: "true"
Dec 19 07:37:10.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-j99lp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:10.879: INFO: stderr: ""
Dec 19 07:37:10.879: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:37:10.879: INFO: validating pod update-demo-nautilus-j99lp
Dec 19 07:37:10.892: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:37:10.892: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:37:10.892: INFO: update-demo-nautilus-j99lp is verified up and running
Dec 19 07:37:10.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-p86t2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:10.942: INFO: stderr: ""
Dec 19 07:37:10.942: INFO: stdout: "true"
Dec 19 07:37:10.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-p86t2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:10.991: INFO: stderr: ""
Dec 19 07:37:10.991: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:37:10.991: INFO: validating pod update-demo-nautilus-p86t2
Dec 19 07:37:11.040: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:37:11.040: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:37:11.040: INFO: update-demo-nautilus-p86t2 is verified up and running
STEP: rolling-update to new replication controller
Dec 19 07:37:11.041: INFO: scanned /root for discovery docs: <nil>
Dec 19 07:37:11.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7210'
Dec 19 07:37:33.270: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 19 07:37:33.270: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 19 07:37:33.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7210'
Dec 19 07:37:33.345: INFO: stderr: ""
Dec 19 07:37:33.345: INFO: stdout: "update-demo-kitten-xgshz update-demo-kitten-xmrhg "
Dec 19 07:37:33.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-kitten-xgshz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:33.395: INFO: stderr: ""
Dec 19 07:37:33.395: INFO: stdout: "true"
Dec 19 07:37:33.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-kitten-xgshz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:33.457: INFO: stderr: ""
Dec 19 07:37:33.457: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 19 07:37:33.457: INFO: validating pod update-demo-kitten-xgshz
Dec 19 07:37:33.524: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 19 07:37:33.524: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 19 07:37:33.524: INFO: update-demo-kitten-xgshz is verified up and running
Dec 19 07:37:33.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-kitten-xmrhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:33.576: INFO: stderr: ""
Dec 19 07:37:33.576: INFO: stdout: "true"
Dec 19 07:37:33.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-kitten-xmrhg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7210'
Dec 19 07:37:33.628: INFO: stderr: ""
Dec 19 07:37:33.628: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 19 07:37:33.628: INFO: validating pod update-demo-kitten-xmrhg
Dec 19 07:37:33.640: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 19 07:37:33.640: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 19 07:37:33.640: INFO: update-demo-kitten-xmrhg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:37:33.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7210" for this suite.
Dec 19 07:37:55.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:37:55.693: INFO: namespace kubectl-7210 deletion completed in 22.050814983s

• [SLOW TEST:50.453 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:37:55.693: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec 19 07:37:55.718: INFO: Waiting up to 5m0s for pod "client-containers-918c1705-b3c1-4957-823d-e001271f69eb" in namespace "containers-6629" to be "success or failure"
Dec 19 07:37:55.719: INFO: Pod "client-containers-918c1705-b3c1-4957-823d-e001271f69eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.411326ms
Dec 19 07:37:57.721: INFO: Pod "client-containers-918c1705-b3c1-4957-823d-e001271f69eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003369808s
Dec 19 07:37:59.724: INFO: Pod "client-containers-918c1705-b3c1-4957-823d-e001271f69eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005934944s
STEP: Saw pod success
Dec 19 07:37:59.724: INFO: Pod "client-containers-918c1705-b3c1-4957-823d-e001271f69eb" satisfied condition "success or failure"
Dec 19 07:37:59.725: INFO: Trying to get logs from node 192.168.0.132 pod client-containers-918c1705-b3c1-4957-823d-e001271f69eb container test-container: <nil>
STEP: delete the pod
Dec 19 07:37:59.741: INFO: Waiting for pod client-containers-918c1705-b3c1-4957-823d-e001271f69eb to disappear
Dec 19 07:37:59.743: INFO: Pod client-containers-918c1705-b3c1-4957-823d-e001271f69eb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:37:59.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6629" for this suite.
Dec 19 07:38:05.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:38:05.802: INFO: namespace containers-6629 deletion completed in 6.057011285s

• [SLOW TEST:10.110 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:38:05.803: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1593/configmap-test-ff22eb49-c31d-4e0c-a9f0-6432e02396f5
STEP: Creating a pod to test consume configMaps
Dec 19 07:38:05.835: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038" in namespace "configmap-1593" to be "success or failure"
Dec 19 07:38:05.837: INFO: Pod "pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038": Phase="Pending", Reason="", readiness=false. Elapsed: 1.644499ms
Dec 19 07:38:07.839: INFO: Pod "pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003744602s
STEP: Saw pod success
Dec 19 07:38:07.839: INFO: Pod "pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038" satisfied condition "success or failure"
Dec 19 07:38:07.840: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038 container env-test: <nil>
STEP: delete the pod
Dec 19 07:38:07.852: INFO: Waiting for pod pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038 to disappear
Dec 19 07:38:07.854: INFO: Pod pod-configmaps-c2e8fffe-a738-4669-9082-df5913405038 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:38:07.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1593" for this suite.
Dec 19 07:38:13.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:38:13.979: INFO: namespace configmap-1593 deletion completed in 6.122979901s

• [SLOW TEST:8.176 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:38:13.979: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-4ec8f329-70b5-4cde-9168-9b875471cd17
STEP: Creating a pod to test consume secrets
Dec 19 07:38:14.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316" in namespace "projected-6376" to be "success or failure"
Dec 19 07:38:14.014: INFO: Pod "pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316": Phase="Pending", Reason="", readiness=false. Elapsed: 2.781121ms
Dec 19 07:38:16.016: INFO: Pod "pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004888504s
STEP: Saw pod success
Dec 19 07:38:16.016: INFO: Pod "pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316" satisfied condition "success or failure"
Dec 19 07:38:16.017: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:38:16.027: INFO: Waiting for pod pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316 to disappear
Dec 19 07:38:16.029: INFO: Pod pod-projected-secrets-3b22ab7f-aecd-45b7-9140-b14915af6316 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:38:16.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6376" for this suite.
Dec 19 07:38:22.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:38:22.082: INFO: namespace projected-6376 deletion completed in 6.05132359s

• [SLOW TEST:8.103 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:38:22.082: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-039b67f7-0c67-4afc-b749-f44620387e8c in namespace container-probe-5410
Dec 19 07:38:24.120: INFO: Started pod liveness-039b67f7-0c67-4afc-b749-f44620387e8c in namespace container-probe-5410
STEP: checking the pod's current state and verifying that restartCount is present
Dec 19 07:38:24.122: INFO: Initial restart count of pod liveness-039b67f7-0c67-4afc-b749-f44620387e8c is 0
Dec 19 07:38:46.148: INFO: Restart count of pod container-probe-5410/liveness-039b67f7-0c67-4afc-b749-f44620387e8c is now 1 (22.025991991s elapsed)
Dec 19 07:39:06.169: INFO: Restart count of pod container-probe-5410/liveness-039b67f7-0c67-4afc-b749-f44620387e8c is now 2 (42.047535177s elapsed)
Dec 19 07:39:26.190: INFO: Restart count of pod container-probe-5410/liveness-039b67f7-0c67-4afc-b749-f44620387e8c is now 3 (1m2.068558439s elapsed)
Dec 19 07:39:46.212: INFO: Restart count of pod container-probe-5410/liveness-039b67f7-0c67-4afc-b749-f44620387e8c is now 4 (1m22.089829416s elapsed)
Dec 19 07:40:48.279: INFO: Restart count of pod container-probe-5410/liveness-039b67f7-0c67-4afc-b749-f44620387e8c is now 5 (2m24.157586023s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:40:48.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5410" for this suite.
Dec 19 07:40:54.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:40:54.338: INFO: namespace container-probe-5410 deletion completed in 6.049369651s

• [SLOW TEST:152.257 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:40:54.339: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 19 07:40:54.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-4728'
Dec 19 07:40:54.500: INFO: stderr: ""
Dec 19 07:40:54.500: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 19 07:40:54.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4728'
Dec 19 07:40:54.553: INFO: stderr: ""
Dec 19 07:40:54.553: INFO: stdout: "update-demo-nautilus-p8f2q update-demo-nautilus-wl2qm "
Dec 19 07:40:54.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-p8f2q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4728'
Dec 19 07:40:54.602: INFO: stderr: ""
Dec 19 07:40:54.602: INFO: stdout: ""
Dec 19 07:40:54.602: INFO: update-demo-nautilus-p8f2q is created but not running
Dec 19 07:40:59.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4728'
Dec 19 07:40:59.660: INFO: stderr: ""
Dec 19 07:40:59.660: INFO: stdout: "update-demo-nautilus-p8f2q update-demo-nautilus-wl2qm "
Dec 19 07:40:59.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-p8f2q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4728'
Dec 19 07:40:59.710: INFO: stderr: ""
Dec 19 07:40:59.710: INFO: stdout: "true"
Dec 19 07:40:59.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-p8f2q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4728'
Dec 19 07:40:59.760: INFO: stderr: ""
Dec 19 07:40:59.760: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:40:59.760: INFO: validating pod update-demo-nautilus-p8f2q
Dec 19 07:40:59.771: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:40:59.771: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:40:59.771: INFO: update-demo-nautilus-p8f2q is verified up and running
Dec 19 07:40:59.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wl2qm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4728'
Dec 19 07:40:59.820: INFO: stderr: ""
Dec 19 07:40:59.820: INFO: stdout: "true"
Dec 19 07:40:59.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods update-demo-nautilus-wl2qm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4728'
Dec 19 07:40:59.870: INFO: stderr: ""
Dec 19 07:40:59.870: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 19 07:40:59.870: INFO: validating pod update-demo-nautilus-wl2qm
Dec 19 07:40:59.905: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 19 07:40:59.905: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 19 07:40:59.905: INFO: update-demo-nautilus-wl2qm is verified up and running
STEP: using delete to clean up resources
Dec 19 07:40:59.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete --grace-period=0 --force -f - --namespace=kubectl-4728'
Dec 19 07:40:59.959: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 19 07:40:59.959: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 19 07:40:59.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4728'
Dec 19 07:41:00.013: INFO: stderr: "No resources found.\n"
Dec 19 07:41:00.013: INFO: stdout: ""
Dec 19 07:41:00.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pods -l name=update-demo --namespace=kubectl-4728 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 19 07:41:00.067: INFO: stderr: ""
Dec 19 07:41:00.067: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:41:00.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4728" for this suite.
Dec 19 07:41:06.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:41:06.124: INFO: namespace kubectl-4728 deletion completed in 6.053987199s

• [SLOW TEST:11.785 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:41:06.124: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 19 07:41:11.158: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:41:12.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6046" for this suite.
Dec 19 07:41:34.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:41:34.226: INFO: namespace replicaset-6046 deletion completed in 22.05963844s

• [SLOW TEST:28.103 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:41:34.227: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-be9b9b51-76c3-4f73-9d94-d6dfd3487932
STEP: Creating a pod to test consume configMaps
Dec 19 07:41:34.256: INFO: Waiting up to 5m0s for pod "pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11" in namespace "configmap-7241" to be "success or failure"
Dec 19 07:41:34.257: INFO: Pod "pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11": Phase="Pending", Reason="", readiness=false. Elapsed: 1.09748ms
Dec 19 07:41:36.259: INFO: Pod "pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003246702s
Dec 19 07:41:38.262: INFO: Pod "pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005404341s
STEP: Saw pod success
Dec 19 07:41:38.262: INFO: Pod "pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11" satisfied condition "success or failure"
Dec 19 07:41:38.263: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:41:38.276: INFO: Waiting for pod pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11 to disappear
Dec 19 07:41:38.277: INFO: Pod pod-configmaps-ddd53611-dca9-460c-9457-60073e5b4e11 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:41:38.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7241" for this suite.
Dec 19 07:41:44.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:41:44.329: INFO: namespace configmap-7241 deletion completed in 6.049958118s

• [SLOW TEST:10.103 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:41:44.329: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 19 07:41:48.865: INFO: Successfully updated pod "labelsupdatec5099c33-06bc-4093-b3f8-f5ac3addcdec"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:41:50.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1646" for this suite.
Dec 19 07:42:12.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:42:13.015: INFO: namespace projected-1646 deletion completed in 22.134967778s

• [SLOW TEST:28.686 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:42:13.015: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 19 07:42:13.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7656,SelfLink:/api/v1/namespaces/watch-7656/configmaps/e2e-watch-test-resource-version,UID:002a1824-1743-4107-9db9-9f4eed639c5c,ResourceVersion:673157,Generation:0,CreationTimestamp:2019-12-19 07:42:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 19 07:42:13.044: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7656,SelfLink:/api/v1/namespaces/watch-7656/configmaps/e2e-watch-test-resource-version,UID:002a1824-1743-4107-9db9-9f4eed639c5c,ResourceVersion:673158,Generation:0,CreationTimestamp:2019-12-19 07:42:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:42:13.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7656" for this suite.
Dec 19 07:42:19.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:42:19.099: INFO: namespace watch-7656 deletion completed in 6.052834782s

• [SLOW TEST:6.084 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:42:19.099: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 19 07:42:19.134: INFO: Number of nodes with available pods: 0
Dec 19 07:42:19.134: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:20.138: INFO: Number of nodes with available pods: 0
Dec 19 07:42:20.138: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:21.142: INFO: Number of nodes with available pods: 2
Dec 19 07:42:21.142: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 19 07:42:21.153: INFO: Number of nodes with available pods: 1
Dec 19 07:42:21.153: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:22.158: INFO: Number of nodes with available pods: 1
Dec 19 07:42:22.158: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:23.157: INFO: Number of nodes with available pods: 1
Dec 19 07:42:23.157: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:24.157: INFO: Number of nodes with available pods: 1
Dec 19 07:42:24.157: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:25.157: INFO: Number of nodes with available pods: 1
Dec 19 07:42:25.157: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:26.157: INFO: Number of nodes with available pods: 1
Dec 19 07:42:26.157: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:42:27.158: INFO: Number of nodes with available pods: 2
Dec 19 07:42:27.158: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2385, will wait for the garbage collector to delete the pods
Dec 19 07:42:27.215: INFO: Deleting DaemonSet.extensions daemon-set took: 3.795266ms
Dec 19 07:42:27.315: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.136399ms
Dec 19 07:42:37.917: INFO: Number of nodes with available pods: 0
Dec 19 07:42:37.917: INFO: Number of running nodes: 0, number of available pods: 0
Dec 19 07:42:37.918: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2385/daemonsets","resourceVersion":"673280"},"items":null}

Dec 19 07:42:37.920: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2385/pods","resourceVersion":"673280"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:42:37.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2385" for this suite.
Dec 19 07:42:43.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:42:43.976: INFO: namespace daemonsets-2385 deletion completed in 6.049304434s

• [SLOW TEST:24.877 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:42:43.976: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 19 07:42:43.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 create -f - --namespace=kubectl-679'
Dec 19 07:42:44.118: INFO: stderr: ""
Dec 19 07:42:44.118: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 19 07:42:45.120: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:42:45.120: INFO: Found 0 / 1
Dec 19 07:42:46.120: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:42:46.120: INFO: Found 0 / 1
Dec 19 07:42:47.120: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:42:47.120: INFO: Found 1 / 1
Dec 19 07:42:47.120: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 19 07:42:47.122: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:42:47.122: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 19 07:42:47.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 patch pod redis-master-pll72 --namespace=kubectl-679 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 19 07:42:47.177: INFO: stderr: ""
Dec 19 07:42:47.177: INFO: stdout: "pod/redis-master-pll72 patched\n"
STEP: checking annotations
Dec 19 07:42:47.178: INFO: Selector matched 1 pods for map[app:redis]
Dec 19 07:42:47.179: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:42:47.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-679" for this suite.
Dec 19 07:43:09.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:43:09.231: INFO: namespace kubectl-679 deletion completed in 22.050526955s

• [SLOW TEST:25.255 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:43:09.231: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:43:13.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-35" for this suite.
Dec 19 07:43:57.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:43:57.317: INFO: namespace kubelet-test-35 deletion completed in 44.049332695s

• [SLOW TEST:48.085 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:43:57.317: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-55d8d625-8b11-4b19-9ddc-55a1a30781d8
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-55d8d625-8b11-4b19-9ddc-55a1a30781d8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:45:17.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5251" for this suite.
Dec 19 07:45:39.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:45:39.623: INFO: namespace configmap-5251 deletion completed in 22.05953543s

• [SLOW TEST:102.307 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:45:39.623: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec 19 07:45:39.647: INFO: Waiting up to 5m0s for pod "var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f" in namespace "var-expansion-4107" to be "success or failure"
Dec 19 07:45:39.649: INFO: Pod "var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.890595ms
Dec 19 07:45:41.651: INFO: Pod "var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004002943s
Dec 19 07:45:43.653: INFO: Pod "var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006082072s
STEP: Saw pod success
Dec 19 07:45:43.653: INFO: Pod "var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f" satisfied condition "success or failure"
Dec 19 07:45:43.655: INFO: Trying to get logs from node 192.168.0.132 pod var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f container dapi-container: <nil>
STEP: delete the pod
Dec 19 07:45:43.666: INFO: Waiting for pod var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f to disappear
Dec 19 07:45:43.667: INFO: Pod var-expansion-0a967a43-e468-4780-a34c-fdc41bb1441f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:45:43.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4107" for this suite.
Dec 19 07:45:49.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:45:49.719: INFO: namespace var-expansion-4107 deletion completed in 6.049775301s

• [SLOW TEST:10.096 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:45:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:45:49.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803" in namespace "downward-api-3162" to be "success or failure"
Dec 19 07:45:49.742: INFO: Pod "downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803": Phase="Pending", Reason="", readiness=false. Elapsed: 1.6729ms
Dec 19 07:45:51.744: INFO: Pod "downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003821326s
Dec 19 07:45:53.746: INFO: Pod "downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005836234s
STEP: Saw pod success
Dec 19 07:45:53.746: INFO: Pod "downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803" satisfied condition "success or failure"
Dec 19 07:45:53.748: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803 container client-container: <nil>
STEP: delete the pod
Dec 19 07:45:53.762: INFO: Waiting for pod downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803 to disappear
Dec 19 07:45:53.764: INFO: Pod downwardapi-volume-3d528b5b-0d5c-466c-9e07-5c74333b0803 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:45:53.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3162" for this suite.
Dec 19 07:45:59.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:45:59.815: INFO: namespace downward-api-3162 deletion completed in 6.04946379s

• [SLOW TEST:10.096 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:45:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-bf81a9d8-8e9e-4b5b-ad71-a9c1c024b327 in namespace container-probe-9106
Dec 19 07:46:03.842: INFO: Started pod busybox-bf81a9d8-8e9e-4b5b-ad71-a9c1c024b327 in namespace container-probe-9106
STEP: checking the pod's current state and verifying that restartCount is present
Dec 19 07:46:03.844: INFO: Initial restart count of pod busybox-bf81a9d8-8e9e-4b5b-ad71-a9c1c024b327 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:04.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9106" for this suite.
Dec 19 07:50:10.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:50:10.170: INFO: namespace container-probe-9106 deletion completed in 6.051023827s

• [SLOW TEST:250.354 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:50:10.170: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec 19 07:50:10.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 api-versions'
Dec 19 07:50:10.246: INFO: stderr: ""
Dec 19 07:50:10.246: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrbac.cce.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nversion.cce.io/v1beta1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:10.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6953" for this suite.
Dec 19 07:50:16.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:50:16.296: INFO: namespace kubectl-6953 deletion completed in 6.048079855s

• [SLOW TEST:6.126 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:50:16.296: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 19 07:50:16.312: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec 19 07:50:16.701: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 19 07:50:18.733: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338616, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338616, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338616, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338616, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-76d4c54cd6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 07:50:21.453: INFO: Waited 714.185656ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:22.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9034" for this suite.
Dec 19 07:50:28.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:50:28.239: INFO: namespace aggregator-9034 deletion completed in 6.144623164s

• [SLOW TEST:11.943 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:50:28.239: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 19 07:50:29.281: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1219 07:50:29.281659      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:29.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7158" for this suite.
Dec 19 07:50:35.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:50:35.333: INFO: namespace gc-7158 deletion completed in 6.049868899s

• [SLOW TEST:7.093 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:50:35.333: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 19 07:50:38.367: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:38.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4959" for this suite.
Dec 19 07:50:44.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:50:44.427: INFO: namespace container-runtime-4959 deletion completed in 6.049718691s

• [SLOW TEST:9.094 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:50:44.427: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:50:44.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d" in namespace "downward-api-7520" to be "success or failure"
Dec 19 07:50:44.449: INFO: Pod "downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140526ms
Dec 19 07:50:46.451: INFO: Pod "downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005259825s
Dec 19 07:50:48.454: INFO: Pod "downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007576099s
STEP: Saw pod success
Dec 19 07:50:48.454: INFO: Pod "downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d" satisfied condition "success or failure"
Dec 19 07:50:48.455: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d container client-container: <nil>
STEP: delete the pod
Dec 19 07:50:48.467: INFO: Waiting for pod downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d to disappear
Dec 19 07:50:48.469: INFO: Pod downwardapi-volume-6a65cd9f-bd8e-4c28-a2a6-623a8ac8dc8d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:48.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7520" for this suite.
Dec 19 07:50:54.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:50:54.520: INFO: namespace downward-api-7520 deletion completed in 6.049382389s

• [SLOW TEST:10.093 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:50:54.520: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:50:59.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-116" for this suite.
Dec 19 07:51:21.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:51:21.609: INFO: namespace replication-controller-116 deletion completed in 22.049013342s

• [SLOW TEST:27.090 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:51:21.610: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec 19 07:51:21.631: INFO: Waiting up to 5m0s for pod "client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f" in namespace "containers-5468" to be "success or failure"
Dec 19 07:51:21.634: INFO: Pod "client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024705ms
Dec 19 07:51:23.636: INFO: Pod "client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00522185s
Dec 19 07:51:25.638: INFO: Pod "client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007437726s
STEP: Saw pod success
Dec 19 07:51:25.638: INFO: Pod "client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f" satisfied condition "success or failure"
Dec 19 07:51:25.640: INFO: Trying to get logs from node 192.168.0.132 pod client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f container test-container: <nil>
STEP: delete the pod
Dec 19 07:51:25.649: INFO: Waiting for pod client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f to disappear
Dec 19 07:51:25.650: INFO: Pod client-containers-ba60b1c9-cfaf-4476-965b-f3b70df5700f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:51:25.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5468" for this suite.
Dec 19 07:51:31.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:51:31.703: INFO: namespace containers-5468 deletion completed in 6.050505371s

• [SLOW TEST:10.094 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:51:31.703: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:51:33.742: INFO: Waiting up to 5m0s for pod "client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601" in namespace "pods-8584" to be "success or failure"
Dec 19 07:51:33.746: INFO: Pod "client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601": Phase="Pending", Reason="", readiness=false. Elapsed: 4.347378ms
Dec 19 07:51:35.748: INFO: Pod "client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006346491s
STEP: Saw pod success
Dec 19 07:51:35.748: INFO: Pod "client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601" satisfied condition "success or failure"
Dec 19 07:51:35.750: INFO: Trying to get logs from node 192.168.0.132 pod client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601 container env3cont: <nil>
STEP: delete the pod
Dec 19 07:51:35.760: INFO: Waiting for pod client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601 to disappear
Dec 19 07:51:35.761: INFO: Pod client-envvars-b114db1f-49e5-4cf4-b71f-eb20c715b601 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:51:35.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8584" for this suite.
Dec 19 07:52:13.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:52:13.811: INFO: namespace pods-8584 deletion completed in 38.048271222s

• [SLOW TEST:42.108 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:52:13.812: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-a785b63a-0502-4bd8-bece-5e79d1c87b55
STEP: Creating a pod to test consume configMaps
Dec 19 07:52:13.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d" in namespace "configmap-8264" to be "success or failure"
Dec 19 07:52:13.836: INFO: Pod "pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.766891ms
Dec 19 07:52:15.838: INFO: Pod "pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003778122s
Dec 19 07:52:17.840: INFO: Pod "pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0058079s
STEP: Saw pod success
Dec 19 07:52:17.840: INFO: Pod "pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d" satisfied condition "success or failure"
Dec 19 07:52:17.842: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d container configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 07:52:17.859: INFO: Waiting for pod pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d to disappear
Dec 19 07:52:17.862: INFO: Pod pod-configmaps-7a4c6fdc-70f5-4ca6-84e4-824d921c996d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:52:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8264" for this suite.
Dec 19 07:52:23.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:52:23.912: INFO: namespace configmap-8264 deletion completed in 6.048402563s

• [SLOW TEST:10.101 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:52:23.912: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:52:23.938: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 19 07:52:23.946: INFO: Number of nodes with available pods: 0
Dec 19 07:52:23.946: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:52:24.950: INFO: Number of nodes with available pods: 0
Dec 19 07:52:24.950: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:52:25.949: INFO: Number of nodes with available pods: 1
Dec 19 07:52:25.949: INFO: Node 192.168.0.205 is running more than one daemon pod
Dec 19 07:52:26.950: INFO: Number of nodes with available pods: 2
Dec 19 07:52:26.950: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 19 07:52:26.962: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:26.962: INFO: Wrong image for pod: daemon-set-c9brx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:27.966: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:27.966: INFO: Wrong image for pod: daemon-set-c9brx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:28.967: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:28.967: INFO: Wrong image for pod: daemon-set-c9brx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:29.967: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:29.967: INFO: Wrong image for pod: daemon-set-c9brx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:29.967: INFO: Pod daemon-set-c9brx is not available
Dec 19 07:52:30.967: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:30.967: INFO: Pod daemon-set-mcnfv is not available
Dec 19 07:52:31.967: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:31.967: INFO: Pod daemon-set-mcnfv is not available
Dec 19 07:52:32.966: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:33.966: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:33.967: INFO: Pod daemon-set-6jlfm is not available
Dec 19 07:52:34.966: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:34.966: INFO: Pod daemon-set-6jlfm is not available
Dec 19 07:52:35.967: INFO: Wrong image for pod: daemon-set-6jlfm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 19 07:52:35.967: INFO: Pod daemon-set-6jlfm is not available
Dec 19 07:52:36.966: INFO: Pod daemon-set-qmxbx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 19 07:52:36.971: INFO: Number of nodes with available pods: 1
Dec 19 07:52:36.971: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:52:37.976: INFO: Number of nodes with available pods: 1
Dec 19 07:52:37.976: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:52:38.975: INFO: Number of nodes with available pods: 1
Dec 19 07:52:38.975: INFO: Node 192.168.0.132 is running more than one daemon pod
Dec 19 07:52:39.976: INFO: Number of nodes with available pods: 2
Dec 19 07:52:39.976: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8938, will wait for the garbage collector to delete the pods
Dec 19 07:52:40.039: INFO: Deleting DaemonSet.extensions daemon-set took: 3.562075ms
Dec 19 07:52:40.139: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.154487ms
Dec 19 07:52:43.141: INFO: Number of nodes with available pods: 0
Dec 19 07:52:43.141: INFO: Number of running nodes: 0, number of available pods: 0
Dec 19 07:52:43.142: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8938/daemonsets","resourceVersion":"675695"},"items":null}

Dec 19 07:52:43.143: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8938/pods","resourceVersion":"675695"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:52:43.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8938" for this suite.
Dec 19 07:52:49.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:52:49.202: INFO: namespace daemonsets-8938 deletion completed in 6.048970569s

• [SLOW TEST:25.290 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:52:49.203: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 19 07:52:49.222: INFO: Waiting up to 5m0s for pod "pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5" in namespace "emptydir-5841" to be "success or failure"
Dec 19 07:52:49.223: INFO: Pod "pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.152372ms
Dec 19 07:52:51.226: INFO: Pod "pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00407882s
STEP: Saw pod success
Dec 19 07:52:51.226: INFO: Pod "pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5" satisfied condition "success or failure"
Dec 19 07:52:51.228: INFO: Trying to get logs from node 192.168.0.132 pod pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5 container test-container: <nil>
STEP: delete the pod
Dec 19 07:52:51.238: INFO: Waiting for pod pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5 to disappear
Dec 19 07:52:51.240: INFO: Pod pod-00ebe6ef-0a39-4030-9f98-443e65d9d1d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:52:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5841" for this suite.
Dec 19 07:52:57.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:52:57.297: INFO: namespace emptydir-5841 deletion completed in 6.055386974s

• [SLOW TEST:8.095 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:52:57.297: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-65138dee-65af-4047-836b-73cc748f6e95
STEP: Creating a pod to test consume secrets
Dec 19 07:52:57.320: INFO: Waiting up to 5m0s for pod "pod-secrets-66113ef4-35cc-4454-9f67-536540e14586" in namespace "secrets-8420" to be "success or failure"
Dec 19 07:52:57.322: INFO: Pod "pod-secrets-66113ef4-35cc-4454-9f67-536540e14586": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491147ms
Dec 19 07:52:59.323: INFO: Pod "pod-secrets-66113ef4-35cc-4454-9f67-536540e14586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003097142s
STEP: Saw pod success
Dec 19 07:52:59.323: INFO: Pod "pod-secrets-66113ef4-35cc-4454-9f67-536540e14586" satisfied condition "success or failure"
Dec 19 07:52:59.325: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-66113ef4-35cc-4454-9f67-536540e14586 container secret-env-test: <nil>
STEP: delete the pod
Dec 19 07:52:59.333: INFO: Waiting for pod pod-secrets-66113ef4-35cc-4454-9f67-536540e14586 to disappear
Dec 19 07:52:59.335: INFO: Pod pod-secrets-66113ef4-35cc-4454-9f67-536540e14586 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:52:59.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8420" for this suite.
Dec 19 07:53:05.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:53:05.391: INFO: namespace secrets-8420 deletion completed in 6.054066545s

• [SLOW TEST:8.094 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:53:05.391: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 07:53:05.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5305'
Dec 19 07:53:05.668: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 19 07:53:05.668: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Dec 19 07:53:05.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete jobs e2e-test-nginx-job --namespace=kubectl-5305'
Dec 19 07:53:05.726: INFO: stderr: ""
Dec 19 07:53:05.726: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:53:05.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5305" for this suite.
Dec 19 07:53:27.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:53:27.777: INFO: namespace kubectl-5305 deletion completed in 22.049561611s

• [SLOW TEST:22.386 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:53:27.777: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-c6f94683-edbb-44a4-afe4-20fe5764b3a6
STEP: Creating a pod to test consume secrets
Dec 19 07:53:27.805: INFO: Waiting up to 5m0s for pod "pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f" in namespace "secrets-7667" to be "success or failure"
Dec 19 07:53:27.808: INFO: Pod "pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634748ms
Dec 19 07:53:29.810: INFO: Pod "pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00476542s
Dec 19 07:53:31.812: INFO: Pod "pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00741734s
STEP: Saw pod success
Dec 19 07:53:31.812: INFO: Pod "pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f" satisfied condition "success or failure"
Dec 19 07:53:31.814: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:53:31.825: INFO: Waiting for pod pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f to disappear
Dec 19 07:53:31.827: INFO: Pod pod-secrets-6d97de1c-ee2c-4053-b595-4070323f637f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:53:31.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7667" for this suite.
Dec 19 07:53:37.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:53:37.881: INFO: namespace secrets-7667 deletion completed in 6.052657672s

• [SLOW TEST:10.104 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:53:37.882: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 19 07:53:42.422: INFO: Successfully updated pod "annotationupdate530ebbb5-0b22-4de4-be12-bf5714e9d381"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:53:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3491" for this suite.
Dec 19 07:54:06.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:54:06.483: INFO: namespace projected-3491 deletion completed in 22.049240384s

• [SLOW TEST:28.601 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:54:06.483: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-a6034187-372d-40d4-bb5f-27711dc15304
STEP: Creating a pod to test consume secrets
Dec 19 07:54:06.510: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897" in namespace "projected-9442" to be "success or failure"
Dec 19 07:54:06.511: INFO: Pod "pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897": Phase="Pending", Reason="", readiness=false. Elapsed: 1.436015ms
Dec 19 07:54:08.514: INFO: Pod "pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003672664s
Dec 19 07:54:10.516: INFO: Pod "pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005839054s
STEP: Saw pod success
Dec 19 07:54:10.516: INFO: Pod "pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897" satisfied condition "success or failure"
Dec 19 07:54:10.517: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897 container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:54:10.528: INFO: Waiting for pod pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897 to disappear
Dec 19 07:54:10.530: INFO: Pod pod-projected-secrets-f98a0718-5b05-4193-8d0f-95db60838897 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:54:10.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9442" for this suite.
Dec 19 07:54:16.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:54:16.586: INFO: namespace projected-9442 deletion completed in 6.053989937s

• [SLOW TEST:10.104 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:54:16.586: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:54:16.605: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 19 07:54:18.622: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:54:19.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-223" for this suite.
Dec 19 07:54:25.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:54:25.685: INFO: namespace replication-controller-223 deletion completed in 6.056696803s

• [SLOW TEST:9.099 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:54:25.685: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 07:54:25.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91" in namespace "projected-9329" to be "success or failure"
Dec 19 07:54:25.709: INFO: Pod "downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347899ms
Dec 19 07:54:27.711: INFO: Pod "downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00387789s
Dec 19 07:54:29.714: INFO: Pod "downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006282308s
STEP: Saw pod success
Dec 19 07:54:29.714: INFO: Pod "downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91" satisfied condition "success or failure"
Dec 19 07:54:29.715: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91 container client-container: <nil>
STEP: delete the pod
Dec 19 07:54:29.725: INFO: Waiting for pod downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91 to disappear
Dec 19 07:54:29.727: INFO: Pod downwardapi-volume-3de41e81-6056-43b9-8191-f30b2863ec91 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:54:29.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9329" for this suite.
Dec 19 07:54:35.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:54:35.780: INFO: namespace projected-9329 deletion completed in 6.050489169s

• [SLOW TEST:10.095 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:54:35.780: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec 19 07:54:35.805: INFO: Waiting up to 5m0s for pod "client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f" in namespace "containers-2127" to be "success or failure"
Dec 19 07:54:35.808: INFO: Pod "client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296408ms
Dec 19 07:54:37.810: INFO: Pod "client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004783247s
STEP: Saw pod success
Dec 19 07:54:37.810: INFO: Pod "client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f" satisfied condition "success or failure"
Dec 19 07:54:37.812: INFO: Trying to get logs from node 192.168.0.132 pod client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f container test-container: <nil>
STEP: delete the pod
Dec 19 07:54:37.823: INFO: Waiting for pod client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f to disappear
Dec 19 07:54:37.827: INFO: Pod client-containers-8ce6428d-1551-44b8-9d84-74d14a96f61f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:54:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2127" for this suite.
Dec 19 07:54:43.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:54:43.884: INFO: namespace containers-2127 deletion completed in 6.054795865s

• [SLOW TEST:8.104 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:54:43.884: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 19 07:54:46.433: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3888c618-145e-45cd-b142-532a612407e0"
Dec 19 07:54:46.433: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3888c618-145e-45cd-b142-532a612407e0" in namespace "pods-9081" to be "terminated due to deadline exceeded"
Dec 19 07:54:46.437: INFO: Pod "pod-update-activedeadlineseconds-3888c618-145e-45cd-b142-532a612407e0": Phase="Running", Reason="", readiness=true. Elapsed: 4.386599ms
Dec 19 07:54:48.440: INFO: Pod "pod-update-activedeadlineseconds-3888c618-145e-45cd-b142-532a612407e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006738608s
Dec 19 07:54:50.442: INFO: Pod "pod-update-activedeadlineseconds-3888c618-145e-45cd-b142-532a612407e0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.008838376s
Dec 19 07:54:50.442: INFO: Pod "pod-update-activedeadlineseconds-3888c618-145e-45cd-b142-532a612407e0" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:54:50.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9081" for this suite.
Dec 19 07:54:56.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:54:56.493: INFO: namespace pods-9081 deletion completed in 6.049930639s

• [SLOW TEST:12.610 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:54:56.494: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1800480a-e2fe-49cd-930f-8f99c062f605
STEP: Creating a pod to test consume secrets
Dec 19 07:54:56.518: INFO: Waiting up to 5m0s for pod "pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f" in namespace "secrets-9896" to be "success or failure"
Dec 19 07:54:56.520: INFO: Pod "pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.13618ms
Dec 19 07:54:58.522: INFO: Pod "pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004183025s
Dec 19 07:55:00.525: INFO: Pod "pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006485177s
STEP: Saw pod success
Dec 19 07:55:00.525: INFO: Pod "pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f" satisfied condition "success or failure"
Dec 19 07:55:00.526: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 07:55:00.540: INFO: Waiting for pod pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f to disappear
Dec 19 07:55:00.543: INFO: Pod pod-secrets-f67ba7a9-26c4-4e2c-9015-ac5988186f2f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:55:00.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9896" for this suite.
Dec 19 07:55:06.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:55:06.608: INFO: namespace secrets-9896 deletion completed in 6.063478984s

• [SLOW TEST:10.114 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:55:06.608: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 19 07:55:06.627: INFO: PodSpec: initContainers in spec.initContainers
Dec 19 07:55:53.226: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-a4141260-fabd-4c43-a079-8d72f83153aa", GenerateName:"", Namespace:"init-container-8273", SelfLink:"/api/v1/namespaces/init-container-8273/pods/pod-init-a4141260-fabd-4c43-a079-8d72f83153aa", UID:"ed29ed6d-e0b3-4d37-a7e0-3b278210b4c1", ResourceVersion:"676566", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63712338906, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"627990190"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-799pv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00271f180), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-799pv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-799pv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-799pv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003845428), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"192.168.0.132", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002b5e900), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038454b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0038454d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0038454d8), DNSConfig:(*v1.PodDNSConfig)(0xc0022757c0), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0038454e7), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338906, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338906, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338906, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712338906, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.132", PodIP:"172.16.0.19", StartTime:(*v1.Time)(0xc001b02740), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b94fc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002b95030)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://ea9408e090364c39e5f0e970be1e04090d44d8cd38c828827a46e540d1043b58"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b02860), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b027c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:55:53.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8273" for this suite.
Dec 19 07:56:15.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:56:15.283: INFO: namespace init-container-8273 deletion completed in 22.051792131s

• [SLOW TEST:68.675 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:56:15.283: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 07:56:15.302: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:56:19.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8475" for this suite.
Dec 19 07:57:09.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:57:09.418: INFO: namespace pods-8475 deletion completed in 50.054047532s

• [SLOW TEST:54.135 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:57:09.418: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-ch9f
STEP: Creating a pod to test atomic-volume-subpath
Dec 19 07:57:09.448: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ch9f" in namespace "subpath-6029" to be "success or failure"
Dec 19 07:57:09.452: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.451981ms
Dec 19 07:57:11.454: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005514423s
Dec 19 07:57:13.456: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 4.007666974s
Dec 19 07:57:15.458: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 6.009978221s
Dec 19 07:57:17.460: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 8.012080794s
Dec 19 07:57:19.463: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 10.01453693s
Dec 19 07:57:21.465: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 12.016594852s
Dec 19 07:57:23.467: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 14.018826884s
Dec 19 07:57:25.469: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 16.021040575s
Dec 19 07:57:27.472: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 18.023565537s
Dec 19 07:57:29.474: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 20.025705647s
Dec 19 07:57:31.476: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Running", Reason="", readiness=true. Elapsed: 22.027916929s
Dec 19 07:57:33.479: INFO: Pod "pod-subpath-test-configmap-ch9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.030304275s
STEP: Saw pod success
Dec 19 07:57:33.479: INFO: Pod "pod-subpath-test-configmap-ch9f" satisfied condition "success or failure"
Dec 19 07:57:33.481: INFO: Trying to get logs from node 192.168.0.132 pod pod-subpath-test-configmap-ch9f container test-container-subpath-configmap-ch9f: <nil>
STEP: delete the pod
Dec 19 07:57:33.492: INFO: Waiting for pod pod-subpath-test-configmap-ch9f to disappear
Dec 19 07:57:33.494: INFO: Pod pod-subpath-test-configmap-ch9f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ch9f
Dec 19 07:57:33.494: INFO: Deleting pod "pod-subpath-test-configmap-ch9f" in namespace "subpath-6029"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:57:33.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6029" for this suite.
Dec 19 07:57:39.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:57:39.553: INFO: namespace subpath-6029 deletion completed in 6.054774432s

• [SLOW TEST:30.135 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:57:39.554: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 19 07:57:43.584: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-317660674 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 19 07:57:58.638: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:57:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5430" for this suite.
Dec 19 07:58:04.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:58:04.693: INFO: namespace pods-5430 deletion completed in 6.051362297s

• [SLOW TEST:25.139 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:58:04.693: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 07:58:04.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-7785'
Dec 19 07:58:04.770: INFO: stderr: ""
Dec 19 07:58:04.770: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec 19 07:58:09.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 get pod e2e-test-nginx-pod --namespace=kubectl-7785 -o json'
Dec 19 07:58:09.875: INFO: stderr: ""
Dec 19 07:58:09.875: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-19T07:58:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-7785\",\n        \"resourceVersion\": \"677089\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7785/pods/e2e-test-nginx-pod\",\n        \"uid\": \"67a70284-fc58-4f38-b10d-81ffa44030a6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hhdt9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsConfig\": {\n            \"options\": [\n                {\n                    \"name\": \"single-request-reopen\",\n                    \"value\": \"\"\n                },\n                {\n                    \"name\": \"timeout\",\n                    \"value\": \"2\"\n                }\n            ]\n        },\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"192.168.0.132\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hhdt9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hhdt9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-19T07:58:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-19T07:58:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-19T07:58:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-19T07:58:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://cd50aa054b9dc92f11567516cfe10bbbfde2c41c9828ea04e593370c5f883917\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-19T07:58:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.132\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.0.23\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.0.23\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-19T07:58:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 19 07:58:09.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 replace -f - --namespace=kubectl-7785'
Dec 19 07:58:10.063: INFO: stderr: ""
Dec 19 07:58:10.063: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Dec 19 07:58:10.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete pods e2e-test-nginx-pod --namespace=kubectl-7785'
Dec 19 07:58:16.626: INFO: stderr: ""
Dec 19 07:58:16.626: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:58:16.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7785" for this suite.
Dec 19 07:58:22.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:58:22.683: INFO: namespace kubectl-7785 deletion completed in 6.054004555s

• [SLOW TEST:17.990 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:58:22.683: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 19 07:58:30.718: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:30.718: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:30.773: INFO: Exec stderr: ""
Dec 19 07:58:30.773: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:30.773: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:30.830: INFO: Exec stderr: ""
Dec 19 07:58:30.830: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:30.830: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:30.888: INFO: Exec stderr: ""
Dec 19 07:58:30.888: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:30.888: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:30.942: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 19 07:58:30.942: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:30.942: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:31.003: INFO: Exec stderr: ""
Dec 19 07:58:31.003: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:31.003: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:31.053: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 19 07:58:31.053: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:31.053: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:31.110: INFO: Exec stderr: ""
Dec 19 07:58:31.110: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:31.110: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:31.160: INFO: Exec stderr: ""
Dec 19 07:58:31.160: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:31.160: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:31.211: INFO: Exec stderr: ""
Dec 19 07:58:31.211: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2771 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 19 07:58:31.211: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
Dec 19 07:58:31.266: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:58:31.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2771" for this suite.
Dec 19 07:59:21.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:59:21.325: INFO: namespace e2e-kubelet-etc-hosts-2771 deletion completed in 50.056349892s

• [SLOW TEST:58.643 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:59:21.325: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec 19 07:59:21.349: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4027" to be "success or failure"
Dec 19 07:59:21.354: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.964763ms
Dec 19 07:59:23.356: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007157998s
Dec 19 07:59:25.358: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008899664s
STEP: Saw pod success
Dec 19 07:59:25.358: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 19 07:59:25.360: INFO: Trying to get logs from node 192.168.0.132 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 19 07:59:25.371: INFO: Waiting for pod pod-host-path-test to disappear
Dec 19 07:59:25.373: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:59:25.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4027" for this suite.
Dec 19 07:59:31.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:59:31.438: INFO: namespace hostpath-4027 deletion completed in 6.062036008s

• [SLOW TEST:10.112 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:59:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 19 07:59:37.486: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1219 07:59:37.486616      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:59:37.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9136" for this suite.
Dec 19 07:59:43.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 07:59:43.537: INFO: namespace gc-9136 deletion completed in 6.049485792s

• [SLOW TEST:12.100 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 07:59:43.538: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 07:59:47.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7846" for this suite.
Dec 19 08:00:27.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:00:27.633: INFO: namespace kubelet-test-7846 deletion completed in 40.056193776s

• [SLOW TEST:44.095 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:00:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 08:00:27.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8" in namespace "downward-api-9876" to be "success or failure"
Dec 19 08:00:27.658: INFO: Pod "downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360884ms
Dec 19 08:00:29.660: INFO: Pod "downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005512598s
STEP: Saw pod success
Dec 19 08:00:29.660: INFO: Pod "downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8" satisfied condition "success or failure"
Dec 19 08:00:29.662: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8 container client-container: <nil>
STEP: delete the pod
Dec 19 08:00:29.680: INFO: Waiting for pod downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8 to disappear
Dec 19 08:00:29.683: INFO: Pod downwardapi-volume-12dfb53b-e122-4a08-8eb4-394ddcc41ec8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:00:29.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9876" for this suite.
Dec 19 08:00:35.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:00:35.740: INFO: namespace downward-api-9876 deletion completed in 6.054264917s

• [SLOW TEST:8.106 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:00:35.740: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 08:00:35.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1" in namespace "downward-api-4127" to be "success or failure"
Dec 19 08:00:35.764: INFO: Pod "downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.167171ms
Dec 19 08:00:37.767: INFO: Pod "downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004396439s
STEP: Saw pod success
Dec 19 08:00:37.767: INFO: Pod "downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1" satisfied condition "success or failure"
Dec 19 08:00:37.768: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1 container client-container: <nil>
STEP: delete the pod
Dec 19 08:00:37.780: INFO: Waiting for pod downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1 to disappear
Dec 19 08:00:37.784: INFO: Pod downwardapi-volume-565611df-1cb4-4c0e-9c64-e39bdf2b66e1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:00:37.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4127" for this suite.
Dec 19 08:00:43.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:00:43.838: INFO: namespace downward-api-4127 deletion completed in 6.052558758s

• [SLOW TEST:8.099 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:00:43.839: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec 19 08:00:47.868: INFO: Pod pod-hostip-7fcd0956-dc2a-4241-80bf-a061c5fd6369 has hostIP: 192.168.0.132
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:00:47.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5264" for this suite.
Dec 19 08:01:09.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:01:09.919: INFO: namespace pods-5264 deletion completed in 22.049086625s

• [SLOW TEST:26.080 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:01:09.919: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 08:01:09.943: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9" in namespace "downward-api-6185" to be "success or failure"
Dec 19 08:01:09.944: INFO: Pod "downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.820346ms
Dec 19 08:01:11.947: INFO: Pod "downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004315578s
Dec 19 08:01:13.949: INFO: Pod "downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006353507s
STEP: Saw pod success
Dec 19 08:01:13.949: INFO: Pod "downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9" satisfied condition "success or failure"
Dec 19 08:01:13.951: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9 container client-container: <nil>
STEP: delete the pod
Dec 19 08:01:13.960: INFO: Waiting for pod downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9 to disappear
Dec 19 08:01:13.961: INFO: Pod downwardapi-volume-21563500-b4e4-45eb-b9e3-256544de03e9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:01:13.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6185" for this suite.
Dec 19 08:01:19.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:01:20.014: INFO: namespace downward-api-6185 deletion completed in 6.051485652s

• [SLOW TEST:10.096 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:01:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 19 08:01:24.548: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8441 pod-service-account-46b4a9c0-ce0a-4c82-9edd-a1432d5e3767 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 19 08:01:24.736: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8441 pod-service-account-46b4a9c0-ce0a-4c82-9edd-a1432d5e3767 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 19 08:01:24.842: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8441 pod-service-account-46b4a9c0-ce0a-4c82-9edd-a1432d5e3767 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:01:24.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8441" for this suite.
Dec 19 08:01:30.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:01:31.004: INFO: namespace svcaccounts-8441 deletion completed in 6.051950071s

• [SLOW TEST:10.990 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:01:31.005: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 08:01:31.028: INFO: Waiting up to 5m0s for pod "downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594" in namespace "downward-api-9238" to be "success or failure"
Dec 19 08:01:31.032: INFO: Pod "downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594": Phase="Pending", Reason="", readiness=false. Elapsed: 3.722945ms
Dec 19 08:01:33.034: INFO: Pod "downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005902183s
Dec 19 08:01:35.037: INFO: Pod "downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008597123s
STEP: Saw pod success
Dec 19 08:01:35.037: INFO: Pod "downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594" satisfied condition "success or failure"
Dec 19 08:01:35.039: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594 container client-container: <nil>
STEP: delete the pod
Dec 19 08:01:35.050: INFO: Waiting for pod downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594 to disappear
Dec 19 08:01:35.052: INFO: Pod downwardapi-volume-def03609-0e55-49e5-9284-bfe685456594 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:01:35.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9238" for this suite.
Dec 19 08:01:41.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:01:41.114: INFO: namespace downward-api-9238 deletion completed in 6.05981184s

• [SLOW TEST:10.109 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:01:41.114: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-878ecad2-f3d2-4726-9af4-ede1e06e6e57
STEP: Creating a pod to test consume configMaps
Dec 19 08:01:41.147: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233" in namespace "projected-2025" to be "success or failure"
Dec 19 08:01:41.149: INFO: Pod "pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90249ms
Dec 19 08:01:43.152: INFO: Pod "pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00416653s
Dec 19 08:01:45.154: INFO: Pod "pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006554692s
STEP: Saw pod success
Dec 19 08:01:45.154: INFO: Pod "pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233" satisfied condition "success or failure"
Dec 19 08:01:45.156: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 08:01:45.171: INFO: Waiting for pod pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233 to disappear
Dec 19 08:01:45.172: INFO: Pod pod-projected-configmaps-fd6f23d6-2a5b-43e6-88f3-dea29bb38233 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:01:45.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2025" for this suite.
Dec 19 08:01:51.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:01:51.223: INFO: namespace projected-2025 deletion completed in 6.047919399s

• [SLOW TEST:10.109 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:01:51.223: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-c40962dc-66b7-486d-86ce-c9097f3c5cc8
STEP: Creating configMap with name cm-test-opt-upd-ec21d600-ef9b-4ab0-aea9-d8b2d95a5b4a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c40962dc-66b7-486d-86ce-c9097f3c5cc8
STEP: Updating configmap cm-test-opt-upd-ec21d600-ef9b-4ab0-aea9-d8b2d95a5b4a
STEP: Creating configMap with name cm-test-opt-create-19f7f647-8035-4246-9878-26cb5d61d841
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:03:15.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8523" for this suite.
Dec 19 08:03:37.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:03:37.554: INFO: namespace configmap-8523 deletion completed in 22.050993848s

• [SLOW TEST:106.331 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:03:37.554: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 19 08:03:37.572: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:03:41.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5835" for this suite.
Dec 19 08:04:03.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:04:03.919: INFO: namespace init-container-5835 deletion completed in 22.052114683s

• [SLOW TEST:26.365 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:04:03.919: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:04:07.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4221" for this suite.
Dec 19 08:04:47.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:04:48.012: INFO: namespace kubelet-test-4221 deletion completed in 40.060750084s

• [SLOW TEST:44.093 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:04:48.012: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6249.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6249.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 19 08:04:52.065: INFO: DNS probes using dns-6249/dns-test-98e81c61-0c84-48d2-a919-2a5f04720082 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:04:52.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6249" for this suite.
Dec 19 08:04:58.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:04:58.129: INFO: namespace dns-6249 deletion completed in 6.054216973s

• [SLOW TEST:10.116 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:04:58.129: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9209
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9209
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9209
Dec 19 08:04:58.160: INFO: Found 0 stateful pods, waiting for 1
Dec 19 08:05:08.163: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 19 08:05:08.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 08:05:08.498: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 08:05:08.498: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 08:05:08.498: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 08:05:08.501: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 19 08:05:18.503: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 08:05:18.503: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 08:05:18.510: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999823s
Dec 19 08:05:19.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997769742s
Dec 19 08:05:20.515: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995410618s
Dec 19 08:05:21.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992990365s
Dec 19 08:05:22.519: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.990738236s
Dec 19 08:05:23.522: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.988436503s
Dec 19 08:05:24.524: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.985759099s
Dec 19 08:05:25.527: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.983354938s
Dec 19 08:05:26.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981025477s
Dec 19 08:05:27.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 978.26738ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9209
Dec 19 08:05:28.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 08:05:28.639: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 08:05:28.639: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 08:05:28.639: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 08:05:28.641: INFO: Found 1 stateful pods, waiting for 3
Dec 19 08:05:38.644: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 08:05:38.644: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 19 08:05:38.644: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 19 08:05:38.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 08:05:38.756: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 08:05:38.756: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 08:05:38.756: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 08:05:38.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 08:05:39.013: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 08:05:39.013: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 08:05:39.013: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 08:05:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 19 08:05:39.137: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 19 08:05:39.137: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 19 08:05:39.137: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 19 08:05:39.137: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 08:05:39.139: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 19 08:05:49.144: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 08:05:49.144: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 08:05:49.144: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 19 08:05:49.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999829s
Dec 19 08:05:50.157: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998025528s
Dec 19 08:05:51.159: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995077733s
Dec 19 08:05:52.162: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992327521s
Dec 19 08:05:53.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989295749s
Dec 19 08:05:54.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986357076s
Dec 19 08:05:55.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983913321s
Dec 19 08:05:56.173: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.98158069s
Dec 19 08:05:57.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.978994856s
Dec 19 08:05:58.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.56916ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9209
Dec 19 08:05:59.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 08:05:59.286: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 08:05:59.286: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 08:05:59.286: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 08:05:59.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 08:05:59.392: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 08:05:59.392: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 08:05:59.392: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 08:05:59.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 exec --namespace=statefulset-9209 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 19 08:05:59.496: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 19 08:05:59.496: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 19 08:05:59.496: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 19 08:05:59.496: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 19 08:06:29.520: INFO: Deleting all statefulset in ns statefulset-9209
Dec 19 08:06:29.522: INFO: Scaling statefulset ss to 0
Dec 19 08:06:29.527: INFO: Waiting for statefulset status.replicas updated to 0
Dec 19 08:06:29.528: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:06:29.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9209" for this suite.
Dec 19 08:06:35.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:06:35.590: INFO: namespace statefulset-9209 deletion completed in 6.054437792s

• [SLOW TEST:97.461 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:06:35.590: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 19 08:06:35.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7431'
Dec 19 08:06:35.664: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 19 08:06:35.664: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec 19 08:06:35.678: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-zvp2q]
Dec 19 08:06:35.678: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-zvp2q" in namespace "kubectl-7431" to be "running and ready"
Dec 19 08:06:35.686: INFO: Pod "e2e-test-nginx-rc-zvp2q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.340982ms
Dec 19 08:06:37.689: INFO: Pod "e2e-test-nginx-rc-zvp2q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01068645s
Dec 19 08:06:39.691: INFO: Pod "e2e-test-nginx-rc-zvp2q": Phase="Running", Reason="", readiness=true. Elapsed: 4.013044129s
Dec 19 08:06:39.691: INFO: Pod "e2e-test-nginx-rc-zvp2q" satisfied condition "running and ready"
Dec 19 08:06:39.691: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-zvp2q]
Dec 19 08:06:39.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 logs rc/e2e-test-nginx-rc --namespace=kubectl-7431'
Dec 19 08:06:39.757: INFO: stderr: ""
Dec 19 08:06:39.757: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Dec 19 08:06:39.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 delete rc e2e-test-nginx-rc --namespace=kubectl-7431'
Dec 19 08:06:39.812: INFO: stderr: ""
Dec 19 08:06:39.812: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:06:39.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7431" for this suite.
Dec 19 08:07:01.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:07:01.875: INFO: namespace kubectl-7431 deletion completed in 22.060392347s

• [SLOW TEST:26.285 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:07:01.876: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:07:27.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7564" for this suite.
Dec 19 08:07:33.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:07:34.013: INFO: namespace namespaces-7564 deletion completed in 6.051902513s
STEP: Destroying namespace "nsdeletetest-446" for this suite.
Dec 19 08:07:34.014: INFO: Namespace nsdeletetest-446 was already deleted
STEP: Destroying namespace "nsdeletetest-2812" for this suite.
Dec 19 08:07:40.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:07:40.070: INFO: namespace nsdeletetest-2812 deletion completed in 6.05575804s

• [SLOW TEST:38.195 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:07:40.070: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3887.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3887.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3887.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3887.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3887.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3887.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 19 08:07:44.119: INFO: DNS probes using dns-3887/dns-test-d2cded8a-6e70-402d-84e6-d1d7c6910d7b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:07:44.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3887" for this suite.
Dec 19 08:07:50.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:07:50.177: INFO: namespace dns-3887 deletion completed in 6.048948323s

• [SLOW TEST:10.106 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:07:50.177: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec 19 08:07:50.201: INFO: Waiting up to 5m0s for pod "var-expansion-9bc6573b-824f-4820-be14-777245a087f0" in namespace "var-expansion-7282" to be "success or failure"
Dec 19 08:07:50.204: INFO: Pod "var-expansion-9bc6573b-824f-4820-be14-777245a087f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479135ms
Dec 19 08:07:52.206: INFO: Pod "var-expansion-9bc6573b-824f-4820-be14-777245a087f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005594511s
Dec 19 08:07:54.208: INFO: Pod "var-expansion-9bc6573b-824f-4820-be14-777245a087f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00768007s
STEP: Saw pod success
Dec 19 08:07:54.208: INFO: Pod "var-expansion-9bc6573b-824f-4820-be14-777245a087f0" satisfied condition "success or failure"
Dec 19 08:07:54.210: INFO: Trying to get logs from node 192.168.0.132 pod var-expansion-9bc6573b-824f-4820-be14-777245a087f0 container dapi-container: <nil>
STEP: delete the pod
Dec 19 08:07:54.220: INFO: Waiting for pod var-expansion-9bc6573b-824f-4820-be14-777245a087f0 to disappear
Dec 19 08:07:54.223: INFO: Pod var-expansion-9bc6573b-824f-4820-be14-777245a087f0 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:07:54.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7282" for this suite.
Dec 19 08:08:00.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:08:00.277: INFO: namespace var-expansion-7282 deletion completed in 6.052303193s

• [SLOW TEST:10.100 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:08:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 08:08:00.300: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 19 08:08:05.303: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 19 08:08:05.303: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 19 08:08:07.308: INFO: Creating deployment "test-rollover-deployment"
Dec 19 08:08:07.312: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 19 08:08:09.316: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 19 08:08:09.319: INFO: Ensure that both replica sets have 1 created replica
Dec 19 08:08:09.322: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 19 08:08:09.326: INFO: Updating deployment test-rollover-deployment
Dec 19 08:08:09.326: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 19 08:08:11.331: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 19 08:08:11.334: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 19 08:08:11.337: INFO: all replica sets need to contain the pod-template-hash label
Dec 19 08:08:11.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339689, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 08:08:13.342: INFO: all replica sets need to contain the pod-template-hash label
Dec 19 08:08:13.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339691, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 08:08:15.342: INFO: all replica sets need to contain the pod-template-hash label
Dec 19 08:08:15.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339691, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 08:08:17.342: INFO: all replica sets need to contain the pod-template-hash label
Dec 19 08:08:17.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339691, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 08:08:19.341: INFO: all replica sets need to contain the pod-template-hash label
Dec 19 08:08:19.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339691, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 08:08:21.341: INFO: all replica sets need to contain the pod-template-hash label
Dec 19 08:08:21.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339691, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712339687, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 19 08:08:23.342: INFO: 
Dec 19 08:08:23.342: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 19 08:08:23.347: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9452,SelfLink:/apis/apps/v1/namespaces/deployment-9452/deployments/test-rollover-deployment,UID:910d180b-4411-4bbd-ab65-073c2c614b69,ResourceVersion:679743,Generation:2,CreationTimestamp:2019-12-19 08:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-19 08:08:07 +0000 UTC 2019-12-19 08:08:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-19 08:08:21 +0000 UTC 2019-12-19 08:08:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 19 08:08:23.349: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-9452,SelfLink:/apis/apps/v1/namespaces/deployment-9452/replicasets/test-rollover-deployment-854595fc44,UID:7ff4d38f-ef6f-4055-aa9f-4b4fc5aa1184,ResourceVersion:679736,Generation:2,CreationTimestamp:2019-12-19 08:08:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 910d180b-4411-4bbd-ab65-073c2c614b69 0xc000393b77 0xc000393b78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 19 08:08:23.349: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 19 08:08:23.349: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9452,SelfLink:/apis/apps/v1/namespaces/deployment-9452/replicasets/test-rollover-controller,UID:2edd988e-0790-4423-ad50-51ea9a053164,ResourceVersion:679742,Generation:2,CreationTimestamp:2019-12-19 08:08:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 910d180b-4411-4bbd-ab65-073c2c614b69 0xc0003938a7 0xc0003938a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 08:08:23.349: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-9452,SelfLink:/apis/apps/v1/namespaces/deployment-9452/replicasets/test-rollover-deployment-9b8b997cf,UID:874a2d1a-6b96-4b08-9d4d-ac78133ee103,ResourceVersion:679689,Generation:2,CreationTimestamp:2019-12-19 08:08:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 910d180b-4411-4bbd-ab65-073c2c614b69 0xc000393d70 0xc000393d71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 19 08:08:23.351: INFO: Pod "test-rollover-deployment-854595fc44-hswrl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-hswrl,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-9452,SelfLink:/api/v1/namespaces/deployment-9452/pods/test-rollover-deployment-854595fc44-hswrl,UID:f88b1f7b-6d49-43c3-a76e-48fed7cd8141,ResourceVersion:679700,Generation:0,CreationTimestamp:2019-12-19 08:08:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 7ff4d38f-ef6f-4055-aa9f-4b4fc5aa1184 0xc0009617b7 0xc0009617b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mr4qv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mr4qv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-mr4qv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:192.168.0.132,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0009618e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000961920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[],Searches:[],Options:[{single-request-reopen 0xc002feabc0} {timeout 0xc002feabd0}],},ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 08:08:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 08:08:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 08:08:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-19 08:08:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.132,PodIP:172.16.0.29,StartTime:2019-12-19 08:08:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-19 08:08:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4c768bf22939488ec97545025a6a6bedc993117d6fda8663b98dc43773bb00d3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:08:23.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9452" for this suite.
Dec 19 08:08:29.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:08:29.405: INFO: namespace deployment-9452 deletion completed in 6.051740052s

• [SLOW TEST:29.128 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:08:29.405: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 19 08:08:29.428: INFO: Waiting up to 5m0s for pod "pod-a9cdf975-19f2-4999-8e64-3debccde0f70" in namespace "emptydir-1107" to be "success or failure"
Dec 19 08:08:29.430: INFO: Pod "pod-a9cdf975-19f2-4999-8e64-3debccde0f70": Phase="Pending", Reason="", readiness=false. Elapsed: 1.844926ms
Dec 19 08:08:31.432: INFO: Pod "pod-a9cdf975-19f2-4999-8e64-3debccde0f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004152755s
STEP: Saw pod success
Dec 19 08:08:31.432: INFO: Pod "pod-a9cdf975-19f2-4999-8e64-3debccde0f70" satisfied condition "success or failure"
Dec 19 08:08:31.434: INFO: Trying to get logs from node 192.168.0.132 pod pod-a9cdf975-19f2-4999-8e64-3debccde0f70 container test-container: <nil>
STEP: delete the pod
Dec 19 08:08:31.448: INFO: Waiting for pod pod-a9cdf975-19f2-4999-8e64-3debccde0f70 to disappear
Dec 19 08:08:31.449: INFO: Pod pod-a9cdf975-19f2-4999-8e64-3debccde0f70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:08:31.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1107" for this suite.
Dec 19 08:08:37.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:08:37.502: INFO: namespace emptydir-1107 deletion completed in 6.050502208s

• [SLOW TEST:8.097 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:08:37.502: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-8af4d4b9-7916-4e80-aa06-62aae1ff1021
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:08:37.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2924" for this suite.
Dec 19 08:08:43.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:08:43.577: INFO: namespace secrets-2924 deletion completed in 6.052167724s

• [SLOW TEST:6.075 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:08:43.577: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec 19 08:08:43.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 cluster-info'
Dec 19 08:08:43.649: INFO: stderr: ""
Dec 19 08:08:43.649: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.247.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.247.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:08:43.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3022" for this suite.
Dec 19 08:08:49.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:08:49.734: INFO: namespace kubectl-3022 deletion completed in 6.083101429s

• [SLOW TEST:6.157 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:08:49.734: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-a9d82c7a-6a77-4139-add6-9e037040119e
STEP: Creating a pod to test consume configMaps
Dec 19 08:08:49.756: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083" in namespace "projected-7304" to be "success or failure"
Dec 19 08:08:49.759: INFO: Pod "pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.488861ms
Dec 19 08:08:51.761: INFO: Pod "pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00478503s
STEP: Saw pod success
Dec 19 08:08:51.761: INFO: Pod "pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083" satisfied condition "success or failure"
Dec 19 08:08:51.763: INFO: Trying to get logs from node 192.168.0.132 pod pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 19 08:08:51.773: INFO: Waiting for pod pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083 to disappear
Dec 19 08:08:51.775: INFO: Pod pod-projected-configmaps-1d25ffcd-be5a-4eb5-8938-1b0d12871083 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:08:51.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7304" for this suite.
Dec 19 08:08:57.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:08:58.031: INFO: namespace projected-7304 deletion completed in 6.254171777s

• [SLOW TEST:8.297 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:08:58.032: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-f6ba5b73-43ea-4522-9194-3c75d6393c88
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:08:58.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1886" for this suite.
Dec 19 08:09:04.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:09:04.111: INFO: namespace configmap-1886 deletion completed in 6.052012923s

• [SLOW TEST:6.080 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:09:04.111: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 19 08:09:04.130: INFO: Creating ReplicaSet my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433
Dec 19 08:09:04.133: INFO: Pod name my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433: Found 0 pods out of 1
Dec 19 08:09:09.136: INFO: Pod name my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433: Found 1 pods out of 1
Dec 19 08:09:09.136: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433" is running
Dec 19 08:09:09.137: INFO: Pod "my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433-t4wbr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 08:09:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 08:09:06 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 08:09:06 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-19 08:09:04 +0000 UTC Reason: Message:}])
Dec 19 08:09:09.137: INFO: Trying to dial the pod
Dec 19 08:09:14.147: INFO: Controller my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433: Got expected result from replica 1 [my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433-t4wbr]: "my-hostname-basic-f0f23d45-8d1e-4899-860a-a7b132a99433-t4wbr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:09:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1310" for this suite.
Dec 19 08:09:20.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:09:20.201: INFO: namespace replicaset-1310 deletion completed in 6.052161729s

• [SLOW TEST:16.089 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:09:20.201: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 19 08:09:20.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2" in namespace "projected-2254" to be "success or failure"
Dec 19 08:09:20.228: INFO: Pod "downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.381903ms
Dec 19 08:09:22.231: INFO: Pod "downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00393317s
STEP: Saw pod success
Dec 19 08:09:22.231: INFO: Pod "downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2" satisfied condition "success or failure"
Dec 19 08:09:22.233: INFO: Trying to get logs from node 192.168.0.132 pod downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2 container client-container: <nil>
STEP: delete the pod
Dec 19 08:09:22.244: INFO: Waiting for pod downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2 to disappear
Dec 19 08:09:22.246: INFO: Pod downwardapi-volume-96c7c0f2-721b-4a5c-8f56-500d429746e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:09:22.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2254" for this suite.
Dec 19 08:09:28.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:09:28.303: INFO: namespace projected-2254 deletion completed in 6.054661166s

• [SLOW TEST:8.102 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:09:28.303: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c453265b-b525-4203-bada-25124e0a01e7
STEP: Creating a pod to test consume secrets
Dec 19 08:09:28.346: INFO: Waiting up to 5m0s for pod "pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001" in namespace "secrets-3647" to be "success or failure"
Dec 19 08:09:28.350: INFO: Pod "pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125921ms
Dec 19 08:09:30.352: INFO: Pod "pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00635197s
STEP: Saw pod success
Dec 19 08:09:30.352: INFO: Pod "pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001" satisfied condition "success or failure"
Dec 19 08:09:30.354: INFO: Trying to get logs from node 192.168.0.132 pod pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001 container secret-volume-test: <nil>
STEP: delete the pod
Dec 19 08:09:30.364: INFO: Waiting for pod pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001 to disappear
Dec 19 08:09:30.366: INFO: Pod pod-secrets-70b5d163-4c01-4296-a56d-8e83ce2ac001 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:09:30.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3647" for this suite.
Dec 19 08:09:36.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:09:36.421: INFO: namespace secrets-3647 deletion completed in 6.053306871s
STEP: Destroying namespace "secret-namespace-7160" for this suite.
Dec 19 08:09:42.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:09:42.478: INFO: namespace secret-namespace-7160 deletion completed in 6.057502763s

• [SLOW TEST:14.175 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:09:42.478: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-11b42268-6c98-4dd5-8728-bc9e9c8270dd
STEP: Creating configMap with name cm-test-opt-upd-9b7b25e5-302c-411c-a32c-3dd59bc9bd6a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-11b42268-6c98-4dd5-8728-bc9e9c8270dd
STEP: Updating configmap cm-test-opt-upd-9b7b25e5-302c-411c-a32c-3dd59bc9bd6a
STEP: Creating configMap with name cm-test-opt-create-04a06b61-5572-4b2a-9da3-829ae8aefe4b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:11:12.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7937" for this suite.
Dec 19 08:11:34.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:11:34.853: INFO: namespace projected-7937 deletion completed in 22.055139081s

• [SLOW TEST:112.374 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:11:34.853: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-7f51bd0a-eb5d-43cd-88b6-51467b7f85d8
STEP: Creating secret with name s-test-opt-upd-254b0173-41f8-4bd2-b985-11b0cc98e893
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7f51bd0a-eb5d-43cd-88b6-51467b7f85d8
STEP: Updating secret s-test-opt-upd-254b0173-41f8-4bd2-b985-11b0cc98e893
STEP: Creating secret with name s-test-opt-create-8039dc76-ab76-490c-81ae-e61407b80c07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:11:40.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4288" for this suite.
Dec 19 08:12:02.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:12:02.979: INFO: namespace projected-4288 deletion completed in 22.051141005s

• [SLOW TEST:28.126 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:12:02.979: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-9051/secret-test-6c286419-5e79-4653-ad4c-bfda67109d35
STEP: Creating a pod to test consume secrets
Dec 19 08:12:03.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1" in namespace "secrets-9051" to be "success or failure"
Dec 19 08:12:03.008: INFO: Pod "pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.165877ms
Dec 19 08:12:05.010: INFO: Pod "pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005280528s
Dec 19 08:12:07.012: INFO: Pod "pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007702718s
STEP: Saw pod success
Dec 19 08:12:07.012: INFO: Pod "pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1" satisfied condition "success or failure"
Dec 19 08:12:07.014: INFO: Trying to get logs from node 192.168.0.132 pod pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1 container env-test: <nil>
STEP: delete the pod
Dec 19 08:12:07.024: INFO: Waiting for pod pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1 to disappear
Dec 19 08:12:07.026: INFO: Pod pod-configmaps-e754545d-89ed-4e16-8e41-641ff2c87ab1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:12:07.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9051" for this suite.
Dec 19 08:12:13.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:12:13.081: INFO: namespace secrets-9051 deletion completed in 6.053458121s

• [SLOW TEST:10.102 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:12:13.082: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec 19 08:12:13.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-317660674 --namespace=kubectl-1975 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 19 08:12:15.507: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 19 08:12:15.507: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:12:17.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1975" for this suite.
Dec 19 08:12:23.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:12:23.568: INFO: namespace kubectl-1975 deletion completed in 6.056080725s

• [SLOW TEST:10.487 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:12:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 19 08:12:25.596: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:12:25.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9682" for this suite.
Dec 19 08:12:31.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:12:31.663: INFO: namespace container-runtime-9682 deletion completed in 6.054034928s

• [SLOW TEST:8.095 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 19 08:12:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-317660674
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec 19 08:12:31.686: INFO: Waiting up to 5m0s for pod "client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72" in namespace "containers-3118" to be "success or failure"
Dec 19 08:12:31.688: INFO: Pod "client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.473988ms
Dec 19 08:12:33.690: INFO: Pod "client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004200665s
STEP: Saw pod success
Dec 19 08:12:33.690: INFO: Pod "client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72" satisfied condition "success or failure"
Dec 19 08:12:33.693: INFO: Trying to get logs from node 192.168.0.132 pod client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72 container test-container: <nil>
STEP: delete the pod
Dec 19 08:12:33.705: INFO: Waiting for pod client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72 to disappear
Dec 19 08:12:33.711: INFO: Pod client-containers-659f4e5c-75e6-4c26-8edf-e0b8b48d5a72 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 19 08:12:33.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3118" for this suite.
Dec 19 08:12:39.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 19 08:12:39.763: INFO: namespace containers-3118 deletion completed in 6.049387461s

• [SLOW TEST:8.100 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSDec 19 08:12:39.763: INFO: Running AfterSuite actions on all nodes
Dec 19 08:12:39.775: INFO: Running AfterSuite actions on node 1
Dec 19 08:12:39.775: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5557.445 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h32m39.473295436s
Test Suite Passed
