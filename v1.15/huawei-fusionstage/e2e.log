I1211 02:10:43.801927      18 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-194493201
I1211 02:10:43.802058      18 e2e.go:241] Starting e2e run "e0e95ece-a9e4-4765-9531-073364cffcdc" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1576030241 - Will randomize all specs
Will run 215 of 4411 specs

Dec 11 02:10:43.991: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:10:43.993: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 11 02:10:44.146: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 11 02:10:44.354: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 11 02:10:44.354: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Dec 11 02:10:44.354: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 11 02:10:44.380: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'icagent' (0 seconds elapsed)
Dec 11 02:10:44.380: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kubenode' (0 seconds elapsed)
Dec 11 02:10:44.380: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Dec 11 02:10:44.380: INFO: e2e test version: v1.15.0
Dec 11 02:10:44.401: INFO: kube-apiserver version: v1.15.0-FusionStage8.0-B040
SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:10:44.401: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
Dec 11 02:10:44.491: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec 11 02:10:48.638: INFO: Pod pod-hostip-b0e3ea64-3297-43ab-a03b-68209150b4a9 has hostIP: 10.247.6.203
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:10:48.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4932" for this suite.
Dec 11 02:11:10.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:11:11.607: INFO: namespace pods-4932 deletion completed in 22.94315334s

â€¢ [SLOW TEST:27.206 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:11:11.607: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 11 02:11:11.706: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 11 02:11:11.750: INFO: Waiting for terminating namespaces to be deleted...
Dec 11 02:11:11.772: INFO: 
Logging pods the kubelet thinks is on node 10.247.6.203 before test
Dec 11 02:11:11.878: INFO: sonobuoy-e2e-job-7ef775b06a5742ab from sonobuoy started at 2019-12-11 02:10:34 +0000 UTC (2 container statuses recorded)
Dec 11 02:11:11.878: INFO: 	Container e2e ready: true, restart count 0
Dec 11 02:11:11.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 02:11:11.878: INFO: kubenode-vl77r from kube-system started at 2019-12-06 03:07:00 +0000 UTC (0 container statuses recorded)
Dec 11 02:11:11.878: INFO: sonobuoy from sonobuoy started at 2019-12-11 02:10:31 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:11.878: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 11 02:11:11.878: INFO: sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-lbzbk from sonobuoy started at 2019-12-11 02:10:34 +0000 UTC (2 container statuses recorded)
Dec 11 02:11:11.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 02:11:11.878: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 11 02:11:11.878: INFO: icagent-4fhcw from kube-system started at 2019-12-06 03:07:10 +0000 UTC (0 container statuses recorded)
Dec 11 02:11:11.878: INFO: 
Logging pods the kubelet thinks is on node kcsp41-master0 before test
Dec 11 02:11:12.029: INFO: etcd-backup-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container etcd-backup ready: true, restart count 0
Dec 11 02:11:12.029: INFO: icagent-9rdrh from kube-system started at 2019-12-06 03:02:21 +0000 UTC (0 container statuses recorded)
Dec 11 02:11:12.029: INFO: etcd-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container etcd-container ready: true, restart count 0
Dec 11 02:11:12.029: INFO: kube-apiserver-kcsp41-master0 from kube-system started at 2019-12-11 01:44:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 11 02:11:12.029: INFO: cam-tiller-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container cam-tiller ready: true, restart count 0
Dec 11 02:11:12.029: INFO: sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-clmpn from sonobuoy started at 2019-12-11 02:10:14 +0000 UTC (2 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 02:11:12.029: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 11 02:11:12.029: INFO: kube-canal-controller-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container kube-canal-controller ready: true, restart count 0
Dec 11 02:11:12.029: INFO: kube-controller-manager-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container kube-controller-manager ready: true, restart count 4
Dec 11 02:11:12.029: INFO: coredns-556846b6b7-dnbzk from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container coredns ready: true, restart count 0
Dec 11 02:11:12.029: INFO: etcd-event-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container etcd-event-container ready: true, restart count 0
Dec 11 02:11:12.029: INFO: cfe-kube-webhook-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container webhook ready: true, restart count 0
Dec 11 02:11:12.029: INFO: kube-event-controller-6bc74bfc9c-nfh7p from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container kube-event-controller ready: true, restart count 4
Dec 11 02:11:12.029: INFO: kubenode-c625w from kube-system started at 2019-12-06 03:02:14 +0000 UTC (0 container statuses recorded)
Dec 11 02:11:12.029: INFO: etcd-network-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container etcd-network-container ready: true, restart count 0
Dec 11 02:11:12.029: INFO: kube-canal-apiserver-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container kube-canal-apiserver ready: true, restart count 0
Dec 11 02:11:12.029: INFO: kube-scheduler-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container kube-scheduler ready: true, restart count 4
Dec 11 02:11:12.029: INFO: provision-controller-manager-kcsp41-master0 from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 02:11:12.029: INFO: 	Container provision-controller-manager ready: true, restart count 21
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node 10.247.6.203
STEP: verifying the node has the label node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod cam-tiller-kcsp41-master0 requesting resource cpu=50m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod cfe-kube-webhook-kcsp41-master0 requesting resource cpu=50m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod coredns-556846b6b7-dnbzk requesting resource cpu=100m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod etcd-backup-kcsp41-master0 requesting resource cpu=100m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod etcd-event-server-kcsp41-master0 requesting resource cpu=100m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod etcd-network-server-kcsp41-master0 requesting resource cpu=300m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod etcd-server-kcsp41-master0 requesting resource cpu=450m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod icagent-4fhcw requesting resource cpu=0m on Node 10.247.6.203
Dec 11 02:11:12.357: INFO: Pod icagent-9rdrh requesting resource cpu=0m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kube-apiserver-kcsp41-master0 requesting resource cpu=250m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kube-canal-apiserver-kcsp41-master0 requesting resource cpu=250m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kube-canal-controller-kcsp41-master0 requesting resource cpu=200m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kube-controller-manager-kcsp41-master0 requesting resource cpu=200m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kube-event-controller-6bc74bfc9c-nfh7p requesting resource cpu=200m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kube-scheduler-kcsp41-master0 requesting resource cpu=100m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kubenode-c625w requesting resource cpu=0m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod kubenode-vl77r requesting resource cpu=0m on Node 10.247.6.203
Dec 11 02:11:12.357: INFO: Pod provision-controller-manager-kcsp41-master0 requesting resource cpu=200m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.247.6.203
Dec 11 02:11:12.357: INFO: Pod sonobuoy-e2e-job-7ef775b06a5742ab requesting resource cpu=0m on Node 10.247.6.203
Dec 11 02:11:12.357: INFO: Pod sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-clmpn requesting resource cpu=0m on Node kcsp41-master0
Dec 11 02:11:12.357: INFO: Pod sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-lbzbk requesting resource cpu=0m on Node 10.247.6.203
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40.15df2f2a33db3d80], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2150/filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40 to kcsp41-master0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40.15df2f2a47339fc1], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volumes for pod "filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40_sched-pred-2150(3150b173-7392-49d5-a0f6-d35608285393)"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40.15df2f2a8bf44297], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40.15df2f2a9ccc3b55], Reason = [SuccessfulCreate], Message = [Created container filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40.15df2f2aa6d6819e], Reason = [Started], Message = [Started container filler-pod-63b3a56c-331d-4f88-b7ca-d4fcb16e3c40]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e448f78c-a406-4427-9b2d-26856c61199d.15df2f2a32452627], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2150/filler-pod-e448f78c-a406-4427-9b2d-26856c61199d to 10.247.6.203]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e448f78c-a406-4427-9b2d-26856c61199d.15df2f2ecfa43c63], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volumes for pod "filler-pod-e448f78c-a406-4427-9b2d-26856c61199d_sched-pred-2150(c09a31c2-5ac7-40d7-8af9-27607fb9f234)"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e448f78c-a406-4427-9b2d-26856c61199d.15df2f2f0cb4cf73], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e448f78c-a406-4427-9b2d-26856c61199d.15df2f2f1cea971d], Reason = [SuccessfulCreate], Message = [Created container filler-pod-e448f78c-a406-4427-9b2d-26856c61199d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e448f78c-a406-4427-9b2d-26856c61199d.15df2f2f266c4f8f], Reason = [Started], Message = [Started container filler-pod-e448f78c-a406-4427-9b2d-26856c61199d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15df2f2b2a5e5978], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node kcsp41-master0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.247.6.203
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:11:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2150" for this suite.
Dec 11 02:11:23.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:11:24.689: INFO: namespace sched-pred-2150 deletion completed in 6.932833445s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:13.082 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:11:24.690: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 11 02:11:24.818: INFO: Waiting up to 5m0s for pod "pod-ee5d238f-67b6-4515-9c69-a973d4e5f828" in namespace "emptydir-9048" to be "success or failure"
Dec 11 02:11:24.840: INFO: Pod "pod-ee5d238f-67b6-4515-9c69-a973d4e5f828": Phase="Pending", Reason="", readiness=false. Elapsed: 22.299356ms
Dec 11 02:11:26.863: INFO: Pod "pod-ee5d238f-67b6-4515-9c69-a973d4e5f828": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04519215s
Dec 11 02:11:28.886: INFO: Pod "pod-ee5d238f-67b6-4515-9c69-a973d4e5f828": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068374573s
STEP: Saw pod success
Dec 11 02:11:28.886: INFO: Pod "pod-ee5d238f-67b6-4515-9c69-a973d4e5f828" satisfied condition "success or failure"
Dec 11 02:11:28.909: INFO: Trying to get logs from node 10.247.6.203 pod pod-ee5d238f-67b6-4515-9c69-a973d4e5f828 container test-container: <nil>
STEP: delete the pod
Dec 11 02:11:28.984: INFO: Waiting for pod pod-ee5d238f-67b6-4515-9c69-a973d4e5f828 to disappear
Dec 11 02:11:29.006: INFO: Pod pod-ee5d238f-67b6-4515-9c69-a973d4e5f828 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:11:29.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9048" for this suite.
Dec 11 02:11:35.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:11:35.979: INFO: namespace emptydir-9048 deletion completed in 6.949856014s

â€¢ [SLOW TEST:11.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:11:35.979: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 11 02:11:36.068: INFO: namespace kubectl-1140
Dec 11 02:11:36.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1140'
Dec 11 02:11:37.864: INFO: stderr: ""
Dec 11 02:11:37.864: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 11 02:11:38.888: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:11:38.888: INFO: Found 0 / 1
Dec 11 02:11:39.887: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:11:39.887: INFO: Found 0 / 1
Dec 11 02:11:40.887: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:11:40.887: INFO: Found 1 / 1
Dec 11 02:11:40.887: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 11 02:11:40.910: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:11:40.910: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 11 02:11:40.910: INFO: wait on redis-master startup in kubectl-1140 
Dec 11 02:11:40.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 logs redis-master-9hpmb redis-master --namespace=kubectl-1140'
Dec 11 02:11:41.139: INFO: stderr: ""
Dec 11 02:11:41.139: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Dec 02:11:39.762 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Dec 02:11:39.762 # Server started, Redis version 3.2.12\n1:M 11 Dec 02:11:39.762 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Dec 02:11:39.762 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec 11 02:11:41.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1140'
Dec 11 02:11:41.341: INFO: stderr: ""
Dec 11 02:11:41.341: INFO: stdout: "service/rm2 exposed\n"
Dec 11 02:11:41.363: INFO: Service rm2 in namespace kubectl-1140 found.
STEP: exposing service
Dec 11 02:11:43.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1140'
Dec 11 02:11:43.602: INFO: stderr: ""
Dec 11 02:11:43.602: INFO: stdout: "service/rm3 exposed\n"
Dec 11 02:11:43.624: INFO: Service rm3 in namespace kubectl-1140 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:11:45.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1140" for this suite.
Dec 11 02:12:07.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:12:08.637: INFO: namespace kubectl-1140 deletion completed in 22.943376918s

â€¢ [SLOW TEST:32.658 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:12:08.638: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 11 02:12:13.396: INFO: Successfully updated pod "pod-update-activedeadlineseconds-04113541-c214-4fec-bebc-112cbba8d7ca"
Dec 11 02:12:13.396: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-04113541-c214-4fec-bebc-112cbba8d7ca" in namespace "pods-2893" to be "terminated due to deadline exceeded"
Dec 11 02:12:13.418: INFO: Pod "pod-update-activedeadlineseconds-04113541-c214-4fec-bebc-112cbba8d7ca": Phase="Running", Reason="", readiness=true. Elapsed: 22.204902ms
Dec 11 02:12:15.442: INFO: Pod "pod-update-activedeadlineseconds-04113541-c214-4fec-bebc-112cbba8d7ca": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.046226265s
Dec 11 02:12:15.442: INFO: Pod "pod-update-activedeadlineseconds-04113541-c214-4fec-bebc-112cbba8d7ca" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:12:15.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2893" for this suite.
Dec 11 02:12:21.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:12:22.425: INFO: namespace pods-2893 deletion completed in 6.959857947s

â€¢ [SLOW TEST:13.788 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:12:22.425: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 11 02:12:22.683: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9782,SelfLink:/api/v1/namespaces/watch-9782/configmaps/e2e-watch-test-resource-version,UID:3c44ffcd-9a58-4382-b832-a3f60f433435,ResourceVersion:1010659,Generation:0,CreationTimestamp:2019-12-11 02:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 11 02:12:22.684: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9782,SelfLink:/api/v1/namespaces/watch-9782/configmaps/e2e-watch-test-resource-version,UID:3c44ffcd-9a58-4382-b832-a3f60f433435,ResourceVersion:1010660,Generation:0,CreationTimestamp:2019-12-11 02:12:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:12:22.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9782" for this suite.
Dec 11 02:12:28.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:12:29.669: INFO: namespace watch-9782 deletion completed in 6.961800694s

â€¢ [SLOW TEST:7.243 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:12:29.669: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:12:29.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1759'
Dec 11 02:12:30.089: INFO: stderr: ""
Dec 11 02:12:30.089: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 11 02:12:30.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1759'
Dec 11 02:12:30.401: INFO: stderr: ""
Dec 11 02:12:30.401: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 11 02:12:31.424: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:12:31.424: INFO: Found 0 / 1
Dec 11 02:12:32.424: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:12:32.424: INFO: Found 1 / 1
Dec 11 02:12:32.424: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 11 02:12:32.447: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 02:12:32.447: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 11 02:12:32.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 describe pod redis-master-qgnd9 --namespace=kubectl-1759'
Dec 11 02:12:32.646: INFO: stderr: ""
Dec 11 02:12:32.646: INFO: stdout: "Name:           redis-master-qgnd9\nNamespace:      kubectl-1759\nNode:           10.247.6.203/10.247.6.203\nStart Time:     Wed, 11 Dec 2019 02:12:30 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.16.1.196\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://3a420335b9cc4271b4ad478a36b496679231966c7fd4367e5d8f8b20219af13c\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 11 Dec 2019 02:12:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBE_POD_NODE_NAME:   (v1:spec.nodeName)\n      KUBE_POD_NAMESPACE:  kubectl-1759 (v1:metadata.namespace)\n      KUBE_POD_CLUSTERID:  236cc8a2-17d4-11ea-bf02-0255ac1b0093\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mrfhk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mrfhk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mrfhk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason                 Age              From                   Message\n  ----    ------                 ----             ----                   -------\n  Normal  Scheduled              22s              default-scheduler      Successfully assigned kubectl-1759/redis-master-qgnd9 to 10.247.6.203\n  Normal  SuccessfulMountVolume  1s (x2 over 2s)  kubelet, 10.247.6.203  Successfully mounted volumes for pod \"redis-master-qgnd9_kubectl-1759(0dfa7573-bfd6-4f09-acbb-0fa023ee7972)\"\n  Normal  Pulled                 1s               kubelet, 10.247.6.203  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  SuccessfulCreate       1s               kubelet, 10.247.6.203  Created container redis-master\n  Normal  Started                1s               kubelet, 10.247.6.203  Started container redis-master\n"
Dec 11 02:12:32.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 describe rc redis-master --namespace=kubectl-1759'
Dec 11 02:12:32.885: INFO: stderr: ""
Dec 11 02:12:32.885: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1759\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  22s   replication-controller  Created pod: redis-master-qgnd9\n"
Dec 11 02:12:32.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 describe service redis-master --namespace=kubectl-1759'
Dec 11 02:12:33.110: INFO: stderr: ""
Dec 11 02:12:33.110: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1759\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                192.168.133.66\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.1.196:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 11 02:12:33.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 describe node 10.247.6.203'
Dec 11 02:12:33.404: INFO: stderr: ""
Dec 11 02:12:33.404: INFO: stdout: "Name:               10.247.6.203\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    com.huawei.machinetype=vm\n                    com.huawei.project/name=default\n                    failure-domain.beta.kubernetes.io/zone=cn-south-3b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.247.6.203\n                    kubernetes.io/os=linux\n                    os.architecture=x86_64\n                    os.name=EulerOS_2.0_SP5\n                    os.version=3.10.0-862.14.1.0.h183.eulerosv2r7.x86_64\nAnnotations:        ExternalIP: 10.247.6.203\n                    alpha.kubernetes.io/install-node-extend-param: {\"managementNetwork\":\"10.247.0.0/16\",\"dataNetwork\":\"10.247.0.0/16\"}\n                    huawei.com/gpu-status: []\n                    node.alpha.kubernetes.io/ips: {\"name\":\"eth0\",\"ips\":[\"10.247.6.203\"]}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.provision.alpha.kubernetes.io/data-volumes: [{\"volumeName\":\"\",\"volumeSize\":0,\"volumeType\":\"\"}]\n                    node.provision.alpha.kubernetes.io/flavor: \n                    node.provision.alpha.kubernetes.io/image: \n                    node.provision.alpha.kubernetes.io/kube-agent-version: 8.00.32\n                    node.provision.alpha.kubernetes.io/login-secret-name: paas\n                    node.provision.alpha.kubernetes.io/node-address: 10.247.6.203\n                    node.provision.alpha.kubernetes.io/node-port: 22\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 06 Dec 2019 03:05:47 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                    Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                    ------  -----------------                 ------------------                ------                       -------\n  NodeStartInstallation   True    Fri, 06 Dec 2019 03:05:47 +0000   Fri, 06 Dec 2019 03:05:47 +0000                                \n  MemoryPressure          False   Wed, 11 Dec 2019 02:12:29 +0000   Fri, 06 Dec 2019 03:06:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure            False   Wed, 11 Dec 2019 02:12:29 +0000   Fri, 06 Dec 2019 03:06:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure             False   Wed, 11 Dec 2019 02:12:29 +0000   Fri, 06 Dec 2019 03:06:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                   True    Wed, 11 Dec 2019 02:12:29 +0000   Fri, 06 Dec 2019 03:06:56 +0000   KubeletReady                 kubelet is posting ready status\n  NodeInstall             True    Fri, 06 Dec 2019 03:06:39 +0000   Fri, 06 Dec 2019 03:06:39 +0000                                \nAddresses:\n  InternalIP:  10.247.6.203\n  Hostname:    10.247.6.203\n  DataIP:      10.247.6.203\n  ExternalIP:  10.247.6.203\nCapacity:\n cpu:                4\n ephemeral-storage:  14381064Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3863748Ki\n pods:               110\nAllocatable:\n cpu:                3\n ephemeral-storage:  13253588561\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3761348Ki\n pods:               110\nSystem Info:\n Machine ID:                 55b811bc0e5a4e95ac4bc4df5f8f656e\n System UUID:                4bfa90d5-c5d8-4dff-9cb5-15fd0366ecbc\n Boot ID:                    a2e69653-6b88-43d9-b9bf-88fe2b866611\n Kernel Version:             3.10.0-862.14.1.0.h183.eulerosv2r7.x86_64\n OS Image:                   EulerOS 2.0 (SP5)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.11.2\n Kubelet Version:            v1.15.0-FusionStage8.0-B040-dirty-dirty\n Kube-Proxy Version:         v1.15.0-FusionStage8.0-B040-dirty-dirty\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                icagent-4fhcw                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h\n  kube-system                kubenode-vl77r                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d23h\n  kubectl-1759               redis-master-qgnd9                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         23s\n  sonobuoy                   sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m21s\n  sonobuoy                   sonobuoy-e2e-job-7ef775b06a5742ab                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m19s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-lbzbk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m19s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\nEvents:              <none>\n"
Dec 11 02:12:33.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 describe namespace kubectl-1759'
Dec 11 02:12:33.621: INFO: stderr: ""
Dec 11 02:12:33.621: INFO: stdout: "Name:         kubectl-1759\nLabels:       e2e-framework=kubectl\n              e2e-run=e0e95ece-a9e4-4765-9531-073364cffcdc\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:12:33.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1759" for this suite.
Dec 11 02:12:55.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:12:56.565: INFO: namespace kubectl-1759 deletion completed in 22.921568973s

â€¢ [SLOW TEST:26.897 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:12:56.566: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-nxwq
STEP: Creating a pod to test atomic-volume-subpath
Dec 11 02:12:56.730: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nxwq" in namespace "subpath-6373" to be "success or failure"
Dec 11 02:12:56.751: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Pending", Reason="", readiness=false. Elapsed: 21.47641ms
Dec 11 02:12:58.774: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044410296s
Dec 11 02:13:00.796: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 4.066619396s
Dec 11 02:13:02.819: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 6.08895569s
Dec 11 02:13:04.841: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 8.111335545s
Dec 11 02:13:06.864: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 10.134244163s
Dec 11 02:13:08.887: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 12.157864027s
Dec 11 02:13:10.910: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 14.179954566s
Dec 11 02:13:12.932: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 16.20205008s
Dec 11 02:13:14.954: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 18.223987322s
Dec 11 02:13:16.976: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 20.2467072s
Dec 11 02:13:18.999: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Running", Reason="", readiness=true. Elapsed: 22.269061293s
Dec 11 02:13:21.022: INFO: Pod "pod-subpath-test-projected-nxwq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.29223192s
STEP: Saw pod success
Dec 11 02:13:21.022: INFO: Pod "pod-subpath-test-projected-nxwq" satisfied condition "success or failure"
Dec 11 02:13:21.044: INFO: Trying to get logs from node 10.247.6.203 pod pod-subpath-test-projected-nxwq container test-container-subpath-projected-nxwq: <nil>
STEP: delete the pod
Dec 11 02:13:21.135: INFO: Waiting for pod pod-subpath-test-projected-nxwq to disappear
Dec 11 02:13:21.156: INFO: Pod pod-subpath-test-projected-nxwq no longer exists
STEP: Deleting pod pod-subpath-test-projected-nxwq
Dec 11 02:13:21.157: INFO: Deleting pod "pod-subpath-test-projected-nxwq" in namespace "subpath-6373"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:13:21.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6373" for this suite.
Dec 11 02:13:27.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:13:28.152: INFO: namespace subpath-6373 deletion completed in 6.950964494s

â€¢ [SLOW TEST:31.586 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:13:28.152: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 11 02:13:28.271: INFO: Waiting up to 5m0s for pod "pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c" in namespace "emptydir-1319" to be "success or failure"
Dec 11 02:13:28.293: INFO: Pod "pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.697965ms
Dec 11 02:13:30.317: INFO: Pod "pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045214875s
Dec 11 02:13:32.340: INFO: Pod "pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068218546s
STEP: Saw pod success
Dec 11 02:13:32.340: INFO: Pod "pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c" satisfied condition "success or failure"
Dec 11 02:13:32.362: INFO: Trying to get logs from node 10.247.6.203 pod pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c container test-container: <nil>
STEP: delete the pod
Dec 11 02:13:32.436: INFO: Waiting for pod pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c to disappear
Dec 11 02:13:32.458: INFO: Pod pod-7dae75d6-77f7-4668-8ff4-d2c526f31c4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:13:32.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1319" for this suite.
Dec 11 02:13:38.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:13:39.397: INFO: namespace emptydir-1319 deletion completed in 6.91761871s

â€¢ [SLOW TEST:11.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:13:39.398: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2e0a5270-8861-4fc0-afbb-cea3b948eb86
STEP: Creating a pod to test consume secrets
Dec 11 02:13:39.631: INFO: Waiting up to 5m0s for pod "pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8" in namespace "secrets-4367" to be "success or failure"
Dec 11 02:13:39.654: INFO: Pod "pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8": Phase="Pending", Reason="", readiness=false. Elapsed: 23.681858ms
Dec 11 02:13:41.683: INFO: Pod "pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052383711s
STEP: Saw pod success
Dec 11 02:13:41.683: INFO: Pod "pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8" satisfied condition "success or failure"
Dec 11 02:13:41.709: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8 container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 02:13:41.784: INFO: Waiting for pod pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8 to disappear
Dec 11 02:13:41.805: INFO: Pod pod-secrets-723b58f5-d549-4800-843a-4e4fe4b8afd8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:13:41.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4367" for this suite.
Dec 11 02:13:47.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:13:48.748: INFO: namespace secrets-4367 deletion completed in 6.919682279s
STEP: Destroying namespace "secret-namespace-9446" for this suite.
Dec 11 02:13:54.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:13:55.686: INFO: namespace secret-namespace-9446 deletion completed in 6.938530228s

â€¢ [SLOW TEST:16.289 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:13:55.687: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-422b74da-9811-42b2-8aaa-125838bf79d0
STEP: Creating a pod to test consume secrets
Dec 11 02:13:55.832: INFO: Waiting up to 5m0s for pod "pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1" in namespace "secrets-240" to be "success or failure"
Dec 11 02:13:55.854: INFO: Pod "pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.998359ms
Dec 11 02:13:57.879: INFO: Pod "pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046510275s
STEP: Saw pod success
Dec 11 02:13:57.879: INFO: Pod "pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1" satisfied condition "success or failure"
Dec 11 02:13:57.901: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1 container secret-env-test: <nil>
STEP: delete the pod
Dec 11 02:13:57.991: INFO: Waiting for pod pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1 to disappear
Dec 11 02:13:58.013: INFO: Pod pod-secrets-a738dee1-768e-413e-94e7-3b1e521cffa1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:13:58.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-240" for this suite.
Dec 11 02:14:04.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:14:04.975: INFO: namespace secrets-240 deletion completed in 6.939777203s

â€¢ [SLOW TEST:9.288 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:14:04.975: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:14:05.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8518" for this suite.
Dec 11 02:14:27.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:14:28.069: INFO: namespace pods-8518 deletion completed in 22.931936412s

â€¢ [SLOW TEST:23.094 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:14:28.069: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-cece1def-d9d0-41cb-a645-494356b9ffb3
STEP: Creating a pod to test consume secrets
Dec 11 02:14:28.213: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84" in namespace "projected-6600" to be "success or failure"
Dec 11 02:14:28.235: INFO: Pod "pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84": Phase="Pending", Reason="", readiness=false. Elapsed: 21.900869ms
Dec 11 02:14:30.258: INFO: Pod "pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044606152s
Dec 11 02:14:32.280: INFO: Pod "pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066825002s
STEP: Saw pod success
Dec 11 02:14:32.280: INFO: Pod "pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84" satisfied condition "success or failure"
Dec 11 02:14:32.303: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 11 02:14:32.378: INFO: Waiting for pod pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84 to disappear
Dec 11 02:14:32.399: INFO: Pod pod-projected-secrets-8b22e84f-ffe0-45f7-b036-605ceb5e8e84 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:14:32.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6600" for this suite.
Dec 11 02:14:38.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:14:39.364: INFO: namespace projected-6600 deletion completed in 6.940983016s

â€¢ [SLOW TEST:11.295 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:14:39.364: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 11 02:14:39.542: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-935,SelfLink:/api/v1/namespaces/watch-935/configmaps/e2e-watch-test-watch-closed,UID:b9bc2b03-6335-4810-a721-361bf8b024cd,ResourceVersion:1011127,Generation:0,CreationTimestamp:2019-12-11 02:14:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 11 02:14:39.542: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-935,SelfLink:/api/v1/namespaces/watch-935/configmaps/e2e-watch-test-watch-closed,UID:b9bc2b03-6335-4810-a721-361bf8b024cd,ResourceVersion:1011128,Generation:0,CreationTimestamp:2019-12-11 02:14:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 11 02:14:39.630: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-935,SelfLink:/api/v1/namespaces/watch-935/configmaps/e2e-watch-test-watch-closed,UID:b9bc2b03-6335-4810-a721-361bf8b024cd,ResourceVersion:1011129,Generation:0,CreationTimestamp:2019-12-11 02:14:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 11 02:14:39.630: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-935,SelfLink:/api/v1/namespaces/watch-935/configmaps/e2e-watch-test-watch-closed,UID:b9bc2b03-6335-4810-a721-361bf8b024cd,ResourceVersion:1011130,Generation:0,CreationTimestamp:2019-12-11 02:14:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:14:39.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-935" for this suite.
Dec 11 02:14:45.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:14:46.592: INFO: namespace watch-935 deletion completed in 6.939711102s

â€¢ [SLOW TEST:7.228 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:14:46.592: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 11 02:14:48.829: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:14:48.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4120" for this suite.
Dec 11 02:14:54.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:14:55.822: INFO: namespace container-runtime-4120 deletion completed in 6.91814887s

â€¢ [SLOW TEST:9.230 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:14:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8237
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 11 02:14:55.910: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 11 02:15:22.305: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.205:8080/dial?request=hostName&protocol=udp&host=172.16.0.178&port=8081&tries=1'] Namespace:pod-network-test-8237 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:15:22.305: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:15:22.816: INFO: Waiting for endpoints: map[]
Dec 11 02:15:22.837: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.205:8080/dial?request=hostName&protocol=udp&host=172.16.1.204&port=8081&tries=1'] Namespace:pod-network-test-8237 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:15:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:15:23.200: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:15:23.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8237" for this suite.
Dec 11 02:15:45.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:15:46.161: INFO: namespace pod-network-test-8237 deletion completed in 22.938490263s

â€¢ [SLOW TEST:50.338 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:15:46.161: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 11 02:15:49.389: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:15:49.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2776" for this suite.
Dec 11 02:15:55.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:15:56.396: INFO: namespace container-runtime-2776 deletion completed in 6.93613446s

â€¢ [SLOW TEST:10.235 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:15:56.396: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 11 02:16:06.630: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:16:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1105" for this suite.
W1211 02:16:06.630707      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 11 02:16:12.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:16:13.584: INFO: namespace gc-1105 deletion completed in 6.930509638s

â€¢ [SLOW TEST:17.188 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:16:13.584: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 02:16:13.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9" in namespace "downward-api-8996" to be "success or failure"
Dec 11 02:16:13.724: INFO: Pod "downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.702263ms
Dec 11 02:16:15.746: INFO: Pod "downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044250528s
Dec 11 02:16:17.769: INFO: Pod "downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067016119s
STEP: Saw pod success
Dec 11 02:16:17.769: INFO: Pod "downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9" satisfied condition "success or failure"
Dec 11 02:16:17.791: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9 container client-container: <nil>
STEP: delete the pod
Dec 11 02:16:17.866: INFO: Waiting for pod downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9 to disappear
Dec 11 02:16:17.887: INFO: Pod downwardapi-volume-1e2a8924-350b-4809-96ad-092419add4c9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:16:17.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8996" for this suite.
Dec 11 02:16:23.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:16:24.844: INFO: namespace downward-api-8996 deletion completed in 6.934497847s

â€¢ [SLOW TEST:11.260 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:16:24.845: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-6f1b930f-f567-498e-aace-9ddf5a5fa165
STEP: Creating a pod to test consume configMaps
Dec 11 02:16:24.986: INFO: Waiting up to 5m0s for pod "pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5" in namespace "configmap-2898" to be "success or failure"
Dec 11 02:16:25.008: INFO: Pod "pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.838489ms
Dec 11 02:16:27.031: INFO: Pod "pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04452194s
STEP: Saw pod success
Dec 11 02:16:27.031: INFO: Pod "pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5" satisfied condition "success or failure"
Dec 11 02:16:27.052: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:16:27.126: INFO: Waiting for pod pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5 to disappear
Dec 11 02:16:27.148: INFO: Pod pod-configmaps-f0c96bcf-987c-44af-8828-bb527b1086e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:16:27.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2898" for this suite.
Dec 11 02:16:33.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:16:34.105: INFO: namespace configmap-2898 deletion completed in 6.934470347s

â€¢ [SLOW TEST:9.260 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:16:34.105: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c9a6dd88-54db-4bd1-aa53-0a02b50a36ce
STEP: Creating a pod to test consume configMaps
Dec 11 02:16:34.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959" in namespace "configmap-972" to be "success or failure"
Dec 11 02:16:34.279: INFO: Pod "pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959": Phase="Pending", Reason="", readiness=false. Elapsed: 24.725031ms
Dec 11 02:16:36.303: INFO: Pod "pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04800529s
Dec 11 02:16:38.325: INFO: Pod "pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069957937s
STEP: Saw pod success
Dec 11 02:16:38.325: INFO: Pod "pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959" satisfied condition "success or failure"
Dec 11 02:16:38.346: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:16:38.418: INFO: Waiting for pod pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959 to disappear
Dec 11 02:16:38.439: INFO: Pod pod-configmaps-da4fac08-1423-4e7e-9a30-312b54639959 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:16:38.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-972" for this suite.
Dec 11 02:16:44.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:16:45.401: INFO: namespace configmap-972 deletion completed in 6.939208318s

â€¢ [SLOW TEST:11.296 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:16:45.401: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:16:45.495: INFO: Creating ReplicaSet my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1
Dec 11 02:16:45.540: INFO: Pod name my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1: Found 1 pods out of 1
Dec 11 02:16:45.540: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1" is running
Dec 11 02:16:49.583: INFO: Pod "my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1-sdfqd" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-11 02:16:26 +0000 UTC Reason: Message:}])
Dec 11 02:16:49.583: INFO: Trying to dial the pod
Dec 11 02:16:54.716: INFO: Controller my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1: Got expected result from replica 1 [my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1-sdfqd]: "my-hostname-basic-f9a6318b-ad26-4cb0-90c0-815f7d950ad1-sdfqd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:16:54.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3261" for this suite.
Dec 11 02:17:00.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:17:01.674: INFO: namespace replicaset-3261 deletion completed in 6.935804745s

â€¢ [SLOW TEST:16.273 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:17:01.674: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-c895de61-b0a9-4f79-a9a0-a5f9ad83dacc in namespace container-probe-579
Dec 11 02:17:05.835: INFO: Started pod liveness-c895de61-b0a9-4f79-a9a0-a5f9ad83dacc in namespace container-probe-579
STEP: checking the pod's current state and verifying that restartCount is present
Dec 11 02:17:05.857: INFO: Initial restart count of pod liveness-c895de61-b0a9-4f79-a9a0-a5f9ad83dacc is 0
Dec 11 02:17:24.080: INFO: Restart count of pod container-probe-579/liveness-c895de61-b0a9-4f79-a9a0-a5f9ad83dacc is now 1 (18.223345047s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:17:24.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-579" for this suite.
Dec 11 02:17:30.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:17:31.096: INFO: namespace container-probe-579 deletion completed in 6.96301978s

â€¢ [SLOW TEST:29.421 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:17:31.096: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 11 02:17:32.372: INFO: Pod name wrapped-volume-race-b8231ed6-6a04-4170-a3c9-75995cedb6e9: Found 1 pods out of 5
Dec 11 02:17:37.437: INFO: Pod name wrapped-volume-race-b8231ed6-6a04-4170-a3c9-75995cedb6e9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b8231ed6-6a04-4170-a3c9-75995cedb6e9 in namespace emptydir-wrapper-4111, will wait for the garbage collector to delete the pods
Dec 11 02:17:51.739: INFO: Deleting ReplicationController wrapped-volume-race-b8231ed6-6a04-4170-a3c9-75995cedb6e9 took: 23.998686ms
Dec 11 02:17:52.239: INFO: Terminating ReplicationController wrapped-volume-race-b8231ed6-6a04-4170-a3c9-75995cedb6e9 pods took: 500.240744ms
STEP: Creating RC which spawns configmap-volume pods
Dec 11 02:18:29.818: INFO: Pod name wrapped-volume-race-75cbdd77-afca-42c3-bef7-5c750f253a8e: Found 1 pods out of 5
Dec 11 02:18:34.885: INFO: Pod name wrapped-volume-race-75cbdd77-afca-42c3-bef7-5c750f253a8e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-75cbdd77-afca-42c3-bef7-5c750f253a8e in namespace emptydir-wrapper-4111, will wait for the garbage collector to delete the pods
Dec 11 02:18:47.173: INFO: Deleting ReplicationController wrapped-volume-race-75cbdd77-afca-42c3-bef7-5c750f253a8e took: 23.879061ms
Dec 11 02:18:47.673: INFO: Terminating ReplicationController wrapped-volume-race-75cbdd77-afca-42c3-bef7-5c750f253a8e pods took: 500.229786ms
STEP: Creating RC which spawns configmap-volume pods
Dec 11 02:19:22.946: INFO: Pod name wrapped-volume-race-20b8f0e4-7a6b-45a5-950a-568be07a3060: Found 1 pods out of 5
Dec 11 02:19:28.011: INFO: Pod name wrapped-volume-race-20b8f0e4-7a6b-45a5-950a-568be07a3060: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-20b8f0e4-7a6b-45a5-950a-568be07a3060 in namespace emptydir-wrapper-4111, will wait for the garbage collector to delete the pods
Dec 11 02:19:40.312: INFO: Deleting ReplicationController wrapped-volume-race-20b8f0e4-7a6b-45a5-950a-568be07a3060 took: 23.818806ms
Dec 11 02:19:40.912: INFO: Terminating ReplicationController wrapped-volume-race-20b8f0e4-7a6b-45a5-950a-568be07a3060 pods took: 600.199976ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:20:20.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4111" for this suite.
Dec 11 02:20:26.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:20:27.784: INFO: namespace emptydir-wrapper-4111 deletion completed in 6.934637787s

â€¢ [SLOW TEST:176.688 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:20:27.784: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 11 02:20:36.059: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:36.059: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:36.407: INFO: Exec stderr: ""
Dec 11 02:20:36.407: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:36.407: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:36.748: INFO: Exec stderr: ""
Dec 11 02:20:36.748: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:36.748: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:37.092: INFO: Exec stderr: ""
Dec 11 02:20:37.092: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:37.092: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:37.435: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 11 02:20:37.435: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:37.435: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:37.776: INFO: Exec stderr: ""
Dec 11 02:20:37.776: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:37.776: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:38.120: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 11 02:20:38.120: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:38.120: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:38.481: INFO: Exec stderr: ""
Dec 11 02:20:38.481: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:38.481: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:38.826: INFO: Exec stderr: ""
Dec 11 02:20:38.826: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:38.826: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:39.168: INFO: Exec stderr: ""
Dec 11 02:20:39.168: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2855 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:20:39.168: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:20:39.521: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:20:39.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2855" for this suite.
Dec 11 02:21:29.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:21:30.493: INFO: namespace e2e-kubelet-etc-hosts-2855 deletion completed in 50.950615341s

â€¢ [SLOW TEST:62.710 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:21:30.494: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-661007c9-9379-4f67-a7d5-3eac9dff39b6
STEP: Creating secret with name s-test-opt-upd-daae4f8e-2c99-4c2d-8204-85c431456339
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-661007c9-9379-4f67-a7d5-3eac9dff39b6
STEP: Updating secret s-test-opt-upd-daae4f8e-2c99-4c2d-8204-85c431456339
STEP: Creating secret with name s-test-opt-create-bd1b78e8-51b3-4dc6-8ee4-55951dbbfaed
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:23:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-534" for this suite.
Dec 11 02:23:28.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:23:28.911: INFO: namespace secrets-534 deletion completed in 22.931142991s

â€¢ [SLOW TEST:118.418 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:23:28.912: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec 11 02:23:29.028: INFO: Waiting up to 5m0s for pod "client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677" in namespace "containers-8718" to be "success or failure"
Dec 11 02:23:29.050: INFO: Pod "client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677": Phase="Pending", Reason="", readiness=false. Elapsed: 21.756237ms
Dec 11 02:23:31.073: INFO: Pod "client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044388521s
Dec 11 02:23:33.096: INFO: Pod "client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067453908s
STEP: Saw pod success
Dec 11 02:23:33.096: INFO: Pod "client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677" satisfied condition "success or failure"
Dec 11 02:23:33.118: INFO: Trying to get logs from node 10.247.6.203 pod client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677 container test-container: <nil>
STEP: delete the pod
Dec 11 02:23:33.193: INFO: Waiting for pod client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677 to disappear
Dec 11 02:23:33.215: INFO: Pod client-containers-11eb0554-d33f-430a-8b67-ebe36d1aa677 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:23:33.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8718" for this suite.
Dec 11 02:23:39.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:23:40.163: INFO: namespace containers-8718 deletion completed in 6.926259547s

â€¢ [SLOW TEST:11.251 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:23:40.163: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 11 02:23:44.923: INFO: Successfully updated pod "pod-update-b9aee3a2-094e-49c0-a6b2-f84bc1f69354"
STEP: verifying the updated pod is in kubernetes
Dec 11 02:23:44.969: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:23:44.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9615" for this suite.
Dec 11 02:24:07.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:24:07.927: INFO: namespace pods-9615 deletion completed in 22.933730099s

â€¢ [SLOW TEST:27.763 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:24:07.927: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 11 02:24:16.273: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 11 02:24:16.295: INFO: Pod pod-with-poststart-http-hook still exists
Dec 11 02:24:18.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 11 02:24:18.318: INFO: Pod pod-with-poststart-http-hook still exists
Dec 11 02:24:20.295: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 11 02:24:20.317: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:24:20.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8379" for this suite.
Dec 11 02:24:42.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:24:43.267: INFO: namespace container-lifecycle-hook-8379 deletion completed in 22.924267146s

â€¢ [SLOW TEST:35.340 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:24:43.267: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:24:43.354: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:24:47.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5356" for this suite.
Dec 11 02:25:25.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:25:26.509: INFO: namespace pods-5356 deletion completed in 38.925509441s

â€¢ [SLOW TEST:43.242 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:25:26.509: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1211 02:25:36.957010      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 11 02:25:36.957: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:25:36.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4518" for this suite.
Dec 11 02:25:43.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:25:43.890: INFO: namespace gc-4518 deletion completed in 6.911376018s

â€¢ [SLOW TEST:17.381 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:25:43.890: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 02:25:44.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838" in namespace "projected-2104" to be "success or failure"
Dec 11 02:25:44.027: INFO: Pod "downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838": Phase="Pending", Reason="", readiness=false. Elapsed: 21.584076ms
Dec 11 02:25:46.050: INFO: Pod "downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044234305s
Dec 11 02:25:48.072: INFO: Pod "downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066986752s
STEP: Saw pod success
Dec 11 02:25:48.072: INFO: Pod "downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838" satisfied condition "success or failure"
Dec 11 02:25:48.094: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838 container client-container: <nil>
STEP: delete the pod
Dec 11 02:25:48.168: INFO: Waiting for pod downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838 to disappear
Dec 11 02:25:48.189: INFO: Pod downwardapi-volume-90966d19-e163-4e7a-9dde-69a63dee4838 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:25:48.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2104" for this suite.
Dec 11 02:25:54.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:25:55.140: INFO: namespace projected-2104 deletion completed in 6.928553552s

â€¢ [SLOW TEST:11.250 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:25:55.140: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1f9bbfa2-a69f-454e-974b-0102e8fec29a
STEP: Creating a pod to test consume secrets
Dec 11 02:25:55.282: INFO: Waiting up to 5m0s for pod "pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8" in namespace "secrets-3814" to be "success or failure"
Dec 11 02:25:55.304: INFO: Pod "pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 21.697667ms
Dec 11 02:25:57.326: INFO: Pod "pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04433688s
Dec 11 02:25:59.349: INFO: Pod "pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066726184s
STEP: Saw pod success
Dec 11 02:25:59.349: INFO: Pod "pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8" satisfied condition "success or failure"
Dec 11 02:25:59.371: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8 container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 02:25:59.449: INFO: Waiting for pod pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8 to disappear
Dec 11 02:25:59.470: INFO: Pod pod-secrets-a292d865-257e-4df1-8725-b7a4d8161cc8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:25:59.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3814" for this suite.
Dec 11 02:26:05.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:26:06.402: INFO: namespace secrets-3814 deletion completed in 6.908157157s

â€¢ [SLOW TEST:11.261 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:26:06.402: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:26:08.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8862" for this suite.
Dec 11 02:26:48.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:26:49.587: INFO: namespace kubelet-test-8862 deletion completed in 40.915749409s

â€¢ [SLOW TEST:43.185 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:26:49.587: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec 11 02:26:49.748: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-194493201 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:26:49.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3475" for this suite.
Dec 11 02:26:56.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:26:56.867: INFO: namespace kubectl-3475 deletion completed in 6.912673761s

â€¢ [SLOW TEST:7.280 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:26:56.867: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 11 02:26:57.139: INFO: Number of nodes with available pods: 0
Dec 11 02:26:57.139: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 02:26:58.184: INFO: Number of nodes with available pods: 0
Dec 11 02:26:58.184: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 02:26:59.183: INFO: Number of nodes with available pods: 0
Dec 11 02:26:59.183: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 02:27:00.183: INFO: Number of nodes with available pods: 2
Dec 11 02:27:00.183: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 11 02:27:00.298: INFO: Number of nodes with available pods: 1
Dec 11 02:27:00.298: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 02:27:01.343: INFO: Number of nodes with available pods: 1
Dec 11 02:27:01.343: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 02:27:02.344: INFO: Number of nodes with available pods: 1
Dec 11 02:27:02.344: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 02:27:03.343: INFO: Number of nodes with available pods: 2
Dec 11 02:27:03.343: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6692, will wait for the garbage collector to delete the pods
Dec 11 02:27:03.481: INFO: Deleting DaemonSet.extensions daemon-set took: 23.244305ms
Dec 11 02:27:03.881: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.258184ms
Dec 11 02:27:09.703: INFO: Number of nodes with available pods: 0
Dec 11 02:27:09.703: INFO: Number of running nodes: 0, number of available pods: 0
Dec 11 02:27:09.726: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6692/daemonsets","resourceVersion":"1013620"},"items":null}

Dec 11 02:27:09.748: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6692/pods","resourceVersion":"1013620"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:27:09.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6692" for this suite.
Dec 11 02:27:15.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:27:16.764: INFO: namespace daemonsets-6692 deletion completed in 6.924651351s

â€¢ [SLOW TEST:19.896 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:27:16.764: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec 11 02:27:16.879: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-883" to be "success or failure"
Dec 11 02:27:16.901: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 21.444067ms
Dec 11 02:27:18.923: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043958913s
Dec 11 02:27:20.946: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066496691s
STEP: Saw pod success
Dec 11 02:27:20.946: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 11 02:27:20.968: INFO: Trying to get logs from node 10.247.6.203 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 11 02:27:21.041: INFO: Waiting for pod pod-host-path-test to disappear
Dec 11 02:27:21.062: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:27:21.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-883" for this suite.
Dec 11 02:27:27.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:27:28.007: INFO: namespace hostpath-883 deletion completed in 6.923128383s

â€¢ [SLOW TEST:11.243 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:27:28.007: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5724
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 11 02:27:28.168: INFO: Found 1 stateful pods, waiting for 3
Dec 11 02:27:38.191: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:27:38.191: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:27:38.191: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 11 02:27:38.312: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 11 02:27:38.409: INFO: Updating stateful set ss2
Dec 11 02:27:38.452: INFO: Waiting for Pod statefulset-5724/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec 11 02:27:48.585: INFO: Found 2 stateful pods, waiting for 3
Dec 11 02:27:58.608: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:27:58.608: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:27:58.608: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 11 02:27:58.707: INFO: Updating stateful set ss2
Dec 11 02:27:58.753: INFO: Waiting for Pod statefulset-5724/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 11 02:28:08.853: INFO: Updating stateful set ss2
Dec 11 02:28:08.897: INFO: Waiting for StatefulSet statefulset-5724/ss2 to complete update
Dec 11 02:28:08.897: INFO: Waiting for Pod statefulset-5724/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 11 02:28:18.942: INFO: Deleting all statefulset in ns statefulset-5724
Dec 11 02:28:18.964: INFO: Scaling statefulset ss2 to 0
Dec 11 02:28:39.052: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 02:28:39.074: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:28:39.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5724" for this suite.
Dec 11 02:28:45.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:28:46.085: INFO: namespace statefulset-5724 deletion completed in 6.922513632s

â€¢ [SLOW TEST:78.078 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:28:46.086: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 11 02:28:46.200: INFO: Waiting up to 5m0s for pod "pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b" in namespace "emptydir-7879" to be "success or failure"
Dec 11 02:28:46.222: INFO: Pod "pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.595649ms
Dec 11 02:28:48.244: INFO: Pod "pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044030409s
Dec 11 02:28:50.266: INFO: Pod "pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066145924s
STEP: Saw pod success
Dec 11 02:28:50.266: INFO: Pod "pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b" satisfied condition "success or failure"
Dec 11 02:28:50.288: INFO: Trying to get logs from node 10.247.6.203 pod pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b container test-container: <nil>
STEP: delete the pod
Dec 11 02:28:50.366: INFO: Waiting for pod pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b to disappear
Dec 11 02:28:50.388: INFO: Pod pod-5cfe3d52-2d8c-40f4-a383-9d28d9c7551b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:28:50.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7879" for this suite.
Dec 11 02:28:56.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:28:57.344: INFO: namespace emptydir-7879 deletion completed in 6.934881788s

â€¢ [SLOW TEST:11.259 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:28:57.345: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 02:28:57.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6135'
Dec 11 02:28:58.670: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 11 02:28:58.670: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec 11 02:28:58.714: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-64tsn]
Dec 11 02:28:58.714: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-64tsn" in namespace "kubectl-6135" to be "running and ready"
Dec 11 02:28:58.736: INFO: Pod "e2e-test-nginx-rc-64tsn": Phase="Pending", Reason="", readiness=false. Elapsed: 21.722494ms
Dec 11 02:29:00.759: INFO: Pod "e2e-test-nginx-rc-64tsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044654123s
Dec 11 02:29:02.781: INFO: Pod "e2e-test-nginx-rc-64tsn": Phase="Running", Reason="", readiness=true. Elapsed: 4.066931882s
Dec 11 02:29:02.781: INFO: Pod "e2e-test-nginx-rc-64tsn" satisfied condition "running and ready"
Dec 11 02:29:02.781: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-64tsn]
Dec 11 02:29:02.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 logs rc/e2e-test-nginx-rc --namespace=kubectl-6135'
Dec 11 02:29:03.009: INFO: stderr: ""
Dec 11 02:29:03.009: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Dec 11 02:29:03.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete rc e2e-test-nginx-rc --namespace=kubectl-6135'
Dec 11 02:29:03.181: INFO: stderr: ""
Dec 11 02:29:03.181: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:29:03.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6135" for this suite.
Dec 11 02:29:25.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:29:26.130: INFO: namespace kubectl-6135 deletion completed in 22.926883194s

â€¢ [SLOW TEST:28.786 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:29:26.130: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ad49b92c-8167-413e-9242-23ac7903c783 in namespace container-probe-8217
Dec 11 02:29:30.292: INFO: Started pod busybox-ad49b92c-8167-413e-9242-23ac7903c783 in namespace container-probe-8217
STEP: checking the pod's current state and verifying that restartCount is present
Dec 11 02:29:30.314: INFO: Initial restart count of pod busybox-ad49b92c-8167-413e-9242-23ac7903c783 is 0
Dec 11 02:30:14.833: INFO: Restart count of pod container-probe-8217/busybox-ad49b92c-8167-413e-9242-23ac7903c783 is now 1 (44.519491878s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:30:14.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8217" for this suite.
Dec 11 02:30:20.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:30:21.816: INFO: namespace container-probe-8217 deletion completed in 6.932132471s

â€¢ [SLOW TEST:55.685 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:30:21.816: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7048
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 11 02:30:21.977: INFO: Found 1 stateful pods, waiting for 3
Dec 11 02:30:32.000: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:30:32.000: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:30:32.000: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:30:32.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7048 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 02:30:32.462: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 02:30:32.462: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 02:30:32.462: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 11 02:30:42.607: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 11 02:30:42.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7048 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 02:30:43.053: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 02:30:43.053: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 02:30:43.053: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 02:30:53.185: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Dec 11 02:30:53.185: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 11 02:30:53.185: INFO: Waiting for Pod statefulset-7048/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 11 02:30:53.185: INFO: Waiting for Pod statefulset-7048/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 11 02:31:03.230: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Dec 11 02:31:03.230: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 11 02:31:03.230: INFO: Waiting for Pod statefulset-7048/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 11 02:31:13.230: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Dec 11 02:31:13.230: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec 11 02:31:23.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7048 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 02:31:23.607: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 02:31:23.607: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 02:31:23.607: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 02:31:33.750: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 11 02:31:33.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7048 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 02:31:34.196: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 02:31:34.196: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 02:31:34.196: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 02:31:54.329: INFO: Waiting for StatefulSet statefulset-7048/ss2 to complete update
Dec 11 02:31:54.329: INFO: Waiting for Pod statefulset-7048/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 11 02:32:04.374: INFO: Deleting all statefulset in ns statefulset-7048
Dec 11 02:32:04.396: INFO: Scaling statefulset ss2 to 0
Dec 11 02:32:34.486: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 02:32:34.508: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:32:34.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7048" for this suite.
Dec 11 02:32:40.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:32:41.508: INFO: namespace statefulset-7048 deletion completed in 6.910759268s

â€¢ [SLOW TEST:139.692 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:32:41.508: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec 11 02:32:41.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-7806'
Dec 11 02:32:41.923: INFO: stderr: ""
Dec 11 02:32:41.923: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 11 02:32:41.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7806'
Dec 11 02:32:42.077: INFO: stderr: ""
Dec 11 02:32:42.077: INFO: stdout: "update-demo-nautilus-dxwfj update-demo-nautilus-j8b5l "
Dec 11 02:32:42.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-dxwfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:32:42.231: INFO: stderr: ""
Dec 11 02:32:42.231: INFO: stdout: ""
Dec 11 02:32:42.231: INFO: update-demo-nautilus-dxwfj is created but not running
Dec 11 02:32:47.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7806'
Dec 11 02:32:47.387: INFO: stderr: ""
Dec 11 02:32:47.387: INFO: stdout: "update-demo-nautilus-dxwfj update-demo-nautilus-j8b5l "
Dec 11 02:32:47.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-dxwfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:32:47.535: INFO: stderr: ""
Dec 11 02:32:47.535: INFO: stdout: "true"
Dec 11 02:32:47.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-dxwfj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:32:47.682: INFO: stderr: ""
Dec 11 02:32:47.682: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 02:32:47.682: INFO: validating pod update-demo-nautilus-dxwfj
Dec 11 02:32:47.768: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 02:32:47.768: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 02:32:47.768: INFO: update-demo-nautilus-dxwfj is verified up and running
Dec 11 02:32:47.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-j8b5l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:32:47.918: INFO: stderr: ""
Dec 11 02:32:47.918: INFO: stdout: "true"
Dec 11 02:32:47.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-j8b5l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:32:48.064: INFO: stderr: ""
Dec 11 02:32:48.064: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 02:32:48.064: INFO: validating pod update-demo-nautilus-j8b5l
Dec 11 02:32:48.089: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 02:32:48.089: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 02:32:48.089: INFO: update-demo-nautilus-j8b5l is verified up and running
STEP: rolling-update to new replication controller
Dec 11 02:32:48.090: INFO: scanned /root for discovery docs: <nil>
Dec 11 02:32:48.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7806'
Dec 11 02:33:03.334: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 11 02:33:03.334: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 11 02:33:03.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7806'
Dec 11 02:33:03.482: INFO: stderr: ""
Dec 11 02:33:03.482: INFO: stdout: "update-demo-kitten-7rjkh update-demo-kitten-9rzfs "
Dec 11 02:33:03.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-kitten-7rjkh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:33:03.627: INFO: stderr: ""
Dec 11 02:33:03.627: INFO: stdout: "true"
Dec 11 02:33:03.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-kitten-7rjkh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:33:03.775: INFO: stderr: ""
Dec 11 02:33:03.775: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 11 02:33:03.775: INFO: validating pod update-demo-kitten-7rjkh
Dec 11 02:33:03.860: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 11 02:33:03.860: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 11 02:33:03.860: INFO: update-demo-kitten-7rjkh is verified up and running
Dec 11 02:33:03.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-kitten-9rzfs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:33:04.008: INFO: stderr: ""
Dec 11 02:33:04.008: INFO: stdout: "true"
Dec 11 02:33:04.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-kitten-9rzfs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7806'
Dec 11 02:33:04.155: INFO: stderr: ""
Dec 11 02:33:04.155: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 11 02:33:04.155: INFO: validating pod update-demo-kitten-9rzfs
Dec 11 02:33:04.180: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 11 02:33:04.180: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 11 02:33:04.180: INFO: update-demo-kitten-9rzfs is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:33:04.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7806" for this suite.
Dec 11 02:33:26.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:33:27.130: INFO: namespace kubectl-7806 deletion completed in 22.92687921s

â€¢ [SLOW TEST:45.622 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:33:27.130: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:33:27.287: INFO: (0) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.266914ms)
Dec 11 02:33:27.332: INFO: (1) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.650392ms)
Dec 11 02:33:27.377: INFO: (2) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.26958ms)
Dec 11 02:33:27.421: INFO: (3) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.417788ms)
Dec 11 02:33:27.465: INFO: (4) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.01707ms)
Dec 11 02:33:27.511: INFO: (5) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.199884ms)
Dec 11 02:33:27.556: INFO: (6) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.904159ms)
Dec 11 02:33:27.601: INFO: (7) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.665768ms)
Dec 11 02:33:27.646: INFO: (8) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.542012ms)
Dec 11 02:33:27.691: INFO: (9) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.098953ms)
Dec 11 02:33:27.735: INFO: (10) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.486201ms)
Dec 11 02:33:27.779: INFO: (11) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 43.996394ms)
Dec 11 02:33:27.824: INFO: (12) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.059357ms)
Dec 11 02:33:27.868: INFO: (13) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.838543ms)
Dec 11 02:33:27.914: INFO: (14) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.608557ms)
Dec 11 02:33:27.959: INFO: (15) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.721156ms)
Dec 11 02:33:28.005: INFO: (16) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.992712ms)
Dec 11 02:33:28.051: INFO: (17) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 46.10681ms)
Dec 11 02:33:28.095: INFO: (18) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.198088ms)
Dec 11 02:33:28.140: INFO: (19) /api/v1/nodes/10.247.6.203:10250/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.543963ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:33:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2290" for this suite.
Dec 11 02:33:34.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:33:35.104: INFO: namespace proxy-2290 deletion completed in 6.940140453s

â€¢ [SLOW TEST:7.974 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:33:35.104: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2514
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 11 02:33:35.193: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 11 02:33:57.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.0.195:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2514 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:33:57.622: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:33:58.082: INFO: Found all expected endpoints: [netserver-0]
Dec 11 02:33:58.103: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.1.10:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2514 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 02:33:58.103: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 02:33:58.453: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:33:58.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2514" for this suite.
Dec 11 02:34:20.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:34:21.406: INFO: namespace pod-network-test-2514 deletion completed in 22.930549302s

â€¢ [SLOW TEST:46.302 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:34:21.406: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf in namespace container-probe-1058
Dec 11 02:34:25.567: INFO: Started pod liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf in namespace container-probe-1058
STEP: checking the pod's current state and verifying that restartCount is present
Dec 11 02:34:25.588: INFO: Initial restart count of pod liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf is 0
Dec 11 02:34:37.744: INFO: Restart count of pod container-probe-1058/liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf is now 1 (12.155575977s elapsed)
Dec 11 02:34:57.967: INFO: Restart count of pod container-probe-1058/liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf is now 2 (32.379069316s elapsed)
Dec 11 02:35:18.191: INFO: Restart count of pod container-probe-1058/liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf is now 3 (52.602677497s elapsed)
Dec 11 02:35:38.418: INFO: Restart count of pod container-probe-1058/liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf is now 4 (1m12.829554795s elapsed)
Dec 11 02:36:39.088: INFO: Restart count of pod container-probe-1058/liveness-5119a72a-d286-4d7e-b6da-c0c2f27697cf is now 5 (2m13.500068668s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:36:39.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1058" for this suite.
Dec 11 02:36:45.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:36:46.072: INFO: namespace container-probe-1058 deletion completed in 6.932224173s

â€¢ [SLOW TEST:144.667 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:36:46.073: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec 11 02:36:46.781: INFO: created pod pod-service-account-defaultsa
Dec 11 02:36:46.781: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 11 02:36:46.809: INFO: created pod pod-service-account-mountsa
Dec 11 02:36:46.809: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 11 02:36:46.836: INFO: created pod pod-service-account-nomountsa
Dec 11 02:36:46.836: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 11 02:36:46.863: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 11 02:36:46.863: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 11 02:36:46.889: INFO: created pod pod-service-account-mountsa-mountspec
Dec 11 02:36:46.889: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 11 02:36:46.915: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 11 02:36:46.915: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 11 02:36:46.942: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 11 02:36:46.942: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 11 02:36:46.973: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 11 02:36:46.973: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 11 02:36:46.999: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 11 02:36:46.999: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:36:46.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6574" for this suite.
Dec 11 02:37:09.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:37:09.958: INFO: namespace svcaccounts-6574 deletion completed in 22.935683086s

â€¢ [SLOW TEST:23.885 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:37:09.958: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:37:32.142: INFO: Container started at 2019-12-11 02:37:11 +0000 UTC, pod became ready at 2019-12-11 02:37:31 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:37:32.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7015" for this suite.
Dec 11 02:37:54.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:37:55.092: INFO: namespace container-probe-7015 deletion completed in 22.927823507s

â€¢ [SLOW TEST:45.134 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:37:55.093: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 11 02:38:01.390: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 11 02:38:01.412: INFO: Pod pod-with-prestop-http-hook still exists
Dec 11 02:38:03.412: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 11 02:38:03.434: INFO: Pod pod-with-prestop-http-hook still exists
Dec 11 02:38:05.412: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 11 02:38:05.435: INFO: Pod pod-with-prestop-http-hook still exists
Dec 11 02:38:07.412: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 11 02:38:07.434: INFO: Pod pod-with-prestop-http-hook still exists
Dec 11 02:38:09.412: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 11 02:38:09.434: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:38:09.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6842" for this suite.
Dec 11 02:38:31.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:38:32.433: INFO: namespace container-lifecycle-hook-6842 deletion completed in 22.929426731s

â€¢ [SLOW TEST:37.340 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:38:32.433: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-45332096-d4e8-47c0-8a45-3f233314dd84
STEP: Creating a pod to test consume configMaps
Dec 11 02:38:32.578: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5" in namespace "configmap-1866" to be "success or failure"
Dec 11 02:38:32.601: INFO: Pod "pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.708513ms
Dec 11 02:38:34.629: INFO: Pod "pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050358012s
STEP: Saw pod success
Dec 11 02:38:34.629: INFO: Pod "pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5" satisfied condition "success or failure"
Dec 11 02:38:34.650: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:38:34.725: INFO: Waiting for pod pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5 to disappear
Dec 11 02:38:34.746: INFO: Pod pod-configmaps-0a5285bf-430d-4110-9ada-1fea3d837db5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:38:34.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1866" for this suite.
Dec 11 02:38:40.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:38:41.707: INFO: namespace configmap-1866 deletion completed in 6.938271172s

â€¢ [SLOW TEST:9.273 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:38:41.707: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec 11 02:38:41.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 api-versions'
Dec 11 02:38:41.995: INFO: stderr: ""
Dec 11 02:38:41.995: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npaas/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:38:41.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3593" for this suite.
Dec 11 02:38:48.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:38:48.949: INFO: namespace kubectl-3593 deletion completed in 6.93173625s

â€¢ [SLOW TEST:7.242 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:38:48.949: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 02:38:49.065: INFO: Waiting up to 5m0s for pod "downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187" in namespace "downward-api-2866" to be "success or failure"
Dec 11 02:38:49.087: INFO: Pod "downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187": Phase="Pending", Reason="", readiness=false. Elapsed: 21.540033ms
Dec 11 02:38:51.110: INFO: Pod "downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044170157s
Dec 11 02:38:53.132: INFO: Pod "downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066525811s
STEP: Saw pod success
Dec 11 02:38:53.132: INFO: Pod "downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187" satisfied condition "success or failure"
Dec 11 02:38:53.154: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187 container client-container: <nil>
STEP: delete the pod
Dec 11 02:38:53.233: INFO: Waiting for pod downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187 to disappear
Dec 11 02:38:53.255: INFO: Pod downwardapi-volume-119b3763-a8d6-4cf0-af55-b9af61db9187 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:38:53.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2866" for this suite.
Dec 11 02:38:59.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:39:00.214: INFO: namespace downward-api-2866 deletion completed in 6.936561555s

â€¢ [SLOW TEST:11.265 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:39:00.214: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 02:39:00.332: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce" in namespace "downward-api-6562" to be "success or failure"
Dec 11 02:39:00.353: INFO: Pod "downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce": Phase="Pending", Reason="", readiness=false. Elapsed: 21.538679ms
Dec 11 02:39:02.375: INFO: Pod "downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043629378s
Dec 11 02:39:04.398: INFO: Pod "downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066459921s
STEP: Saw pod success
Dec 11 02:39:04.398: INFO: Pod "downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce" satisfied condition "success or failure"
Dec 11 02:39:04.420: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce container client-container: <nil>
STEP: delete the pod
Dec 11 02:39:04.497: INFO: Waiting for pod downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce to disappear
Dec 11 02:39:04.519: INFO: Pod downwardapi-volume-47adeca9-44e3-49aa-905e-aa71414c6fce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:39:04.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6562" for this suite.
Dec 11 02:39:10.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:39:11.479: INFO: namespace downward-api-6562 deletion completed in 6.938445832s

â€¢ [SLOW TEST:11.266 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:39:11.480: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-cd1e4cc4-cb3c-44c4-af48-0fef3cbb00f1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-cd1e4cc4-cb3c-44c4-af48-0fef3cbb00f1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:40:34.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9993" for this suite.
Dec 11 02:40:48.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:40:49.182: INFO: namespace configmap-9993 deletion completed in 14.929045905s

â€¢ [SLOW TEST:97.703 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:40:49.183: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 11 02:40:52.409: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:40:52.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2874" for this suite.
Dec 11 02:40:58.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:40:59.395: INFO: namespace container-runtime-2874 deletion completed in 6.916518434s

â€¢ [SLOW TEST:10.213 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:40:59.396: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-3ef2a36c-d284-4ec0-a2d8-e6c744e70210
STEP: Creating a pod to test consume configMaps
Dec 11 02:40:59.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba809000-b41a-4725-9479-af5676038306" in namespace "configmap-1440" to be "success or failure"
Dec 11 02:40:59.558: INFO: Pod "pod-configmaps-ba809000-b41a-4725-9479-af5676038306": Phase="Pending", Reason="", readiness=false. Elapsed: 21.509476ms
Dec 11 02:41:01.581: INFO: Pod "pod-configmaps-ba809000-b41a-4725-9479-af5676038306": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044333795s
Dec 11 02:41:03.603: INFO: Pod "pod-configmaps-ba809000-b41a-4725-9479-af5676038306": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066755443s
STEP: Saw pod success
Dec 11 02:41:03.603: INFO: Pod "pod-configmaps-ba809000-b41a-4725-9479-af5676038306" satisfied condition "success or failure"
Dec 11 02:41:03.625: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-ba809000-b41a-4725-9479-af5676038306 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:41:03.698: INFO: Waiting for pod pod-configmaps-ba809000-b41a-4725-9479-af5676038306 to disappear
Dec 11 02:41:03.719: INFO: Pod pod-configmaps-ba809000-b41a-4725-9479-af5676038306 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:41:03.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1440" for this suite.
Dec 11 02:41:09.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:41:10.669: INFO: namespace configmap-1440 deletion completed in 6.927647396s

â€¢ [SLOW TEST:11.273 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:41:10.669: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:41:10.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 version'
Dec 11 02:41:10.923: INFO: stderr: ""
Dec 11 02:41:10.923: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15+\", GitVersion:\"v1.15.0-FusionStage8.0-B040\", GitCommit:\"a21c0bae7d6423e9e41faec6bada938b93db575a\", GitTreeState:\"clean\", BuildDate:\"2019-12-11T01:39:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:41:10.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3507" for this suite.
Dec 11 02:41:17.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:41:17.875: INFO: namespace kubectl-3507 deletion completed in 6.927018847s

â€¢ [SLOW TEST:7.206 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:41:17.875: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 11 02:41:17.967: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 11 02:41:18.010: INFO: Waiting for terminating namespaces to be deleted...
Dec 11 02:41:18.032: INFO: 
Logging pods the kubelet thinks is on node 10.247.6.203 before test
Dec 11 02:41:18.117: INFO: icagent-4fhcw from kube-system started at 2019-12-06 03:07:10 +0000 UTC (0 container statuses recorded)
Dec 11 02:41:18.117: INFO: sonobuoy-e2e-job-7ef775b06a5742ab from sonobuoy started at 2019-12-11 02:10:34 +0000 UTC (2 container statuses recorded)
Dec 11 02:41:18.117: INFO: 	Container e2e ready: true, restart count 0
Dec 11 02:41:18.117: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 02:41:18.117: INFO: kubenode-vl77r from kube-system started at 2019-12-06 03:07:00 +0000 UTC (0 container statuses recorded)
Dec 11 02:41:18.117: INFO: sonobuoy from sonobuoy started at 2019-12-11 02:10:31 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.117: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 11 02:41:18.117: INFO: sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-lbzbk from sonobuoy started at 2019-12-11 02:10:34 +0000 UTC (2 container statuses recorded)
Dec 11 02:41:18.117: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 02:41:18.117: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 11 02:41:18.117: INFO: 
Logging pods the kubelet thinks is on node kcsp41-master0 before test
Dec 11 02:41:18.204: INFO: etcd-event-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.204: INFO: 	Container etcd-event-container ready: true, restart count 0
Dec 11 02:41:18.205: INFO: cfe-kube-webhook-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container webhook ready: true, restart count 0
Dec 11 02:41:18.205: INFO: kube-event-controller-6bc74bfc9c-nfh7p from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container kube-event-controller ready: true, restart count 4
Dec 11 02:41:18.205: INFO: kubenode-c625w from kube-system started at 2019-12-06 03:02:14 +0000 UTC (0 container statuses recorded)
Dec 11 02:41:18.205: INFO: etcd-network-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container etcd-network-container ready: true, restart count 0
Dec 11 02:41:18.205: INFO: kube-canal-apiserver-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container kube-canal-apiserver ready: true, restart count 0
Dec 11 02:41:18.205: INFO: kube-scheduler-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container kube-scheduler ready: true, restart count 4
Dec 11 02:41:18.205: INFO: provision-controller-manager-kcsp41-master0 from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container provision-controller-manager ready: true, restart count 21
Dec 11 02:41:18.205: INFO: etcd-backup-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container etcd-backup ready: true, restart count 0
Dec 11 02:41:18.205: INFO: icagent-9rdrh from kube-system started at 2019-12-06 03:02:21 +0000 UTC (0 container statuses recorded)
Dec 11 02:41:18.205: INFO: etcd-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container etcd-container ready: true, restart count 0
Dec 11 02:41:18.205: INFO: kube-apiserver-kcsp41-master0 from kube-system started at 2019-12-11 01:44:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 11 02:41:18.205: INFO: cam-tiller-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container cam-tiller ready: true, restart count 0
Dec 11 02:41:18.205: INFO: sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-clmpn from sonobuoy started at 2019-12-11 02:10:14 +0000 UTC (2 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 02:41:18.205: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 11 02:41:18.205: INFO: kube-canal-controller-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container kube-canal-controller ready: true, restart count 0
Dec 11 02:41:18.205: INFO: kube-controller-manager-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container kube-controller-manager ready: true, restart count 4
Dec 11 02:41:18.205: INFO: coredns-556846b6b7-dnbzk from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 02:41:18.205: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15df30ceacaab209], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:41:19.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9985" for this suite.
Dec 11 02:41:25.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:41:26.295: INFO: namespace sched-pred-9985 deletion completed in 6.910304796s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:8.420 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:41:26.295: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 11 02:41:26.409: INFO: Waiting up to 5m0s for pod "downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a" in namespace "downward-api-315" to be "success or failure"
Dec 11 02:41:26.431: INFO: Pod "downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.36281ms
Dec 11 02:41:28.453: INFO: Pod "downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044136462s
Dec 11 02:41:30.476: INFO: Pod "downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066380742s
STEP: Saw pod success
Dec 11 02:41:30.476: INFO: Pod "downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a" satisfied condition "success or failure"
Dec 11 02:41:30.497: INFO: Trying to get logs from node 10.247.6.203 pod downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a container dapi-container: <nil>
STEP: delete the pod
Dec 11 02:41:30.591: INFO: Waiting for pod downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a to disappear
Dec 11 02:41:30.612: INFO: Pod downward-api-34ce1078-1c45-401c-8c94-db11bcf32c2a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:41:30.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-315" for this suite.
Dec 11 02:41:36.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:41:37.558: INFO: namespace downward-api-315 deletion completed in 6.923567002s

â€¢ [SLOW TEST:11.262 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:41:37.558: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:41:39.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7665" for this suite.
Dec 11 02:42:19.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:42:20.737: INFO: namespace kubelet-test-7665 deletion completed in 40.929507965s

â€¢ [SLOW TEST:43.180 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:42:20.738: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:42:26.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-989" for this suite.
Dec 11 02:42:32.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:42:33.251: INFO: namespace watch-989 deletion completed in 6.984565281s

â€¢ [SLOW TEST:12.513 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:42:33.251: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 02:42:33.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37" in namespace "downward-api-2180" to be "success or failure"
Dec 11 02:42:33.389: INFO: Pod "downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37": Phase="Pending", Reason="", readiness=false. Elapsed: 21.468603ms
Dec 11 02:42:35.411: INFO: Pod "downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043879953s
Dec 11 02:42:37.434: INFO: Pod "downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066614806s
STEP: Saw pod success
Dec 11 02:42:37.434: INFO: Pod "downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37" satisfied condition "success or failure"
Dec 11 02:42:37.457: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37 container client-container: <nil>
STEP: delete the pod
Dec 11 02:42:37.539: INFO: Waiting for pod downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37 to disappear
Dec 11 02:42:37.561: INFO: Pod downwardapi-volume-47b24490-a77f-4645-8d69-d72625cb1b37 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:42:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2180" for this suite.
Dec 11 02:42:43.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:42:44.514: INFO: namespace downward-api-2180 deletion completed in 6.930881242s

â€¢ [SLOW TEST:11.263 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:42:44.514: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 11 02:42:44.630: INFO: Waiting up to 5m0s for pod "pod-8884be9e-0d2d-4ada-9a12-45714af8e717" in namespace "emptydir-1789" to be "success or failure"
Dec 11 02:42:44.651: INFO: Pod "pod-8884be9e-0d2d-4ada-9a12-45714af8e717": Phase="Pending", Reason="", readiness=false. Elapsed: 21.486045ms
Dec 11 02:42:46.674: INFO: Pod "pod-8884be9e-0d2d-4ada-9a12-45714af8e717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043965917s
Dec 11 02:42:48.696: INFO: Pod "pod-8884be9e-0d2d-4ada-9a12-45714af8e717": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066342285s
STEP: Saw pod success
Dec 11 02:42:48.696: INFO: Pod "pod-8884be9e-0d2d-4ada-9a12-45714af8e717" satisfied condition "success or failure"
Dec 11 02:42:48.718: INFO: Trying to get logs from node 10.247.6.203 pod pod-8884be9e-0d2d-4ada-9a12-45714af8e717 container test-container: <nil>
STEP: delete the pod
Dec 11 02:42:48.793: INFO: Waiting for pod pod-8884be9e-0d2d-4ada-9a12-45714af8e717 to disappear
Dec 11 02:42:48.814: INFO: Pod pod-8884be9e-0d2d-4ada-9a12-45714af8e717 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:42:48.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1789" for this suite.
Dec 11 02:42:54.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:42:55.754: INFO: namespace emptydir-1789 deletion completed in 6.917345439s

â€¢ [SLOW TEST:11.239 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:42:55.754: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:42:55.890: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 11 02:42:59.936: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 11 02:43:04.124: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-6993,SelfLink:/apis/apps/v1/namespaces/deployment-6993/deployments/test-cleanup-deployment,UID:839db647-2888-4eb3-a021-85056dee12f0,ResourceVersion:1016848,Generation:1,CreationTimestamp:2019-12-11 02:42:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:1,MaxSurge:1,},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-11 02:42:40 +0000 UTC 2019-12-11 02:42:40 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-11 02:42:42 +0000 UTC 2019-12-11 02:42:40 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-564d7d68bf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 11 02:43:04.146: INFO: New ReplicaSet "test-cleanup-deployment-564d7d68bf" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-564d7d68bf,GenerateName:,Namespace:deployment-6993,SelfLink:/apis/apps/v1/namespaces/deployment-6993/replicasets/test-cleanup-deployment-564d7d68bf,UID:7d68bee3-7569-4125-9cfb-870a2c1039f7,ResourceVersion:1016847,Generation:1,CreationTimestamp:2019-12-11 02:42:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 564d7d68bf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/internalStrategyType: RollingUpdate,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 839db647-2888-4eb3-a021-85056dee12f0 0xc003e30867 0xc003e30868}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 564d7d68bf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 564d7d68bf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 11 02:43:04.169: INFO: Pod "test-cleanup-deployment-564d7d68bf-gbv5h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-564d7d68bf-gbv5h,GenerateName:test-cleanup-deployment-564d7d68bf-,Namespace:deployment-6993,SelfLink:/api/v1/namespaces/deployment-6993/pods/test-cleanup-deployment-564d7d68bf-gbv5h,UID:f8ce9d4f-a6ab-483d-969b-6df26ff22cf1,ResourceVersion:1016846,Generation:0,CreationTimestamp:2019-12-11 02:42:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 564d7d68bf,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-564d7d68bf 7d68bee3-7569-4125-9cfb-870a2c1039f7 0xc003e30e67 0xc003e30e68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r5n85 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r5n85,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-r5n85 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003e30ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003e30f10}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:43:00 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:43:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:43:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:42:40 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.34,StartTime:2019-12-11 02:43:00 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-11 02:43:01 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://84ac2c8cd10f445a0d33b61aae126679f6c3cf019694184b92f63c8563bcf83b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:43:04.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6993" for this suite.
Dec 11 02:43:10.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:43:11.126: INFO: namespace deployment-6993 deletion completed in 6.934865003s

â€¢ [SLOW TEST:15.372 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:43:11.126: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-53688f1b-010a-453c-b0e6-a0430c528f1d
STEP: Creating secret with name secret-projected-all-test-volume-66dd477b-a8d7-4e08-aa79-df5089f0eb90
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 11 02:43:11.304: INFO: Waiting up to 5m0s for pod "projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237" in namespace "projected-8309" to be "success or failure"
Dec 11 02:43:11.326: INFO: Pod "projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237": Phase="Pending", Reason="", readiness=false. Elapsed: 21.996435ms
Dec 11 02:43:13.348: INFO: Pod "projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044292954s
Dec 11 02:43:15.371: INFO: Pod "projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0672937s
STEP: Saw pod success
Dec 11 02:43:15.371: INFO: Pod "projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237" satisfied condition "success or failure"
Dec 11 02:43:15.393: INFO: Trying to get logs from node 10.247.6.203 pod projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 11 02:43:15.468: INFO: Waiting for pod projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237 to disappear
Dec 11 02:43:15.490: INFO: Pod projected-volume-123bb275-da38-4891-ac3b-9f233c4dd237 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:43:15.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8309" for this suite.
Dec 11 02:43:21.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:43:22.417: INFO: namespace projected-8309 deletion completed in 6.905985906s

â€¢ [SLOW TEST:11.291 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:43:22.418: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3801/secret-test-7e6b2016-50f6-4d88-b5d3-dff2fe460662
STEP: Creating a pod to test consume secrets
Dec 11 02:43:22.557: INFO: Waiting up to 5m0s for pod "pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf" in namespace "secrets-3801" to be "success or failure"
Dec 11 02:43:22.578: INFO: Pod "pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 21.421686ms
Dec 11 02:43:24.601: INFO: Pod "pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044147031s
Dec 11 02:43:26.624: INFO: Pod "pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066945375s
STEP: Saw pod success
Dec 11 02:43:26.624: INFO: Pod "pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf" satisfied condition "success or failure"
Dec 11 02:43:26.645: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf container env-test: <nil>
STEP: delete the pod
Dec 11 02:43:26.738: INFO: Waiting for pod pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf to disappear
Dec 11 02:43:26.760: INFO: Pod pod-configmaps-141fd197-04a9-4429-b243-1f3b7d1ff9cf no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:43:26.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3801" for this suite.
Dec 11 02:43:32.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:43:33.718: INFO: namespace secrets-3801 deletion completed in 6.936760653s

â€¢ [SLOW TEST:11.301 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:43:33.719: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:43:35.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3215" for this suite.
Dec 11 02:43:58.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:43:58.893: INFO: namespace replication-controller-3215 deletion completed in 22.925770523s

â€¢ [SLOW TEST:25.175 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:43:58.894: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-02333932-2108-48ef-9d48-0d77ca713e9a in namespace container-probe-283
Dec 11 02:44:01.053: INFO: Started pod test-webserver-02333932-2108-48ef-9d48-0d77ca713e9a in namespace container-probe-283
STEP: checking the pod's current state and verifying that restartCount is present
Dec 11 02:44:01.075: INFO: Initial restart count of pod test-webserver-02333932-2108-48ef-9d48-0d77ca713e9a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:48:01.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-283" for this suite.
Dec 11 02:48:07.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:48:08.744: INFO: namespace container-probe-283 deletion completed in 6.925999588s

â€¢ [SLOW TEST:249.851 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:48:08.744: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-bc1dcfe0-2684-4964-bea5-599e37730e16
STEP: Creating a pod to test consume configMaps
Dec 11 02:48:08.885: INFO: Waiting up to 5m0s for pod "pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788" in namespace "configmap-3090" to be "success or failure"
Dec 11 02:48:08.907: INFO: Pod "pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788": Phase="Pending", Reason="", readiness=false. Elapsed: 21.632498ms
Dec 11 02:48:10.929: INFO: Pod "pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043527019s
Dec 11 02:48:12.952: INFO: Pod "pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066245392s
STEP: Saw pod success
Dec 11 02:48:12.952: INFO: Pod "pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788" satisfied condition "success or failure"
Dec 11 02:48:12.973: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:48:13.048: INFO: Waiting for pod pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788 to disappear
Dec 11 02:48:13.069: INFO: Pod pod-configmaps-23a74770-6cfe-41fd-adee-81ea34380788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:48:13.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3090" for this suite.
Dec 11 02:48:19.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:48:20.000: INFO: namespace configmap-3090 deletion completed in 6.909065917s

â€¢ [SLOW TEST:11.256 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:48:20.001: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6099
I1211 02:48:20.112636      18 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6099, replica count: 1
I1211 02:48:21.162992      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1211 02:48:22.163171      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 11 02:48:22.292: INFO: Created: latency-svc-nz5q2
Dec 11 02:48:22.295: INFO: Got endpoints: latency-svc-nz5q2 [31.789719ms]
Dec 11 02:48:22.323: INFO: Created: latency-svc-8vjs2
Dec 11 02:48:22.329: INFO: Got endpoints: latency-svc-8vjs2 [34.506295ms]
Dec 11 02:48:22.335: INFO: Created: latency-svc-4nl9c
Dec 11 02:48:22.335: INFO: Created: latency-svc-47kz9
Dec 11 02:48:22.340: INFO: Got endpoints: latency-svc-4nl9c [44.571424ms]
Dec 11 02:48:22.341: INFO: Got endpoints: latency-svc-47kz9 [45.33527ms]
Dec 11 02:48:22.349: INFO: Created: latency-svc-jv4sd
Dec 11 02:48:22.353: INFO: Got endpoints: latency-svc-jv4sd [58.047682ms]
Dec 11 02:48:22.356: INFO: Created: latency-svc-ms6vp
Dec 11 02:48:22.358: INFO: Got endpoints: latency-svc-ms6vp [62.218671ms]
Dec 11 02:48:22.360: INFO: Created: latency-svc-db5sj
Dec 11 02:48:22.360: INFO: Created: latency-svc-mpwct
Dec 11 02:48:22.366: INFO: Got endpoints: latency-svc-db5sj [70.376574ms]
Dec 11 02:48:22.366: INFO: Got endpoints: latency-svc-mpwct [70.195519ms]
Dec 11 02:48:22.370: INFO: Created: latency-svc-6kxmq
Dec 11 02:48:22.374: INFO: Got endpoints: latency-svc-6kxmq [78.477665ms]
Dec 11 02:48:22.376: INFO: Created: latency-svc-5bfz6
Dec 11 02:48:22.381: INFO: Got endpoints: latency-svc-5bfz6 [85.491161ms]
Dec 11 02:48:22.382: INFO: Created: latency-svc-rjtrp
Dec 11 02:48:22.389: INFO: Got endpoints: latency-svc-rjtrp [93.337226ms]
Dec 11 02:48:22.390: INFO: Created: latency-svc-8glj9
Dec 11 02:48:22.391: INFO: Got endpoints: latency-svc-8glj9 [94.979135ms]
Dec 11 02:48:22.402: INFO: Created: latency-svc-k9gjv
Dec 11 02:48:22.410: INFO: Got endpoints: latency-svc-k9gjv [113.908892ms]
Dec 11 02:48:22.410: INFO: Created: latency-svc-k5nhc
Dec 11 02:48:22.416: INFO: Got endpoints: latency-svc-k5nhc [120.161412ms]
Dec 11 02:48:22.420: INFO: Created: latency-svc-gdr2f
Dec 11 02:48:22.425: INFO: Got endpoints: latency-svc-gdr2f [129.092604ms]
Dec 11 02:48:22.431: INFO: Created: latency-svc-4szd2
Dec 11 02:48:22.431: INFO: Created: latency-svc-9hkkr
Dec 11 02:48:22.440: INFO: Got endpoints: latency-svc-4szd2 [144.284724ms]
Dec 11 02:48:22.440: INFO: Got endpoints: latency-svc-9hkkr [49.20091ms]
Dec 11 02:48:22.441: INFO: Created: latency-svc-pqwrh
Dec 11 02:48:22.447: INFO: Got endpoints: latency-svc-pqwrh [117.495275ms]
Dec 11 02:48:22.448: INFO: Created: latency-svc-gpv4v
Dec 11 02:48:22.452: INFO: Got endpoints: latency-svc-gpv4v [112.35836ms]
Dec 11 02:48:22.457: INFO: Created: latency-svc-fvlh6
Dec 11 02:48:22.464: INFO: Got endpoints: latency-svc-fvlh6 [123.727031ms]
Dec 11 02:48:22.471: INFO: Created: latency-svc-xpsdr
Dec 11 02:48:22.471: INFO: Created: latency-svc-2568s
Dec 11 02:48:22.477: INFO: Got endpoints: latency-svc-2568s [123.419004ms]
Dec 11 02:48:22.477: INFO: Got endpoints: latency-svc-xpsdr [118.741234ms]
Dec 11 02:48:22.478: INFO: Created: latency-svc-z6dnx
Dec 11 02:48:22.484: INFO: Created: latency-svc-vbg26
Dec 11 02:48:22.484: INFO: Got endpoints: latency-svc-z6dnx [118.611477ms]
Dec 11 02:48:22.489: INFO: Got endpoints: latency-svc-vbg26 [123.387997ms]
Dec 11 02:48:22.503: INFO: Created: latency-svc-xrwfr
Dec 11 02:48:22.503: INFO: Created: latency-svc-tf6bs
Dec 11 02:48:22.511: INFO: Got endpoints: latency-svc-xrwfr [129.691974ms]
Dec 11 02:48:22.511: INFO: Got endpoints: latency-svc-tf6bs [137.051199ms]
Dec 11 02:48:22.513: INFO: Created: latency-svc-vlb68
Dec 11 02:48:22.513: INFO: Created: latency-svc-kmsq8
Dec 11 02:48:22.525: INFO: Got endpoints: latency-svc-kmsq8 [115.560601ms]
Dec 11 02:48:22.525: INFO: Got endpoints: latency-svc-vlb68 [136.319583ms]
Dec 11 02:48:22.527: INFO: Created: latency-svc-r8d7j
Dec 11 02:48:22.535: INFO: Got endpoints: latency-svc-r8d7j [118.518188ms]
Dec 11 02:48:22.535: INFO: Created: latency-svc-tnxwb
Dec 11 02:48:22.537: INFO: Created: latency-svc-dsm9z
Dec 11 02:48:22.541: INFO: Got endpoints: latency-svc-tnxwb [116.225506ms]
Dec 11 02:48:22.542: INFO: Created: latency-svc-swrlv
Dec 11 02:48:22.542: INFO: Got endpoints: latency-svc-dsm9z [102.503655ms]
Dec 11 02:48:22.549: INFO: Created: latency-svc-qwpc5
Dec 11 02:48:22.549: INFO: Got endpoints: latency-svc-swrlv [109.235096ms]
Dec 11 02:48:22.565: INFO: Got endpoints: latency-svc-qwpc5 [117.67531ms]
Dec 11 02:48:22.567: INFO: Created: latency-svc-z7t6d
Dec 11 02:48:22.576: INFO: Got endpoints: latency-svc-z7t6d [123.483449ms]
Dec 11 02:48:22.576: INFO: Created: latency-svc-l2zsf
Dec 11 02:48:22.580: INFO: Created: latency-svc-kx85d
Dec 11 02:48:22.581: INFO: Got endpoints: latency-svc-l2zsf [116.874459ms]
Dec 11 02:48:22.586: INFO: Created: latency-svc-4zmw5
Dec 11 02:48:22.595: INFO: Created: latency-svc-jk76q
Dec 11 02:48:22.598: INFO: Got endpoints: latency-svc-kx85d [120.585182ms]
Dec 11 02:48:22.598: INFO: Created: latency-svc-lh72k
Dec 11 02:48:22.602: INFO: Created: latency-svc-j6mss
Dec 11 02:48:22.616: INFO: Created: latency-svc-pzxpp
Dec 11 02:48:22.626: INFO: Created: latency-svc-g4ffw
Dec 11 02:48:22.629: INFO: Created: latency-svc-t7c8p
Dec 11 02:48:22.633: INFO: Created: latency-svc-dfbzx
Dec 11 02:48:22.645: INFO: Created: latency-svc-q58tp
Dec 11 02:48:22.645: INFO: Got endpoints: latency-svc-4zmw5 [168.333769ms]
Dec 11 02:48:22.645: INFO: Created: latency-svc-j9sts
Dec 11 02:48:22.652: INFO: Created: latency-svc-78g7z
Dec 11 02:48:22.665: INFO: Created: latency-svc-kpv9j
Dec 11 02:48:22.677: INFO: Created: latency-svc-zdqvp
Dec 11 02:48:22.677: INFO: Created: latency-svc-nwmr7
Dec 11 02:48:22.677: INFO: Created: latency-svc-nbjl2
Dec 11 02:48:22.680: INFO: Created: latency-svc-pnr59
Dec 11 02:48:22.695: INFO: Got endpoints: latency-svc-jk76q [210.47467ms]
Dec 11 02:48:22.723: INFO: Created: latency-svc-8fzxk
Dec 11 02:48:22.745: INFO: Got endpoints: latency-svc-lh72k [255.174867ms]
Dec 11 02:48:22.773: INFO: Created: latency-svc-m28r8
Dec 11 02:48:22.795: INFO: Got endpoints: latency-svc-j6mss [284.248198ms]
Dec 11 02:48:22.823: INFO: Created: latency-svc-6w6kf
Dec 11 02:48:22.845: INFO: Got endpoints: latency-svc-pzxpp [334.064095ms]
Dec 11 02:48:22.876: INFO: Created: latency-svc-jcqr8
Dec 11 02:48:22.895: INFO: Got endpoints: latency-svc-g4ffw [369.416591ms]
Dec 11 02:48:22.923: INFO: Created: latency-svc-qhkdg
Dec 11 02:48:22.945: INFO: Got endpoints: latency-svc-t7c8p [419.666807ms]
Dec 11 02:48:22.981: INFO: Created: latency-svc-qngb7
Dec 11 02:48:22.995: INFO: Got endpoints: latency-svc-dfbzx [459.968961ms]
Dec 11 02:48:23.021: INFO: Created: latency-svc-kjsng
Dec 11 02:48:23.046: INFO: Got endpoints: latency-svc-j9sts [504.660638ms]
Dec 11 02:48:23.076: INFO: Created: latency-svc-4zqvj
Dec 11 02:48:23.097: INFO: Got endpoints: latency-svc-q58tp [555.038317ms]
Dec 11 02:48:23.125: INFO: Created: latency-svc-f6jkz
Dec 11 02:48:23.145: INFO: Got endpoints: latency-svc-78g7z [595.169731ms]
Dec 11 02:48:23.176: INFO: Created: latency-svc-j5rvc
Dec 11 02:48:23.194: INFO: Got endpoints: latency-svc-kpv9j [629.789318ms]
Dec 11 02:48:23.223: INFO: Created: latency-svc-5m42d
Dec 11 02:48:23.245: INFO: Got endpoints: latency-svc-nbjl2 [669.31729ms]
Dec 11 02:48:23.271: INFO: Created: latency-svc-qffvz
Dec 11 02:48:23.296: INFO: Got endpoints: latency-svc-zdqvp [714.287619ms]
Dec 11 02:48:23.324: INFO: Created: latency-svc-kzh2w
Dec 11 02:48:23.346: INFO: Got endpoints: latency-svc-nwmr7 [748.155666ms]
Dec 11 02:48:23.376: INFO: Created: latency-svc-tj4bp
Dec 11 02:48:23.395: INFO: Got endpoints: latency-svc-pnr59 [749.97638ms]
Dec 11 02:48:23.425: INFO: Created: latency-svc-z6gq9
Dec 11 02:48:23.445: INFO: Got endpoints: latency-svc-8fzxk [749.656896ms]
Dec 11 02:48:23.473: INFO: Created: latency-svc-vnrlq
Dec 11 02:48:23.495: INFO: Got endpoints: latency-svc-m28r8 [750.228504ms]
Dec 11 02:48:23.523: INFO: Created: latency-svc-khldl
Dec 11 02:48:23.545: INFO: Got endpoints: latency-svc-6w6kf [749.621138ms]
Dec 11 02:48:23.573: INFO: Created: latency-svc-75b98
Dec 11 02:48:23.595: INFO: Got endpoints: latency-svc-jcqr8 [749.372362ms]
Dec 11 02:48:23.624: INFO: Created: latency-svc-v9qzr
Dec 11 02:48:23.645: INFO: Got endpoints: latency-svc-qhkdg [749.833556ms]
Dec 11 02:48:23.679: INFO: Created: latency-svc-kfk2r
Dec 11 02:48:23.695: INFO: Got endpoints: latency-svc-qngb7 [749.674818ms]
Dec 11 02:48:23.725: INFO: Created: latency-svc-frskn
Dec 11 02:48:23.745: INFO: Got endpoints: latency-svc-kjsng [750.271366ms]
Dec 11 02:48:23.773: INFO: Created: latency-svc-95827
Dec 11 02:48:23.795: INFO: Got endpoints: latency-svc-4zqvj [749.219801ms]
Dec 11 02:48:23.824: INFO: Created: latency-svc-dpnbg
Dec 11 02:48:23.846: INFO: Got endpoints: latency-svc-f6jkz [748.637429ms]
Dec 11 02:48:23.877: INFO: Created: latency-svc-t6xhz
Dec 11 02:48:23.896: INFO: Got endpoints: latency-svc-j5rvc [750.924506ms]
Dec 11 02:48:23.924: INFO: Created: latency-svc-z56bp
Dec 11 02:48:23.944: INFO: Got endpoints: latency-svc-5m42d [749.833446ms]
Dec 11 02:48:23.995: INFO: Created: latency-svc-6gkzr
Dec 11 02:48:23.996: INFO: Got endpoints: latency-svc-qffvz [751.294794ms]
Dec 11 02:48:24.025: INFO: Created: latency-svc-86rl4
Dec 11 02:48:24.044: INFO: Got endpoints: latency-svc-kzh2w [748.565282ms]
Dec 11 02:48:24.072: INFO: Created: latency-svc-4s8h4
Dec 11 02:48:24.095: INFO: Got endpoints: latency-svc-tj4bp [748.898072ms]
Dec 11 02:48:24.122: INFO: Created: latency-svc-mw74t
Dec 11 02:48:24.145: INFO: Got endpoints: latency-svc-z6gq9 [749.289619ms]
Dec 11 02:48:24.179: INFO: Created: latency-svc-mf5sd
Dec 11 02:48:24.194: INFO: Got endpoints: latency-svc-vnrlq [749.790671ms]
Dec 11 02:48:24.226: INFO: Created: latency-svc-snqhf
Dec 11 02:48:24.248: INFO: Got endpoints: latency-svc-khldl [753.610407ms]
Dec 11 02:48:24.276: INFO: Created: latency-svc-rjz2d
Dec 11 02:48:24.296: INFO: Got endpoints: latency-svc-75b98 [750.972352ms]
Dec 11 02:48:24.324: INFO: Created: latency-svc-27wqh
Dec 11 02:48:24.345: INFO: Got endpoints: latency-svc-v9qzr [750.812002ms]
Dec 11 02:48:24.373: INFO: Created: latency-svc-wzg4t
Dec 11 02:48:24.395: INFO: Got endpoints: latency-svc-kfk2r [750.322815ms]
Dec 11 02:48:24.423: INFO: Created: latency-svc-745p4
Dec 11 02:48:24.445: INFO: Got endpoints: latency-svc-frskn [749.80643ms]
Dec 11 02:48:24.473: INFO: Created: latency-svc-pbjkg
Dec 11 02:48:24.495: INFO: Got endpoints: latency-svc-95827 [749.994739ms]
Dec 11 02:48:24.523: INFO: Created: latency-svc-v76sj
Dec 11 02:48:24.544: INFO: Got endpoints: latency-svc-dpnbg [749.259796ms]
Dec 11 02:48:24.572: INFO: Created: latency-svc-jts8f
Dec 11 02:48:24.595: INFO: Got endpoints: latency-svc-t6xhz [748.644392ms]
Dec 11 02:48:24.621: INFO: Created: latency-svc-867r9
Dec 11 02:48:24.644: INFO: Got endpoints: latency-svc-z56bp [748.943941ms]
Dec 11 02:48:24.673: INFO: Created: latency-svc-pn9jn
Dec 11 02:48:24.694: INFO: Got endpoints: latency-svc-6gkzr [749.952559ms]
Dec 11 02:48:24.720: INFO: Created: latency-svc-h8rhk
Dec 11 02:48:24.745: INFO: Got endpoints: latency-svc-86rl4 [748.436607ms]
Dec 11 02:48:24.773: INFO: Created: latency-svc-9v2vl
Dec 11 02:48:24.795: INFO: Got endpoints: latency-svc-4s8h4 [750.620831ms]
Dec 11 02:48:24.835: INFO: Created: latency-svc-skmqk
Dec 11 02:48:24.846: INFO: Got endpoints: latency-svc-mw74t [751.367312ms]
Dec 11 02:48:24.874: INFO: Created: latency-svc-57sr6
Dec 11 02:48:24.895: INFO: Got endpoints: latency-svc-mf5sd [749.93472ms]
Dec 11 02:48:24.923: INFO: Created: latency-svc-fp75j
Dec 11 02:48:24.946: INFO: Got endpoints: latency-svc-snqhf [751.609293ms]
Dec 11 02:48:24.974: INFO: Created: latency-svc-gmrxx
Dec 11 02:48:24.996: INFO: Got endpoints: latency-svc-rjz2d [747.494163ms]
Dec 11 02:48:25.026: INFO: Created: latency-svc-wbmcj
Dec 11 02:48:25.045: INFO: Got endpoints: latency-svc-27wqh [748.849644ms]
Dec 11 02:48:25.083: INFO: Created: latency-svc-grsgw
Dec 11 02:48:25.095: INFO: Got endpoints: latency-svc-wzg4t [749.372466ms]
Dec 11 02:48:25.124: INFO: Created: latency-svc-bzkr5
Dec 11 02:48:25.145: INFO: Got endpoints: latency-svc-745p4 [749.878017ms]
Dec 11 02:48:25.171: INFO: Created: latency-svc-mglqf
Dec 11 02:48:25.195: INFO: Got endpoints: latency-svc-pbjkg [749.763893ms]
Dec 11 02:48:25.221: INFO: Created: latency-svc-b6s7m
Dec 11 02:48:25.245: INFO: Got endpoints: latency-svc-v76sj [749.571331ms]
Dec 11 02:48:25.273: INFO: Created: latency-svc-2mbd2
Dec 11 02:48:25.294: INFO: Got endpoints: latency-svc-jts8f [749.939385ms]
Dec 11 02:48:25.321: INFO: Created: latency-svc-qtqc2
Dec 11 02:48:25.345: INFO: Got endpoints: latency-svc-867r9 [749.834207ms]
Dec 11 02:48:25.372: INFO: Created: latency-svc-9g75t
Dec 11 02:48:25.395: INFO: Got endpoints: latency-svc-pn9jn [750.035736ms]
Dec 11 02:48:25.423: INFO: Created: latency-svc-4rqfx
Dec 11 02:48:25.445: INFO: Got endpoints: latency-svc-h8rhk [750.962158ms]
Dec 11 02:48:25.473: INFO: Created: latency-svc-jz4t6
Dec 11 02:48:25.497: INFO: Got endpoints: latency-svc-9v2vl [752.726772ms]
Dec 11 02:48:25.524: INFO: Created: latency-svc-p2p4c
Dec 11 02:48:25.544: INFO: Got endpoints: latency-svc-skmqk [749.579321ms]
Dec 11 02:48:25.573: INFO: Created: latency-svc-nljwb
Dec 11 02:48:25.595: INFO: Got endpoints: latency-svc-57sr6 [748.5502ms]
Dec 11 02:48:25.622: INFO: Created: latency-svc-tv84z
Dec 11 02:48:25.645: INFO: Got endpoints: latency-svc-fp75j [749.892611ms]
Dec 11 02:48:25.673: INFO: Created: latency-svc-6xf9m
Dec 11 02:48:25.695: INFO: Got endpoints: latency-svc-gmrxx [748.537489ms]
Dec 11 02:48:25.721: INFO: Created: latency-svc-5g679
Dec 11 02:48:25.745: INFO: Got endpoints: latency-svc-wbmcj [748.576934ms]
Dec 11 02:48:25.771: INFO: Created: latency-svc-jwxzf
Dec 11 02:48:25.796: INFO: Got endpoints: latency-svc-grsgw [751.402693ms]
Dec 11 02:48:25.824: INFO: Created: latency-svc-nj7fx
Dec 11 02:48:25.845: INFO: Got endpoints: latency-svc-bzkr5 [749.785938ms]
Dec 11 02:48:25.873: INFO: Created: latency-svc-v72fq
Dec 11 02:48:25.895: INFO: Got endpoints: latency-svc-mglqf [750.123639ms]
Dec 11 02:48:25.926: INFO: Created: latency-svc-z57z4
Dec 11 02:48:25.944: INFO: Got endpoints: latency-svc-b6s7m [749.941868ms]
Dec 11 02:48:25.974: INFO: Created: latency-svc-t2rmx
Dec 11 02:48:25.995: INFO: Got endpoints: latency-svc-2mbd2 [749.922873ms]
Dec 11 02:48:26.022: INFO: Created: latency-svc-7sw78
Dec 11 02:48:26.045: INFO: Got endpoints: latency-svc-qtqc2 [750.324252ms]
Dec 11 02:48:26.075: INFO: Created: latency-svc-l9lct
Dec 11 02:48:26.095: INFO: Got endpoints: latency-svc-9g75t [750.152168ms]
Dec 11 02:48:26.123: INFO: Created: latency-svc-6lgrw
Dec 11 02:48:26.145: INFO: Got endpoints: latency-svc-4rqfx [750.357655ms]
Dec 11 02:48:26.172: INFO: Created: latency-svc-v5krr
Dec 11 02:48:26.196: INFO: Got endpoints: latency-svc-jz4t6 [750.608691ms]
Dec 11 02:48:26.223: INFO: Created: latency-svc-289k8
Dec 11 02:48:26.245: INFO: Got endpoints: latency-svc-p2p4c [747.141736ms]
Dec 11 02:48:26.274: INFO: Created: latency-svc-rs98q
Dec 11 02:48:26.295: INFO: Got endpoints: latency-svc-nljwb [750.166745ms]
Dec 11 02:48:26.321: INFO: Created: latency-svc-4fxpn
Dec 11 02:48:26.345: INFO: Got endpoints: latency-svc-tv84z [749.881061ms]
Dec 11 02:48:26.372: INFO: Created: latency-svc-dv66z
Dec 11 02:48:26.395: INFO: Got endpoints: latency-svc-6xf9m [749.930663ms]
Dec 11 02:48:26.422: INFO: Created: latency-svc-9bt82
Dec 11 02:48:26.445: INFO: Got endpoints: latency-svc-5g679 [749.91983ms]
Dec 11 02:48:26.471: INFO: Created: latency-svc-kzlsb
Dec 11 02:48:26.496: INFO: Got endpoints: latency-svc-jwxzf [751.160058ms]
Dec 11 02:48:26.523: INFO: Created: latency-svc-v92zj
Dec 11 02:48:26.545: INFO: Got endpoints: latency-svc-nj7fx [748.643686ms]
Dec 11 02:48:26.573: INFO: Created: latency-svc-n47rj
Dec 11 02:48:26.595: INFO: Got endpoints: latency-svc-v72fq [750.215805ms]
Dec 11 02:48:26.624: INFO: Created: latency-svc-tkxv8
Dec 11 02:48:26.644: INFO: Got endpoints: latency-svc-z57z4 [749.326518ms]
Dec 11 02:48:26.672: INFO: Created: latency-svc-lvgkr
Dec 11 02:48:26.695: INFO: Got endpoints: latency-svc-t2rmx [750.905747ms]
Dec 11 02:48:26.723: INFO: Created: latency-svc-m2k7q
Dec 11 02:48:26.744: INFO: Got endpoints: latency-svc-7sw78 [749.641841ms]
Dec 11 02:48:26.771: INFO: Created: latency-svc-cxzvt
Dec 11 02:48:26.795: INFO: Got endpoints: latency-svc-l9lct [749.981849ms]
Dec 11 02:48:26.821: INFO: Created: latency-svc-qgdhm
Dec 11 02:48:26.845: INFO: Got endpoints: latency-svc-6lgrw [750.12521ms]
Dec 11 02:48:26.877: INFO: Created: latency-svc-78kbl
Dec 11 02:48:26.894: INFO: Got endpoints: latency-svc-v5krr [749.487571ms]
Dec 11 02:48:26.921: INFO: Created: latency-svc-xv5s7
Dec 11 02:48:26.945: INFO: Got endpoints: latency-svc-289k8 [748.673441ms]
Dec 11 02:48:27.001: INFO: Got endpoints: latency-svc-rs98q [756.286312ms]
Dec 11 02:48:27.001: INFO: Created: latency-svc-tbl8k
Dec 11 02:48:27.028: INFO: Created: latency-svc-tbvs7
Dec 11 02:48:27.045: INFO: Got endpoints: latency-svc-4fxpn [749.939464ms]
Dec 11 02:48:27.071: INFO: Created: latency-svc-9t7bk
Dec 11 02:48:27.095: INFO: Got endpoints: latency-svc-dv66z [750.00855ms]
Dec 11 02:48:27.137: INFO: Created: latency-svc-x5k8d
Dec 11 02:48:27.145: INFO: Got endpoints: latency-svc-9bt82 [749.993957ms]
Dec 11 02:48:27.179: INFO: Created: latency-svc-sxrl9
Dec 11 02:48:27.194: INFO: Got endpoints: latency-svc-kzlsb [749.565771ms]
Dec 11 02:48:27.222: INFO: Created: latency-svc-g5vk9
Dec 11 02:48:27.245: INFO: Got endpoints: latency-svc-v92zj [748.845362ms]
Dec 11 02:48:27.270: INFO: Created: latency-svc-bhf6q
Dec 11 02:48:27.295: INFO: Got endpoints: latency-svc-n47rj [749.861343ms]
Dec 11 02:48:27.321: INFO: Created: latency-svc-gb4k2
Dec 11 02:48:27.345: INFO: Got endpoints: latency-svc-tkxv8 [749.931323ms]
Dec 11 02:48:27.373: INFO: Created: latency-svc-vx6hs
Dec 11 02:48:27.395: INFO: Got endpoints: latency-svc-lvgkr [750.370378ms]
Dec 11 02:48:27.422: INFO: Created: latency-svc-jgf7j
Dec 11 02:48:27.445: INFO: Got endpoints: latency-svc-m2k7q [749.124179ms]
Dec 11 02:48:27.474: INFO: Created: latency-svc-wc8gf
Dec 11 02:48:27.494: INFO: Got endpoints: latency-svc-cxzvt [750.099034ms]
Dec 11 02:48:27.523: INFO: Created: latency-svc-n2k7z
Dec 11 02:48:27.545: INFO: Got endpoints: latency-svc-qgdhm [749.890641ms]
Dec 11 02:48:27.571: INFO: Created: latency-svc-jdchb
Dec 11 02:48:27.594: INFO: Got endpoints: latency-svc-78kbl [749.39235ms]
Dec 11 02:48:27.622: INFO: Created: latency-svc-rw8vr
Dec 11 02:48:27.647: INFO: Got endpoints: latency-svc-xv5s7 [752.725619ms]
Dec 11 02:48:27.698: INFO: Got endpoints: latency-svc-tbl8k [753.432094ms]
Dec 11 02:48:27.699: INFO: Created: latency-svc-flqfx
Dec 11 02:48:27.726: INFO: Created: latency-svc-8bvrq
Dec 11 02:48:27.745: INFO: Got endpoints: latency-svc-tbvs7 [743.658797ms]
Dec 11 02:48:27.772: INFO: Created: latency-svc-lcs77
Dec 11 02:48:27.795: INFO: Got endpoints: latency-svc-9t7bk [750.287613ms]
Dec 11 02:48:27.822: INFO: Created: latency-svc-qgf6n
Dec 11 02:48:27.845: INFO: Got endpoints: latency-svc-x5k8d [750.18048ms]
Dec 11 02:48:27.871: INFO: Created: latency-svc-wz2nz
Dec 11 02:48:27.895: INFO: Got endpoints: latency-svc-sxrl9 [750.086984ms]
Dec 11 02:48:27.921: INFO: Created: latency-svc-p9dd2
Dec 11 02:48:27.945: INFO: Got endpoints: latency-svc-g5vk9 [750.326954ms]
Dec 11 02:48:27.972: INFO: Created: latency-svc-nc2qg
Dec 11 02:48:27.995: INFO: Got endpoints: latency-svc-bhf6q [750.228396ms]
Dec 11 02:48:28.022: INFO: Created: latency-svc-54rdd
Dec 11 02:48:28.045: INFO: Got endpoints: latency-svc-gb4k2 [749.989755ms]
Dec 11 02:48:28.071: INFO: Created: latency-svc-x69ns
Dec 11 02:48:28.095: INFO: Got endpoints: latency-svc-vx6hs [749.770923ms]
Dec 11 02:48:28.130: INFO: Created: latency-svc-7csjs
Dec 11 02:48:28.149: INFO: Got endpoints: latency-svc-jgf7j [754.736597ms]
Dec 11 02:48:28.182: INFO: Created: latency-svc-pjmnt
Dec 11 02:48:28.196: INFO: Got endpoints: latency-svc-wc8gf [751.32711ms]
Dec 11 02:48:28.225: INFO: Created: latency-svc-xvxdq
Dec 11 02:48:28.245: INFO: Got endpoints: latency-svc-n2k7z [750.3327ms]
Dec 11 02:48:28.275: INFO: Created: latency-svc-p2vwk
Dec 11 02:48:28.295: INFO: Got endpoints: latency-svc-jdchb [750.567484ms]
Dec 11 02:48:28.326: INFO: Created: latency-svc-q6cb4
Dec 11 02:48:28.344: INFO: Got endpoints: latency-svc-rw8vr [750.060319ms]
Dec 11 02:48:28.374: INFO: Created: latency-svc-vxd85
Dec 11 02:48:28.395: INFO: Got endpoints: latency-svc-flqfx [747.266907ms]
Dec 11 02:48:28.422: INFO: Created: latency-svc-h9mj7
Dec 11 02:48:28.445: INFO: Got endpoints: latency-svc-8bvrq [746.815742ms]
Dec 11 02:48:28.472: INFO: Created: latency-svc-mvz7n
Dec 11 02:48:28.495: INFO: Got endpoints: latency-svc-lcs77 [750.019353ms]
Dec 11 02:48:28.522: INFO: Created: latency-svc-kcxqc
Dec 11 02:48:28.545: INFO: Got endpoints: latency-svc-qgf6n [750.411401ms]
Dec 11 02:48:28.572: INFO: Created: latency-svc-4g44k
Dec 11 02:48:28.595: INFO: Got endpoints: latency-svc-wz2nz [750.043001ms]
Dec 11 02:48:28.622: INFO: Created: latency-svc-t7tkk
Dec 11 02:48:28.644: INFO: Got endpoints: latency-svc-p9dd2 [749.800275ms]
Dec 11 02:48:28.672: INFO: Created: latency-svc-984px
Dec 11 02:48:28.695: INFO: Got endpoints: latency-svc-nc2qg [749.891678ms]
Dec 11 02:48:28.723: INFO: Created: latency-svc-5ndx2
Dec 11 02:48:28.744: INFO: Got endpoints: latency-svc-54rdd [749.578731ms]
Dec 11 02:48:28.772: INFO: Created: latency-svc-t5ldd
Dec 11 02:48:28.795: INFO: Got endpoints: latency-svc-x69ns [750.14824ms]
Dec 11 02:48:28.823: INFO: Created: latency-svc-gsx49
Dec 11 02:48:28.844: INFO: Got endpoints: latency-svc-7csjs [749.834182ms]
Dec 11 02:48:28.874: INFO: Created: latency-svc-w8sg9
Dec 11 02:48:28.896: INFO: Got endpoints: latency-svc-pjmnt [746.518694ms]
Dec 11 02:48:28.924: INFO: Created: latency-svc-xz7nx
Dec 11 02:48:28.945: INFO: Got endpoints: latency-svc-xvxdq [749.520815ms]
Dec 11 02:48:28.972: INFO: Created: latency-svc-vk776
Dec 11 02:48:28.995: INFO: Got endpoints: latency-svc-p2vwk [750.506226ms]
Dec 11 02:48:29.023: INFO: Created: latency-svc-8fmb9
Dec 11 02:48:29.045: INFO: Got endpoints: latency-svc-q6cb4 [749.625416ms]
Dec 11 02:48:29.072: INFO: Created: latency-svc-92hdx
Dec 11 02:48:29.095: INFO: Got endpoints: latency-svc-vxd85 [750.268583ms]
Dec 11 02:48:29.122: INFO: Created: latency-svc-vtczp
Dec 11 02:48:29.145: INFO: Got endpoints: latency-svc-h9mj7 [750.079284ms]
Dec 11 02:48:29.171: INFO: Created: latency-svc-tngll
Dec 11 02:48:29.197: INFO: Got endpoints: latency-svc-mvz7n [751.830756ms]
Dec 11 02:48:29.226: INFO: Created: latency-svc-2w5dn
Dec 11 02:48:29.244: INFO: Got endpoints: latency-svc-kcxqc [749.631945ms]
Dec 11 02:48:29.282: INFO: Created: latency-svc-5vzwz
Dec 11 02:48:29.297: INFO: Got endpoints: latency-svc-4g44k [751.973895ms]
Dec 11 02:48:29.325: INFO: Created: latency-svc-7bzqw
Dec 11 02:48:29.345: INFO: Got endpoints: latency-svc-t7tkk [749.840792ms]
Dec 11 02:48:29.372: INFO: Created: latency-svc-hrdwp
Dec 11 02:48:29.394: INFO: Got endpoints: latency-svc-984px [749.842849ms]
Dec 11 02:48:29.421: INFO: Created: latency-svc-8jwkz
Dec 11 02:48:29.445: INFO: Got endpoints: latency-svc-5ndx2 [750.086231ms]
Dec 11 02:48:29.472: INFO: Created: latency-svc-dqt4n
Dec 11 02:48:29.496: INFO: Got endpoints: latency-svc-t5ldd [751.98081ms]
Dec 11 02:48:29.523: INFO: Created: latency-svc-hxbhh
Dec 11 02:48:29.548: INFO: Got endpoints: latency-svc-gsx49 [753.492727ms]
Dec 11 02:48:29.576: INFO: Created: latency-svc-hcqlh
Dec 11 02:48:29.595: INFO: Got endpoints: latency-svc-w8sg9 [750.048795ms]
Dec 11 02:48:29.624: INFO: Created: latency-svc-2zzds
Dec 11 02:48:29.647: INFO: Got endpoints: latency-svc-xz7nx [750.704745ms]
Dec 11 02:48:29.678: INFO: Created: latency-svc-tpf4b
Dec 11 02:48:29.695: INFO: Got endpoints: latency-svc-vk776 [749.708367ms]
Dec 11 02:48:29.724: INFO: Created: latency-svc-5skpf
Dec 11 02:48:29.746: INFO: Got endpoints: latency-svc-8fmb9 [751.178289ms]
Dec 11 02:48:29.776: INFO: Created: latency-svc-jf28t
Dec 11 02:48:29.795: INFO: Got endpoints: latency-svc-92hdx [749.603597ms]
Dec 11 02:48:29.823: INFO: Created: latency-svc-8276k
Dec 11 02:48:29.845: INFO: Got endpoints: latency-svc-vtczp [749.866546ms]
Dec 11 02:48:29.872: INFO: Created: latency-svc-q85xm
Dec 11 02:48:29.895: INFO: Got endpoints: latency-svc-tngll [750.024063ms]
Dec 11 02:48:29.925: INFO: Created: latency-svc-7vm25
Dec 11 02:48:29.946: INFO: Got endpoints: latency-svc-2w5dn [748.718104ms]
Dec 11 02:48:29.973: INFO: Created: latency-svc-xks4m
Dec 11 02:48:29.994: INFO: Got endpoints: latency-svc-5vzwz [750.05919ms]
Dec 11 02:48:30.022: INFO: Created: latency-svc-gg29x
Dec 11 02:48:30.045: INFO: Got endpoints: latency-svc-7bzqw [747.352116ms]
Dec 11 02:48:30.072: INFO: Created: latency-svc-wv8rh
Dec 11 02:48:30.095: INFO: Got endpoints: latency-svc-hrdwp [750.538939ms]
Dec 11 02:48:30.126: INFO: Created: latency-svc-sxmfl
Dec 11 02:48:30.145: INFO: Got endpoints: latency-svc-8jwkz [750.858883ms]
Dec 11 02:48:30.195: INFO: Got endpoints: latency-svc-dqt4n [750.025095ms]
Dec 11 02:48:30.245: INFO: Got endpoints: latency-svc-hxbhh [748.141467ms]
Dec 11 02:48:30.295: INFO: Got endpoints: latency-svc-hcqlh [746.495998ms]
Dec 11 02:48:30.345: INFO: Got endpoints: latency-svc-2zzds [750.1683ms]
Dec 11 02:48:30.395: INFO: Got endpoints: latency-svc-tpf4b [747.87759ms]
Dec 11 02:48:30.445: INFO: Got endpoints: latency-svc-5skpf [749.532844ms]
Dec 11 02:48:30.495: INFO: Got endpoints: latency-svc-jf28t [748.12844ms]
Dec 11 02:48:30.545: INFO: Got endpoints: latency-svc-8276k [750.110214ms]
Dec 11 02:48:30.595: INFO: Got endpoints: latency-svc-q85xm [750.264405ms]
Dec 11 02:48:30.645: INFO: Got endpoints: latency-svc-7vm25 [749.99658ms]
Dec 11 02:48:30.695: INFO: Got endpoints: latency-svc-xks4m [749.518824ms]
Dec 11 02:48:30.745: INFO: Got endpoints: latency-svc-gg29x [750.292814ms]
Dec 11 02:48:30.796: INFO: Got endpoints: latency-svc-wv8rh [750.869578ms]
Dec 11 02:48:30.846: INFO: Got endpoints: latency-svc-sxmfl [750.242367ms]
Dec 11 02:48:30.846: INFO: Latencies: [34.506295ms 44.571424ms 45.33527ms 49.20091ms 58.047682ms 62.218671ms 70.195519ms 70.376574ms 78.477665ms 85.491161ms 93.337226ms 94.979135ms 102.503655ms 109.235096ms 112.35836ms 113.908892ms 115.560601ms 116.225506ms 116.874459ms 117.495275ms 117.67531ms 118.518188ms 118.611477ms 118.741234ms 120.161412ms 120.585182ms 123.387997ms 123.419004ms 123.483449ms 123.727031ms 129.092604ms 129.691974ms 136.319583ms 137.051199ms 144.284724ms 168.333769ms 210.47467ms 255.174867ms 284.248198ms 334.064095ms 369.416591ms 419.666807ms 459.968961ms 504.660638ms 555.038317ms 595.169731ms 629.789318ms 669.31729ms 714.287619ms 743.658797ms 746.495998ms 746.518694ms 746.815742ms 747.141736ms 747.266907ms 747.352116ms 747.494163ms 747.87759ms 748.12844ms 748.141467ms 748.155666ms 748.436607ms 748.537489ms 748.5502ms 748.565282ms 748.576934ms 748.637429ms 748.643686ms 748.644392ms 748.673441ms 748.718104ms 748.845362ms 748.849644ms 748.898072ms 748.943941ms 749.124179ms 749.219801ms 749.259796ms 749.289619ms 749.326518ms 749.372362ms 749.372466ms 749.39235ms 749.487571ms 749.518824ms 749.520815ms 749.532844ms 749.565771ms 749.571331ms 749.578731ms 749.579321ms 749.603597ms 749.621138ms 749.625416ms 749.631945ms 749.641841ms 749.656896ms 749.674818ms 749.708367ms 749.763893ms 749.770923ms 749.785938ms 749.790671ms 749.800275ms 749.80643ms 749.833446ms 749.833556ms 749.834182ms 749.834207ms 749.840792ms 749.842849ms 749.861343ms 749.866546ms 749.878017ms 749.881061ms 749.890641ms 749.891678ms 749.892611ms 749.91983ms 749.922873ms 749.930663ms 749.931323ms 749.93472ms 749.939385ms 749.939464ms 749.941868ms 749.952559ms 749.97638ms 749.981849ms 749.989755ms 749.993957ms 749.994739ms 749.99658ms 750.00855ms 750.019353ms 750.024063ms 750.025095ms 750.035736ms 750.043001ms 750.048795ms 750.05919ms 750.060319ms 750.079284ms 750.086231ms 750.086984ms 750.099034ms 750.110214ms 750.123639ms 750.12521ms 750.14824ms 750.152168ms 750.166745ms 750.1683ms 750.18048ms 750.215805ms 750.228396ms 750.228504ms 750.242367ms 750.264405ms 750.268583ms 750.271366ms 750.287613ms 750.292814ms 750.322815ms 750.324252ms 750.326954ms 750.3327ms 750.357655ms 750.370378ms 750.411401ms 750.506226ms 750.538939ms 750.567484ms 750.608691ms 750.620831ms 750.704745ms 750.812002ms 750.858883ms 750.869578ms 750.905747ms 750.924506ms 750.962158ms 750.972352ms 751.160058ms 751.178289ms 751.294794ms 751.32711ms 751.367312ms 751.402693ms 751.609293ms 751.830756ms 751.973895ms 751.98081ms 752.725619ms 752.726772ms 753.432094ms 753.492727ms 753.610407ms 754.736597ms 756.286312ms]
Dec 11 02:48:30.846: INFO: 50 %ile: 749.770923ms
Dec 11 02:48:30.846: INFO: 90 %ile: 750.924506ms
Dec 11 02:48:30.846: INFO: 99 %ile: 754.736597ms
Dec 11 02:48:30.846: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:48:30.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6099" for this suite.
Dec 11 02:48:38.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:48:39.781: INFO: namespace svc-latency-6099 deletion completed in 8.912810063s

â€¢ [SLOW TEST:19.780 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:48:39.781: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-500ebc11-b723-49d8-b1e3-b422ff0c87e8
STEP: Creating a pod to test consume configMaps
Dec 11 02:48:39.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d" in namespace "projected-3967" to be "success or failure"
Dec 11 02:48:39.967: INFO: Pod "pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.51954ms
Dec 11 02:48:41.990: INFO: Pod "pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043899392s
Dec 11 02:48:44.012: INFO: Pod "pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066071739s
STEP: Saw pod success
Dec 11 02:48:44.012: INFO: Pod "pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d" satisfied condition "success or failure"
Dec 11 02:48:44.034: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:48:44.112: INFO: Waiting for pod pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d to disappear
Dec 11 02:48:44.133: INFO: Pod pod-projected-configmaps-b45b2a84-2f72-4c07-b0d2-97cff2f9071d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:48:44.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3967" for this suite.
Dec 11 02:48:50.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:48:51.075: INFO: namespace projected-3967 deletion completed in 6.920427552s

â€¢ [SLOW TEST:11.294 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:48:51.076: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 11 02:48:51.193: INFO: Waiting up to 5m0s for pod "pod-d418ae78-080f-466d-8fdd-29840d90fe45" in namespace "emptydir-7520" to be "success or failure"
Dec 11 02:48:51.216: INFO: Pod "pod-d418ae78-080f-466d-8fdd-29840d90fe45": Phase="Pending", Reason="", readiness=false. Elapsed: 22.734903ms
Dec 11 02:48:53.238: INFO: Pod "pod-d418ae78-080f-466d-8fdd-29840d90fe45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045378172s
Dec 11 02:48:55.261: INFO: Pod "pod-d418ae78-080f-466d-8fdd-29840d90fe45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068149818s
STEP: Saw pod success
Dec 11 02:48:55.261: INFO: Pod "pod-d418ae78-080f-466d-8fdd-29840d90fe45" satisfied condition "success or failure"
Dec 11 02:48:55.283: INFO: Trying to get logs from node 10.247.6.203 pod pod-d418ae78-080f-466d-8fdd-29840d90fe45 container test-container: <nil>
STEP: delete the pod
Dec 11 02:48:55.356: INFO: Waiting for pod pod-d418ae78-080f-466d-8fdd-29840d90fe45 to disappear
Dec 11 02:48:55.377: INFO: Pod pod-d418ae78-080f-466d-8fdd-29840d90fe45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:48:55.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7520" for this suite.
Dec 11 02:49:01.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:49:02.351: INFO: namespace emptydir-7520 deletion completed in 6.951495884s

â€¢ [SLOW TEST:11.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:49:02.351: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-1fac668b-cb36-42dc-b23e-d7ea2b03c0e3
STEP: Creating a pod to test consume configMaps
Dec 11 02:49:02.500: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1" in namespace "projected-633" to be "success or failure"
Dec 11 02:49:02.521: INFO: Pod "pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.730933ms
Dec 11 02:49:04.546: INFO: Pod "pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046449786s
STEP: Saw pod success
Dec 11 02:49:04.546: INFO: Pod "pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1" satisfied condition "success or failure"
Dec 11 02:49:04.569: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:49:04.642: INFO: Waiting for pod pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1 to disappear
Dec 11 02:49:04.664: INFO: Pod pod-projected-configmaps-6909eb18-9367-4b08-85b4-3a028b4eadc1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:49:04.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-633" for this suite.
Dec 11 02:49:10.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:49:11.635: INFO: namespace projected-633 deletion completed in 6.945277109s

â€¢ [SLOW TEST:9.284 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:49:11.635: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:49:11.724: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:49:14.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8581" for this suite.
Dec 11 02:49:52.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:49:52.994: INFO: namespace pods-8581 deletion completed in 38.935620631s

â€¢ [SLOW TEST:41.359 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:49:52.995: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-2bgk
STEP: Creating a pod to test atomic-volume-subpath
Dec 11 02:49:53.157: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-2bgk" in namespace "subpath-2487" to be "success or failure"
Dec 11 02:49:53.179: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Pending", Reason="", readiness=false. Elapsed: 21.639714ms
Dec 11 02:49:55.202: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044267144s
Dec 11 02:49:57.224: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 4.066678864s
Dec 11 02:49:59.247: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 6.08962246s
Dec 11 02:50:01.270: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 8.112637437s
Dec 11 02:50:03.293: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 10.135229766s
Dec 11 02:50:05.315: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 12.157566857s
Dec 11 02:50:07.337: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 14.179607507s
Dec 11 02:50:09.360: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 16.202090654s
Dec 11 02:50:11.382: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 18.224565888s
Dec 11 02:50:13.404: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Running", Reason="", readiness=true. Elapsed: 20.2470503s
Dec 11 02:50:15.427: INFO: Pod "pod-subpath-test-secret-2bgk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.269611744s
STEP: Saw pod success
Dec 11 02:50:15.427: INFO: Pod "pod-subpath-test-secret-2bgk" satisfied condition "success or failure"
Dec 11 02:50:15.449: INFO: Trying to get logs from node 10.247.6.203 pod pod-subpath-test-secret-2bgk container test-container-subpath-secret-2bgk: <nil>
STEP: delete the pod
Dec 11 02:50:15.542: INFO: Waiting for pod pod-subpath-test-secret-2bgk to disappear
Dec 11 02:50:15.563: INFO: Pod pod-subpath-test-secret-2bgk no longer exists
STEP: Deleting pod pod-subpath-test-secret-2bgk
Dec 11 02:50:15.563: INFO: Deleting pod "pod-subpath-test-secret-2bgk" in namespace "subpath-2487"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:50:15.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2487" for this suite.
Dec 11 02:50:21.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:50:22.554: INFO: namespace subpath-2487 deletion completed in 6.947379954s

â€¢ [SLOW TEST:29.559 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:50:22.554: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:50:28.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9508" for this suite.
Dec 11 02:50:35.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:50:35.863: INFO: namespace namespaces-9508 deletion completed in 6.930530303s
STEP: Destroying namespace "nsdeletetest-537" for this suite.
Dec 11 02:50:35.887: INFO: Namespace nsdeletetest-537 was already deleted
STEP: Destroying namespace "nsdeletetest-4928" for this suite.
Dec 11 02:50:41.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:50:42.817: INFO: namespace nsdeletetest-4928 deletion completed in 6.930753906s

â€¢ [SLOW TEST:20.263 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:50:42.818: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-dab6ff1f-c5cc-4bde-8fbe-d3c6c10ffb34
STEP: Creating a pod to test consume configMaps
Dec 11 02:50:42.955: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957" in namespace "projected-2701" to be "success or failure"
Dec 11 02:50:42.977: INFO: Pod "pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957": Phase="Pending", Reason="", readiness=false. Elapsed: 21.440059ms
Dec 11 02:50:45.001: INFO: Pod "pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04608293s
Dec 11 02:50:47.024: INFO: Pod "pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06886957s
STEP: Saw pod success
Dec 11 02:50:47.024: INFO: Pod "pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957" satisfied condition "success or failure"
Dec 11 02:50:47.046: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:50:47.119: INFO: Waiting for pod pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957 to disappear
Dec 11 02:50:47.149: INFO: Pod pod-projected-configmaps-ee308b91-29fa-4652-9c8a-34833af06957 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:50:47.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2701" for this suite.
Dec 11 02:50:53.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:50:54.105: INFO: namespace projected-2701 deletion completed in 6.933832888s

â€¢ [SLOW TEST:11.288 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:50:54.106: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:50:54.297: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7d6d2a7b-cf78-46d4-af5b-403cd2dd97cf", Controller:(*bool)(0xc000da940a), BlockOwnerDeletion:(*bool)(0xc000da940b)}}
Dec 11 02:50:54.324: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"caa15461-e58c-4de0-984b-9c875acd23cd", Controller:(*bool)(0xc0022982e2), BlockOwnerDeletion:(*bool)(0xc0022982e3)}}
Dec 11 02:50:54.350: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2b924dc4-98d9-4535-bc11-2a2341cea5e2", Controller:(*bool)(0xc002400402), BlockOwnerDeletion:(*bool)(0xc002400403)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:50:59.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8684" for this suite.
Dec 11 02:51:05.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:51:06.373: INFO: namespace gc-8684 deletion completed in 6.955221531s

â€¢ [SLOW TEST:12.267 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:51:06.373: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9ac82a08-fa8b-41a5-8bcd-7fd5e10b68c1
STEP: Creating a pod to test consume configMaps
Dec 11 02:51:06.514: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6" in namespace "configmap-8804" to be "success or failure"
Dec 11 02:51:06.535: INFO: Pod "pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.5395ms
Dec 11 02:51:08.558: INFO: Pod "pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044123112s
Dec 11 02:51:10.580: INFO: Pod "pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066741416s
STEP: Saw pod success
Dec 11 02:51:10.580: INFO: Pod "pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6" satisfied condition "success or failure"
Dec 11 02:51:10.602: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 02:51:10.797: INFO: Waiting for pod pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6 to disappear
Dec 11 02:51:10.818: INFO: Pod pod-configmaps-e2d82cd4-35c8-4152-9046-d5aea5849cc6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:51:10.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8804" for this suite.
Dec 11 02:51:16.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:51:17.775: INFO: namespace configmap-8804 deletion completed in 6.934583739s

â€¢ [SLOW TEST:11.402 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:51:17.775: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 11 02:51:17.870: INFO: PodSpec: initContainers in spec.initContainers
Dec 11 02:52:06.371: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8b25e24c-8484-41f0-b4f8-ab1e168dc2f5", GenerateName:"", Namespace:"init-container-2764", SelfLink:"/api/v1/namespaces/init-container-2764/pods/pod-init-8b25e24c-8484-41f0-b4f8-ab1e168dc2f5", UID:"dcab91c4-dc33-4dc2-b239-1df22b28145c", ResourceVersion:"1019659", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711629458, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"870893536"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qltnw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001c7d940), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qltnw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qltnw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBE_POD_NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc0023d5900)}, v1.EnvVar{Name:"KUBE_POD_NAMESPACE", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc0023d5980)}, v1.EnvVar{Name:"KUBE_POD_CLUSTERID", Value:"236cc8a2-17d4-11ea-bf02-0255ac1b0093", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qltnw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002112d48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.247.6.203", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0020e6a20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002112dd0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002112df0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002112df8), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711629477, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711629477, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711629477, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711629458, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.247.6.203", PodIP:"172.16.1.51", StartTime:(*v1.Time)(0xc0023d59c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008aea10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008aeaf0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker://sha256:758ec7f3a1ee85f8f08399b55641bfb13e8c1109287ddc5e22b68c3d653152ee", ContainerID:"docker://146f6311ac4234298025cdf3ced43f227845ad81753b560ff99e90a44e895879"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023d5a00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023d59e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:52:06.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2764" for this suite.
Dec 11 02:52:28.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:52:29.362: INFO: namespace init-container-2764 deletion completed in 22.967930749s

â€¢ [SLOW TEST:71.586 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:52:29.362: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9685.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9685.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9685.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9685.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9685.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9685.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 11 02:52:35.939: INFO: DNS probes using dns-9685/dns-test-00cda69b-d7f2-4acb-bca2-58a09a5de691 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:52:35.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9685" for this suite.
Dec 11 02:52:42.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:52:42.912: INFO: namespace dns-9685 deletion completed in 6.920726764s

â€¢ [SLOW TEST:13.551 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:52:42.913: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Dec 11 02:52:43.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-2878'
Dec 11 02:52:43.848: INFO: stderr: ""
Dec 11 02:52:43.848: INFO: stdout: "pod/pause created\n"
Dec 11 02:52:43.848: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 11 02:52:43.848: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2878" to be "running and ready"
Dec 11 02:52:43.869: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 21.42292ms
Dec 11 02:52:45.892: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04405834s
Dec 11 02:52:47.914: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.066367161s
Dec 11 02:52:47.914: INFO: Pod "pause" satisfied condition "running and ready"
Dec 11 02:52:47.914: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 11 02:52:47.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 label pods pause testing-label=testing-label-value --namespace=kubectl-2878'
Dec 11 02:52:48.116: INFO: stderr: ""
Dec 11 02:52:48.116: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 11 02:52:48.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pod pause -L testing-label --namespace=kubectl-2878'
Dec 11 02:52:48.277: INFO: stderr: ""
Dec 11 02:52:48.277: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 11 02:52:48.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 label pods pause testing-label- --namespace=kubectl-2878'
Dec 11 02:52:48.455: INFO: stderr: ""
Dec 11 02:52:48.455: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 11 02:52:48.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pod pause -L testing-label --namespace=kubectl-2878'
Dec 11 02:52:48.612: INFO: stderr: ""
Dec 11 02:52:48.612: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Dec 11 02:52:48.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-2878'
Dec 11 02:52:48.789: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 02:52:48.789: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 11 02:52:48.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get rc,svc -l name=pause --no-headers --namespace=kubectl-2878'
Dec 11 02:52:48.970: INFO: stderr: "No resources found.\n"
Dec 11 02:52:48.970: INFO: stdout: ""
Dec 11 02:52:48.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -l name=pause --namespace=kubectl-2878 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 11 02:52:49.121: INFO: stderr: ""
Dec 11 02:52:49.121: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:52:49.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2878" for this suite.
Dec 11 02:52:55.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:52:56.123: INFO: namespace kubectl-2878 deletion completed in 6.978472087s

â€¢ [SLOW TEST:13.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:52:56.123: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7792
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-7792
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7792
Dec 11 02:52:56.296: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec 11 02:53:06.318: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 11 02:53:06.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 02:53:06.853: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 02:53:06.853: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 02:53:06.853: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 02:53:06.875: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 11 02:53:16.900: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 02:53:16.900: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 02:53:16.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999965s
Dec 11 02:53:18.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.973433086s
Dec 11 02:53:19.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.950360569s
Dec 11 02:53:20.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.927512768s
Dec 11 02:53:21.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.904081093s
Dec 11 02:53:22.116: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.881196453s
Dec 11 02:53:23.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.855291547s
Dec 11 02:53:24.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.832694771s
Dec 11 02:53:25.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.809967858s
Dec 11 02:53:26.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 787.429721ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7792
Dec 11 02:53:27.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 02:53:27.738: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 02:53:27.738: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 02:53:27.738: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 02:53:27.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 02:53:28.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 11 02:53:28.135: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 02:53:28.135: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 02:53:28.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 02:53:28.649: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 11 02:53:28.649: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 02:53:28.649: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 02:53:28.672: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:53:28.672: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 02:53:28.672: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 11 02:53:28.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 02:53:29.231: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 02:53:29.231: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 02:53:29.231: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 02:53:29.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 02:53:29.631: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 02:53:29.631: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 02:53:29.631: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 02:53:29.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-7792 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 02:53:30.154: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 02:53:30.154: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 02:53:30.154: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 02:53:30.154: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 02:53:30.176: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 11 02:53:40.221: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 02:53:40.221: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 02:53:40.221: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 02:53:40.287: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 11 02:53:40.287: INFO: ss-0  10.247.6.203    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:36 +0000 UTC  }]
Dec 11 02:53:40.287: INFO: ss-1  kcsp41-master0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  }]
Dec 11 02:53:40.287: INFO: ss-2  10.247.6.203    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  }]
Dec 11 02:53:40.287: INFO: 
Dec 11 02:53:40.287: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 11 02:53:41.310: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Dec 11 02:53:41.310: INFO: ss-0  10.247.6.203    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:36 +0000 UTC  }]
Dec 11 02:53:41.310: INFO: ss-1  kcsp41-master0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  }]
Dec 11 02:53:41.310: INFO: ss-2  10.247.6.203    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  }]
Dec 11 02:53:41.310: INFO: 
Dec 11 02:53:41.310: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 11 02:53:42.332: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Dec 11 02:53:42.332: INFO: ss-0  10.247.6.203  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:36 +0000 UTC  }]
Dec 11 02:53:42.332: INFO: ss-2  10.247.6.203  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:53:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 02:52:57 +0000 UTC  }]
Dec 11 02:53:42.332: INFO: 
Dec 11 02:53:42.332: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 11 02:53:43.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.932650259s
Dec 11 02:53:44.378: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.908965707s
Dec 11 02:53:45.401: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.886382101s
Dec 11 02:53:46.424: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.863933193s
Dec 11 02:53:47.447: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.840904591s
Dec 11 02:53:48.470: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.818219456s
Dec 11 02:53:49.493: INFO: Verifying statefulset ss doesn't scale past 0 for another 794.727555ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7792
Dec 11 02:53:50.515: INFO: Scaling statefulset ss to 0
Dec 11 02:53:50.581: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 11 02:53:50.602: INFO: Deleting all statefulset in ns statefulset-7792
Dec 11 02:53:50.624: INFO: Scaling statefulset ss to 0
Dec 11 02:53:50.691: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 02:53:50.713: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:53:50.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7792" for this suite.
Dec 11 02:53:56.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:53:57.717: INFO: namespace statefulset-7792 deletion completed in 6.912463511s

â€¢ [SLOW TEST:61.594 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:53:57.717: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 02:53:57.811: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:53:58.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-204" for this suite.
Dec 11 02:54:04.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:54:04.991: INFO: namespace custom-resource-definition-204 deletion completed in 6.906696083s

â€¢ [SLOW TEST:7.274 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:54:04.991: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:55:05.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3762" for this suite.
Dec 11 02:55:27.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:55:28.092: INFO: namespace container-probe-3762 deletion completed in 22.93986262s

â€¢ [SLOW TEST:83.101 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:55:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-45a4b7fd-6ec9-48f3-a5a2-1dabd2ce34e6 in namespace container-probe-9087
Dec 11 02:55:32.251: INFO: Started pod busybox-45a4b7fd-6ec9-48f3-a5a2-1dabd2ce34e6 in namespace container-probe-9087
STEP: checking the pod's current state and verifying that restartCount is present
Dec 11 02:55:32.273: INFO: Initial restart count of pod busybox-45a4b7fd-6ec9-48f3-a5a2-1dabd2ce34e6 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:59:33.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9087" for this suite.
Dec 11 02:59:39.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:59:39.988: INFO: namespace container-probe-9087 deletion completed in 6.936756226s

â€¢ [SLOW TEST:251.896 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:59:39.988: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 11 02:59:44.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec pod-sharedvolume-0cf0dd25-125f-495f-b65f-ca81f055e240 -c busybox-main-container --namespace=emptydir-4933 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 11 02:59:44.716: INFO: stderr: ""
Dec 11 02:59:44.716: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:59:44.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4933" for this suite.
Dec 11 02:59:50.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 02:59:51.667: INFO: namespace emptydir-4933 deletion completed in 6.928789897s

â€¢ [SLOW TEST:11.679 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 02:59:51.668: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-422fd936-7df9-4807-9ad9-788e03e7fad0
STEP: Creating a pod to test consume secrets
Dec 11 02:59:51.808: INFO: Waiting up to 5m0s for pod "pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99" in namespace "secrets-8267" to be "success or failure"
Dec 11 02:59:51.829: INFO: Pod "pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99": Phase="Pending", Reason="", readiness=false. Elapsed: 21.515414ms
Dec 11 02:59:53.853: INFO: Pod "pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045625448s
STEP: Saw pod success
Dec 11 02:59:53.853: INFO: Pod "pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99" satisfied condition "success or failure"
Dec 11 02:59:53.875: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99 container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 02:59:53.995: INFO: Waiting for pod pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99 to disappear
Dec 11 02:59:54.016: INFO: Pod pod-secrets-728a84c3-8086-4b0e-ae7e-b114183bfd99 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 02:59:54.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8267" for this suite.
Dec 11 03:00:00.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:00.966: INFO: namespace secrets-8267 deletion completed in 6.927724438s

â€¢ [SLOW TEST:9.298 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:00.966: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-79fb85f6-c7fc-45af-8927-4f94ced37c19
STEP: Creating a pod to test consume secrets
Dec 11 03:00:01.157: INFO: Waiting up to 5m0s for pod "pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f" in namespace "secrets-9516" to be "success or failure"
Dec 11 03:00:01.179: INFO: Pod "pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.859305ms
Dec 11 03:00:03.202: INFO: Pod "pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044577401s
STEP: Saw pod success
Dec 11 03:00:03.202: INFO: Pod "pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f" satisfied condition "success or failure"
Dec 11 03:00:03.224: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:00:03.318: INFO: Waiting for pod pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f to disappear
Dec 11 03:00:03.340: INFO: Pod pod-secrets-f21d8e7f-a16e-41ce-ad74-601a68e8df3f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:00:03.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9516" for this suite.
Dec 11 03:00:09.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:10.271: INFO: namespace secrets-9516 deletion completed in 6.908525668s

â€¢ [SLOW TEST:9.305 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:10.271: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec 11 03:00:10.388: INFO: Waiting up to 5m0s for pod "client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66" in namespace "containers-7480" to be "success or failure"
Dec 11 03:00:10.410: INFO: Pod "client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66": Phase="Pending", Reason="", readiness=false. Elapsed: 22.162409ms
Dec 11 03:00:12.433: INFO: Pod "client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045055606s
Dec 11 03:00:14.456: INFO: Pod "client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067638763s
STEP: Saw pod success
Dec 11 03:00:14.456: INFO: Pod "client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66" satisfied condition "success or failure"
Dec 11 03:00:14.478: INFO: Trying to get logs from node 10.247.6.203 pod client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66 container test-container: <nil>
STEP: delete the pod
Dec 11 03:00:14.618: INFO: Waiting for pod client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66 to disappear
Dec 11 03:00:14.640: INFO: Pod client-containers-8317c5f2-684e-4cc4-8f79-d92c6e6e0c66 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:00:14.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7480" for this suite.
Dec 11 03:00:20.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:21.590: INFO: namespace containers-7480 deletion completed in 6.928015199s

â€¢ [SLOW TEST:11.318 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:21.590: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:00:21.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c" in namespace "downward-api-2550" to be "success or failure"
Dec 11 03:00:21.731: INFO: Pod "downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.691085ms
Dec 11 03:00:23.754: INFO: Pod "downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046357719s
Dec 11 03:00:25.776: INFO: Pod "downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06845503s
STEP: Saw pod success
Dec 11 03:00:25.776: INFO: Pod "downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c" satisfied condition "success or failure"
Dec 11 03:00:25.798: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c container client-container: <nil>
STEP: delete the pod
Dec 11 03:00:25.873: INFO: Waiting for pod downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c to disappear
Dec 11 03:00:25.895: INFO: Pod downwardapi-volume-4ceecb02-f092-443a-b73f-20fee1d4a79c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:00:25.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2550" for this suite.
Dec 11 03:00:31.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:32.850: INFO: namespace downward-api-2550 deletion completed in 6.932879295s

â€¢ [SLOW TEST:11.260 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:32.850: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-f89ae862-d98b-4fd6-ba34-25e1284b9b7f
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:00:32.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7848" for this suite.
Dec 11 03:00:39.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:39.900: INFO: namespace configmap-7848 deletion completed in 6.916882728s

â€¢ [SLOW TEST:7.050 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:39.901: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-978a8361-e5f4-46f5-8c7c-42041f7edb61
STEP: Creating a pod to test consume configMaps
Dec 11 03:00:40.040: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80" in namespace "projected-1938" to be "success or failure"
Dec 11 03:00:40.062: INFO: Pod "pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80": Phase="Pending", Reason="", readiness=false. Elapsed: 22.063726ms
Dec 11 03:00:42.084: INFO: Pod "pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044149129s
Dec 11 03:00:44.107: INFO: Pod "pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067045809s
STEP: Saw pod success
Dec 11 03:00:44.107: INFO: Pod "pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80" satisfied condition "success or failure"
Dec 11 03:00:44.129: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 03:00:44.202: INFO: Waiting for pod pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80 to disappear
Dec 11 03:00:44.224: INFO: Pod pod-projected-configmaps-182f1f22-5198-4096-ba91-2a88fbbf9c80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:00:44.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1938" for this suite.
Dec 11 03:00:50.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:51.158: INFO: namespace projected-1938 deletion completed in 6.911695361s

â€¢ [SLOW TEST:11.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:51.158: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:00:51.324: INFO: (0) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 47.066899ms)
Dec 11 03:00:51.368: INFO: (1) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.297561ms)
Dec 11 03:00:51.412: INFO: (2) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.306204ms)
Dec 11 03:00:51.457: INFO: (3) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.699865ms)
Dec 11 03:00:51.501: INFO: (4) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.366013ms)
Dec 11 03:00:51.545: INFO: (5) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 43.991236ms)
Dec 11 03:00:51.590: INFO: (6) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.572664ms)
Dec 11 03:00:51.635: INFO: (7) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.956364ms)
Dec 11 03:00:51.679: INFO: (8) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.162113ms)
Dec 11 03:00:51.724: INFO: (9) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.496246ms)
Dec 11 03:00:51.768: INFO: (10) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 43.957877ms)
Dec 11 03:00:51.812: INFO: (11) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.497943ms)
Dec 11 03:00:51.857: INFO: (12) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.329756ms)
Dec 11 03:00:51.901: INFO: (13) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.088735ms)
Dec 11 03:00:51.946: INFO: (14) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.974722ms)
Dec 11 03:00:51.990: INFO: (15) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.345692ms)
Dec 11 03:00:52.034: INFO: (16) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 43.940426ms)
Dec 11 03:00:52.078: INFO: (17) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 44.224219ms)
Dec 11 03:00:52.124: INFO: (18) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.94661ms)
Dec 11 03:00:52.170: INFO: (19) /api/v1/nodes/10.247.6.203/proxy/logs/: <pre>
<a href="aide/">aide/</a>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a ... (200; 45.441517ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:00:52.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7892" for this suite.
Dec 11 03:00:58.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:00:59.132: INFO: namespace proxy-7892 deletion completed in 6.939786388s

â€¢ [SLOW TEST:7.974 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:00:59.132: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8711/configmap-test-c6267aa7-d4fe-4d26-9bdb-a727f21ea359
STEP: Creating a pod to test consume configMaps
Dec 11 03:00:59.291: INFO: Waiting up to 5m0s for pod "pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d" in namespace "configmap-8711" to be "success or failure"
Dec 11 03:00:59.312: INFO: Pod "pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.568881ms
Dec 11 03:01:01.335: INFO: Pod "pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044093256s
Dec 11 03:01:03.357: INFO: Pod "pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066538152s
STEP: Saw pod success
Dec 11 03:01:03.357: INFO: Pod "pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d" satisfied condition "success or failure"
Dec 11 03:01:03.379: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d container env-test: <nil>
STEP: delete the pod
Dec 11 03:01:03.470: INFO: Waiting for pod pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d to disappear
Dec 11 03:01:03.492: INFO: Pod pod-configmaps-b97295fb-c09c-4302-9625-e1fe73fdc75d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:01:03.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8711" for this suite.
Dec 11 03:01:09.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:01:10.441: INFO: namespace configmap-8711 deletion completed in 6.926703369s

â€¢ [SLOW TEST:11.309 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:01:10.441: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:01:14.678: INFO: Waiting up to 5m0s for pod "client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea" in namespace "pods-7038" to be "success or failure"
Dec 11 03:01:14.701: INFO: Pod "client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea": Phase="Pending", Reason="", readiness=false. Elapsed: 22.201889ms
Dec 11 03:01:16.723: INFO: Pod "client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044580861s
Dec 11 03:01:18.746: INFO: Pod "client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067185144s
STEP: Saw pod success
Dec 11 03:01:18.746: INFO: Pod "client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea" satisfied condition "success or failure"
Dec 11 03:01:18.767: INFO: Trying to get logs from node 10.247.6.203 pod client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea container env3cont: <nil>
STEP: delete the pod
Dec 11 03:01:18.861: INFO: Waiting for pod client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea to disappear
Dec 11 03:01:18.882: INFO: Pod client-envvars-6ec3f90e-3e62-496e-942f-cb454b278aea no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:01:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7038" for this suite.
Dec 11 03:01:58.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:01:59.810: INFO: namespace pods-7038 deletion completed in 40.905246669s

â€¢ [SLOW TEST:49.368 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:01:59.810: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-blr7
STEP: Creating a pod to test atomic-volume-subpath
Dec 11 03:01:59.975: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-blr7" in namespace "subpath-3626" to be "success or failure"
Dec 11 03:01:59.996: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Pending", Reason="", readiness=false. Elapsed: 21.389856ms
Dec 11 03:02:02.019: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044192816s
Dec 11 03:02:04.041: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 4.066473248s
Dec 11 03:02:06.064: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 6.088827346s
Dec 11 03:02:08.086: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 8.111097535s
Dec 11 03:02:10.108: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 10.133408037s
Dec 11 03:02:12.131: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 12.155787429s
Dec 11 03:02:14.153: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 14.178287783s
Dec 11 03:02:16.176: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 16.200793836s
Dec 11 03:02:18.198: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 18.222858472s
Dec 11 03:02:20.220: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 20.24492334s
Dec 11 03:02:22.242: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Running", Reason="", readiness=true. Elapsed: 22.267389266s
Dec 11 03:02:24.265: INFO: Pod "pod-subpath-test-configmap-blr7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.290053438s
STEP: Saw pod success
Dec 11 03:02:24.265: INFO: Pod "pod-subpath-test-configmap-blr7" satisfied condition "success or failure"
Dec 11 03:02:24.287: INFO: Trying to get logs from node 10.247.6.203 pod pod-subpath-test-configmap-blr7 container test-container-subpath-configmap-blr7: <nil>
STEP: delete the pod
Dec 11 03:02:24.378: INFO: Waiting for pod pod-subpath-test-configmap-blr7 to disappear
Dec 11 03:02:24.423: INFO: Pod pod-subpath-test-configmap-blr7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-blr7
Dec 11 03:02:24.423: INFO: Deleting pod "pod-subpath-test-configmap-blr7" in namespace "subpath-3626"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:02:24.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3626" for this suite.
Dec 11 03:02:30.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:02:31.378: INFO: namespace subpath-3626 deletion completed in 6.911630268s

â€¢ [SLOW TEST:31.568 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:02:31.378: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:02:31.496: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2" in namespace "projected-8767" to be "success or failure"
Dec 11 03:02:31.517: INFO: Pod "downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.644702ms
Dec 11 03:02:33.540: INFO: Pod "downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044140482s
Dec 11 03:02:35.563: INFO: Pod "downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066929578s
STEP: Saw pod success
Dec 11 03:02:35.563: INFO: Pod "downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2" satisfied condition "success or failure"
Dec 11 03:02:35.584: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2 container client-container: <nil>
STEP: delete the pod
Dec 11 03:02:35.658: INFO: Waiting for pod downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2 to disappear
Dec 11 03:02:35.680: INFO: Pod downwardapi-volume-f85735a7-a030-45b8-a85c-541121c2aea2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:02:35.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8767" for this suite.
Dec 11 03:02:41.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:02:42.641: INFO: namespace projected-8767 deletion completed in 6.939397487s

â€¢ [SLOW TEST:11.263 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:02:42.641: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9
Dec 11 03:02:42.773: INFO: Pod name my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9: Found 1 pods out of 1
Dec 11 03:02:42.773: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9" are running
Dec 11 03:02:46.818: INFO: Pod "my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9-zdvzc" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-11 03:02:23 +0000 UTC Reason: Message:}])
Dec 11 03:02:46.818: INFO: Trying to dial the pod
Dec 11 03:02:51.947: INFO: Controller my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9: Got expected result from replica 1 [my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9-zdvzc]: "my-hostname-basic-7bd4006a-dc13-47e5-bcc3-c4f5a34971e9-zdvzc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:02:51.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2899" for this suite.
Dec 11 03:02:58.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:02:58.874: INFO: namespace replication-controller-2899 deletion completed in 6.905335222s

â€¢ [SLOW TEST:16.233 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:02:58.875: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-39ef6612-4b0f-4d33-af1f-0f609da3dd9d
STEP: Creating a pod to test consume secrets
Dec 11 03:02:59.016: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162" in namespace "projected-212" to be "success or failure"
Dec 11 03:02:59.037: INFO: Pod "pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162": Phase="Pending", Reason="", readiness=false. Elapsed: 21.441987ms
Dec 11 03:03:01.060: INFO: Pod "pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044543056s
STEP: Saw pod success
Dec 11 03:03:01.060: INFO: Pod "pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162" satisfied condition "success or failure"
Dec 11 03:03:01.082: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162 container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:03:01.157: INFO: Waiting for pod pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162 to disappear
Dec 11 03:03:01.178: INFO: Pod pod-projected-secrets-8e3013cb-401a-489e-b52b-ab4193a61162 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:03:01.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-212" for this suite.
Dec 11 03:03:07.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:03:08.125: INFO: namespace projected-212 deletion completed in 6.925581381s

â€¢ [SLOW TEST:9.250 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:03:08.125: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 11 03:03:10.362: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-194493201 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 11 03:03:20.598: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:03:20.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1249" for this suite.
Dec 11 03:03:26.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:03:27.566: INFO: namespace pods-1249 deletion completed in 6.924143282s

â€¢ [SLOW TEST:19.440 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:03:27.566: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec 11 03:03:27.682: INFO: Waiting up to 5m0s for pod "var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec" in namespace "var-expansion-8611" to be "success or failure"
Dec 11 03:03:27.703: INFO: Pod "var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec": Phase="Pending", Reason="", readiness=false. Elapsed: 21.395353ms
Dec 11 03:03:29.726: INFO: Pod "var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043712117s
STEP: Saw pod success
Dec 11 03:03:29.726: INFO: Pod "var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec" satisfied condition "success or failure"
Dec 11 03:03:29.748: INFO: Trying to get logs from node 10.247.6.203 pod var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:03:29.823: INFO: Waiting for pod var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec to disappear
Dec 11 03:03:29.844: INFO: Pod var-expansion-2d891b89-3da5-4c6d-b527-c36369adffec no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:03:29.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8611" for this suite.
Dec 11 03:03:35.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:03:36.796: INFO: namespace var-expansion-8611 deletion completed in 6.929260189s

â€¢ [SLOW TEST:9.230 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:03:36.796: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 11 03:03:36.883: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:03:40.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4255" for this suite.
Dec 11 03:03:46.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:03:47.835: INFO: namespace init-container-4255 deletion completed in 6.909615385s

â€¢ [SLOW TEST:11.040 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:03:47.836: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 11 03:03:47.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-316'
Dec 11 03:03:48.993: INFO: stderr: ""
Dec 11 03:03:48.993: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 11 03:03:48.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-316'
Dec 11 03:03:49.154: INFO: stderr: ""
Dec 11 03:03:49.154: INFO: stdout: "update-demo-nautilus-tj4zs update-demo-nautilus-xrt84 "
Dec 11 03:03:49.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-tj4zs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:03:49.308: INFO: stderr: ""
Dec 11 03:03:49.308: INFO: stdout: ""
Dec 11 03:03:49.308: INFO: update-demo-nautilus-tj4zs is created but not running
Dec 11 03:03:54.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-316'
Dec 11 03:03:54.465: INFO: stderr: ""
Dec 11 03:03:54.465: INFO: stdout: "update-demo-nautilus-tj4zs update-demo-nautilus-xrt84 "
Dec 11 03:03:54.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-tj4zs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:03:54.620: INFO: stderr: ""
Dec 11 03:03:54.620: INFO: stdout: "true"
Dec 11 03:03:54.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-tj4zs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:03:54.771: INFO: stderr: ""
Dec 11 03:03:54.771: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:03:54.771: INFO: validating pod update-demo-nautilus-tj4zs
Dec 11 03:03:54.858: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:03:54.858: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:03:54.858: INFO: update-demo-nautilus-tj4zs is verified up and running
Dec 11 03:03:54.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-xrt84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:03:55.015: INFO: stderr: ""
Dec 11 03:03:55.015: INFO: stdout: "true"
Dec 11 03:03:55.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-xrt84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:03:55.170: INFO: stderr: ""
Dec 11 03:03:55.170: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:03:55.170: INFO: validating pod update-demo-nautilus-xrt84
Dec 11 03:03:55.194: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:03:55.194: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:03:55.194: INFO: update-demo-nautilus-xrt84 is verified up and running
STEP: scaling down the replication controller
Dec 11 03:03:55.196: INFO: scanned /root for discovery docs: <nil>
Dec 11 03:03:55.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-316'
Dec 11 03:03:55.405: INFO: stderr: ""
Dec 11 03:03:55.405: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 11 03:03:55.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-316'
Dec 11 03:03:55.567: INFO: stderr: ""
Dec 11 03:03:55.567: INFO: stdout: "update-demo-nautilus-tj4zs update-demo-nautilus-xrt84 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 11 03:04:00.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-316'
Dec 11 03:04:00.729: INFO: stderr: ""
Dec 11 03:04:00.729: INFO: stdout: "update-demo-nautilus-xrt84 "
Dec 11 03:04:00.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-xrt84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:00.887: INFO: stderr: ""
Dec 11 03:04:00.887: INFO: stdout: "true"
Dec 11 03:04:00.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-xrt84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:01.044: INFO: stderr: ""
Dec 11 03:04:01.044: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:04:01.044: INFO: validating pod update-demo-nautilus-xrt84
Dec 11 03:04:01.067: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:04:01.067: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:04:01.067: INFO: update-demo-nautilus-xrt84 is verified up and running
STEP: scaling up the replication controller
Dec 11 03:04:01.068: INFO: scanned /root for discovery docs: <nil>
Dec 11 03:04:01.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-316'
Dec 11 03:04:01.280: INFO: stderr: ""
Dec 11 03:04:01.280: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 11 03:04:01.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-316'
Dec 11 03:04:01.439: INFO: stderr: ""
Dec 11 03:04:01.439: INFO: stdout: "update-demo-nautilus-fvtcp update-demo-nautilus-xrt84 "
Dec 11 03:04:01.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-fvtcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:01.588: INFO: stderr: ""
Dec 11 03:04:01.588: INFO: stdout: ""
Dec 11 03:04:01.588: INFO: update-demo-nautilus-fvtcp is created but not running
Dec 11 03:04:06.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-316'
Dec 11 03:04:06.742: INFO: stderr: ""
Dec 11 03:04:06.742: INFO: stdout: "update-demo-nautilus-fvtcp update-demo-nautilus-xrt84 "
Dec 11 03:04:06.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-fvtcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:06.888: INFO: stderr: ""
Dec 11 03:04:06.888: INFO: stdout: "true"
Dec 11 03:04:06.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-fvtcp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:07.043: INFO: stderr: ""
Dec 11 03:04:07.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:04:07.043: INFO: validating pod update-demo-nautilus-fvtcp
Dec 11 03:04:07.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:04:07.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:04:07.129: INFO: update-demo-nautilus-fvtcp is verified up and running
Dec 11 03:04:07.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-xrt84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:07.275: INFO: stderr: ""
Dec 11 03:04:07.275: INFO: stdout: "true"
Dec 11 03:04:07.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-xrt84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-316'
Dec 11 03:04:07.424: INFO: stderr: ""
Dec 11 03:04:07.424: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:04:07.424: INFO: validating pod update-demo-nautilus-xrt84
Dec 11 03:04:07.446: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:04:07.446: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:04:07.446: INFO: update-demo-nautilus-xrt84 is verified up and running
STEP: using delete to clean up resources
Dec 11 03:04:07.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-316'
Dec 11 03:04:07.614: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:04:07.614: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 11 03:04:07.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-316'
Dec 11 03:04:07.789: INFO: stderr: "No resources found.\n"
Dec 11 03:04:07.789: INFO: stdout: ""
Dec 11 03:04:07.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -l name=update-demo --namespace=kubectl-316 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 11 03:04:07.936: INFO: stderr: ""
Dec 11 03:04:07.936: INFO: stdout: "update-demo-nautilus-fvtcp\nupdate-demo-nautilus-xrt84\n"
Dec 11 03:04:08.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-316'
Dec 11 03:04:08.619: INFO: stderr: "No resources found.\n"
Dec 11 03:04:08.619: INFO: stdout: ""
Dec 11 03:04:08.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -l name=update-demo --namespace=kubectl-316 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 11 03:04:08.768: INFO: stderr: ""
Dec 11 03:04:08.768: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:04:08.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-316" for this suite.
Dec 11 03:04:30.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:04:31.698: INFO: namespace kubectl-316 deletion completed in 22.90795994s

â€¢ [SLOW TEST:43.862 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:04:31.699: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 03:04:31.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4661'
Dec 11 03:04:31.944: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 11 03:04:31.944: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Dec 11 03:04:31.997: INFO: scanned /root for discovery docs: <nil>
Dec 11 03:04:31.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4661'
Dec 11 03:04:43.265: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 11 03:04:43.265: INFO: stdout: "Created e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e\nScaling up e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec 11 03:04:43.265: INFO: stdout: "Created e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e\nScaling up e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec 11 03:04:43.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4661'
Dec 11 03:04:43.421: INFO: stderr: ""
Dec 11 03:04:43.421: INFO: stdout: "e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e-2bzwx "
Dec 11 03:04:43.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e-2bzwx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Dec 11 03:04:43.574: INFO: stderr: ""
Dec 11 03:04:43.574: INFO: stdout: "true"
Dec 11 03:04:43.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e-2bzwx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4661'
Dec 11 03:04:43.724: INFO: stderr: ""
Dec 11 03:04:43.724: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec 11 03:04:43.724: INFO: e2e-test-nginx-rc-6c31b74af9eb31fdf51a0b4f3032c85e-2bzwx is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Dec 11 03:04:43.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete rc e2e-test-nginx-rc --namespace=kubectl-4661'
Dec 11 03:04:43.902: INFO: stderr: ""
Dec 11 03:04:43.902: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:04:43.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4661" for this suite.
Dec 11 03:04:49.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:04:50.831: INFO: namespace kubectl-4661 deletion completed in 6.906876757s

â€¢ [SLOW TEST:19.133 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:04:50.832: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-a7350e56-6944-43be-9acd-8ac859d0ed17
STEP: Creating a pod to test consume secrets
Dec 11 03:04:50.972: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3" in namespace "projected-4637" to be "success or failure"
Dec 11 03:04:50.993: INFO: Pod "pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.635695ms
Dec 11 03:04:53.016: INFO: Pod "pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044210067s
STEP: Saw pod success
Dec 11 03:04:53.016: INFO: Pod "pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3" satisfied condition "success or failure"
Dec 11 03:04:53.038: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:04:53.118: INFO: Waiting for pod pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3 to disappear
Dec 11 03:04:53.140: INFO: Pod pod-projected-secrets-785e0be1-8cb0-46d6-b860-6650d21d48a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:04:53.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4637" for this suite.
Dec 11 03:04:59.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:05:00.099: INFO: namespace projected-4637 deletion completed in 6.937173811s

â€¢ [SLOW TEST:9.268 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:05:00.099: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 11 03:05:00.187: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec 11 03:05:01.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-dd7d7f796\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:05:03.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711630281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-dd7d7f796\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:05:19.634: INFO: Waited 14.578127456s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:05:20.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2341" for this suite.
Dec 11 03:05:26.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:05:27.810: INFO: namespace aggregator-2341 deletion completed in 6.954518132s

â€¢ [SLOW TEST:27.711 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:05:27.811: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1484
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 11 03:05:27.902: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 11 03:05:50.327: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.0.200 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1484 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 03:05:50.327: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 03:05:51.714: INFO: Found all expected endpoints: [netserver-0]
Dec 11 03:05:51.737: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.1.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1484 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 03:05:51.737: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 03:05:53.079: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:05:53.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1484" for this suite.
Dec 11 03:06:15.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:06:16.043: INFO: namespace pod-network-test-1484 deletion completed in 22.941071352s

â€¢ [SLOW TEST:48.232 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:06:16.043: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:06:16.161: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44" in namespace "downward-api-5983" to be "success or failure"
Dec 11 03:06:16.183: INFO: Pod "downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44": Phase="Pending", Reason="", readiness=false. Elapsed: 22.176319ms
Dec 11 03:06:18.205: INFO: Pod "downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044685001s
Dec 11 03:06:20.228: INFO: Pod "downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067368888s
STEP: Saw pod success
Dec 11 03:06:20.228: INFO: Pod "downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44" satisfied condition "success or failure"
Dec 11 03:06:20.250: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44 container client-container: <nil>
STEP: delete the pod
Dec 11 03:06:20.325: INFO: Waiting for pod downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44 to disappear
Dec 11 03:06:20.346: INFO: Pod downwardapi-volume-57c9eea5-76f7-4d14-8870-a1f921a0ef44 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:06:20.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5983" for this suite.
Dec 11 03:06:26.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:06:27.297: INFO: namespace downward-api-5983 deletion completed in 6.929393175s

â€¢ [SLOW TEST:11.254 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:06:27.298: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-99cd2c74-5ee2-4386-915d-4cd665cd8b56
STEP: Creating a pod to test consume secrets
Dec 11 03:06:27.442: INFO: Waiting up to 5m0s for pod "pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2" in namespace "secrets-1914" to be "success or failure"
Dec 11 03:06:27.464: INFO: Pod "pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.851287ms
Dec 11 03:06:29.487: INFO: Pod "pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044881244s
Dec 11 03:06:31.510: INFO: Pod "pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067355104s
STEP: Saw pod success
Dec 11 03:06:31.510: INFO: Pod "pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2" satisfied condition "success or failure"
Dec 11 03:06:31.532: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2 container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:06:31.607: INFO: Waiting for pod pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2 to disappear
Dec 11 03:06:31.628: INFO: Pod pod-secrets-9e9149c2-e1bd-4917-b05f-b505d2e680d2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:06:31.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1914" for this suite.
Dec 11 03:06:37.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:06:38.619: INFO: namespace secrets-1914 deletion completed in 6.969255695s

â€¢ [SLOW TEST:11.322 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:06:38.620: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 11 03:06:42.871: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-761ea218-17d5-42b7-b30f-ab454f6a9905,GenerateName:,Namespace:events-9844,SelfLink:/api/v1/namespaces/events-9844/pods/send-events-761ea218-17d5-42b7-b30f-ab454f6a9905,UID:191963fd-b857-4698-af1e-1b208974dfc3,ResourceVersion:1022391,Generation:0,CreationTimestamp:2019-12-11 03:06:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 755236495,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-v7w5s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v7w5s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-v7w5s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003aa1400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003aa1420}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:06:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:06:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:06:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:06:19 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.85,StartTime:2019-12-11 03:06:38 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-11 03:06:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker://sha256:c6b8a28d5611cf83d297661ddd7f40672d286eafd7eb3852267e634e8eee0948 docker://54d3d681f66ba049550f5cebee42d277385ab6a4bf3e831000432c0447945a63}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec 11 03:06:44.894: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 11 03:06:46.917: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:06:46.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9844" for this suite.
Dec 11 03:07:25.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:07:25.898: INFO: namespace events-9844 deletion completed in 38.933673126s

â€¢ [SLOW TEST:47.278 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:07:25.898: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 11 03:07:26.015: INFO: Waiting up to 5m0s for pod "pod-8e2ae055-56ee-41a8-b7da-a5a240510452" in namespace "emptydir-5466" to be "success or failure"
Dec 11 03:07:26.037: INFO: Pod "pod-8e2ae055-56ee-41a8-b7da-a5a240510452": Phase="Pending", Reason="", readiness=false. Elapsed: 21.735298ms
Dec 11 03:07:28.062: INFO: Pod "pod-8e2ae055-56ee-41a8-b7da-a5a240510452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046465081s
STEP: Saw pod success
Dec 11 03:07:28.062: INFO: Pod "pod-8e2ae055-56ee-41a8-b7da-a5a240510452" satisfied condition "success or failure"
Dec 11 03:07:28.088: INFO: Trying to get logs from node 10.247.6.203 pod pod-8e2ae055-56ee-41a8-b7da-a5a240510452 container test-container: <nil>
STEP: delete the pod
Dec 11 03:07:28.163: INFO: Waiting for pod pod-8e2ae055-56ee-41a8-b7da-a5a240510452 to disappear
Dec 11 03:07:28.184: INFO: Pod pod-8e2ae055-56ee-41a8-b7da-a5a240510452 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:07:28.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5466" for this suite.
Dec 11 03:07:34.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:07:35.116: INFO: namespace emptydir-5466 deletion completed in 6.909364544s

â€¢ [SLOW TEST:9.218 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:07:35.116: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-l675
STEP: Creating a pod to test atomic-volume-subpath
Dec 11 03:07:35.286: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l675" in namespace "subpath-7163" to be "success or failure"
Dec 11 03:07:35.307: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Pending", Reason="", readiness=false. Elapsed: 21.541475ms
Dec 11 03:07:37.330: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044169517s
Dec 11 03:07:39.352: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 4.066614923s
Dec 11 03:07:41.376: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 6.089760277s
Dec 11 03:07:43.398: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 8.111993506s
Dec 11 03:07:45.420: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 10.134474538s
Dec 11 03:07:47.443: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 12.156789307s
Dec 11 03:07:49.465: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 14.179417557s
Dec 11 03:07:51.488: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 16.201820751s
Dec 11 03:07:53.510: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 18.224039579s
Dec 11 03:07:55.532: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 20.246367855s
Dec 11 03:07:57.555: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Running", Reason="", readiness=true. Elapsed: 22.268780018s
Dec 11 03:07:59.577: INFO: Pod "pod-subpath-test-downwardapi-l675": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.291463105s
STEP: Saw pod success
Dec 11 03:07:59.577: INFO: Pod "pod-subpath-test-downwardapi-l675" satisfied condition "success or failure"
Dec 11 03:07:59.599: INFO: Trying to get logs from node 10.247.6.203 pod pod-subpath-test-downwardapi-l675 container test-container-subpath-downwardapi-l675: <nil>
STEP: delete the pod
Dec 11 03:07:59.690: INFO: Waiting for pod pod-subpath-test-downwardapi-l675 to disappear
Dec 11 03:07:59.711: INFO: Pod pod-subpath-test-downwardapi-l675 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-l675
Dec 11 03:07:59.711: INFO: Deleting pod "pod-subpath-test-downwardapi-l675" in namespace "subpath-7163"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:07:59.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7163" for this suite.
Dec 11 03:08:05.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:08:06.666: INFO: namespace subpath-7163 deletion completed in 6.911128772s

â€¢ [SLOW TEST:31.550 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:08:06.666: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:08:06.787: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe" in namespace "projected-3174" to be "success or failure"
Dec 11 03:08:06.808: INFO: Pod "downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 21.768407ms
Dec 11 03:08:08.831: INFO: Pod "downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.044349338s
Dec 11 03:08:10.854: INFO: Pod "downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066993051s
STEP: Saw pod success
Dec 11 03:08:10.854: INFO: Pod "downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe" satisfied condition "success or failure"
Dec 11 03:08:10.875: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe container client-container: <nil>
STEP: delete the pod
Dec 11 03:08:10.948: INFO: Waiting for pod downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe to disappear
Dec 11 03:08:10.969: INFO: Pod downwardapi-volume-0df70db8-d338-4299-9097-e6f0c6a1e2fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:08:10.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3174" for this suite.
Dec 11 03:08:17.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:08:17.916: INFO: namespace projected-3174 deletion completed in 6.92469529s

â€¢ [SLOW TEST:11.249 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:08:17.916: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 11 03:08:22.211: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:22.233: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:24.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:24.256: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:26.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:26.255: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:28.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:28.255: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:30.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:30.255: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:32.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:32.255: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:34.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:34.255: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:36.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:36.255: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:38.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:38.256: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:40.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:40.256: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 11 03:08:42.233: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 11 03:08:42.256: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:08:42.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5317" for this suite.
Dec 11 03:09:04.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:09:05.260: INFO: namespace container-lifecycle-hook-5317 deletion completed in 22.934684566s

â€¢ [SLOW TEST:47.343 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:09:05.260: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Dec 11 03:09:05.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-8437'
Dec 11 03:09:05.978: INFO: stderr: ""
Dec 11 03:09:05.978: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec 11 03:09:07.001: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:09:07.001: INFO: Found 0 / 1
Dec 11 03:09:08.001: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:09:08.001: INFO: Found 0 / 1
Dec 11 03:09:09.001: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:09:09.001: INFO: Found 1 / 1
Dec 11 03:09:09.001: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 11 03:09:09.023: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:09:09.023: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec 11 03:09:09.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 logs redis-master-2sjxn redis-master --namespace=kubectl-8437'
Dec 11 03:09:09.238: INFO: stderr: ""
Dec 11 03:09:09.238: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Dec 03:09:07.798 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Dec 03:09:07.798 # Server started, Redis version 3.2.12\n1:M 11 Dec 03:09:07.798 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Dec 03:09:07.798 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec 11 03:09:09.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 log redis-master-2sjxn redis-master --namespace=kubectl-8437 --tail=1'
Dec 11 03:09:09.438: INFO: stderr: ""
Dec 11 03:09:09.438: INFO: stdout: "1:M 11 Dec 03:09:07.798 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec 11 03:09:09.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 log redis-master-2sjxn redis-master --namespace=kubectl-8437 --limit-bytes=1'
Dec 11 03:09:09.650: INFO: stderr: ""
Dec 11 03:09:09.650: INFO: stdout: " "
STEP: exposing timestamps
Dec 11 03:09:09.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 log redis-master-2sjxn redis-master --namespace=kubectl-8437 --tail=1 --timestamps'
Dec 11 03:09:09.851: INFO: stderr: ""
Dec 11 03:09:09.851: INFO: stdout: "2019-12-11T03:09:07.798652154Z 1:M 11 Dec 03:09:07.798 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec 11 03:09:12.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 log redis-master-2sjxn redis-master --namespace=kubectl-8437 --since=1s'
Dec 11 03:09:12.547: INFO: stderr: ""
Dec 11 03:09:12.547: INFO: stdout: ""
Dec 11 03:09:12.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 log redis-master-2sjxn redis-master --namespace=kubectl-8437 --since=24h'
Dec 11 03:09:12.769: INFO: stderr: ""
Dec 11 03:09:12.769: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 11 Dec 03:09:07.798 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 11 Dec 03:09:07.798 # Server started, Redis version 3.2.12\n1:M 11 Dec 03:09:07.798 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 11 Dec 03:09:07.798 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Dec 11 03:09:12.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-8437'
Dec 11 03:09:12.947: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:09:12.947: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec 11 03:09:12.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8437'
Dec 11 03:09:13.120: INFO: stderr: "No resources found.\n"
Dec 11 03:09:13.120: INFO: stdout: ""
Dec 11 03:09:13.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -l name=nginx --namespace=kubectl-8437 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 11 03:09:13.277: INFO: stderr: ""
Dec 11 03:09:13.277: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:09:13.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8437" for this suite.
Dec 11 03:09:19.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:09:20.236: INFO: namespace kubectl-8437 deletion completed in 6.937753758s

â€¢ [SLOW TEST:14.977 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:09:20.237: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 11 03:09:20.324: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:09:24.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1230" for this suite.
Dec 11 03:09:30.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:09:31.483: INFO: namespace init-container-1230 deletion completed in 6.911867352s

â€¢ [SLOW TEST:11.246 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:09:31.483: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-9a9f68d4-ea8a-4e8c-8da6-79d85cb48ea5
STEP: Creating a pod to test consume secrets
Dec 11 03:09:31.629: INFO: Waiting up to 5m0s for pod "pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e" in namespace "secrets-9654" to be "success or failure"
Dec 11 03:09:31.653: INFO: Pod "pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.093768ms
Dec 11 03:09:33.675: INFO: Pod "pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045400475s
STEP: Saw pod success
Dec 11 03:09:33.675: INFO: Pod "pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e" satisfied condition "success or failure"
Dec 11 03:09:33.697: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:09:33.772: INFO: Waiting for pod pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e to disappear
Dec 11 03:09:33.794: INFO: Pod pod-secrets-ed551eee-f1d9-4dcc-b151-dfb560715b3e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:09:33.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9654" for this suite.
Dec 11 03:09:39.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:09:40.757: INFO: namespace secrets-9654 deletion completed in 6.940072927s

â€¢ [SLOW TEST:9.274 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:09:40.757: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 11 03:09:40.888: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 11 03:09:48.091: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:09:48.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4272" for this suite.
Dec 11 03:09:54.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:09:55.074: INFO: namespace pods-4272 deletion completed in 6.939229316s

â€¢ [SLOW TEST:14.317 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:09:55.074: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 11 03:10:01.326: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:10:01.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1211 03:10:01.326608      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8192" for this suite.
Dec 11 03:10:07.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:10:08.313: INFO: namespace gc-8192 deletion completed in 6.954476722s

â€¢ [SLOW TEST:13.239 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:10:08.314: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec 11 03:10:08.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 cluster-info'
Dec 11 03:10:08.554: INFO: stderr: ""
Dec 11 03:10:08.554: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://192.168.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://192.168.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:10:08.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3849" for this suite.
Dec 11 03:10:14.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:10:15.529: INFO: namespace kubectl-3849 deletion completed in 6.95257373s

â€¢ [SLOW TEST:7.215 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:10:15.529: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c8df62f0-704d-4aa0-b9e9-4e04353efe82
STEP: Creating a pod to test consume secrets
Dec 11 03:10:15.668: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d" in namespace "projected-8006" to be "success or failure"
Dec 11 03:10:15.690: INFO: Pod "pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.653298ms
Dec 11 03:10:17.712: INFO: Pod "pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043837628s
Dec 11 03:10:19.735: INFO: Pod "pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067258855s
STEP: Saw pod success
Dec 11 03:10:19.735: INFO: Pod "pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d" satisfied condition "success or failure"
Dec 11 03:10:19.757: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:10:19.839: INFO: Waiting for pod pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d to disappear
Dec 11 03:10:19.861: INFO: Pod pod-projected-secrets-d8c893a9-ea73-4b8c-a234-9c8a17d9204d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:10:19.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8006" for this suite.
Dec 11 03:10:25.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:10:26.857: INFO: namespace projected-8006 deletion completed in 6.972907335s

â€¢ [SLOW TEST:11.328 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:10:26.857: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-1735
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1735
STEP: Deleting pre-stop pod
Dec 11 03:10:40.257: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:10:40.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1735" for this suite.
Dec 11 03:11:20.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:11:21.235: INFO: namespace prestop-1735 deletion completed in 40.92696739s

â€¢ [SLOW TEST:54.378 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:11:21.235: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:11:21.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5816" for this suite.
Dec 11 03:11:43.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:11:44.322: INFO: namespace kubelet-test-5816 deletion completed in 22.925688007s

â€¢ [SLOW TEST:23.087 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:11:44.322: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:11:44.437: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe" in namespace "projected-4899" to be "success or failure"
Dec 11 03:11:44.459: INFO: Pod "downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe": Phase="Pending", Reason="", readiness=false. Elapsed: 21.725402ms
Dec 11 03:11:46.483: INFO: Pod "downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04531126s
Dec 11 03:11:48.506: INFO: Pod "downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068397272s
STEP: Saw pod success
Dec 11 03:11:48.506: INFO: Pod "downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe" satisfied condition "success or failure"
Dec 11 03:11:48.529: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe container client-container: <nil>
STEP: delete the pod
Dec 11 03:11:48.604: INFO: Waiting for pod downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe to disappear
Dec 11 03:11:48.625: INFO: Pod downwardapi-volume-b8f0adad-f19e-4ba1-b07d-66869f48eafe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:11:48.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4899" for this suite.
Dec 11 03:11:54.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:11:55.597: INFO: namespace projected-4899 deletion completed in 6.949103582s

â€¢ [SLOW TEST:11.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:11:55.597: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:11:55.714: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0" in namespace "projected-6387" to be "success or failure"
Dec 11 03:11:55.736: INFO: Pod "downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.125405ms
Dec 11 03:11:57.758: INFO: Pod "downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044399889s
Dec 11 03:11:59.781: INFO: Pod "downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066949124s
STEP: Saw pod success
Dec 11 03:11:59.781: INFO: Pod "downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0" satisfied condition "success or failure"
Dec 11 03:11:59.803: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0 container client-container: <nil>
STEP: delete the pod
Dec 11 03:11:59.875: INFO: Waiting for pod downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0 to disappear
Dec 11 03:11:59.897: INFO: Pod downwardapi-volume-5e1970a9-9e64-4cd8-a3f7-93cce74eefa0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:11:59.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6387" for this suite.
Dec 11 03:12:05.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:12:06.824: INFO: namespace projected-6387 deletion completed in 6.90474573s

â€¢ [SLOW TEST:11.227 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:12:06.824: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 11 03:12:06.941: INFO: Waiting up to 5m0s for pod "pod-f4469f02-28ea-408c-82af-b3e02d932717" in namespace "emptydir-3726" to be "success or failure"
Dec 11 03:12:06.962: INFO: Pod "pod-f4469f02-28ea-408c-82af-b3e02d932717": Phase="Pending", Reason="", readiness=false. Elapsed: 21.443975ms
Dec 11 03:12:08.985: INFO: Pod "pod-f4469f02-28ea-408c-82af-b3e02d932717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04423793s
Dec 11 03:12:11.008: INFO: Pod "pod-f4469f02-28ea-408c-82af-b3e02d932717": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066916173s
STEP: Saw pod success
Dec 11 03:12:11.008: INFO: Pod "pod-f4469f02-28ea-408c-82af-b3e02d932717" satisfied condition "success or failure"
Dec 11 03:12:11.030: INFO: Trying to get logs from node 10.247.6.203 pod pod-f4469f02-28ea-408c-82af-b3e02d932717 container test-container: <nil>
STEP: delete the pod
Dec 11 03:12:11.112: INFO: Waiting for pod pod-f4469f02-28ea-408c-82af-b3e02d932717 to disappear
Dec 11 03:12:11.134: INFO: Pod pod-f4469f02-28ea-408c-82af-b3e02d932717 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:12:11.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3726" for this suite.
Dec 11 03:12:17.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:12:18.087: INFO: namespace emptydir-3726 deletion completed in 6.930768531s

â€¢ [SLOW TEST:11.263 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:12:18.087: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:12:18.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb" in namespace "projected-5033" to be "success or failure"
Dec 11 03:12:18.224: INFO: Pod "downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb": Phase="Pending", Reason="", readiness=false. Elapsed: 21.615435ms
Dec 11 03:12:20.247: INFO: Pod "downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044555969s
Dec 11 03:12:22.270: INFO: Pod "downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067300047s
STEP: Saw pod success
Dec 11 03:12:22.270: INFO: Pod "downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb" satisfied condition "success or failure"
Dec 11 03:12:22.292: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb container client-container: <nil>
STEP: delete the pod
Dec 11 03:12:22.367: INFO: Waiting for pod downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb to disappear
Dec 11 03:12:22.388: INFO: Pod downwardapi-volume-b9c0d83b-f7e5-45e2-aa5c-bff66b9d18eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:12:22.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5033" for this suite.
Dec 11 03:12:28.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:12:29.327: INFO: namespace projected-5033 deletion completed in 6.917271492s

â€¢ [SLOW TEST:11.240 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:12:29.328: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:12:29.419: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 11 03:12:30.578: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:12:30.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7116" for this suite.
Dec 11 03:12:36.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:12:37.543: INFO: namespace replication-controller-7116 deletion completed in 6.921280366s

â€¢ [SLOW TEST:8.215 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:12:37.558: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 11 03:12:37.781: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5656,SelfLink:/api/v1/namespaces/watch-5656/configmaps/e2e-watch-test-label-changed,UID:e4649d15-368e-4092-af13-937a74e89ca2,ResourceVersion:1023626,Generation:0,CreationTimestamp:2019-12-11 03:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 11 03:12:37.781: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5656,SelfLink:/api/v1/namespaces/watch-5656/configmaps/e2e-watch-test-label-changed,UID:e4649d15-368e-4092-af13-937a74e89ca2,ResourceVersion:1023627,Generation:0,CreationTimestamp:2019-12-11 03:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 11 03:12:37.781: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5656,SelfLink:/api/v1/namespaces/watch-5656/configmaps/e2e-watch-test-label-changed,UID:e4649d15-368e-4092-af13-937a74e89ca2,ResourceVersion:1023629,Generation:0,CreationTimestamp:2019-12-11 03:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 11 03:12:47.936: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5656,SelfLink:/api/v1/namespaces/watch-5656/configmaps/e2e-watch-test-label-changed,UID:e4649d15-368e-4092-af13-937a74e89ca2,ResourceVersion:1023654,Generation:0,CreationTimestamp:2019-12-11 03:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 11 03:12:47.936: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5656,SelfLink:/api/v1/namespaces/watch-5656/configmaps/e2e-watch-test-label-changed,UID:e4649d15-368e-4092-af13-937a74e89ca2,ResourceVersion:1023655,Generation:0,CreationTimestamp:2019-12-11 03:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 11 03:12:47.936: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5656,SelfLink:/api/v1/namespaces/watch-5656/configmaps/e2e-watch-test-label-changed,UID:e4649d15-368e-4092-af13-937a74e89ca2,ResourceVersion:1023656,Generation:0,CreationTimestamp:2019-12-11 03:12:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:12:47.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5656" for this suite.
Dec 11 03:12:54.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:12:54.888: INFO: namespace watch-5656 deletion completed in 6.929748207s

â€¢ [SLOW TEST:17.330 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:12:54.888: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 11 03:12:55.004: INFO: Waiting up to 5m0s for pod "pod-b5f26f6e-407d-4f6e-b54f-e724e768319f" in namespace "emptydir-9393" to be "success or failure"
Dec 11 03:12:55.026: INFO: Pod "pod-b5f26f6e-407d-4f6e-b54f-e724e768319f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.583878ms
Dec 11 03:12:57.051: INFO: Pod "pod-b5f26f6e-407d-4f6e-b54f-e724e768319f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046259685s
STEP: Saw pod success
Dec 11 03:12:57.051: INFO: Pod "pod-b5f26f6e-407d-4f6e-b54f-e724e768319f" satisfied condition "success or failure"
Dec 11 03:12:57.072: INFO: Trying to get logs from node 10.247.6.203 pod pod-b5f26f6e-407d-4f6e-b54f-e724e768319f container test-container: <nil>
STEP: delete the pod
Dec 11 03:12:57.146: INFO: Waiting for pod pod-b5f26f6e-407d-4f6e-b54f-e724e768319f to disappear
Dec 11 03:12:57.167: INFO: Pod pod-b5f26f6e-407d-4f6e-b54f-e724e768319f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:12:57.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9393" for this suite.
Dec 11 03:13:03.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:13:04.141: INFO: namespace emptydir-9393 deletion completed in 6.95226565s

â€¢ [SLOW TEST:9.253 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:13:04.142: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 11 03:13:08.945: INFO: Successfully updated pod "annotationupdatecb6d35be-4ede-406a-a60c-20fef10bbfdd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:13:11.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4904" for this suite.
Dec 11 03:13:33.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:13:33.985: INFO: namespace downward-api-4904 deletion completed in 22.905732524s

â€¢ [SLOW TEST:29.844 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:13:33.985: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 11 03:13:34.073: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:13:38.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1486" for this suite.
Dec 11 03:14:00.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:14:01.440: INFO: namespace init-container-1486 deletion completed in 22.93135969s

â€¢ [SLOW TEST:27.454 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:14:01.440: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-60400ba5-3b67-4ebd-8763-8aded2b8dedd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:14:05.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-25" for this suite.
Dec 11 03:14:27.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:14:28.733: INFO: namespace configmap-25 deletion completed in 22.909806371s

â€¢ [SLOW TEST:27.293 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:14:28.734: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 03:14:28.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-373'
Dec 11 03:14:29.652: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 11 03:14:29.652: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Dec 11 03:14:29.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete jobs e2e-test-nginx-job --namespace=kubectl-373'
Dec 11 03:14:29.867: INFO: stderr: ""
Dec 11 03:14:29.867: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:14:29.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-373" for this suite.
Dec 11 03:14:51.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:14:52.792: INFO: namespace kubectl-373 deletion completed in 22.902619804s

â€¢ [SLOW TEST:24.058 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:14:52.792: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5196.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5196.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 11 03:14:57.129: INFO: File jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-d7cd753f-e4b7-4cda-9595-14ad9e0b1a8f contains '' instead of 'foo.example.com.'
Dec 11 03:14:57.129: INFO: Lookups using dns-5196/dns-test-d7cd753f-e4b7-4cda-9595-14ad9e0b1a8f failed for: [jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local]

Dec 11 03:15:02.215: INFO: DNS probes using dns-test-d7cd753f-e4b7-4cda-9595-14ad9e0b1a8f succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5196.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5196.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 11 03:15:08.474: INFO: File wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:08.517: INFO: File jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:08.517: INFO: Lookups using dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 failed for: [wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local]

Dec 11 03:15:13.560: INFO: File wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:13.603: INFO: File jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains '' instead of 'bar.example.com.'
Dec 11 03:15:13.603: INFO: Lookups using dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 failed for: [wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local]

Dec 11 03:15:18.560: INFO: File wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:18.603: INFO: File jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:18.603: INFO: Lookups using dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 failed for: [wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local]

Dec 11 03:15:23.561: INFO: File wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:23.603: INFO: File jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local from pod  dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 11 03:15:23.603: INFO: Lookups using dns-5196/dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 failed for: [wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local]

Dec 11 03:15:28.603: INFO: DNS probes using dns-test-fa2a7bd1-1b19-491a-b471-7c7627ecfa07 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5196.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5196.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5196.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5196.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 11 03:15:34.925: INFO: DNS probes using dns-test-c3dc41b8-d6b2-425a-a2bd-fe80b53b487a succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:15:34.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5196" for this suite.
Dec 11 03:15:41.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:15:41.949: INFO: namespace dns-5196 deletion completed in 6.948049358s

â€¢ [SLOW TEST:49.157 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:15:41.950: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-e4379653-da6c-451b-9728-411e0c80659d
STEP: Creating a pod to test consume configMaps
Dec 11 03:15:42.100: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405" in namespace "projected-7660" to be "success or failure"
Dec 11 03:15:42.122: INFO: Pod "pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405": Phase="Pending", Reason="", readiness=false. Elapsed: 21.554648ms
Dec 11 03:15:44.144: INFO: Pod "pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044090806s
STEP: Saw pod success
Dec 11 03:15:44.144: INFO: Pod "pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405" satisfied condition "success or failure"
Dec 11 03:15:44.166: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 03:15:44.243: INFO: Waiting for pod pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405 to disappear
Dec 11 03:15:44.264: INFO: Pod pod-projected-configmaps-fb00ea20-c329-4090-ba15-45bdac044405 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:15:44.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7660" for this suite.
Dec 11 03:15:50.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:15:51.212: INFO: namespace projected-7660 deletion completed in 6.92525454s

â€¢ [SLOW TEST:9.262 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:15:51.212: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec 11 03:15:51.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 --namespace=kubectl-1806 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 11 03:15:55.016: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 11 03:15:55.016: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:15:57.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1806" for this suite.
Dec 11 03:16:03.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:16:04.020: INFO: namespace kubectl-1806 deletion completed in 6.937401004s

â€¢ [SLOW TEST:12.807 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:16:04.020: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5n48z in namespace proxy-9825
I1211 03:16:04.162132      18 runners.go:180] Created replication controller with name: proxy-service-5n48z, namespace: proxy-9825, replica count: 1
I1211 03:16:05.212423      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1211 03:16:06.212678      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1211 03:16:07.212832      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:08.213013      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:09.213225      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:10.213424      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:11.213635      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:12.213844      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:13.214048      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:14.214254      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1211 03:16:15.214403      18 runners.go:180] proxy-service-5n48z Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 11 03:16:15.236: INFO: setup took 11.126117443s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 11 03:16:15.321: INFO: (0) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 84.965631ms)
Dec 11 03:16:15.321: INFO: (0) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 85.011489ms)
Dec 11 03:16:15.321: INFO: (0) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 85.224313ms)
Dec 11 03:16:15.325: INFO: (0) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 89.517022ms)
Dec 11 03:16:15.325: INFO: (0) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 89.196751ms)
Dec 11 03:16:15.325: INFO: (0) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 89.257371ms)
Dec 11 03:16:15.326: INFO: (0) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 89.512317ms)
Dec 11 03:16:15.342: INFO: (0) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 105.504317ms)
Dec 11 03:16:15.342: INFO: (0) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 105.451618ms)
Dec 11 03:16:15.342: INFO: (0) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 105.510909ms)
Dec 11 03:16:15.342: INFO: (0) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 105.509824ms)
Dec 11 03:16:15.366: INFO: (0) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 130.522413ms)
Dec 11 03:16:15.368: INFO: (0) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 131.314798ms)
Dec 11 03:16:15.369: INFO: (0) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 132.522993ms)
Dec 11 03:16:15.369: INFO: (0) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 133.135549ms)
Dec 11 03:16:15.372: INFO: (0) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 135.499868ms)
Dec 11 03:16:15.414: INFO: (1) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 42.493782ms)
Dec 11 03:16:15.417: INFO: (1) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 45.052155ms)
Dec 11 03:16:15.417: INFO: (1) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 45.156589ms)
Dec 11 03:16:15.418: INFO: (1) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 45.713403ms)
Dec 11 03:16:15.418: INFO: (1) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 45.706305ms)
Dec 11 03:16:15.418: INFO: (1) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 45.830313ms)
Dec 11 03:16:15.418: INFO: (1) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 45.741454ms)
Dec 11 03:16:15.418: INFO: (1) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 45.755473ms)
Dec 11 03:16:15.418: INFO: (1) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 45.835585ms)
Dec 11 03:16:15.419: INFO: (1) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 46.681916ms)
Dec 11 03:16:15.419: INFO: (1) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 46.652681ms)
Dec 11 03:16:15.435: INFO: (1) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 62.588029ms)
Dec 11 03:16:15.438: INFO: (1) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 66.234778ms)
Dec 11 03:16:15.439: INFO: (1) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 66.640588ms)
Dec 11 03:16:15.439: INFO: (1) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 66.583062ms)
Dec 11 03:16:15.439: INFO: (1) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 66.848346ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.65541ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 44.499397ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 44.269329ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 44.523902ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 44.544005ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 44.677523ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.283482ms)
Dec 11 03:16:15.484: INFO: (2) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 44.499812ms)
Dec 11 03:16:15.485: INFO: (2) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 45.179454ms)
Dec 11 03:16:15.485: INFO: (2) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 45.251203ms)
Dec 11 03:16:15.485: INFO: (2) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 45.066979ms)
Dec 11 03:16:15.485: INFO: (2) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 45.328635ms)
Dec 11 03:16:15.504: INFO: (2) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 63.986831ms)
Dec 11 03:16:15.505: INFO: (2) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 65.212638ms)
Dec 11 03:16:15.505: INFO: (2) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 65.962323ms)
Dec 11 03:16:15.505: INFO: (2) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 65.774376ms)
Dec 11 03:16:15.549: INFO: (3) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.60715ms)
Dec 11 03:16:15.549: INFO: (3) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 44.129397ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.908568ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 44.009423ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.976062ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.213318ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 44.53457ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 44.329439ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 44.531754ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.76578ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 44.660535ms)
Dec 11 03:16:15.550: INFO: (3) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 44.83813ms)
Dec 11 03:16:15.570: INFO: (3) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 63.878676ms)
Dec 11 03:16:15.570: INFO: (3) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 64.887873ms)
Dec 11 03:16:15.570: INFO: (3) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 64.669232ms)
Dec 11 03:16:15.571: INFO: (3) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 64.986937ms)
Dec 11 03:16:15.614: INFO: (4) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 43.408404ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.820941ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 44.051885ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.961867ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.059932ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.975567ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 44.17928ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 44.301183ms)
Dec 11 03:16:15.615: INFO: (4) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 44.314169ms)
Dec 11 03:16:15.616: INFO: (4) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 45.374797ms)
Dec 11 03:16:15.616: INFO: (4) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 45.15765ms)
Dec 11 03:16:15.616: INFO: (4) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 45.276598ms)
Dec 11 03:16:15.635: INFO: (4) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 64.18009ms)
Dec 11 03:16:15.635: INFO: (4) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 64.163918ms)
Dec 11 03:16:15.635: INFO: (4) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 64.280751ms)
Dec 11 03:16:15.637: INFO: (4) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 65.706888ms)
Dec 11 03:16:15.680: INFO: (5) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.307344ms)
Dec 11 03:16:15.680: INFO: (5) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.448619ms)
Dec 11 03:16:15.680: INFO: (5) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.343723ms)
Dec 11 03:16:15.680: INFO: (5) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.337543ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.580121ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 43.874285ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 43.65927ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.547279ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 44.017522ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.373346ms)
Dec 11 03:16:15.681: INFO: (5) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 44.506593ms)
Dec 11 03:16:15.699: INFO: (5) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 61.865431ms)
Dec 11 03:16:15.700: INFO: (5) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 63.575537ms)
Dec 11 03:16:15.701: INFO: (5) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 64.470948ms)
Dec 11 03:16:15.701: INFO: (5) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 64.241728ms)
Dec 11 03:16:15.701: INFO: (5) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 64.539027ms)
Dec 11 03:16:15.746: INFO: (6) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 44.170383ms)
Dec 11 03:16:15.746: INFO: (6) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 44.432739ms)
Dec 11 03:16:15.746: INFO: (6) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 44.531007ms)
Dec 11 03:16:15.746: INFO: (6) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.544797ms)
Dec 11 03:16:15.746: INFO: (6) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 44.786763ms)
Dec 11 03:16:15.746: INFO: (6) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 44.617449ms)
Dec 11 03:16:15.747: INFO: (6) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.949534ms)
Dec 11 03:16:15.747: INFO: (6) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 45.01946ms)
Dec 11 03:16:15.747: INFO: (6) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 45.019574ms)
Dec 11 03:16:15.747: INFO: (6) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 45.105425ms)
Dec 11 03:16:15.747: INFO: (6) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 45.008451ms)
Dec 11 03:16:15.747: INFO: (6) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 45.192926ms)
Dec 11 03:16:15.766: INFO: (6) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 64.393999ms)
Dec 11 03:16:15.766: INFO: (6) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 64.6727ms)
Dec 11 03:16:15.766: INFO: (6) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 64.970466ms)
Dec 11 03:16:15.767: INFO: (6) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 65.237273ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 43.676041ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.971581ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.75962ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 43.406197ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.562918ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 43.85988ms)
Dec 11 03:16:15.811: INFO: (7) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.646107ms)
Dec 11 03:16:15.812: INFO: (7) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 44.32789ms)
Dec 11 03:16:15.812: INFO: (7) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 45.135421ms)
Dec 11 03:16:15.812: INFO: (7) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.54944ms)
Dec 11 03:16:15.813: INFO: (7) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 45.395414ms)
Dec 11 03:16:15.813: INFO: (7) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 45.469945ms)
Dec 11 03:16:15.831: INFO: (7) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 63.659517ms)
Dec 11 03:16:15.831: INFO: (7) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 63.620133ms)
Dec 11 03:16:15.832: INFO: (7) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 65.052724ms)
Dec 11 03:16:15.832: INFO: (7) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 64.620621ms)
Dec 11 03:16:15.874: INFO: (8) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 42.043977ms)
Dec 11 03:16:15.875: INFO: (8) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 42.439381ms)
Dec 11 03:16:15.875: INFO: (8) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 42.470023ms)
Dec 11 03:16:15.875: INFO: (8) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 42.766773ms)
Dec 11 03:16:15.875: INFO: (8) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 42.903447ms)
Dec 11 03:16:15.876: INFO: (8) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.182915ms)
Dec 11 03:16:15.876: INFO: (8) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 43.018757ms)
Dec 11 03:16:15.876: INFO: (8) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 42.88524ms)
Dec 11 03:16:15.876: INFO: (8) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 43.85213ms)
Dec 11 03:16:15.877: INFO: (8) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.901293ms)
Dec 11 03:16:15.877: INFO: (8) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 43.818375ms)
Dec 11 03:16:15.877: INFO: (8) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.100934ms)
Dec 11 03:16:15.895: INFO: (8) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 63.098969ms)
Dec 11 03:16:15.896: INFO: (8) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 62.790599ms)
Dec 11 03:16:15.896: INFO: (8) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 63.116676ms)
Dec 11 03:16:15.896: INFO: (8) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 63.462275ms)
Dec 11 03:16:15.938: INFO: (9) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 42.155808ms)
Dec 11 03:16:15.938: INFO: (9) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 41.936571ms)
Dec 11 03:16:15.938: INFO: (9) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 42.009673ms)
Dec 11 03:16:15.939: INFO: (9) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 42.764492ms)
Dec 11 03:16:15.939: INFO: (9) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 42.600945ms)
Dec 11 03:16:15.939: INFO: (9) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 42.666402ms)
Dec 11 03:16:15.939: INFO: (9) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 42.959459ms)
Dec 11 03:16:15.939: INFO: (9) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 42.801891ms)
Dec 11 03:16:15.940: INFO: (9) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 42.821987ms)
Dec 11 03:16:15.940: INFO: (9) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 43.554896ms)
Dec 11 03:16:15.940: INFO: (9) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 43.906684ms)
Dec 11 03:16:15.958: INFO: (9) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 61.285144ms)
Dec 11 03:16:15.958: INFO: (9) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 62.014034ms)
Dec 11 03:16:15.960: INFO: (9) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 63.594218ms)
Dec 11 03:16:15.960: INFO: (9) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 63.380232ms)
Dec 11 03:16:15.960: INFO: (9) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 62.971492ms)
Dec 11 03:16:16.003: INFO: (10) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.421703ms)
Dec 11 03:16:16.003: INFO: (10) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.250423ms)
Dec 11 03:16:16.003: INFO: (10) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.206562ms)
Dec 11 03:16:16.003: INFO: (10) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.80281ms)
Dec 11 03:16:16.004: INFO: (10) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 43.619263ms)
Dec 11 03:16:16.004: INFO: (10) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.412299ms)
Dec 11 03:16:16.004: INFO: (10) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 43.215293ms)
Dec 11 03:16:16.004: INFO: (10) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.248133ms)
Dec 11 03:16:16.004: INFO: (10) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.19608ms)
Dec 11 03:16:16.005: INFO: (10) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 44.459033ms)
Dec 11 03:16:16.005: INFO: (10) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.427444ms)
Dec 11 03:16:16.022: INFO: (10) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 61.542853ms)
Dec 11 03:16:16.024: INFO: (10) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 63.849062ms)
Dec 11 03:16:16.024: INFO: (10) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 64.163542ms)
Dec 11 03:16:16.024: INFO: (10) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 63.698776ms)
Dec 11 03:16:16.024: INFO: (10) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 63.543634ms)
Dec 11 03:16:16.073: INFO: (11) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 48.688294ms)
Dec 11 03:16:16.073: INFO: (11) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 48.304294ms)
Dec 11 03:16:16.073: INFO: (11) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 48.782248ms)
Dec 11 03:16:16.073: INFO: (11) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 49.242362ms)
Dec 11 03:16:16.074: INFO: (11) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 48.994056ms)
Dec 11 03:16:16.074: INFO: (11) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 49.340273ms)
Dec 11 03:16:16.074: INFO: (11) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 49.252366ms)
Dec 11 03:16:16.074: INFO: (11) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 49.206211ms)
Dec 11 03:16:16.074: INFO: (11) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 49.397465ms)
Dec 11 03:16:16.077: INFO: (11) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 52.380351ms)
Dec 11 03:16:16.077: INFO: (11) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 52.705317ms)
Dec 11 03:16:16.077: INFO: (11) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 52.678296ms)
Dec 11 03:16:16.093: INFO: (11) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 69.374593ms)
Dec 11 03:16:16.094: INFO: (11) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 69.393609ms)
Dec 11 03:16:16.094: INFO: (11) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 69.568977ms)
Dec 11 03:16:16.094: INFO: (11) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 69.29703ms)
Dec 11 03:16:16.140: INFO: (12) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 46.216338ms)
Dec 11 03:16:16.140: INFO: (12) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 46.098277ms)
Dec 11 03:16:16.140: INFO: (12) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 45.800616ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 46.137682ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 46.432219ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 45.947937ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 46.502471ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 46.043529ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 46.220902ms)
Dec 11 03:16:16.141: INFO: (12) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 46.297928ms)
Dec 11 03:16:16.142: INFO: (12) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 46.681253ms)
Dec 11 03:16:16.142: INFO: (12) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 46.839733ms)
Dec 11 03:16:16.161: INFO: (12) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 66.07552ms)
Dec 11 03:16:16.161: INFO: (12) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 65.805644ms)
Dec 11 03:16:16.161: INFO: (12) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 66.399896ms)
Dec 11 03:16:16.162: INFO: (12) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 67.169446ms)
Dec 11 03:16:16.205: INFO: (13) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 42.74452ms)
Dec 11 03:16:16.205: INFO: (13) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 42.891686ms)
Dec 11 03:16:16.205: INFO: (13) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.049234ms)
Dec 11 03:16:16.205: INFO: (13) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 43.177638ms)
Dec 11 03:16:16.205: INFO: (13) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 42.870306ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.638842ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 43.898655ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.130841ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.823048ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 43.877941ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 43.790013ms)
Dec 11 03:16:16.206: INFO: (13) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 44.21488ms)
Dec 11 03:16:16.225: INFO: (13) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 62.838936ms)
Dec 11 03:16:16.225: INFO: (13) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 62.944874ms)
Dec 11 03:16:16.225: INFO: (13) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 62.7751ms)
Dec 11 03:16:16.226: INFO: (13) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 63.689588ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.374371ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.70782ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.491816ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 43.478401ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 43.511621ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.50834ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.648411ms)
Dec 11 03:16:16.270: INFO: (14) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.49072ms)
Dec 11 03:16:16.271: INFO: (14) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 44.677406ms)
Dec 11 03:16:16.271: INFO: (14) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 44.848434ms)
Dec 11 03:16:16.271: INFO: (14) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.826328ms)
Dec 11 03:16:16.288: INFO: (14) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 62.095813ms)
Dec 11 03:16:16.290: INFO: (14) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 64.019935ms)
Dec 11 03:16:16.290: INFO: (14) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 63.984625ms)
Dec 11 03:16:16.290: INFO: (14) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 64.093671ms)
Dec 11 03:16:16.290: INFO: (14) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 64.082865ms)
Dec 11 03:16:16.333: INFO: (15) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 42.400051ms)
Dec 11 03:16:16.333: INFO: (15) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 42.182647ms)
Dec 11 03:16:16.333: INFO: (15) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 42.634296ms)
Dec 11 03:16:16.334: INFO: (15) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 42.634112ms)
Dec 11 03:16:16.334: INFO: (15) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 42.805371ms)
Dec 11 03:16:16.334: INFO: (15) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 42.977578ms)
Dec 11 03:16:16.334: INFO: (15) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.049912ms)
Dec 11 03:16:16.335: INFO: (15) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.433403ms)
Dec 11 03:16:16.335: INFO: (15) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 43.65186ms)
Dec 11 03:16:16.335: INFO: (15) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.506002ms)
Dec 11 03:16:16.335: INFO: (15) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 44.272261ms)
Dec 11 03:16:16.353: INFO: (15) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 61.679213ms)
Dec 11 03:16:16.353: INFO: (15) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 62.782968ms)
Dec 11 03:16:16.354: INFO: (15) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 62.648421ms)
Dec 11 03:16:16.355: INFO: (15) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 63.71344ms)
Dec 11 03:16:16.355: INFO: (15) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 64.078059ms)
Dec 11 03:16:16.398: INFO: (16) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 42.550181ms)
Dec 11 03:16:16.398: INFO: (16) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 42.858301ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 43.655935ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.414411ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 43.205919ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 43.272548ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.045805ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 43.345651ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 43.911526ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 44.0268ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 43.534538ms)
Dec 11 03:16:16.399: INFO: (16) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 44.030002ms)
Dec 11 03:16:16.419: INFO: (16) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 62.911134ms)
Dec 11 03:16:16.419: INFO: (16) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 64.174968ms)
Dec 11 03:16:16.419: INFO: (16) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 63.764184ms)
Dec 11 03:16:16.420: INFO: (16) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 63.718647ms)
Dec 11 03:16:16.463: INFO: (17) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.351115ms)
Dec 11 03:16:16.463: INFO: (17) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 43.208746ms)
Dec 11 03:16:16.463: INFO: (17) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.76977ms)
Dec 11 03:16:16.463: INFO: (17) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 43.488802ms)
Dec 11 03:16:16.463: INFO: (17) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.58392ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.812205ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 43.919427ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 43.95769ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 44.028747ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 44.450403ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 44.356855ms)
Dec 11 03:16:16.464: INFO: (17) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.657005ms)
Dec 11 03:16:16.483: INFO: (17) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 63.389423ms)
Dec 11 03:16:16.483: INFO: (17) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 63.49689ms)
Dec 11 03:16:16.483: INFO: (17) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 63.59795ms)
Dec 11 03:16:16.483: INFO: (17) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 63.742599ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 43.840703ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 44.024835ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 44.460893ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 43.751133ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 43.719072ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 44.35843ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 44.004552ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 43.912924ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 44.227959ms)
Dec 11 03:16:16.528: INFO: (18) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 44.194333ms)
Dec 11 03:16:16.529: INFO: (18) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 45.044863ms)
Dec 11 03:16:16.529: INFO: (18) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 45.302027ms)
Dec 11 03:16:16.548: INFO: (18) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 64.346432ms)
Dec 11 03:16:16.548: INFO: (18) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 64.018415ms)
Dec 11 03:16:16.549: INFO: (18) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 64.860129ms)
Dec 11 03:16:16.549: INFO: (18) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 65.051968ms)
Dec 11 03:16:16.594: INFO: (19) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:160/proxy/: foo (200; 44.538212ms)
Dec 11 03:16:16.594: INFO: (19) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:1080/proxy/rewriteme">test<... (200; 44.284094ms)
Dec 11 03:16:16.594: INFO: (19) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:1080/proxy/rewriteme">... (200; 44.505415ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:460/proxy/: tls baz (200; 45.0232ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:162/proxy/: bar (200; 45.292024ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:462/proxy/: tls qux (200; 45.666393ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname2/proxy/: tls qux (200; 45.610393ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/services/https:proxy-service-5n48z:tlsportname1/proxy/: tls baz (200; 45.716601ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn:160/proxy/: foo (200; 45.705083ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/http:proxy-service-5n48z-4crrn:162/proxy/: bar (200; 45.800583ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/proxy-service-5n48z-4crrn/proxy/rewriteme">test</a> (200; 45.671316ms)
Dec 11 03:16:16.595: INFO: (19) /api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/: <a href="/api/v1/namespaces/proxy-9825/pods/https:proxy-service-5n48z-4crrn:443/proxy/tlsrewritem... (200; 45.964619ms)
Dec 11 03:16:16.614: INFO: (19) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname1/proxy/: foo (200; 65.004716ms)
Dec 11 03:16:16.615: INFO: (19) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname1/proxy/: foo (200; 65.39215ms)
Dec 11 03:16:16.615: INFO: (19) /api/v1/namespaces/proxy-9825/services/http:proxy-service-5n48z:portname2/proxy/: bar (200; 65.663372ms)
Dec 11 03:16:16.615: INFO: (19) /api/v1/namespaces/proxy-9825/services/proxy-service-5n48z:portname2/proxy/: bar (200; 65.924188ms)
STEP: deleting ReplicationController proxy-service-5n48z in namespace proxy-9825, will wait for the garbage collector to delete the pods
Dec 11 03:16:16.711: INFO: Deleting ReplicationController proxy-service-5n48z took: 23.859351ms
Dec 11 03:16:17.111: INFO: Terminating ReplicationController proxy-service-5n48z pods took: 400.170817ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:16:18.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9825" for this suite.
Dec 11 03:16:24.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:16:25.644: INFO: namespace proxy-9825 deletion completed in 6.910427005s

â€¢ [SLOW TEST:21.624 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:16:25.644: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec 11 03:16:25.733: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-194493201 proxy --unix-socket=/tmp/kubectl-proxy-unix231907376/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:16:25.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2043" for this suite.
Dec 11 03:16:31.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:16:32.720: INFO: namespace kubectl-2043 deletion completed in 6.910404091s

â€¢ [SLOW TEST:7.076 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:16:32.720: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Dec 11 03:16:32.974: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1211 03:16:32.974345      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:16:32.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-444" for this suite.
Dec 11 03:16:39.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:16:39.925: INFO: namespace gc-444 deletion completed in 6.92980691s

â€¢ [SLOW TEST:7.205 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:16:39.926: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:16:40.044: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a" in namespace "downward-api-4495" to be "success or failure"
Dec 11 03:16:40.066: INFO: Pod "downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.88657ms
Dec 11 03:16:42.088: INFO: Pod "downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044379014s
Dec 11 03:16:44.111: INFO: Pod "downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067104859s
STEP: Saw pod success
Dec 11 03:16:44.111: INFO: Pod "downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a" satisfied condition "success or failure"
Dec 11 03:16:44.133: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a container client-container: <nil>
STEP: delete the pod
Dec 11 03:16:44.209: INFO: Waiting for pod downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a to disappear
Dec 11 03:16:44.230: INFO: Pod downwardapi-volume-ab383b84-66fc-4763-b7e3-a267976ae33a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:16:44.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4495" for this suite.
Dec 11 03:16:50.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:16:51.173: INFO: namespace downward-api-4495 deletion completed in 6.920651425s

â€¢ [SLOW TEST:11.247 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:16:51.173: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 11 03:16:53.958: INFO: Successfully updated pod "labelsupdatef7c75b29-f98c-42e4-b657-c063bd53c7e8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:16:56.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3192" for this suite.
Dec 11 03:17:18.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:17:18.991: INFO: namespace projected-3192 deletion completed in 22.916195806s

â€¢ [SLOW TEST:27.818 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:17:18.991: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5938
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5938
STEP: Creating statefulset with conflicting port in namespace statefulset-5938
STEP: Waiting until pod test-pod will start running in namespace statefulset-5938
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5938
Dec 11 03:17:23.249: INFO: Observed stateful pod in namespace: statefulset-5938, name: ss-0, uid: 05dc618a-5e63-4e7c-a4ca-82a5b996b0e9, status phase: Pending. Waiting for statefulset controller to delete.
Dec 11 03:17:23.387: INFO: Observed stateful pod in namespace: statefulset-5938, name: ss-0, uid: 05dc618a-5e63-4e7c-a4ca-82a5b996b0e9, status phase: Failed. Waiting for statefulset controller to delete.
Dec 11 03:17:23.395: INFO: Observed stateful pod in namespace: statefulset-5938, name: ss-0, uid: 05dc618a-5e63-4e7c-a4ca-82a5b996b0e9, status phase: Failed. Waiting for statefulset controller to delete.
Dec 11 03:17:23.398: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5938
STEP: Removing pod with conflicting port in namespace statefulset-5938
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5938 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 11 03:17:27.512: INFO: Deleting all statefulset in ns statefulset-5938
Dec 11 03:17:27.534: INFO: Scaling statefulset ss to 0
Dec 11 03:17:37.623: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 03:17:37.645: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:17:37.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5938" for this suite.
Dec 11 03:17:43.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:17:44.654: INFO: namespace statefulset-5938 deletion completed in 6.920654516s

â€¢ [SLOW TEST:25.663 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:17:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:17:48.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2796" for this suite.
Dec 11 03:18:28.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:18:29.844: INFO: namespace kubelet-test-2796 deletion completed in 40.917913166s

â€¢ [SLOW TEST:45.189 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:18:29.844: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 11 03:18:32.650: INFO: Successfully updated pod "annotationupdate240c0a4a-10a8-4ec7-8530-76f185113304"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:18:34.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1098" for this suite.
Dec 11 03:18:56.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:18:57.697: INFO: namespace projected-1098 deletion completed in 22.928002695s

â€¢ [SLOW TEST:27.853 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:18:57.698: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 03:18:57.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9397'
Dec 11 03:18:57.952: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 11 03:18:57.952: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Dec 11 03:19:00.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9397'
Dec 11 03:19:00.187: INFO: stderr: ""
Dec 11 03:19:00.187: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:19:00.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9397" for this suite.
Dec 11 03:19:22.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:19:23.146: INFO: namespace kubectl-9397 deletion completed in 22.936432756s

â€¢ [SLOW TEST:25.448 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:19:23.146: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:19:23.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f" in namespace "projected-3726" to be "success or failure"
Dec 11 03:19:23.283: INFO: Pod "downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.550191ms
Dec 11 03:19:25.305: INFO: Pod "downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043747007s
STEP: Saw pod success
Dec 11 03:19:25.305: INFO: Pod "downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f" satisfied condition "success or failure"
Dec 11 03:19:25.327: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f container client-container: <nil>
STEP: delete the pod
Dec 11 03:19:25.401: INFO: Waiting for pod downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f to disappear
Dec 11 03:19:25.422: INFO: Pod downwardapi-volume-e6704351-53c2-4f4c-8996-43f8dc1faf5f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:19:25.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3726" for this suite.
Dec 11 03:19:31.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:19:32.365: INFO: namespace projected-3726 deletion completed in 6.920897637s

â€¢ [SLOW TEST:9.219 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:19:32.365: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2095
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2095
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2095
Dec 11 03:19:32.550: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec 11 03:19:42.572: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 11 03:19:42.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 03:19:43.130: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 03:19:43.130: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 03:19:43.130: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 03:19:43.153: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 11 03:19:53.176: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 03:19:53.176: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 03:19:53.265: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999478s
Dec 11 03:19:54.288: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.977512566s
Dec 11 03:19:55.311: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.954720767s
Dec 11 03:19:56.335: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.93171412s
Dec 11 03:19:57.358: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.907034839s
Dec 11 03:19:58.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.884410764s
Dec 11 03:19:59.403: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.861587605s
Dec 11 03:20:00.426: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.839050018s
Dec 11 03:20:01.449: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.816058617s
Dec 11 03:20:02.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 793.439517ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2095
Dec 11 03:20:03.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 03:20:04.000: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 03:20:04.000: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 03:20:04.000: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 03:20:04.022: INFO: Found 1 stateful pods, waiting for 3
Dec 11 03:20:14.045: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 03:20:14.045: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 11 03:20:14.045: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 11 03:20:14.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 03:20:14.589: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 03:20:14.589: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 03:20:14.589: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 03:20:14.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 03:20:14.970: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 03:20:14.970: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 03:20:14.970: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 03:20:14.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 11 03:20:15.466: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 11 03:20:15.466: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 11 03:20:15.466: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 11 03:20:15.466: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 03:20:15.488: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 11 03:20:25.533: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 03:20:25.533: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 03:20:25.533: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 11 03:20:25.601: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999344s
Dec 11 03:20:26.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.977109405s
Dec 11 03:20:27.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.953407994s
Dec 11 03:20:28.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.930530412s
Dec 11 03:20:29.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.907673675s
Dec 11 03:20:30.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.885097222s
Dec 11 03:20:31.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.862407882s
Dec 11 03:20:32.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.839647189s
Dec 11 03:20:33.783: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.817072254s
Dec 11 03:20:34.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 794.359922ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2095
Dec 11 03:20:35.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 03:20:36.323: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 03:20:36.323: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 03:20:36.323: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 03:20:36.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 03:20:36.728: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 03:20:36.728: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 03:20:36.728: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 03:20:36.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 exec --namespace=statefulset-2095 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 11 03:20:37.225: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 11 03:20:37.225: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 11 03:20:37.225: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 11 03:20:37.225: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 11 03:20:57.314: INFO: Deleting all statefulset in ns statefulset-2095
Dec 11 03:20:57.336: INFO: Scaling statefulset ss to 0
Dec 11 03:20:57.403: INFO: Waiting for statefulset status.replicas updated to 0
Dec 11 03:20:57.424: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:20:57.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2095" for this suite.
Dec 11 03:21:03.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:21:04.421: INFO: namespace statefulset-2095 deletion completed in 6.909004676s

â€¢ [SLOW TEST:92.056 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:21:04.422: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-5b2a0419-e3bb-452b-a31d-9b9b06c753cf
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:21:04.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9650" for this suite.
Dec 11 03:21:10.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:21:11.479: INFO: namespace secrets-9650 deletion completed in 6.92527923s

â€¢ [SLOW TEST:7.058 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:21:11.480: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:21:37.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2906" for this suite.
Dec 11 03:21:43.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:21:44.823: INFO: namespace namespaces-2906 deletion completed in 6.939427202s
STEP: Destroying namespace "nsdeletetest-8838" for this suite.
Dec 11 03:21:44.845: INFO: Namespace nsdeletetest-8838 was already deleted
STEP: Destroying namespace "nsdeletetest-6154" for this suite.
Dec 11 03:21:50.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:21:51.775: INFO: namespace nsdeletetest-6154 deletion completed in 6.929878044s

â€¢ [SLOW TEST:40.296 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:21:51.775: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 11 03:21:51.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-7265'
Dec 11 03:21:52.144: INFO: stderr: ""
Dec 11 03:21:52.144: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 11 03:21:53.167: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:21:53.167: INFO: Found 0 / 1
Dec 11 03:21:54.167: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:21:54.167: INFO: Found 0 / 1
Dec 11 03:21:55.167: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:21:55.167: INFO: Found 1 / 1
Dec 11 03:21:55.167: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 11 03:21:55.189: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:21:55.189: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 11 03:21:55.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 patch pod redis-master-9lwjl --namespace=kubectl-7265 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 11 03:21:55.367: INFO: stderr: ""
Dec 11 03:21:55.367: INFO: stdout: "pod/redis-master-9lwjl patched\n"
STEP: checking annotations
Dec 11 03:21:55.389: INFO: Selector matched 1 pods for map[app:redis]
Dec 11 03:21:55.389: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:21:55.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7265" for this suite.
Dec 11 03:22:17.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:22:18.341: INFO: namespace kubectl-7265 deletion completed in 22.930333866s

â€¢ [SLOW TEST:26.566 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:22:18.341: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3436/configmap-test-4d5c7412-2323-48c2-ba7e-4dd19ece53fd
STEP: Creating a pod to test consume configMaps
Dec 11 03:22:18.478: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71" in namespace "configmap-3436" to be "success or failure"
Dec 11 03:22:18.499: INFO: Pod "pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71": Phase="Pending", Reason="", readiness=false. Elapsed: 21.571067ms
Dec 11 03:22:20.522: INFO: Pod "pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044239602s
Dec 11 03:22:22.545: INFO: Pod "pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066769098s
STEP: Saw pod success
Dec 11 03:22:22.545: INFO: Pod "pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71" satisfied condition "success or failure"
Dec 11 03:22:22.566: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71 container env-test: <nil>
STEP: delete the pod
Dec 11 03:22:22.662: INFO: Waiting for pod pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71 to disappear
Dec 11 03:22:22.683: INFO: Pod pod-configmaps-5a21a01c-1039-4bf3-9d0a-44cde6a97c71 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:22:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3436" for this suite.
Dec 11 03:22:28.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:22:29.635: INFO: namespace configmap-3436 deletion completed in 6.929585749s

â€¢ [SLOW TEST:11.294 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:22:29.636: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 11 03:22:29.777: INFO: Waiting up to 5m0s for pod "pod-9e19fd68-42a8-4b7c-9280-a2b95071266c" in namespace "emptydir-282" to be "success or failure"
Dec 11 03:22:29.799: INFO: Pod "pod-9e19fd68-42a8-4b7c-9280-a2b95071266c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.095883ms
Dec 11 03:22:31.822: INFO: Pod "pod-9e19fd68-42a8-4b7c-9280-a2b95071266c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044617688s
STEP: Saw pod success
Dec 11 03:22:31.822: INFO: Pod "pod-9e19fd68-42a8-4b7c-9280-a2b95071266c" satisfied condition "success or failure"
Dec 11 03:22:31.843: INFO: Trying to get logs from node 10.247.6.203 pod pod-9e19fd68-42a8-4b7c-9280-a2b95071266c container test-container: <nil>
STEP: delete the pod
Dec 11 03:22:31.917: INFO: Waiting for pod pod-9e19fd68-42a8-4b7c-9280-a2b95071266c to disappear
Dec 11 03:22:31.939: INFO: Pod pod-9e19fd68-42a8-4b7c-9280-a2b95071266c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:22:31.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-282" for this suite.
Dec 11 03:22:38.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:22:38.885: INFO: namespace emptydir-282 deletion completed in 6.923497913s

â€¢ [SLOW TEST:9.248 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:22:38.885: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-5q6t
STEP: Creating a pod to test atomic-volume-subpath
Dec 11 03:22:39.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5q6t" in namespace "subpath-480" to be "success or failure"
Dec 11 03:22:39.071: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Pending", Reason="", readiness=false. Elapsed: 21.718608ms
Dec 11 03:22:41.094: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 2.044609132s
Dec 11 03:22:43.116: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 4.066764044s
Dec 11 03:22:45.138: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 6.088961031s
Dec 11 03:22:47.161: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 8.111593915s
Dec 11 03:22:49.183: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 10.133878622s
Dec 11 03:22:51.205: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 12.156141747s
Dec 11 03:22:53.228: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 14.178627384s
Dec 11 03:22:55.250: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 16.200994527s
Dec 11 03:22:57.272: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 18.223477243s
Dec 11 03:22:59.295: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Running", Reason="", readiness=true. Elapsed: 20.245768433s
Dec 11 03:23:01.317: INFO: Pod "pod-subpath-test-configmap-5q6t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.268299089s
STEP: Saw pod success
Dec 11 03:23:01.317: INFO: Pod "pod-subpath-test-configmap-5q6t" satisfied condition "success or failure"
Dec 11 03:23:01.339: INFO: Trying to get logs from node 10.247.6.203 pod pod-subpath-test-configmap-5q6t container test-container-subpath-configmap-5q6t: <nil>
STEP: delete the pod
Dec 11 03:23:01.430: INFO: Waiting for pod pod-subpath-test-configmap-5q6t to disappear
Dec 11 03:23:01.452: INFO: Pod pod-subpath-test-configmap-5q6t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5q6t
Dec 11 03:23:01.452: INFO: Deleting pod "pod-subpath-test-configmap-5q6t" in namespace "subpath-480"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:23:01.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-480" for this suite.
Dec 11 03:23:07.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:23:08.400: INFO: namespace subpath-480 deletion completed in 6.904321325s

â€¢ [SLOW TEST:29.515 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:23:08.400: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec 11 03:23:08.487: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 11 03:23:08.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1394'
Dec 11 03:23:08.762: INFO: stderr: ""
Dec 11 03:23:08.762: INFO: stdout: "service/redis-slave created\n"
Dec 11 03:23:08.762: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 11 03:23:08.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1394'
Dec 11 03:23:09.073: INFO: stderr: ""
Dec 11 03:23:09.073: INFO: stdout: "service/redis-master created\n"
Dec 11 03:23:09.073: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 11 03:23:09.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1394'
Dec 11 03:23:09.351: INFO: stderr: ""
Dec 11 03:23:09.351: INFO: stdout: "service/frontend created\n"
Dec 11 03:23:09.351: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 11 03:23:09.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1394'
Dec 11 03:23:09.630: INFO: stderr: ""
Dec 11 03:23:09.630: INFO: stdout: "deployment.apps/frontend created\n"
Dec 11 03:23:09.630: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 11 03:23:09.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1394'
Dec 11 03:23:09.917: INFO: stderr: ""
Dec 11 03:23:09.917: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 11 03:23:09.917: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 11 03:23:09.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-1394'
Dec 11 03:23:10.224: INFO: stderr: ""
Dec 11 03:23:10.224: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 11 03:23:10.224: INFO: Waiting for all frontend pods to be Running.
Dec 11 03:23:15.275: INFO: Waiting for frontend to serve content.
Dec 11 03:23:15.308: INFO: Trying to add a new entry to the guestbook.
Dec 11 03:23:16.441: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 11 03:23:16.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-1394'
Dec 11 03:23:17.121: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:23:17.121: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 11 03:23:17.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-1394'
Dec 11 03:23:17.311: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:23:17.311: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 11 03:23:17.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-1394'
Dec 11 03:23:17.537: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:23:17.537: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 11 03:23:17.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-1394'
Dec 11 03:23:17.718: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:23:17.718: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 11 03:23:17.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-1394'
Dec 11 03:23:17.892: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:23:17.892: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 11 03:23:17.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-1394'
Dec 11 03:23:18.068: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:23:18.068: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:23:18.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1394" for this suite.
Dec 11 03:24:04.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:24:05.001: INFO: namespace kubectl-1394 deletion completed in 46.909758443s

â€¢ [SLOW TEST:56.601 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:24:05.001: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:24:09.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8380" for this suite.
Dec 11 03:24:15.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:24:16.266: INFO: namespace emptydir-wrapper-8380 deletion completed in 6.934298522s

â€¢ [SLOW TEST:11.265 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:24:16.267: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-ec7cc401-4b0b-4019-a4cb-3d98b1479811
STEP: Creating configMap with name cm-test-opt-upd-b252d60c-a96c-4c99-9afb-1b18156e7474
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ec7cc401-4b0b-4019-a4cb-3d98b1479811
STEP: Updating configmap cm-test-opt-upd-b252d60c-a96c-4c99-9afb-1b18156e7474
STEP: Creating configMap with name cm-test-opt-create-7e9b99cd-71a0-4a61-b9cd-f380b5878116
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:24:22.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3776" for this suite.
Dec 11 03:24:45.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:24:45.931: INFO: namespace projected-3776 deletion completed in 22.933923951s

â€¢ [SLOW TEST:29.664 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:24:45.931: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:24:46.130: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 11 03:24:46.175: INFO: Number of nodes with available pods: 0
Dec 11 03:24:46.175: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 11 03:24:46.273: INFO: Number of nodes with available pods: 0
Dec 11 03:24:46.273: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:47.298: INFO: Number of nodes with available pods: 0
Dec 11 03:24:47.298: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:48.295: INFO: Number of nodes with available pods: 1
Dec 11 03:24:48.295: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 11 03:24:48.392: INFO: Number of nodes with available pods: 0
Dec 11 03:24:48.392: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 11 03:24:48.436: INFO: Number of nodes with available pods: 0
Dec 11 03:24:48.436: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:49.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:49.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:50.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:50.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:51.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:51.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:52.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:52.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:53.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:53.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:54.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:54.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:55.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:55.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:56.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:56.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:57.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:57.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:58.480: INFO: Number of nodes with available pods: 0
Dec 11 03:24:58.480: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:24:59.459: INFO: Number of nodes with available pods: 0
Dec 11 03:24:59.459: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:25:00.462: INFO: Number of nodes with available pods: 1
Dec 11 03:25:00.462: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-894, will wait for the garbage collector to delete the pods
Dec 11 03:25:00.604: INFO: Deleting DaemonSet.extensions daemon-set took: 23.837468ms
Dec 11 03:25:01.005: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.175793ms
Dec 11 03:25:04.527: INFO: Number of nodes with available pods: 0
Dec 11 03:25:04.527: INFO: Number of running nodes: 0, number of available pods: 0
Dec 11 03:25:04.549: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-894/daemonsets","resourceVersion":"1026169"},"items":null}

Dec 11 03:25:04.571: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-894/pods","resourceVersion":"1026169"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:25:04.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-894" for this suite.
Dec 11 03:25:10.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:25:11.607: INFO: namespace daemonsets-894 deletion completed in 6.917742904s

â€¢ [SLOW TEST:25.677 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:25:11.608: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7844.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7844.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 11 03:25:16.242: INFO: DNS probes using dns-7844/dns-test-ed985dbe-66fc-40d3-ae27-1e8cdb86315b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:25:16.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7844" for this suite.
Dec 11 03:25:22.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:25:23.229: INFO: namespace dns-7844 deletion completed in 6.934893114s

â€¢ [SLOW TEST:11.621 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:25:23.229: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec 11 03:25:23.355: INFO: Waiting up to 5m0s for pod "client-containers-57729549-47a4-4fb1-9cac-46218ec5f904" in namespace "containers-9140" to be "success or failure"
Dec 11 03:25:23.376: INFO: Pod "client-containers-57729549-47a4-4fb1-9cac-46218ec5f904": Phase="Pending", Reason="", readiness=false. Elapsed: 21.451102ms
Dec 11 03:25:25.399: INFO: Pod "client-containers-57729549-47a4-4fb1-9cac-46218ec5f904": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044418471s
Dec 11 03:25:27.422: INFO: Pod "client-containers-57729549-47a4-4fb1-9cac-46218ec5f904": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066942974s
STEP: Saw pod success
Dec 11 03:25:27.422: INFO: Pod "client-containers-57729549-47a4-4fb1-9cac-46218ec5f904" satisfied condition "success or failure"
Dec 11 03:25:27.444: INFO: Trying to get logs from node 10.247.6.203 pod client-containers-57729549-47a4-4fb1-9cac-46218ec5f904 container test-container: <nil>
STEP: delete the pod
Dec 11 03:25:27.517: INFO: Waiting for pod client-containers-57729549-47a4-4fb1-9cac-46218ec5f904 to disappear
Dec 11 03:25:27.539: INFO: Pod client-containers-57729549-47a4-4fb1-9cac-46218ec5f904 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:25:27.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9140" for this suite.
Dec 11 03:25:33.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:25:34.471: INFO: namespace containers-9140 deletion completed in 6.909786717s

â€¢ [SLOW TEST:11.242 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:25:34.471: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec 11 03:25:34.589: INFO: Waiting up to 5m0s for pod "client-containers-32282427-f2de-4058-8cbb-157d183ceb54" in namespace "containers-8879" to be "success or failure"
Dec 11 03:25:34.611: INFO: Pod "client-containers-32282427-f2de-4058-8cbb-157d183ceb54": Phase="Pending", Reason="", readiness=false. Elapsed: 21.678851ms
Dec 11 03:25:36.635: INFO: Pod "client-containers-32282427-f2de-4058-8cbb-157d183ceb54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045681189s
Dec 11 03:25:38.657: INFO: Pod "client-containers-32282427-f2de-4058-8cbb-157d183ceb54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068015048s
STEP: Saw pod success
Dec 11 03:25:38.657: INFO: Pod "client-containers-32282427-f2de-4058-8cbb-157d183ceb54" satisfied condition "success or failure"
Dec 11 03:25:38.679: INFO: Trying to get logs from node 10.247.6.203 pod client-containers-32282427-f2de-4058-8cbb-157d183ceb54 container test-container: <nil>
STEP: delete the pod
Dec 11 03:25:38.787: INFO: Waiting for pod client-containers-32282427-f2de-4058-8cbb-157d183ceb54 to disappear
Dec 11 03:25:38.810: INFO: Pod client-containers-32282427-f2de-4058-8cbb-157d183ceb54 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:25:38.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8879" for this suite.
Dec 11 03:25:44.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:25:45.749: INFO: namespace containers-8879 deletion completed in 6.915055705s

â€¢ [SLOW TEST:11.278 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:25:45.749: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:25:45.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97" in namespace "projected-3912" to be "success or failure"
Dec 11 03:25:45.886: INFO: Pod "downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97": Phase="Pending", Reason="", readiness=false. Elapsed: 21.688615ms
Dec 11 03:25:47.909: INFO: Pod "downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044216416s
Dec 11 03:25:49.931: INFO: Pod "downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06626307s
STEP: Saw pod success
Dec 11 03:25:49.931: INFO: Pod "downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97" satisfied condition "success or failure"
Dec 11 03:25:49.952: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97 container client-container: <nil>
STEP: delete the pod
Dec 11 03:25:50.027: INFO: Waiting for pod downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97 to disappear
Dec 11 03:25:50.048: INFO: Pod downwardapi-volume-e31b934f-6efb-4584-9b08-538501ce0d97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:25:50.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3912" for this suite.
Dec 11 03:25:56.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:25:56.992: INFO: namespace projected-3912 deletion completed in 6.921201821s

â€¢ [SLOW TEST:11.243 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:25:56.992: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 11 03:26:00.225: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:26:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5167" for this suite.
Dec 11 03:26:06.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:26:07.322: INFO: namespace container-runtime-5167 deletion completed in 7.024429926s

â€¢ [SLOW TEST:10.330 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:26:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-0a2a3def-d86c-4aba-ae36-287a31ffbff1
STEP: Creating a pod to test consume configMaps
Dec 11 03:26:07.470: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6" in namespace "projected-4691" to be "success or failure"
Dec 11 03:26:07.492: INFO: Pod "pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.648841ms
Dec 11 03:26:09.515: INFO: Pod "pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044446922s
Dec 11 03:26:11.538: INFO: Pod "pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067526387s
STEP: Saw pod success
Dec 11 03:26:11.538: INFO: Pod "pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6" satisfied condition "success or failure"
Dec 11 03:26:11.560: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 03:26:11.635: INFO: Waiting for pod pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6 to disappear
Dec 11 03:26:11.657: INFO: Pod pod-projected-configmaps-685a4e91-f11a-45c6-b8be-f24e8b76e4d6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:26:11.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4691" for this suite.
Dec 11 03:26:17.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:26:18.617: INFO: namespace projected-4691 deletion completed in 6.937639412s

â€¢ [SLOW TEST:11.295 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:26:18.618: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:26:18.707: INFO: Creating deployment "test-recreate-deployment"
Dec 11 03:26:18.729: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 11 03:26:18.775: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 11 03:26:18.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5b4fd7849f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:26:20.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711631559, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-5b4fd7849f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:26:22.819: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 11 03:26:22.864: INFO: Updating deployment test-recreate-deployment
Dec 11 03:26:22.864: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 11 03:26:22.943: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3967,SelfLink:/apis/apps/v1/namespaces/deployment-3967/deployments/test-recreate-deployment,UID:358f85fb-ee0a-4375-bf75-7df06a802a80,ResourceVersion:1026486,Generation:2,CreationTimestamp:2019-12-11 03:25:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-11 03:26:03 +0000 UTC 2019-12-11 03:26:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-11 03:26:03 +0000 UTC 2019-12-11 03:25:59 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-64cbdf85c6" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec 11 03:26:22.966: INFO: New ReplicaSet "test-recreate-deployment-64cbdf85c6" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-64cbdf85c6,GenerateName:,Namespace:deployment-3967,SelfLink:/apis/apps/v1/namespaces/deployment-3967/replicasets/test-recreate-deployment-64cbdf85c6,UID:33dd2b48-ec81-498d-bfb1-f419a06b6fef,ResourceVersion:1026485,Generation:1,CreationTimestamp:2019-12-11 03:26:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 64cbdf85c6,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/internalStrategyType: Recreate,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 358f85fb-ee0a-4375-bf75-7df06a802a80 0xc0029643ff 0xc002964410}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 64cbdf85c6,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 64cbdf85c6,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 11 03:26:22.966: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 11 03:26:22.966: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5b4fd7849f,GenerateName:,Namespace:deployment-3967,SelfLink:/apis/apps/v1/namespaces/deployment-3967/replicasets/test-recreate-deployment-5b4fd7849f,UID:e829c1ce-f0fe-4376-be8f-e06ed7b01754,ResourceVersion:1026477,Generation:2,CreationTimestamp:2019-12-11 03:25:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5b4fd7849f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/internalStrategyType: Recreate,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 358f85fb-ee0a-4375-bf75-7df06a802a80 0xc00296431f 0xc002964330}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b4fd7849f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5b4fd7849f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 11 03:26:22.989: INFO: Pod "test-recreate-deployment-64cbdf85c6-jgll5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-64cbdf85c6-jgll5,GenerateName:test-recreate-deployment-64cbdf85c6-,Namespace:deployment-3967,SelfLink:/api/v1/namespaces/deployment-3967/pods/test-recreate-deployment-64cbdf85c6-jgll5,UID:a9fed622-d2a8-4ac7-9322-87c9c54b0ebf,ResourceVersion:1026487,Generation:0,CreationTimestamp:2019-12-11 03:26:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 64cbdf85c6,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-64cbdf85c6 33dd2b48-ec81-498d-bfb1-f419a06b6fef 0xc002964d07 0xc002964d08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r4tpx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r4tpx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-r4tpx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002964d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002964db0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:26:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:26:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:26:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:26:03 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:26:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:26:22.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3967" for this suite.
Dec 11 03:26:29.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:26:29.954: INFO: namespace deployment-3967 deletion completed in 6.942443561s

â€¢ [SLOW TEST:11.336 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:26:29.954: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 11 03:26:30.076: INFO: Waiting up to 5m0s for pod "downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07" in namespace "downward-api-6122" to be "success or failure"
Dec 11 03:26:30.097: INFO: Pod "downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07": Phase="Pending", Reason="", readiness=false. Elapsed: 21.708576ms
Dec 11 03:26:32.120: INFO: Pod "downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04452609s
Dec 11 03:26:34.143: INFO: Pod "downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067538355s
STEP: Saw pod success
Dec 11 03:26:34.143: INFO: Pod "downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07" satisfied condition "success or failure"
Dec 11 03:26:34.165: INFO: Trying to get logs from node 10.247.6.203 pod downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07 container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:26:34.258: INFO: Waiting for pod downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07 to disappear
Dec 11 03:26:34.279: INFO: Pod downward-api-7ca7bbce-3de5-45f7-9f0e-a7cfb71c3b07 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:26:34.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6122" for this suite.
Dec 11 03:26:40.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:26:41.223: INFO: namespace downward-api-6122 deletion completed in 6.921559297s

â€¢ [SLOW TEST:11.269 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:26:41.223: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec 11 03:27:11.500: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1211 03:27:11.500167      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:27:11.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8793" for this suite.
Dec 11 03:27:17.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:27:18.465: INFO: namespace gc-8793 deletion completed in 6.942479042s

â€¢ [SLOW TEST:37.242 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:27:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 03:27:18.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-176'
Dec 11 03:27:19.326: INFO: stderr: ""
Dec 11 03:27:19.326: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Dec 11 03:27:19.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete pods e2e-test-nginx-pod --namespace=kubectl-176'
Dec 11 03:27:28.415: INFO: stderr: ""
Dec 11 03:27:28.415: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:27:28.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-176" for this suite.
Dec 11 03:27:34.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:27:35.384: INFO: namespace kubectl-176 deletion completed in 6.946751154s

â€¢ [SLOW TEST:16.919 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:27:35.384: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-a15aa92f-a672-48ec-81c3-9916ea5d6daf
STEP: Creating a pod to test consume configMaps
Dec 11 03:27:35.522: INFO: Waiting up to 5m0s for pod "pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657" in namespace "configmap-4606" to be "success or failure"
Dec 11 03:27:35.544: INFO: Pod "pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657": Phase="Pending", Reason="", readiness=false. Elapsed: 21.932316ms
Dec 11 03:27:37.566: INFO: Pod "pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044329529s
Dec 11 03:27:39.589: INFO: Pod "pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066868067s
STEP: Saw pod success
Dec 11 03:27:39.589: INFO: Pod "pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657" satisfied condition "success or failure"
Dec 11 03:27:39.611: INFO: Trying to get logs from node 10.247.6.203 pod pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 03:27:39.687: INFO: Waiting for pod pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657 to disappear
Dec 11 03:27:39.709: INFO: Pod pod-configmaps-61f02bc7-27a5-4121-b69f-49008e40b657 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:27:39.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4606" for this suite.
Dec 11 03:27:45.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:27:46.663: INFO: namespace configmap-4606 deletion completed in 6.931689762s

â€¢ [SLOW TEST:11.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:27:46.663: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-7938
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7938 to expose endpoints map[]
Dec 11 03:27:46.799: INFO: successfully validated that service endpoint-test2 in namespace services-7938 exposes endpoints map[] (21.924104ms elapsed)
STEP: Creating pod pod1 in namespace services-7938
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7938 to expose endpoints map[pod1:[80]]
Dec 11 03:27:48.960: INFO: successfully validated that service endpoint-test2 in namespace services-7938 exposes endpoints map[pod1:[80]] (2.13227926s elapsed)
STEP: Creating pod pod2 in namespace services-7938
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7938 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 11 03:27:52.250: INFO: successfully validated that service endpoint-test2 in namespace services-7938 exposes endpoints map[pod1:[80] pod2:[80]] (3.263792743s elapsed)
STEP: Deleting pod pod1 in namespace services-7938
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7938 to expose endpoints map[pod2:[80]]
Dec 11 03:27:52.316: INFO: successfully validated that service endpoint-test2 in namespace services-7938 exposes endpoints map[pod2:[80]] (43.42238ms elapsed)
STEP: Deleting pod pod2 in namespace services-7938
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7938 to expose endpoints map[]
Dec 11 03:27:52.365: INFO: successfully validated that service endpoint-test2 in namespace services-7938 exposes endpoints map[] (21.57136ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:27:52.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7938" for this suite.
Dec 11 03:28:14.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:28:15.364: INFO: namespace services-7938 deletion completed in 22.946295138s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:28.701 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:28:15.365: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 11 03:28:15.693: INFO: Number of nodes with available pods: 0
Dec 11 03:28:15.693: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:16.739: INFO: Number of nodes with available pods: 0
Dec 11 03:28:16.739: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:17.738: INFO: Number of nodes with available pods: 1
Dec 11 03:28:17.738: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:18.738: INFO: Number of nodes with available pods: 2
Dec 11 03:28:18.738: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 11 03:28:18.850: INFO: Number of nodes with available pods: 1
Dec 11 03:28:18.850: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:19.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:19.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:20.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:20.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:21.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:21.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:22.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:22.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:23.897: INFO: Number of nodes with available pods: 1
Dec 11 03:28:23.897: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:24.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:24.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:25.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:25.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:26.894: INFO: Number of nodes with available pods: 1
Dec 11 03:28:26.894: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:27.898: INFO: Number of nodes with available pods: 1
Dec 11 03:28:27.898: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:28.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:28.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:29.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:29.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:30.895: INFO: Number of nodes with available pods: 1
Dec 11 03:28:30.895: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:28:31.895: INFO: Number of nodes with available pods: 2
Dec 11 03:28:31.895: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9285, will wait for the garbage collector to delete the pods
Dec 11 03:28:32.012: INFO: Deleting DaemonSet.extensions daemon-set took: 24.240097ms
Dec 11 03:28:32.413: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.234265ms
Dec 11 03:28:39.734: INFO: Number of nodes with available pods: 0
Dec 11 03:28:39.734: INFO: Number of running nodes: 0, number of available pods: 0
Dec 11 03:28:39.756: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9285/daemonsets","resourceVersion":"1026974"},"items":null}

Dec 11 03:28:39.778: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9285/pods","resourceVersion":"1026974"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:28:39.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9285" for this suite.
Dec 11 03:28:45.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:28:46.773: INFO: namespace daemonsets-9285 deletion completed in 6.908485199s

â€¢ [SLOW TEST:31.409 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:28:46.774: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:28:46.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35" in namespace "projected-8206" to be "success or failure"
Dec 11 03:28:46.916: INFO: Pod "downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35": Phase="Pending", Reason="", readiness=false. Elapsed: 22.601143ms
Dec 11 03:28:48.938: INFO: Pod "downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045249393s
Dec 11 03:28:50.961: INFO: Pod "downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067997561s
STEP: Saw pod success
Dec 11 03:28:50.961: INFO: Pod "downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35" satisfied condition "success or failure"
Dec 11 03:28:50.983: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35 container client-container: <nil>
STEP: delete the pod
Dec 11 03:28:51.057: INFO: Waiting for pod downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35 to disappear
Dec 11 03:28:51.081: INFO: Pod downwardapi-volume-92504de4-e315-4028-9717-b9c53ef4af35 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:28:51.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8206" for this suite.
Dec 11 03:28:57.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:28:58.012: INFO: namespace projected-8206 deletion completed in 6.909597488s

â€¢ [SLOW TEST:11.239 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:28:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 11 03:28:58.136: INFO: Waiting up to 5m0s for pod "downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd" in namespace "downward-api-370" to be "success or failure"
Dec 11 03:28:58.158: INFO: Pod "downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd": Phase="Pending", Reason="", readiness=false. Elapsed: 21.426624ms
Dec 11 03:29:00.180: INFO: Pod "downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0440672s
Dec 11 03:29:02.203: INFO: Pod "downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067170885s
STEP: Saw pod success
Dec 11 03:29:02.203: INFO: Pod "downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd" satisfied condition "success or failure"
Dec 11 03:29:02.226: INFO: Trying to get logs from node 10.247.6.203 pod downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:29:02.316: INFO: Waiting for pod downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd to disappear
Dec 11 03:29:02.338: INFO: Pod downward-api-63df1ca8-c300-4cc8-a2de-84ab4ced31dd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:29:02.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-370" for this suite.
Dec 11 03:29:08.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:29:09.267: INFO: namespace downward-api-370 deletion completed in 6.907419286s

â€¢ [SLOW TEST:11.255 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:29:09.268: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 11 03:29:14.049: INFO: Successfully updated pod "labelsupdatee5b80eab-45df-43be-be19-00d79c513561"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:29:16.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3447" for this suite.
Dec 11 03:29:38.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:29:39.094: INFO: namespace downward-api-3447 deletion completed in 22.928442759s

â€¢ [SLOW TEST:29.827 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:29:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6587
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6587 to expose endpoints map[]
Dec 11 03:29:39.275: INFO: successfully validated that service multi-endpoint-test in namespace services-6587 exposes endpoints map[] (21.572425ms elapsed)
STEP: Creating pod pod1 in namespace services-6587
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6587 to expose endpoints map[pod1:[100]]
Dec 11 03:29:41.434: INFO: successfully validated that service multi-endpoint-test in namespace services-6587 exposes endpoints map[pod1:[100]] (2.131445646s elapsed)
STEP: Creating pod pod2 in namespace services-6587
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6587 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 11 03:29:43.663: INFO: successfully validated that service multi-endpoint-test in namespace services-6587 exposes endpoints map[pod1:[100] pod2:[101]] (2.201886422s elapsed)
STEP: Deleting pod pod1 in namespace services-6587
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6587 to expose endpoints map[pod2:[101]]
Dec 11 03:29:43.733: INFO: successfully validated that service multi-endpoint-test in namespace services-6587 exposes endpoints map[pod2:[101]] (47.041345ms elapsed)
STEP: Deleting pod pod2 in namespace services-6587
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6587 to expose endpoints map[]
Dec 11 03:29:43.778: INFO: successfully validated that service multi-endpoint-test in namespace services-6587 exposes endpoints map[] (21.470636ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:29:43.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6587" for this suite.
Dec 11 03:30:05.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:30:06.753: INFO: namespace services-6587 deletion completed in 22.919935166s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:27.658 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:30:06.753: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:30:06.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8463" for this suite.
Dec 11 03:30:12.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:30:13.804: INFO: namespace services-8463 deletion completed in 6.920200357s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

â€¢ [SLOW TEST:7.052 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:30:13.805: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 11 03:30:13.980: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027287,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 11 03:30:13.980: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027287,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 11 03:30:24.025: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027319,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 11 03:30:24.025: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027319,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 11 03:30:34.072: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027343,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 11 03:30:34.072: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027343,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 11 03:30:44.096: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027366,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 11 03:30:44.097: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-a,UID:69a1b7aa-ac18-42d6-a5f1-79b547f970a1,ResourceVersion:1027366,Generation:0,CreationTimestamp:2019-12-11 03:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 11 03:30:54.121: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-b,UID:b406ac38-2d48-4680-b56e-8d9fe58a34e1,ResourceVersion:1027390,Generation:0,CreationTimestamp:2019-12-11 03:30:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 11 03:30:54.121: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-b,UID:b406ac38-2d48-4680-b56e-8d9fe58a34e1,ResourceVersion:1027390,Generation:0,CreationTimestamp:2019-12-11 03:30:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 11 03:31:04.145: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-b,UID:b406ac38-2d48-4680-b56e-8d9fe58a34e1,ResourceVersion:1027413,Generation:0,CreationTimestamp:2019-12-11 03:30:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 11 03:31:04.145: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7236,SelfLink:/api/v1/namespaces/watch-7236/configmaps/e2e-watch-test-configmap-b,UID:b406ac38-2d48-4680-b56e-8d9fe58a34e1,ResourceVersion:1027413,Generation:0,CreationTimestamp:2019-12-11 03:30:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:31:14.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7236" for this suite.
Dec 11 03:31:20.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:31:21.112: INFO: namespace watch-7236 deletion completed in 6.943216517s

â€¢ [SLOW TEST:67.308 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:31:21.113: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 11 03:31:29.476: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:29.499: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:31.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:31.523: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:33.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:33.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:35.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:35.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:37.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:37.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:39.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:39.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:41.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:41.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:43.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:43.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:45.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:45.522: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 11 03:31:47.499: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 11 03:31:47.522: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:31:47.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1695" for this suite.
Dec 11 03:32:09.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:32:10.479: INFO: namespace container-lifecycle-hook-1695 deletion completed in 22.935479733s

â€¢ [SLOW TEST:49.367 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:32:10.480: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 11 03:32:13.227: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-341 pod-service-account-9df94d61-0ca2-4543-9b83-cdc971533464 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 11 03:32:13.725: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-341 pod-service-account-9df94d61-0ca2-4543-9b83-cdc971533464 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 11 03:32:14.237: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-341 pod-service-account-9df94d61-0ca2-4543-9b83-cdc971533464 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:32:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-341" for this suite.
Dec 11 03:32:20.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:32:21.765: INFO: namespace svcaccounts-341 deletion completed in 6.973654732s

â€¢ [SLOW TEST:11.285 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:32:21.765: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec 11 03:33:02.035: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1211 03:33:02.035093      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:33:02.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5554" for this suite.
Dec 11 03:33:08.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:33:09.014: INFO: namespace gc-5554 deletion completed in 6.957583018s

â€¢ [SLOW TEST:47.249 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:33:09.015: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-2e194e92-d5a5-4de7-b5c2-da66561f8628
STEP: Creating secret with name s-test-opt-upd-e110853f-3ab2-4502-bfa9-2ccd0d0c4eeb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2e194e92-d5a5-4de7-b5c2-da66561f8628
STEP: Updating secret s-test-opt-upd-e110853f-3ab2-4502-bfa9-2ccd0d0c4eeb
STEP: Creating secret with name s-test-opt-create-455f5d73-e05c-40bd-b26f-629c4e95b086
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:33:15.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8695" for this suite.
Dec 11 03:33:37.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:33:38.654: INFO: namespace projected-8695 deletion completed in 22.939071103s

â€¢ [SLOW TEST:29.639 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:33:38.654: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec 11 03:33:38.786: INFO: Waiting up to 5m0s for pod "var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6" in namespace "var-expansion-4512" to be "success or failure"
Dec 11 03:33:38.807: INFO: Pod "var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.588797ms
Dec 11 03:33:40.831: INFO: Pod "var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045273869s
Dec 11 03:33:42.854: INFO: Pod "var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067813063s
STEP: Saw pod success
Dec 11 03:33:42.854: INFO: Pod "var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6" satisfied condition "success or failure"
Dec 11 03:33:42.875: INFO: Trying to get logs from node 10.247.6.203 pod var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6 container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:33:42.954: INFO: Waiting for pod var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6 to disappear
Dec 11 03:33:42.976: INFO: Pod var-expansion-c0093b74-9759-4c91-9bf7-b21f5ddacfc6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:33:42.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4512" for this suite.
Dec 11 03:33:49.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:33:49.959: INFO: namespace var-expansion-4512 deletion completed in 6.961567614s

â€¢ [SLOW TEST:11.305 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:33:49.960: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-fdc27290-b858-4d31-8d62-66d32db5d837
STEP: Creating configMap with name cm-test-opt-upd-8138f329-3d14-4517-8835-9d65ebe4cd86
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fdc27290-b858-4d31-8d62-66d32db5d837
STEP: Updating configmap cm-test-opt-upd-8138f329-3d14-4517-8835-9d65ebe4cd86
STEP: Creating configMap with name cm-test-opt-create-a586539e-34f0-4a8f-9c86-09c37e2d3aeb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:35:00.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9355" for this suite.
Dec 11 03:35:22.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:35:23.572: INFO: namespace configmap-9355 deletion completed in 22.951763447s

â€¢ [SLOW TEST:93.613 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:35:23.573: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:35:23.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6" in namespace "downward-api-2928" to be "success or failure"
Dec 11 03:35:23.716: INFO: Pod "downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.466353ms
Dec 11 03:35:25.739: INFO: Pod "downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04580592s
Dec 11 03:35:27.762: INFO: Pod "downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069163909s
STEP: Saw pod success
Dec 11 03:35:27.762: INFO: Pod "downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6" satisfied condition "success or failure"
Dec 11 03:35:27.785: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6 container client-container: <nil>
STEP: delete the pod
Dec 11 03:35:27.861: INFO: Waiting for pod downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6 to disappear
Dec 11 03:35:27.883: INFO: Pod downwardapi-volume-24ed8ee5-9552-466a-98ca-df98d69fdae6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:35:27.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2928" for this suite.
Dec 11 03:35:33.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:35:34.840: INFO: namespace downward-api-2928 deletion completed in 6.933282549s

â€¢ [SLOW TEST:11.267 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:35:34.840: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7126, will wait for the garbage collector to delete the pods
Dec 11 03:35:39.073: INFO: Deleting Job.batch foo took: 23.741597ms
Dec 11 03:35:39.473: INFO: Terminating Job.batch foo pods took: 400.203992ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:36:18.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7126" for this suite.
Dec 11 03:36:24.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:36:25.630: INFO: namespace job-7126 deletion completed in 6.912698763s

â€¢ [SLOW TEST:50.790 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:36:25.630: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 11 03:36:25.755: INFO: Waiting up to 5m0s for pod "downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93" in namespace "downward-api-6478" to be "success or failure"
Dec 11 03:36:25.776: INFO: Pod "downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93": Phase="Pending", Reason="", readiness=false. Elapsed: 21.705715ms
Dec 11 03:36:27.799: INFO: Pod "downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044443296s
STEP: Saw pod success
Dec 11 03:36:27.799: INFO: Pod "downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93" satisfied condition "success or failure"
Dec 11 03:36:27.821: INFO: Trying to get logs from node 10.247.6.203 pod downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93 container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:36:27.966: INFO: Waiting for pod downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93 to disappear
Dec 11 03:36:27.987: INFO: Pod downward-api-46c093ad-f0b9-4b0b-a535-cc4ac71fcb93 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:36:27.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6478" for this suite.
Dec 11 03:36:34.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:36:34.942: INFO: namespace downward-api-6478 deletion completed in 6.932151698s

â€¢ [SLOW TEST:9.312 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:36:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:36:35.140: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 11 03:36:35.219: INFO: Number of nodes with available pods: 0
Dec 11 03:36:35.219: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:36:36.265: INFO: Number of nodes with available pods: 0
Dec 11 03:36:36.265: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:36:37.264: INFO: Number of nodes with available pods: 0
Dec 11 03:36:37.264: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:36:38.264: INFO: Number of nodes with available pods: 2
Dec 11 03:36:38.264: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 11 03:36:38.454: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:38.454: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:39.498: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:39.498: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:40.500: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:40.500: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:41.499: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:41.499: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:41.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:42.501: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:42.501: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:42.501: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:43.499: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:43.499: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:43.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:44.499: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:44.499: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:44.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:45.499: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:45.499: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:45.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:46.498: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:46.498: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:46.498: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:47.499: INFO: Wrong image for pod: daemon-set-lp2zn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:47.499: INFO: Pod daemon-set-lp2zn is not available
Dec 11 03:36:47.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:48.498: INFO: Pod daemon-set-9ns5w is not available
Dec 11 03:36:48.498: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:49.498: INFO: Pod daemon-set-9ns5w is not available
Dec 11 03:36:49.498: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:50.499: INFO: Pod daemon-set-9ns5w is not available
Dec 11 03:36:50.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:51.500: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:52.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:52.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:53.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:53.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:54.498: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:54.498: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:55.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:55.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:56.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:56.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:57.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:57.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:58.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:58.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:36:59.499: INFO: Wrong image for pod: daemon-set-mbvh9. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 11 03:36:59.499: INFO: Pod daemon-set-mbvh9 is not available
Dec 11 03:37:00.499: INFO: Pod daemon-set-krvpq is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 11 03:37:00.567: INFO: Number of nodes with available pods: 1
Dec 11 03:37:00.567: INFO: Node kcsp41-master0 is running more than one daemon pod
Dec 11 03:37:01.612: INFO: Number of nodes with available pods: 1
Dec 11 03:37:01.612: INFO: Node kcsp41-master0 is running more than one daemon pod
Dec 11 03:37:02.611: INFO: Number of nodes with available pods: 2
Dec 11 03:37:02.611: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9295, will wait for the garbage collector to delete the pods
Dec 11 03:37:02.818: INFO: Deleting DaemonSet.extensions daemon-set took: 23.274872ms
Dec 11 03:37:03.218: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.183067ms
Dec 11 03:37:08.440: INFO: Number of nodes with available pods: 0
Dec 11 03:37:08.440: INFO: Number of running nodes: 0, number of available pods: 0
Dec 11 03:37:08.462: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9295/daemonsets","resourceVersion":"1028582"},"items":null}

Dec 11 03:37:08.484: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9295/pods","resourceVersion":"1028582"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:37:08.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9295" for this suite.
Dec 11 03:37:14.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:37:15.522: INFO: namespace daemonsets-9295 deletion completed in 6.949563941s

â€¢ [SLOW TEST:40.580 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:37:15.522: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:37:40.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3556" for this suite.
Dec 11 03:37:46.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:37:47.757: INFO: namespace container-runtime-3556 deletion completed in 6.935044445s

â€¢ [SLOW TEST:32.235 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:37:47.757: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 11 03:37:47.874: INFO: Waiting up to 5m0s for pod "downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344" in namespace "downward-api-3705" to be "success or failure"
Dec 11 03:37:47.896: INFO: Pod "downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344": Phase="Pending", Reason="", readiness=false. Elapsed: 21.623917ms
Dec 11 03:37:49.920: INFO: Pod "downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045339924s
Dec 11 03:37:51.942: INFO: Pod "downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068019576s
STEP: Saw pod success
Dec 11 03:37:51.942: INFO: Pod "downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344" satisfied condition "success or failure"
Dec 11 03:37:51.964: INFO: Trying to get logs from node 10.247.6.203 pod downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344 container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:37:52.066: INFO: Waiting for pod downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344 to disappear
Dec 11 03:37:52.088: INFO: Pod downward-api-468d352b-1e4f-4ca3-9e47-be625a1fd344 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:37:52.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3705" for this suite.
Dec 11 03:37:58.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:37:59.037: INFO: namespace downward-api-3705 deletion completed in 6.926959212s

â€¢ [SLOW TEST:11.280 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:37:59.037: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-1131d3a7-fe11-4b6d-9687-a9d2e2d3f173
STEP: Creating a pod to test consume secrets
Dec 11 03:37:59.177: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e" in namespace "projected-2497" to be "success or failure"
Dec 11 03:37:59.200: INFO: Pod "pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.402532ms
Dec 11 03:38:01.222: INFO: Pod "pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045118181s
Dec 11 03:38:03.245: INFO: Pod "pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067813991s
STEP: Saw pod success
Dec 11 03:38:03.245: INFO: Pod "pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e" satisfied condition "success or failure"
Dec 11 03:38:03.267: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:38:03.341: INFO: Waiting for pod pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e to disappear
Dec 11 03:38:03.362: INFO: Pod pod-projected-secrets-65b1233f-b5a2-4c64-aca9-7be4de75395e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:38:03.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2497" for this suite.
Dec 11 03:38:09.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:38:10.336: INFO: namespace projected-2497 deletion completed in 6.951218086s

â€¢ [SLOW TEST:11.299 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:38:10.337: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 11 03:38:10.477: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:38:10.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6382" for this suite.
Dec 11 03:38:16.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:38:17.506: INFO: namespace replication-controller-6382 deletion completed in 6.935260811s

â€¢ [SLOW TEST:7.169 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:38:17.506: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 11 03:38:17.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 create -f - --namespace=kubectl-2035'
Dec 11 03:38:18.596: INFO: stderr: ""
Dec 11 03:38:18.596: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 11 03:38:18.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2035'
Dec 11 03:38:18.803: INFO: stderr: ""
Dec 11 03:38:18.803: INFO: stdout: "update-demo-nautilus-5hnxw update-demo-nautilus-h8z54 "
Dec 11 03:38:18.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-5hnxw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2035'
Dec 11 03:38:18.953: INFO: stderr: ""
Dec 11 03:38:18.953: INFO: stdout: ""
Dec 11 03:38:18.953: INFO: update-demo-nautilus-5hnxw is created but not running
Dec 11 03:38:23.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2035'
Dec 11 03:38:24.104: INFO: stderr: ""
Dec 11 03:38:24.104: INFO: stdout: "update-demo-nautilus-5hnxw update-demo-nautilus-h8z54 "
Dec 11 03:38:24.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-5hnxw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2035'
Dec 11 03:38:24.256: INFO: stderr: ""
Dec 11 03:38:24.256: INFO: stdout: "true"
Dec 11 03:38:24.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-5hnxw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2035'
Dec 11 03:38:24.411: INFO: stderr: ""
Dec 11 03:38:24.411: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:38:24.411: INFO: validating pod update-demo-nautilus-5hnxw
Dec 11 03:38:24.498: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:38:24.498: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:38:24.498: INFO: update-demo-nautilus-5hnxw is verified up and running
Dec 11 03:38:24.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-h8z54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2035'
Dec 11 03:38:24.644: INFO: stderr: ""
Dec 11 03:38:24.644: INFO: stdout: "true"
Dec 11 03:38:24.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods update-demo-nautilus-h8z54 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2035'
Dec 11 03:38:24.792: INFO: stderr: ""
Dec 11 03:38:24.792: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 11 03:38:24.792: INFO: validating pod update-demo-nautilus-h8z54
Dec 11 03:38:24.820: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 11 03:38:24.820: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 11 03:38:24.820: INFO: update-demo-nautilus-h8z54 is verified up and running
STEP: using delete to clean up resources
Dec 11 03:38:24.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete --grace-period=0 --force -f - --namespace=kubectl-2035'
Dec 11 03:38:24.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 11 03:38:24.988: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 11 03:38:24.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2035'
Dec 11 03:38:25.173: INFO: stderr: "No resources found.\n"
Dec 11 03:38:25.173: INFO: stdout: ""
Dec 11 03:38:25.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pods -l name=update-demo --namespace=kubectl-2035 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 11 03:38:25.332: INFO: stderr: ""
Dec 11 03:38:25.332: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:38:25.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2035" for this suite.
Dec 11 03:38:31.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:38:32.276: INFO: namespace kubectl-2035 deletion completed in 6.92222545s

â€¢ [SLOW TEST:14.770 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:38:32.276: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 11 03:38:32.394: INFO: Waiting up to 5m0s for pod "pod-bc359bb4-2d00-435d-a09a-da1d369814bd" in namespace "emptydir-5877" to be "success or failure"
Dec 11 03:38:32.416: INFO: Pod "pod-bc359bb4-2d00-435d-a09a-da1d369814bd": Phase="Pending", Reason="", readiness=false. Elapsed: 21.706023ms
Dec 11 03:38:34.439: INFO: Pod "pod-bc359bb4-2d00-435d-a09a-da1d369814bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044490373s
Dec 11 03:38:36.462: INFO: Pod "pod-bc359bb4-2d00-435d-a09a-da1d369814bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068122683s
STEP: Saw pod success
Dec 11 03:38:36.462: INFO: Pod "pod-bc359bb4-2d00-435d-a09a-da1d369814bd" satisfied condition "success or failure"
Dec 11 03:38:36.484: INFO: Trying to get logs from node 10.247.6.203 pod pod-bc359bb4-2d00-435d-a09a-da1d369814bd container test-container: <nil>
STEP: delete the pod
Dec 11 03:38:36.560: INFO: Waiting for pod pod-bc359bb4-2d00-435d-a09a-da1d369814bd to disappear
Dec 11 03:38:36.582: INFO: Pod pod-bc359bb4-2d00-435d-a09a-da1d369814bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:38:36.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5877" for this suite.
Dec 11 03:38:42.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:38:43.518: INFO: namespace emptydir-5877 deletion completed in 6.913763176s

â€¢ [SLOW TEST:11.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:38:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 11 03:38:43.634: INFO: Waiting up to 5m0s for pod "pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0" in namespace "emptydir-6391" to be "success or failure"
Dec 11 03:38:43.656: INFO: Pod "pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.396481ms
Dec 11 03:38:45.678: INFO: Pod "pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044043549s
Dec 11 03:38:47.701: INFO: Pod "pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067195223s
STEP: Saw pod success
Dec 11 03:38:47.701: INFO: Pod "pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0" satisfied condition "success or failure"
Dec 11 03:38:47.724: INFO: Trying to get logs from node 10.247.6.203 pod pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0 container test-container: <nil>
STEP: delete the pod
Dec 11 03:38:47.807: INFO: Waiting for pod pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0 to disappear
Dec 11 03:38:47.828: INFO: Pod pod-a0be1dca-2277-4f0c-87c7-a88aeaf008c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:38:47.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6391" for this suite.
Dec 11 03:38:53.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:38:54.787: INFO: namespace emptydir-6391 deletion completed in 6.935979958s

â€¢ [SLOW TEST:11.269 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:38:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f791d2c0-c182-4553-b982-71ccd2dda5f3
STEP: Creating a pod to test consume secrets
Dec 11 03:38:54.938: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b" in namespace "projected-6033" to be "success or failure"
Dec 11 03:38:54.960: INFO: Pod "pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.880887ms
Dec 11 03:38:56.983: INFO: Pod "pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.044549644s
Dec 11 03:38:59.005: INFO: Pod "pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067224832s
STEP: Saw pod success
Dec 11 03:38:59.006: INFO: Pod "pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b" satisfied condition "success or failure"
Dec 11 03:38:59.027: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:38:59.101: INFO: Waiting for pod pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b to disappear
Dec 11 03:38:59.123: INFO: Pod pod-projected-secrets-1ec2e158-e9c9-403b-aa97-4f73bfd0dc0b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:38:59.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6033" for this suite.
Dec 11 03:39:05.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:39:06.053: INFO: namespace projected-6033 deletion completed in 6.908512375s

â€¢ [SLOW TEST:11.266 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:39:06.054: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ba50f2c0-1154-4c75-a614-4cb03469fcca
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ba50f2c0-1154-4c75-a614-4cb03469fcca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:39:10.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4373" for this suite.
Dec 11 03:39:32.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:39:33.436: INFO: namespace projected-4373 deletion completed in 22.934624323s

â€¢ [SLOW TEST:27.383 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:39:33.437: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:39:33.525: INFO: Creating deployment "nginx-deployment"
Dec 11 03:39:33.547: INFO: Waiting for observed generation 1
Dec 11 03:39:33.569: INFO: Waiting for all required pods to come up
Dec 11 03:39:33.595: INFO: Pod name nginx: Found 3 pods out of 10
Dec 11 03:39:38.639: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 11 03:39:38.639: INFO: Waiting for deployment "nginx-deployment" to complete
Dec 11 03:39:38.682: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec 11 03:39:38.727: INFO: Updating deployment nginx-deployment
Dec 11 03:39:38.727: INFO: Waiting for observed generation 2
Dec 11 03:39:40.778: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 11 03:39:40.800: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 11 03:39:40.823: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 11 03:39:40.890: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 11 03:39:40.890: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 11 03:39:40.912: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 11 03:39:40.955: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec 11 03:39:40.955: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec 11 03:39:41.000: INFO: Updating deployment nginx-deployment
Dec 11 03:39:41.000: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec 11 03:39:41.065: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 11 03:39:43.131: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 11 03:39:43.177: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-4470,SelfLink:/apis/apps/v1/namespaces/deployment-4470/deployments/nginx-deployment,UID:b2c3d562-31e8-4718-bace-c7cc2d5cdbb1,ResourceVersion:1029318,Generation:3,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-11 03:39:21 +0000 UTC 2019-12-11 03:39:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-11 03:39:21 +0000 UTC 2019-12-11 03:39:14 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-66976cf757" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec 11 03:39:43.202: INFO: New ReplicaSet "nginx-deployment-66976cf757" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757,GenerateName:,Namespace:deployment-4470,SelfLink:/apis/apps/v1/namespaces/deployment-4470/replicasets/nginx-deployment-66976cf757,UID:8ebc05b7-114f-403c-9cdf-af3f38264338,ResourceVersion:1029296,Generation:3,CreationTimestamp:2019-12-11 03:39:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/internalStrategyType: RollingUpdate,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b2c3d562-31e8-4718-bace-c7cc2d5cdbb1 0xc00050cc47 0xc00050cc48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 11 03:39:43.202: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec 11 03:39:43.202: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b,GenerateName:,Namespace:deployment-4470,SelfLink:/apis/apps/v1/namespaces/deployment-4470/replicasets/nginx-deployment-5bc64d997b,UID:bee0589b-f507-41a7-b9df-24618737ba1f,ResourceVersion:1029317,Generation:3,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/internalStrategyType: RollingUpdate,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment b2c3d562-31e8-4718-bace-c7cc2d5cdbb1 0xc00050cac7 0xc00050cac8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec 11 03:39:43.267: INFO: Pod "nginx-deployment-5bc64d997b-5fhgb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-5fhgb,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-5fhgb,UID:efa4adad-fb30-4ed2-a8f1-2185d8fe62d2,ResourceVersion:1029309,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad4897 0xc000ad4898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad4930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad4950}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-5sbrf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-5sbrf,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-5sbrf,UID:2d488162-7fda-4b30-a64f-1faff48f7e9d,ResourceVersion:1029301,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad49c7 0xc000ad49c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad4ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad4af0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-6lm6t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-6lm6t,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-6lm6t,UID:ac19ff9c-cb05-497e-aea5-a29c0a386555,ResourceVersion:1029320,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad4b67 0xc000ad4b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad4c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad4c70}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-6x876" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-6x876,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-6x876,UID:75629021-0e8f-424a-95bd-465bf490b72a,ResourceVersion:1029293,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad4e17 0xc000ad4e18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad4f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad4fa0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-7bl4h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-7bl4h,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-7bl4h,UID:fdda45ba-67c8-4965-8280-ad592904b66b,ResourceVersion:1029328,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad5077 0xc000ad5078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad51c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad51e0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-8fvzt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-8fvzt,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-8fvzt,UID:e7003f0d-4e9f-44ad-940a-df705356b3bc,ResourceVersion:1029290,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad5397 0xc000ad5398}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad5480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad54a0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-b2kgc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-b2kgc,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-b2kgc,UID:ee5f59bf-e862-4c0b-b1da-8c5be9d8ba82,ResourceVersion:1029216,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad5647 0xc000ad5648}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad5770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad5790}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:172.16.0.225,StartTime:2019-12-11 03:39:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:18 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://402f94b42d47d34c75fd82082ef0d4945f3fade81083727b3a51456b1ca49430}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-c4xfl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-c4xfl,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-c4xfl,UID:10c18faf-4369-4a13-beb2-efd4c92fab33,ResourceVersion:1029201,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad5927 0xc000ad5928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad5a30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad5ac0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:172.16.0.222,StartTime:2019-12-11 03:39:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:16 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8744f2efb037df5e51d3b50e29f202fc3830e81fae9f74baedf15092e00a4601}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.268: INFO: Pod "nginx-deployment-5bc64d997b-ccgml" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-ccgml,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-ccgml,UID:30a7b9a8-b5e7-4080-a312-c51f7105f844,ResourceVersion:1029198,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad5c27 0xc000ad5c28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ad5d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ad5d60}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:172.16.0.224,StartTime:2019-12-11 03:39:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8a0092de3ace633839bec025a59a8b5ff8cb106674d3c657429cef89dc3c71f5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-hlgxw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-hlgxw,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-hlgxw,UID:d4e9991e-fba8-4194-88f4-5dbbf1ffbac7,ResourceVersion:1029189,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc000ad5e87 0xc000ad5e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834030}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.194,StartTime:2019-12-11 03:39:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:36 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://48db36e44ee80a477a8f84eda60b732698f4a29ed79ada6862e8cf76621cc044}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-j84rc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-j84rc,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-j84rc,UID:dd77cf04-fdc8-4549-a257-cd500bcebe9b,ResourceVersion:1029195,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc0038340f7 0xc0038340f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038341a0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.195,StartTime:2019-12-11 03:39:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:36 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://43e4e984ec10ed09bbc6e97f9da6236922eb8e2e7b4dbd6f8fb4028842efc633}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-kk7kp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-kk7kp,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-kk7kp,UID:aa660a0d-1cf7-4a02-b413-459f829530b6,ResourceVersion:1029192,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834267 0xc003834268}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038342f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834310}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.193,StartTime:2019-12-11 03:39:33 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:35 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73 docker://3c67fd56c8dba07ae5b83fbc56f0f2280fafb14e7eafd3557a3fe29dfdb36439}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-kztvj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-kztvj,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-kztvj,UID:d48cb0d3-c5cc-404a-b966-1b0007414f8c,ResourceVersion:1029311,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc0038343d7 0xc0038343d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834480}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-mq59f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-mq59f,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-mq59f,UID:df716c54-c3a0-49f2-9a08-c01bdbbb7cc4,ResourceVersion:1029323,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834537 0xc003834538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038345c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038345e0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-nb9n2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-nb9n2,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-nb9n2,UID:aabd9b4d-b794-4b02-b5aa-81f369583f09,ResourceVersion:1029212,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834697 0xc003834698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834740}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:172.16.0.226,StartTime:2019-12-11 03:39:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:18 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ac25673cdc0c6fc141f728d3f0939e357e6328a94a085ceb643827de0b109c3b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-plfkf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-plfkf,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-plfkf,UID:36d7ec34-ff59-4e78-a4bc-a718ad3b8d97,ResourceVersion:1029298,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834807 0xc003834808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834890} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038348b0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-rrnbw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-rrnbw,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-rrnbw,UID:60a49394-90f6-4d3f-a405-1396fec12b41,ResourceVersion:1029315,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834987 0xc003834988}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834a30}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.269: INFO: Pod "nginx-deployment-5bc64d997b-s6kzc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-s6kzc,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-s6kzc,UID:3bc79bd6-37f1-49fb-a64a-bc7048113e94,ResourceVersion:1029325,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834ae7 0xc003834ae8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834b90}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-5bc64d997b-swbfc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-swbfc,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-swbfc,UID:85c97da4-84f4-4915-b087-926e14cc449f,ResourceVersion:1029327,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834c47 0xc003834c48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834cf0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-5bc64d997b-x6fj4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5bc64d997b-x6fj4,GenerateName:nginx-deployment-5bc64d997b-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-5bc64d997b-x6fj4,UID:1bb16083-ab94-4f3b-821f-772c1304255f,ResourceVersion:1029185,Generation:0,CreationTimestamp:2019-12-11 03:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5bc64d997b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5bc64d997b bee0589b-f507-41a7-b9df-24618737ba1f 0xc003834da7 0xc003834da8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834e50}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:14 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:172.16.0.223,StartTime:2019-12-11 03:39:14 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-11 03:39:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://523c089852da4f8b343de0d4d15a2ab218449bee54bce0eee9d35c5ecf3a6f22}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-66976cf757-2gp55" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-2gp55,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-2gp55,UID:72427722-5bb6-4c3b-a200-9a8fce724eef,ResourceVersion:1029254,Generation:0,CreationTimestamp:2019-12-11 03:39:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003834f17 0xc003834f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003834fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003834fc0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-66976cf757-56nsc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-56nsc,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-56nsc,UID:725a4317-c36e-4996-9fa8-5b31f5d01e87,ResourceVersion:1029239,Generation:0,CreationTimestamp:2019-12-11 03:39:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835087 0xc003835088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835130}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-66976cf757-6flx5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-6flx5,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-6flx5,UID:eae9da8c-00ab-4578-b5c5-9678929db2eb,ResourceVersion:1029312,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc0038351f7 0xc0038351f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038352a0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-66976cf757-8xp99" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-8xp99,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-8xp99,UID:d0e80d0d-a23e-479a-b1d2-731a8dbba841,ResourceVersion:1029240,Generation:0,CreationTimestamp:2019-12-11 03:39:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835367 0xc003835368}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038353f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835410}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.270: INFO: Pod "nginx-deployment-66976cf757-ddvdg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-ddvdg,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-ddvdg,UID:9468dc60-c2e0-42c1-ba2d-10a01c88c212,ResourceVersion:1029253,Generation:0,CreationTimestamp:2019-12-11 03:39:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc0038354d7 0xc0038354d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835580}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-dfsdh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-dfsdh,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-dfsdh,UID:a813e20d-cb21-414d-811a-b73d1def5e38,ResourceVersion:1029249,Generation:0,CreationTimestamp:2019-12-11 03:39:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835647 0xc003835648}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038356d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038356f0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:19 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-dlxsg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-dlxsg,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-dlxsg,UID:e5540222-e911-44c9-946c-02883b4bed15,ResourceVersion:1029313,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc0038357b7 0xc0038357b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835860}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-gb2cz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-gb2cz,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-gb2cz,UID:c3539785-047a-414a-bf67-de8253332c97,ResourceVersion:1029316,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835927 0xc003835928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038359b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038359d0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-gzdsn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-gzdsn,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-gzdsn,UID:aaf08eed-a52a-4c9e-8826-5a521afe3fe9,ResourceVersion:1029319,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835a97 0xc003835a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835b40}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-llxtb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-llxtb,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-llxtb,UID:0e40db50-3805-4eea-bda4-6c52d551e109,ResourceVersion:1029297,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835c07 0xc003835c08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835cc0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-n45l5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-n45l5,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-n45l5,UID:3a3ca81f-9ae3-4c8b-8090-12bd2902c74b,ResourceVersion:1029275,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835d37 0xc003835d38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kcsp41-master0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835dc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835de0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.73.193,PodIP:,StartTime:2019-12-11 03:39:21 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-srgrd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-srgrd,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-srgrd,UID:6e03d464-c077-470b-9de7-7e7222e2c239,ResourceVersion:1029321,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc003835ea7 0xc003835ea8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003835f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003835f50}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 11 03:39:43.271: INFO: Pod "nginx-deployment-66976cf757-xk5hg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-66976cf757-xk5hg,GenerateName:nginx-deployment-66976cf757-,Namespace:deployment-4470,SelfLink:/api/v1/namespaces/deployment-4470/pods/nginx-deployment-66976cf757-xk5hg,UID:a5c58e05-4356-4af6-a024-8274c6d73a2d,ResourceVersion:1029310,Generation:0,CreationTimestamp:2019-12-11 03:39:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 66976cf757,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-66976cf757 8ebc05b7-114f-403c-9cdf-af3f38264338 0xc00034c067 0xc00034c068}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sxb29 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sxb29,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-sxb29 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00034c160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00034c190}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:39:21 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:,StartTime:2019-12-11 03:39:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:39:43.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4470" for this suite.
Dec 11 03:39:51.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:39:52.270: INFO: namespace deployment-4470 deletion completed in 8.975624223s

â€¢ [SLOW TEST:18.833 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:39:52.291: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 11 03:39:56.548: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:39:56.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1707" for this suite.
Dec 11 03:40:18.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:40:19.578: INFO: namespace replicaset-1707 deletion completed in 22.932898265s

â€¢ [SLOW TEST:27.287 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:40:19.578: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 11 03:40:19.697: INFO: Waiting up to 5m0s for pod "pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c" in namespace "emptydir-6920" to be "success or failure"
Dec 11 03:40:19.719: INFO: Pod "pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 21.578627ms
Dec 11 03:40:21.742: INFO: Pod "pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044458612s
Dec 11 03:40:23.765: INFO: Pod "pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067268847s
STEP: Saw pod success
Dec 11 03:40:23.765: INFO: Pod "pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c" satisfied condition "success or failure"
Dec 11 03:40:23.786: INFO: Trying to get logs from node 10.247.6.203 pod pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c container test-container: <nil>
STEP: delete the pod
Dec 11 03:40:23.861: INFO: Waiting for pod pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c to disappear
Dec 11 03:40:23.882: INFO: Pod pod-9643fa98-e0a9-4d7f-8f33-b0fc079d9d8c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:40:23.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6920" for this suite.
Dec 11 03:40:29.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:40:30.890: INFO: namespace emptydir-6920 deletion completed in 6.985262377s

â€¢ [SLOW TEST:11.312 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:40:30.890: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 11 03:40:31.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28" in namespace "downward-api-1455" to be "success or failure"
Dec 11 03:40:31.027: INFO: Pod "downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28": Phase="Pending", Reason="", readiness=false. Elapsed: 21.570004ms
Dec 11 03:40:33.053: INFO: Pod "downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047788404s
STEP: Saw pod success
Dec 11 03:40:33.053: INFO: Pod "downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28" satisfied condition "success or failure"
Dec 11 03:40:33.075: INFO: Trying to get logs from node 10.247.6.203 pod downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28 container client-container: <nil>
STEP: delete the pod
Dec 11 03:40:33.149: INFO: Waiting for pod downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28 to disappear
Dec 11 03:40:33.170: INFO: Pod downwardapi-volume-2b4975d7-c721-4106-999a-0a1e1561ee28 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:40:33.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1455" for this suite.
Dec 11 03:40:39.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:40:40.106: INFO: namespace downward-api-1455 deletion completed in 6.913420552s

â€¢ [SLOW TEST:9.216 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:40:40.106: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:40:40.194: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 11 03:40:40.239: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 11 03:40:44.283: INFO: Creating deployment "test-rolling-update-deployment"
Dec 11 03:40:44.306: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 11 03:40:44.356: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 11 03:40:44.378: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6896b6d67f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:40:46.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632424, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6896b6d67f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:40:48.400: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 11 03:40:48.466: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-734,SelfLink:/apis/apps/v1/namespaces/deployment-734/deployments/test-rolling-update-deployment,UID:ea49ccca-d8f5-41cc-b917-f4e7fb039923,ResourceVersion:1029680,Generation:1,CreationTimestamp:2019-12-11 03:40:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:1,MaxSurge:1,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-11 03:40:24 +0000 UTC 2019-12-11 03:40:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-11 03:40:27 +0000 UTC 2019-12-11 03:40:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-6896b6d67f" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 11 03:40:48.489: INFO: New ReplicaSet "test-rolling-update-deployment-6896b6d67f" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-6896b6d67f,GenerateName:,Namespace:deployment-734,SelfLink:/apis/apps/v1/namespaces/deployment-734/replicasets/test-rolling-update-deployment-6896b6d67f,UID:defd3649-c5bf-407d-a2c2-d8215dbabc32,ResourceVersion:1029679,Generation:1,CreationTimestamp:2019-12-11 03:40:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 6896b6d67f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/internalStrategyType: RollingUpdate,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ea49ccca-d8f5-41cc-b917-f4e7fb039923 0xc002c63247 0xc002c63248}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6896b6d67f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 6896b6d67f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 11 03:40:48.489: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 11 03:40:48.489: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-734,SelfLink:/apis/apps/v1/namespaces/deployment-734/replicasets/test-rolling-update-controller,UID:3913e800-eb55-4adf-aeab-a13996174c2b,ResourceVersion:1029665,Generation:2,CreationTimestamp:2019-12-11 03:40:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ea49ccca-d8f5-41cc-b917-f4e7fb039923 0xc002c63167 0xc002c63168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 11 03:40:48.512: INFO: Pod "test-rolling-update-deployment-6896b6d67f-k66lw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-6896b6d67f-k66lw,GenerateName:test-rolling-update-deployment-6896b6d67f-,Namespace:deployment-734,SelfLink:/api/v1/namespaces/deployment-734/pods/test-rolling-update-deployment-6896b6d67f-k66lw,UID:c0b0800e-a74b-488a-9d78-b692ce328900,ResourceVersion:1029678,Generation:0,CreationTimestamp:2019-12-11 03:40:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 6896b6d67f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-6896b6d67f defd3649-c5bf-407d-a2c2-d8215dbabc32 0xc0029be757 0xc0029be758}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hspnn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hspnn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-hspnn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029be7e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029be800}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:40:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:40:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:40:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:40:24 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.217,StartTime:2019-12-11 03:40:44 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-11 03:40:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://cc55d25f16fd2069a7abf9cf90c2222eef1c03312863952f67071417021ba171}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:40:48.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-734" for this suite.
Dec 11 03:40:54.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:40:55.450: INFO: namespace deployment-734 deletion completed in 6.915859701s

â€¢ [SLOW TEST:15.344 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:40:55.451: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9a0a249b-543f-4771-9589-e1ea0c5ac29a
STEP: Creating a pod to test consume secrets
Dec 11 03:40:55.596: INFO: Waiting up to 5m0s for pod "pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb" in namespace "secrets-8575" to be "success or failure"
Dec 11 03:40:55.618: INFO: Pod "pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb": Phase="Pending", Reason="", readiness=false. Elapsed: 21.982814ms
Dec 11 03:40:57.640: INFO: Pod "pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.044222652s
STEP: Saw pod success
Dec 11 03:40:57.640: INFO: Pod "pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb" satisfied condition "success or failure"
Dec 11 03:40:57.662: INFO: Trying to get logs from node 10.247.6.203 pod pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb container secret-volume-test: <nil>
STEP: delete the pod
Dec 11 03:40:57.737: INFO: Waiting for pod pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb to disappear
Dec 11 03:40:57.759: INFO: Pod pod-secrets-057c5626-0fd3-42bf-9347-d21ba7ee31fb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:40:57.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8575" for this suite.
Dec 11 03:41:03.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:41:04.718: INFO: namespace secrets-8575 deletion completed in 6.936941235s

â€¢ [SLOW TEST:9.268 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:41:04.719: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-762
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 11 03:41:04.807: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 11 03:41:25.203: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.220:8080/dial?request=hostName&protocol=http&host=172.16.0.238&port=8080&tries=1'] Namespace:pod-network-test-762 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 03:41:25.203: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 03:41:25.711: INFO: Waiting for endpoints: map[]
Dec 11 03:41:25.732: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.1.220:8080/dial?request=hostName&protocol=http&host=172.16.1.219&port=8080&tries=1'] Namespace:pod-network-test-762 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 11 03:41:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
Dec 11 03:41:26.359: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:41:26.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-762" for this suite.
Dec 11 03:41:48.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:41:49.324: INFO: namespace pod-network-test-762 deletion completed in 22.943292012s

â€¢ [SLOW TEST:44.606 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:41:49.325: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 03:41:49.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5167'
Dec 11 03:41:49.879: INFO: stderr: ""
Dec 11 03:41:49.879: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec 11 03:41:54.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 get pod e2e-test-nginx-pod --namespace=kubectl-5167 -o json'
Dec 11 03:41:55.082: INFO: stderr: ""
Dec 11 03:41:55.082: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-11T03:41:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5167\",\n        \"resourceVersion\": \"1029893\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5167/pods/e2e-test-nginx-pod\",\n        \"uid\": \"8ed1ee28-2157-4bae-b71c-8e875c606331\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBE_POD_NODE_NAME\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"apiVersion\": \"v1\",\n                                \"fieldPath\": \"spec.nodeName\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"KUBE_POD_NAMESPACE\",\n                        \"valueFrom\": {\n                            \"fieldRef\": {\n                                \"apiVersion\": \"v1\",\n                                \"fieldPath\": \"metadata.namespace\"\n                            }\n                        }\n                    },\n                    {\n                        \"name\": \"KUBE_POD_CLUSTERID\",\n                        \"value\": \"236cc8a2-17d4-11ea-bf02-0255ac1b0093\"\n                    }\n                ],\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zn542\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.247.6.203\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zn542\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zn542\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-11T03:41:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-11T03:41:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-11T03:41:52Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-11T03:41:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://351dc7e1d3972c22e69a64039ed0e5043db86080f96a5d24c27eb72b140c687d\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker://sha256:8a2fb25a19f5dc1528b7a3fabe8b3145ff57fe10e4f1edac6c718a3cf4aa4b73\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-11T03:41:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.247.6.203\",\n        \"managementIP\": \"10.247.6.203\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.1.221\",\n        \"podNetworks\": [\n            {\n                \"iP\": [\n                    \"172.16.1.221\"\n                ],\n                \"name\": \"eth0\",\n                \"network\": \"default\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-11T03:41:49Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 11 03:41:55.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 replace -f - --namespace=kubectl-5167'
Dec 11 03:41:55.379: INFO: stderr: ""
Dec 11 03:41:55.379: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Dec 11 03:41:55.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete pods e2e-test-nginx-pod --namespace=kubectl-5167'
Dec 11 03:42:08.353: INFO: stderr: ""
Dec 11 03:42:08.353: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:42:08.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5167" for this suite.
Dec 11 03:42:14.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:42:15.316: INFO: namespace kubectl-5167 deletion completed in 6.937603517s

â€¢ [SLOW TEST:25.991 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:42:15.316: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-434e002f-99ae-418e-b928-857dcd9fa10f
STEP: Creating a pod to test consume configMaps
Dec 11 03:42:15.457: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60" in namespace "projected-7967" to be "success or failure"
Dec 11 03:42:15.478: INFO: Pod "pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60": Phase="Pending", Reason="", readiness=false. Elapsed: 21.520259ms
Dec 11 03:42:17.501: INFO: Pod "pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044018853s
Dec 11 03:42:19.523: INFO: Pod "pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066249306s
STEP: Saw pod success
Dec 11 03:42:19.523: INFO: Pod "pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60" satisfied condition "success or failure"
Dec 11 03:42:19.548: INFO: Trying to get logs from node 10.247.6.203 pod pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 11 03:42:19.621: INFO: Waiting for pod pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60 to disappear
Dec 11 03:42:19.643: INFO: Pod pod-projected-configmaps-4b42d2aa-1bef-4701-bb51-da14613d5f60 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:42:19.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7967" for this suite.
Dec 11 03:42:25.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:42:26.614: INFO: namespace projected-7967 deletion completed in 6.949047106s

â€¢ [SLOW TEST:11.298 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:42:26.614: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 11 03:42:26.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5645'
Dec 11 03:42:26.876: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 11 03:42:26.876: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Dec 11 03:42:26.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-194493201 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5645'
Dec 11 03:42:27.076: INFO: stderr: ""
Dec 11 03:42:27.076: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:42:27.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5645" for this suite.
Dec 11 03:42:33.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:42:34.006: INFO: namespace kubectl-5645 deletion completed in 6.907516263s

â€¢ [SLOW TEST:7.392 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:42:34.006: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:42:38.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3621" for this suite.
Dec 11 03:42:44.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:42:45.126: INFO: namespace kubelet-test-3621 deletion completed in 6.933651797s

â€¢ [SLOW TEST:11.121 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:42:45.127: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:42:45.346: INFO: Create a RollingUpdate DaemonSet
Dec 11 03:42:45.369: INFO: Check that daemon pods launch on every node of the cluster
Dec 11 03:42:45.419: INFO: Number of nodes with available pods: 0
Dec 11 03:42:45.419: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:42:46.464: INFO: Number of nodes with available pods: 0
Dec 11 03:42:46.464: INFO: Node 10.247.6.203 is running more than one daemon pod
Dec 11 03:42:47.463: INFO: Number of nodes with available pods: 1
Dec 11 03:42:47.463: INFO: Node kcsp41-master0 is running more than one daemon pod
Dec 11 03:42:48.463: INFO: Number of nodes with available pods: 2
Dec 11 03:42:48.463: INFO: Number of running nodes: 2, number of available pods: 2
Dec 11 03:42:48.463: INFO: Update the DaemonSet to trigger a rollout
Dec 11 03:42:48.507: INFO: Updating DaemonSet daemon-set
Dec 11 03:42:58.596: INFO: Roll back the DaemonSet before rollout is complete
Dec 11 03:42:58.641: INFO: Updating DaemonSet daemon-set
Dec 11 03:42:58.641: INFO: Make sure DaemonSet rollback is complete
Dec 11 03:42:58.662: INFO: Wrong image for pod: daemon-set-gzf65. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 11 03:42:58.663: INFO: Pod daemon-set-gzf65 is not available
Dec 11 03:42:59.707: INFO: Wrong image for pod: daemon-set-gzf65. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 11 03:42:59.707: INFO: Pod daemon-set-gzf65 is not available
Dec 11 03:43:00.708: INFO: Wrong image for pod: daemon-set-gzf65. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 11 03:43:00.708: INFO: Pod daemon-set-gzf65 is not available
Dec 11 03:43:01.709: INFO: Pod daemon-set-9vv6f is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-90, will wait for the garbage collector to delete the pods
Dec 11 03:43:01.874: INFO: Deleting DaemonSet.extensions daemon-set took: 24.754043ms
Dec 11 03:43:02.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.138618ms
Dec 11 03:43:09.798: INFO: Number of nodes with available pods: 0
Dec 11 03:43:09.798: INFO: Number of running nodes: 0, number of available pods: 0
Dec 11 03:43:09.821: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-90/daemonsets","resourceVersion":"1030190"},"items":null}

Dec 11 03:43:09.845: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-90/pods","resourceVersion":"1030190"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:43:09.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-90" for this suite.
Dec 11 03:43:16.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:43:16.877: INFO: namespace daemonsets-90 deletion completed in 6.93545656s

â€¢ [SLOW TEST:31.750 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:43:16.877: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 11 03:43:16.993: INFO: Waiting up to 5m0s for pod "pod-0806fd06-1b41-4579-80b1-5726f0b79958" in namespace "emptydir-4419" to be "success or failure"
Dec 11 03:43:17.014: INFO: Pod "pod-0806fd06-1b41-4579-80b1-5726f0b79958": Phase="Pending", Reason="", readiness=false. Elapsed: 21.558046ms
Dec 11 03:43:19.037: INFO: Pod "pod-0806fd06-1b41-4579-80b1-5726f0b79958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043900763s
Dec 11 03:43:21.060: INFO: Pod "pod-0806fd06-1b41-4579-80b1-5726f0b79958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066675444s
STEP: Saw pod success
Dec 11 03:43:21.060: INFO: Pod "pod-0806fd06-1b41-4579-80b1-5726f0b79958" satisfied condition "success or failure"
Dec 11 03:43:21.082: INFO: Trying to get logs from node 10.247.6.203 pod pod-0806fd06-1b41-4579-80b1-5726f0b79958 container test-container: <nil>
STEP: delete the pod
Dec 11 03:43:21.157: INFO: Waiting for pod pod-0806fd06-1b41-4579-80b1-5726f0b79958 to disappear
Dec 11 03:43:21.179: INFO: Pod pod-0806fd06-1b41-4579-80b1-5726f0b79958 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:43:21.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4419" for this suite.
Dec 11 03:43:27.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:43:28.133: INFO: namespace emptydir-4419 deletion completed in 6.932128038s

â€¢ [SLOW TEST:11.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:43:28.134: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4919.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4919.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 15.151.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.151.15_udp@PTR;check="$$(dig +tcp +noall +answer +search 15.151.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.151.15_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4919.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4919.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4919.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4919.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4919.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 15.151.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.151.15_udp@PTR;check="$$(dig +tcp +noall +answer +search 15.151.168.192.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/192.168.151.15_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 11 03:43:32.497: INFO: Unable to read wheezy_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:32.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:32.583: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:32.626: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:32.923: INFO: Unable to read jessie_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:32.965: INFO: Unable to read jessie_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:33.008: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:33.050: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:33.305: INFO: Lookups using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 failed for: [wheezy_udp@dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_udp@dns-test-service.dns-4919.svc.cluster.local jessie_tcp@dns-test-service.dns-4919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local]

Dec 11 03:43:38.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.392: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.436: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.479: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.798: INFO: Unable to read jessie_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.842: INFO: Unable to read jessie_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.884: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:38.927: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:39.185: INFO: Lookups using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 failed for: [wheezy_udp@dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_udp@dns-test-service.dns-4919.svc.cluster.local jessie_tcp@dns-test-service.dns-4919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local]

Dec 11 03:43:43.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.391: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.434: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.476: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.775: INFO: Unable to read jessie_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.818: INFO: Unable to read jessie_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:43.904: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:44.163: INFO: Lookups using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 failed for: [wheezy_udp@dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_udp@dns-test-service.dns-4919.svc.cluster.local jessie_tcp@dns-test-service.dns-4919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local]

Dec 11 03:43:48.347: INFO: Unable to read wheezy_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.474: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.773: INFO: Unable to read jessie_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.815: INFO: Unable to read jessie_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.858: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:48.901: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:49.155: INFO: Lookups using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 failed for: [wheezy_udp@dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_udp@dns-test-service.dns-4919.svc.cluster.local jessie_tcp@dns-test-service.dns-4919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local]

Dec 11 03:43:53.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.434: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.476: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.776: INFO: Unable to read jessie_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.819: INFO: Unable to read jessie_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:53.904: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:54.160: INFO: Lookups using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 failed for: [wheezy_udp@dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_udp@dns-test-service.dns-4919.svc.cluster.local jessie_tcp@dns-test-service.dns-4919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local]

Dec 11 03:43:58.348: INFO: Unable to read wheezy_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.390: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.432: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.474: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.775: INFO: Unable to read jessie_udp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.817: INFO: Unable to read jessie_tcp@dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.859: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:58.902: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local from pod dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1: the server could not find the requested resource (get pods dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1)
Dec 11 03:43:59.159: INFO: Lookups using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 failed for: [wheezy_udp@dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@dns-test-service.dns-4919.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_udp@dns-test-service.dns-4919.svc.cluster.local jessie_tcp@dns-test-service.dns-4919.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4919.svc.cluster.local]

Dec 11 03:44:04.157: INFO: DNS probes using dns-4919/dns-test-7ca7c8b7-0c46-4590-9041-e8efdda1f8a1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:44:04.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4919" for this suite.
Dec 11 03:44:10.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:44:11.214: INFO: namespace dns-4919 deletion completed in 6.940258243s

â€¢ [SLOW TEST:43.081 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:44:11.215: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 11 03:44:11.348: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 11 03:44:13.392: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 11 03:44:15.415: INFO: Creating deployment "test-rollover-deployment"
Dec 11 03:44:15.469: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 11 03:44:15.491: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 11 03:44:15.535: INFO: Ensure that both replica sets have 1 created replica
Dec 11 03:44:15.578: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 11 03:44:15.622: INFO: Updating deployment test-rollover-deployment
Dec 11 03:44:15.622: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 11 03:44:15.646: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 11 03:44:15.693: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 11 03:44:15.737: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:15.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632636, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:17.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:17.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632636, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:19.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:19.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:21.781: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:21.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:23.780: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:23.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:25.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:25.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:27.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:27.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:29.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:29.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:31.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:31.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:33.781: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:33.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:35.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:35.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:37.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:37.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:39.783: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:39.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:41.781: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:41.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:43.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:43.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:45.781: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:45.781: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:47.782: INFO: all replica sets need to contain the pod-template-hash label
Dec 11 03:44:47.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632639, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711632635, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5bcddbcff8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 11 03:44:49.782: INFO: 
Dec 11 03:44:49.782: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 11 03:44:49.848: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1652,SelfLink:/apis/apps/v1/namespaces/deployment-1652/deployments/test-rollover-deployment,UID:e9d10734-4a01-4fc6-9f9c-cc7ede3833e5,ResourceVersion:1030527,Generation:2,CreationTimestamp:2019-12-11 03:43:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-11 03:43:55 +0000 UTC 2019-12-11 03:43:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-11 03:44:29 +0000 UTC 2019-12-11 03:43:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5bcddbcff8" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 11 03:44:49.870: INFO: New ReplicaSet "test-rollover-deployment-5bcddbcff8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5bcddbcff8,GenerateName:,Namespace:deployment-1652,SelfLink:/apis/apps/v1/namespaces/deployment-1652/replicasets/test-rollover-deployment-5bcddbcff8,UID:25bec67b-2f9e-4656-982e-4a4a933823b4,ResourceVersion:1030520,Generation:2,CreationTimestamp:2019-12-11 03:43:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5bcddbcff8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/internalStrategyType: RollingUpdate,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e9d10734-4a01-4fc6-9f9c-cc7ede3833e5 0xc001dc5847 0xc001dc5848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5bcddbcff8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5bcddbcff8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 11 03:44:49.870: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 11 03:44:49.870: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1652,SelfLink:/apis/apps/v1/namespaces/deployment-1652/replicasets/test-rollover-controller,UID:d973990d-5f76-4e58-a155-ec332f4a4fa0,ResourceVersion:1030526,Generation:2,CreationTimestamp:2019-12-11 03:43:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e9d10734-4a01-4fc6-9f9c-cc7ede3833e5 0xc001dc5767 0xc001dc5768}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 11 03:44:49.870: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-77dccbff7c,GenerateName:,Namespace:deployment-1652,SelfLink:/apis/apps/v1/namespaces/deployment-1652/replicasets/test-rollover-deployment-77dccbff7c,UID:e351e56e-bbb7-4522-acf7-8aa96577ef58,ResourceVersion:1030428,Generation:2,CreationTimestamp:2019-12-11 03:43:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 77dccbff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/internalStrategyType: RollingUpdate,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e9d10734-4a01-4fc6-9f9c-cc7ede3833e5 0xc001dc5927 0xc001dc5928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77dccbff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 77dccbff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 11 03:44:49.893: INFO: Pod "test-rollover-deployment-5bcddbcff8-8pnwm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5bcddbcff8-8pnwm,GenerateName:test-rollover-deployment-5bcddbcff8-,Namespace:deployment-1652,SelfLink:/api/v1/namespaces/deployment-1652/pods/test-rollover-deployment-5bcddbcff8-8pnwm,UID:ed63148b-55ef-42a5-ad34-23ea20101d98,ResourceVersion:1030441,Generation:0,CreationTimestamp:2019-12-11 03:43:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5bcddbcff8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5bcddbcff8 25bec67b-2f9e-4656-982e-4a4a933823b4 0xc003d28527 0xc003d28528}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8gk6f {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8gk6f,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [{KUBE_POD_NODE_NAME  EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_NAMESPACE  &EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {KUBE_POD_CLUSTERID 236cc8a2-17d4-11ea-bf02-0255ac1b0093 nil}] {map[] map[]} [{default-token-8gk6f true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent &SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.247.6.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003d285b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003d285d0}],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:44:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:44:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:44:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-11 03:43:56 +0000 UTC  }],Message:,Reason:,HostIP:10.247.6.203,PodIP:172.16.1.231,StartTime:2019-12-11 03:44:15 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-11 03:44:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://638fd28910b5546839c87107bbd78249cf1098db56dfa85cb5775bf093bb2f5c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:44:49.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1652" for this suite.
Dec 11 03:44:55.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:44:56.846: INFO: namespace deployment-1652 deletion completed in 6.92987543s

â€¢ [SLOW TEST:45.632 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:44:56.847: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 11 03:44:56.951: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 11 03:44:56.996: INFO: Waiting for terminating namespaces to be deleted...
Dec 11 03:44:57.018: INFO: 
Logging pods the kubelet thinks is on node 10.247.6.203 before test
Dec 11 03:44:57.082: INFO: sonobuoy-e2e-job-7ef775b06a5742ab from sonobuoy started at 2019-12-11 02:10:34 +0000 UTC (2 container statuses recorded)
Dec 11 03:44:57.082: INFO: 	Container e2e ready: true, restart count 0
Dec 11 03:44:57.082: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 11 03:44:57.082: INFO: kubenode-vl77r from kube-system started at 2019-12-06 03:07:00 +0000 UTC (0 container statuses recorded)
Dec 11 03:44:57.082: INFO: sonobuoy from sonobuoy started at 2019-12-11 02:10:31 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.082: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 11 03:44:57.082: INFO: sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-lbzbk from sonobuoy started at 2019-12-11 02:10:34 +0000 UTC (2 container statuses recorded)
Dec 11 03:44:57.082: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 11 03:44:57.082: INFO: 	Container systemd-logs ready: true, restart count 1
Dec 11 03:44:57.082: INFO: icagent-4fhcw from kube-system started at 2019-12-06 03:07:10 +0000 UTC (0 container statuses recorded)
Dec 11 03:44:57.082: INFO: 
Logging pods the kubelet thinks is on node kcsp41-master0 before test
Dec 11 03:44:57.150: INFO: kube-canal-controller-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container kube-canal-controller ready: true, restart count 0
Dec 11 03:44:57.150: INFO: kube-controller-manager-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container kube-controller-manager ready: true, restart count 4
Dec 11 03:44:57.150: INFO: coredns-556846b6b7-dnbzk from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container coredns ready: true, restart count 0
Dec 11 03:44:57.150: INFO: etcd-event-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container etcd-event-container ready: true, restart count 0
Dec 11 03:44:57.150: INFO: cfe-kube-webhook-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container webhook ready: true, restart count 0
Dec 11 03:44:57.150: INFO: kube-event-controller-6bc74bfc9c-nfh7p from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container kube-event-controller ready: true, restart count 4
Dec 11 03:44:57.150: INFO: kubenode-c625w from kube-system started at 2019-12-06 03:02:14 +0000 UTC (0 container statuses recorded)
Dec 11 03:44:57.150: INFO: etcd-network-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container etcd-network-container ready: true, restart count 0
Dec 11 03:44:57.150: INFO: kube-canal-apiserver-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container kube-canal-apiserver ready: true, restart count 0
Dec 11 03:44:57.150: INFO: kube-scheduler-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container kube-scheduler ready: true, restart count 4
Dec 11 03:44:57.150: INFO: provision-controller-manager-kcsp41-master0 from kube-system started at 2019-12-06 03:00:34 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container provision-controller-manager ready: true, restart count 21
Dec 11 03:44:57.150: INFO: etcd-backup-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container etcd-backup ready: true, restart count 0
Dec 11 03:44:57.150: INFO: icagent-9rdrh from kube-system started at 2019-12-06 03:02:21 +0000 UTC (0 container statuses recorded)
Dec 11 03:44:57.150: INFO: etcd-server-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container etcd-container ready: true, restart count 0
Dec 11 03:44:57.150: INFO: kube-apiserver-kcsp41-master0 from kube-system started at 2019-12-11 01:44:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 11 03:44:57.150: INFO: cam-tiller-kcsp41-master0 from kube-system started at 2019-12-06 02:58:50 +0000 UTC (1 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container cam-tiller ready: true, restart count 0
Dec 11 03:44:57.150: INFO: sonobuoy-systemd-logs-daemon-set-a3a36a58b07d4f59-clmpn from sonobuoy started at 2019-12-11 02:10:14 +0000 UTC (2 container statuses recorded)
Dec 11 03:44:57.150: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 11 03:44:57.150: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cf6251d0-d5cf-40a3-9bd6-424eb1fb48bd 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-cf6251d0-d5cf-40a3-9bd6-424eb1fb48bd off the node 10.247.6.203
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cf6251d0-d5cf-40a3-9bd6-424eb1fb48bd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:45:03.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2118" for this suite.
Dec 11 03:45:11.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:45:12.447: INFO: namespace sched-pred-2118 deletion completed in 8.92869057s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

â€¢ [SLOW TEST:15.601 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:45:12.448: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec 11 03:45:12.563: INFO: Waiting up to 5m0s for pod "var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65" in namespace "var-expansion-877" to be "success or failure"
Dec 11 03:45:12.585: INFO: Pod "var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65": Phase="Pending", Reason="", readiness=false. Elapsed: 21.385598ms
Dec 11 03:45:14.607: INFO: Pod "var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044099661s
Dec 11 03:45:16.630: INFO: Pod "var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06655607s
STEP: Saw pod success
Dec 11 03:45:16.630: INFO: Pod "var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65" satisfied condition "success or failure"
Dec 11 03:45:16.652: INFO: Trying to get logs from node 10.247.6.203 pod var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65 container dapi-container: <nil>
STEP: delete the pod
Dec 11 03:45:16.742: INFO: Waiting for pod var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65 to disappear
Dec 11 03:45:16.763: INFO: Pod var-expansion-fb5bbff1-0f62-4d7b-9e75-53b9c3323a65 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:45:16.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-877" for this suite.
Dec 11 03:45:22.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:45:23.725: INFO: namespace var-expansion-877 deletion completed in 6.939601203s

â€¢ [SLOW TEST:11.277 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 11 03:45:23.725: INFO: >>> kubeConfig: /tmp/kubeconfig-194493201
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 11 03:45:23.841: INFO: Waiting up to 5m0s for pod "pod-44057153-3a2c-4f12-94b2-e5df248baa25" in namespace "emptydir-5838" to be "success or failure"
Dec 11 03:45:23.863: INFO: Pod "pod-44057153-3a2c-4f12-94b2-e5df248baa25": Phase="Pending", Reason="", readiness=false. Elapsed: 21.827382ms
Dec 11 03:45:25.886: INFO: Pod "pod-44057153-3a2c-4f12-94b2-e5df248baa25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044601458s
Dec 11 03:45:27.909: INFO: Pod "pod-44057153-3a2c-4f12-94b2-e5df248baa25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067441619s
STEP: Saw pod success
Dec 11 03:45:27.909: INFO: Pod "pod-44057153-3a2c-4f12-94b2-e5df248baa25" satisfied condition "success or failure"
Dec 11 03:45:27.931: INFO: Trying to get logs from node 10.247.6.203 pod pod-44057153-3a2c-4f12-94b2-e5df248baa25 container test-container: <nil>
STEP: delete the pod
Dec 11 03:45:28.010: INFO: Waiting for pod pod-44057153-3a2c-4f12-94b2-e5df248baa25 to disappear
Dec 11 03:45:28.034: INFO: Pod pod-44057153-3a2c-4f12-94b2-e5df248baa25 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 11 03:45:28.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5838" for this suite.
Dec 11 03:45:34.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 11 03:45:34.990: INFO: namespace emptydir-5838 deletion completed in 6.932043785s

â€¢ [SLOW TEST:11.265 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
Dec 11 03:45:34.990: INFO: Running AfterSuite actions on all nodes
Dec 11 03:45:34.992: INFO: Running AfterSuite actions on node 1
Dec 11 03:45:34.992: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5691.006 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h34m53.234415786s
Test Suite Passed
